#!/usr/bin/env python3
"""
åˆ†æåˆ†ç±»æ•°æ®çš„è¯¦ç»†æƒ…å†µ
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from quick_test_causal_engine import QuickTester

def analyze_classification_data():
    # ç”Ÿæˆä¸æµ‹è¯•ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    X, y = make_classification(
        n_samples=2000, n_features=15, n_classes=3,
        n_informative=7, n_redundant=0, n_clusters_per_class=1,
        class_sep=1.0, random_state=42
    )

    print('ğŸ” åˆ†ç±»æ•°æ®åˆ†ææŠ¥å‘Š')
    print('=' * 50)
    print(f'æ•°æ®å½¢çŠ¶: {X.shape}')
    print(f'ç‰¹å¾èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]')
    print(f'ç‰¹å¾å‡å€¼: {X.mean():.2f}, æ ‡å‡†å·®: {X.std():.2f}')
    print()

    # åŸå§‹ç±»åˆ«åˆ†å¸ƒ
    unique, counts = np.unique(y, return_counts=True)
    print('ğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:')
    for label, count in zip(unique, counts):
        print(f'   ç±»åˆ«{label}: {count}ä¸ªæ ·æœ¬ ({count/len(y)*100:.1f}%)')
    print()

    # åˆ†å‰²æ•°æ®æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    print(f'è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}')

    # è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
    unique_train, counts_train = np.unique(y_train, return_counts=True)
    print('ğŸ“Š è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:')
    for label, count in zip(unique_train, counts_train):
        print(f'   ç±»åˆ«{label}: {count}ä¸ªæ ·æœ¬ ({count/len(y_train)*100:.1f}%)')
    print()

    # æ¨¡æ‹Ÿ40%æ ‡ç­¾å¼‚å¸¸
    tester = QuickTester()
    y_train_noisy = tester.add_label_anomalies(y_train, 0.4, 'classification')

    print('ğŸ“Š æ·»åŠ 40%å¼‚å¸¸åçš„è®­ç»ƒé›†åˆ†å¸ƒ:')
    unique_noisy, counts_noisy = np.unique(y_train_noisy, return_counts=True)
    for label, count in zip(unique_noisy, counts_noisy):
        print(f'   ç±»åˆ«{label}: {count}ä¸ªæ ·æœ¬ ({count/len(y_train_noisy)*100:.1f}%)')
    print()

    # åˆ†æå¼‚å¸¸å½±å“
    changed_indices = y_train != y_train_noisy
    print(f'ğŸ¯ å¼‚å¸¸ç»Ÿè®¡:')
    print(f'   å¼‚å¸¸æ ·æœ¬æ•°: {np.sum(changed_indices)} / {len(y_train)} ({np.sum(changed_indices)/len(y_train)*100:.1f}%)')
    print()

    # å¼‚å¸¸è½¬æ¢çŸ©é˜µ
    print('ğŸ”„ æ ‡ç­¾è½¬æ¢çŸ©é˜µ (åŸå§‹â†’å¼‚å¸¸):')
    for orig in unique:
        for new in unique:
            count = np.sum((y_train == orig) & (y_train_noisy == new))
            if count > 0 and orig != new:
                print(f'   {orig}â†’{new}: {count}ä¸ª')
    print()

    # æ•°æ®å¯åˆ†ç¦»æ€§åˆ†æ
    print('ğŸ“ˆ æ•°æ®å¯åˆ†ç¦»æ€§åˆ†æ:')
    
    # åœ¨å¹²å‡€æ•°æ®ä¸Šçš„åŸºå‡†æ€§èƒ½
    clf_clean = LogisticRegression(random_state=42, max_iter=1000)
    clf_clean.fit(X_train, y_train)
    acc_clean = accuracy_score(y_test, clf_clean.predict(X_test))

    # åœ¨å¼‚å¸¸æ•°æ®ä¸Šçš„æ€§èƒ½
    clf_noisy = LogisticRegression(random_state=42, max_iter=1000)
    clf_noisy.fit(X_train, y_train_noisy)
    acc_noisy = accuracy_score(y_test, clf_noisy.predict(X_test))

    print(f'   Logisticå›å½’ - å¹²å‡€æ•°æ®è®­ç»ƒ: {acc_clean:.3f}')
    print(f'   Logisticå›å½’ - 40%å¼‚å¸¸è®­ç»ƒ: {acc_noisy:.3f}')
    print(f'   æ€§èƒ½ä¸‹é™: {(acc_clean-acc_noisy)*100:.1f}ä¸ªç™¾åˆ†ç‚¹')
    print()
    
    # ç±»åˆ«åˆ†ç¦»åº¦åˆ†æ
    from sklearn.metrics import silhouette_score
    from sklearn.cluster import KMeans
    
    # ä½¿ç”¨KMeansè¯„ä¼°å¯åˆ†ç¦»æ€§
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X)
    sil_score = silhouette_score(X, cluster_labels)
    
    print(f'ğŸ¯ æ•°æ®ç‰¹å¾åˆ†æ:')
    print(f'   è½®å»“ç³»æ•° (æ•°æ®å¯åˆ†ç¦»æ€§): {sil_score:.3f}')
    print(f'   ç±»åˆ«åˆ†ç¦»åº¦è®¾ç½®: 1.0 (ä¸­ç­‰éš¾åº¦)')
    print(f'   ç‰¹å¾ä¿¡æ¯é‡: 7/{15} (çº¦47%æœ‰æ•ˆç‰¹å¾)')
    print()
    
    # åˆ†æä¸ºä»€ä¹ˆå¼‚å¸¸å½±å“å¯èƒ½è¾ƒå°
    print('ğŸ¤” å¯èƒ½çš„å¼‚å¸¸å½±å“è¾ƒå°åŸå› :')
    print('   1. æ•°æ®æœ¬èº«è¾ƒæ˜“åˆ†ç±» (class_sep=1.0)')
    print('   2. 3åˆ†ç±»ä»»åŠ¡ï¼Œæ ‡ç­¾ç¿»è½¬åä»æœ‰33%æ¦‚ç‡æ­£ç¡®')
    print('   3. ç‰¹å¾ç»´åº¦è¾ƒé«˜(15ç»´)ï¼Œå†—ä½™ä¿¡æ¯å¯èƒ½å¸®åŠ©æŠµæŠ—å¼‚å¸¸')
    print('   4. æ ·æœ¬é‡å……è¶³(2000ä¸ª)ï¼Œå¼‚å¸¸å½±å“è¢«ç¨€é‡Š')

if __name__ == "__main__":
    analyze_classification_data()
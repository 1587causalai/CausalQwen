"""
çœŸå®æ•°æ®é›†ç¤ºä¾‹ - åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·é¢„æµ‹  
=====================================

è¿™ä¸ªæ•™ç¨‹ä½¿ç”¨åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·æ•°æ®é›†ï¼Œæ¼”ç¤ºå¦‚ä½•ç”¨ CausalQwen å¤„ç†çœŸå®çš„å›å½’ä»»åŠ¡ã€‚
è¯¥æ•°æ®é›†åŒ…å«åŠ åˆ©ç¦å°¼äºšå„åœ°åŒºçš„æˆ¿å±‹å’Œäººå£ä¿¡æ¯ï¼Œç›®æ ‡æ˜¯é¢„æµ‹æˆ¿å±‹ä»·å€¼ä¸­ä½æ•°ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. å­¦ä¼šå¤„ç†çœŸå®çš„å›å½’æ•°æ®
2. ç†è§£æˆ¿ä»·é¢„æµ‹çš„ä¸šåŠ¡èƒŒæ™¯
3. æŒæ¡ç‰¹å¾å·¥ç¨‹æŠ€å·§
4. å­¦ä¼šè§£é‡Šæˆ¿ä»·é¢„æµ‹ç»“æœ
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from utils.simple_models import SimpleCausalRegressor, compare_with_sklearn
from utils.data_helpers import visualize_predictions


def main():
    """ä¸»å‡½æ•°ï¼šåŠ åˆ©ç¦å°¼äºšæˆ¿ä»·é¢„æµ‹å®Œæ•´æµç¨‹"""
    
    print("ğŸ  CausalQwen çœŸå®æ•°æ®ç¤ºä¾‹ - åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·é¢„æµ‹")
    print("=" * 60)
    
    print("\\nğŸ“š å…³äºåŠ åˆ©ç¦å°¼äºšæˆ¿ä»·æ•°æ®é›†:")
    print("è¿™æ˜¯1990å¹´åŠ åˆ©ç¦å°¼äºšäººå£æ™®æŸ¥çš„æ•°æ®ï¼ŒåŒ…å«20,640ä¸ªåœ°åŒºçš„ä¿¡æ¯ã€‚")
    print("æ¯ä¸ªæ ·æœ¬ä»£è¡¨ä¸€ä¸ªäººå£æ™®æŸ¥åŒºå—ï¼ŒåŒ…å«è¯¥åŒºåŸŸçš„æˆ¿å±‹å’Œäººå£ç»Ÿè®¡ä¿¡æ¯ã€‚")
    print("ç›®æ ‡æ˜¯é¢„æµ‹è¯¥åŒºåŸŸæˆ¿å±‹ä»·å€¼çš„ä¸­ä½æ•°ï¼ˆå•ä½ï¼šåä¸‡ç¾å…ƒï¼‰ã€‚")
    
    # 1. åŠ è½½æ•°æ®
    print("\\nğŸ“Š æ­¥éª¤ 1: åŠ è½½åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·æ•°æ®é›†")
    housing = fetch_california_housing()
    X = housing.data
    y = housing.target
    
    feature_names = housing.feature_names
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
    print(f"   æ ·æœ¬æ•°é‡: {X.shape[0]:,}")
    print(f"   ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"   ç›®æ ‡å˜é‡: æˆ¿å±‹ä»·å€¼ä¸­ä½æ•°ï¼ˆåä¸‡ç¾å…ƒï¼‰")
    
    print(f"\\nğŸ·ï¸ ç‰¹å¾è¯´æ˜:")
    feature_descriptions = {
        'MedInc': 'æ”¶å…¥ä¸­ä½æ•°ï¼ˆä¸‡ç¾å…ƒï¼‰',
        'HouseAge': 'æˆ¿å±‹å¹´é¾„ä¸­ä½æ•°',
        'AveRooms': 'å¹³å‡æˆ¿é—´æ•°',
        'AveBedrms': 'å¹³å‡å§å®¤æ•°',
        'Population': 'äººå£æ•°é‡',
        'AveOccup': 'å¹³å‡å±…ä½äººæ•°',
        'Latitude': 'çº¬åº¦',
        'Longitude': 'ç»åº¦'
    }
    
    for i, name in enumerate(feature_names):
        desc = feature_descriptions.get(name, 'æœªçŸ¥ç‰¹å¾')
        print(f"   {i+1}. {name}: {desc}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\\nğŸ” æ­¥éª¤ 2: æ•°æ®æ¢ç´¢")
    explore_housing_data(X, y, feature_names)
    
    # 3. ç‰¹å¾å·¥ç¨‹
    print("\\nğŸ”§ æ­¥éª¤ 3: ç‰¹å¾å·¥ç¨‹")
    X_engineered = engineer_features(X, feature_names)
    new_feature_names = feature_names + ['Rooms_per_household', 'Bedrooms_per_room', 'Population_density']
    
    print("\\næ–°å¢ç‰¹å¾:")
    print("   - Rooms_per_household: æ¯æˆ·æˆ¿é—´æ•° = AveRooms / AveOccup")
    print("   - Bedrooms_per_room: å§å®¤æ¯”ä¾‹ = AveBedrms / AveRooms") 
    print("   - Population_density: äººå£å¯†åº¦çš„ä»£ç†å˜é‡")
    
    # 4. æ•°æ®é¢„å¤„ç†
    print("\\nğŸ”§ æ­¥éª¤ 4: æ•°æ®é¢„å¤„ç†")
    X_train, X_test, y_train, y_test = train_test_split(
        X_engineered, y, test_size=0.2, random_state=42
    )
    
    # æ ‡å‡†åŒ–ç‰¹å¾å’Œç›®æ ‡
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]:,} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]:,} æ ·æœ¬")
    print(f"   ç‰¹å¾å’Œç›®æ ‡å‡å·²æ ‡å‡†åŒ–")
    
    # 5. è®­ç»ƒ CausalQwen æ¨¡å‹
    print("\\nğŸš€ æ­¥éª¤ 5: è®­ç»ƒ CausalQwen å›å½’å™¨")
    
    model = SimpleCausalRegressor(random_state=42)
    model.fit(X_train_scaled, y_train_scaled, epochs=80, verbose=True)
    
    # 6. æ¨¡å‹è¯„ä¼°
    print("\\nğŸ“Š æ­¥éª¤ 6: æ¨¡å‹è¯„ä¼°")
    
    # é¢„æµ‹ï¼ˆæ ‡å‡†åŒ–ç©ºé—´ï¼‰
    predictions_scaled = model.predict(X_test_scaled)
    
    # åæ ‡å‡†åŒ–åˆ°åŸå§‹ç©ºé—´
    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
    y_test_original = y_test
    
    # è®¡ç®—æŒ‡æ ‡
    r2 = r2_score(y_test_original, predictions)
    mae = mean_absolute_error(y_test_original, predictions)
    mse = mean_squared_error(y_test_original, predictions)
    rmse = np.sqrt(mse)
    
    print(f"\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"   RÂ² åˆ†æ•°: {r2:.4f}")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {mae:.4f} (åä¸‡ç¾å…ƒ)")
    print(f"   å‡æ–¹æ ¹è¯¯å·®: {rmse:.4f} (åä¸‡ç¾å…ƒ)")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: ${mae*100000:.0f}")
    print(f"   å‡æ–¹æ ¹è¯¯å·®: ${rmse*100000:.0f}")
    
    # 7. ä¸ç¡®å®šæ€§åˆ†æ
    print("\\nğŸ” æ­¥éª¤ 7: ä¸ç¡®å®šæ€§åˆ†æ")
    analyze_uncertainty(model, X_test_scaled, y_test_original, scaler_y)
    
    # 8. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\\nğŸ“ˆ æ­¥éª¤ 8: ç‰¹å¾é‡è¦æ€§åˆ†æ")
    analyze_feature_importance(model, new_feature_names)
    
    # 9. é¢„æµ‹æ¡ˆä¾‹åˆ†æ
    print("\\nğŸ  æ­¥éª¤ 9: é¢„æµ‹æ¡ˆä¾‹åˆ†æ")
    analyze_prediction_cases(model, X_test_scaled, y_test_original, new_feature_names, scaler_X, scaler_y)
    
    # 10. åœ°ç†åˆ†å¸ƒåˆ†æ
    print("\\nğŸ—ºï¸ æ­¥éª¤ 10: åœ°ç†åˆ†å¸ƒåˆ†æ")
    analyze_geographic_patterns(X_test, y_test_original, predictions, feature_names)
    
    # 11. ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    print("\\nâš–ï¸ æ­¥éª¤ 11: ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¯¹æ¯”")
    comparison_results = compare_with_sklearn(X_engineered, y, task_type='regression')
    
    # 12. å®é™…åº”ç”¨æ¼”ç¤º
    print("\\nğŸ¡ æ­¥éª¤ 12: å®é™…åº”ç”¨æ¼”ç¤º")
    demo_house_price_prediction(model, scaler_X, scaler_y, new_feature_names)
    
    # 13. ç»“æœå¯è§†åŒ–
    print("\\nğŸ“Š æ­¥éª¤ 13: ç»“æœå¯è§†åŒ–")
    visualize_predictions(y_test_original, predictions, 'regression', 'åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·é¢„æµ‹ - CausalQwen ç»“æœ')
    
    # ç»˜åˆ¶è¯¦ç»†åˆ†æå›¾
    plot_detailed_analysis(y_test_original, predictions, X_test, feature_names)
    
    print("\\nğŸ‰ åŠ åˆ©ç¦å°¼äºšæˆ¿ä»·é¢„æµ‹æ•™ç¨‹å®Œæˆï¼")
    print("\\nğŸ¯ å…³é”®æ”¶è·:")
    print(f"   1. åœ¨çœŸå®æˆ¿ä»·æ•°æ®ä¸Šè¾¾åˆ°äº† RÂ² = {r2:.4f}")
    print(f"   2. å¹³å‡é¢„æµ‹è¯¯å·®çº¦ä¸º ${mae*100000:.0f}")
    print("   3. å­¦ä¼šäº†æˆ¿ä»·é¢„æµ‹çš„ç‰¹å¾å·¥ç¨‹")
    print("   4. ç†è§£äº†åœ°ç†å› ç´ å¯¹æˆ¿ä»·çš„å½±å“")
    print("   5. æŒæ¡äº†ä¸ç¡®å®šæ€§é‡åŒ–åœ¨æˆ¿ä»·é¢„æµ‹ä¸­çš„ä»·å€¼")


def explore_housing_data(X, y, feature_names):
    """æ¢ç´¢æˆ¿ä»·æ•°æ®"""
    
    # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
    df = pd.DataFrame(X, columns=feature_names)
    df['price'] = y
    
    print("\\nåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
    print(df.describe())
    
    print(f"\\næˆ¿ä»·åˆ†å¸ƒ:")
    print(f"   æœ€ä½æˆ¿ä»·: ${y.min()*100000:.0f}")
    print(f"   æœ€é«˜æˆ¿ä»·: ${y.max()*100000:.0f}")
    print(f"   å¹³å‡æˆ¿ä»·: ${y.mean()*100000:.0f}")
    print(f"   æˆ¿ä»·ä¸­ä½æ•°: ${np.median(y)*100000:.0f}")
    
    # è®¡ç®—ç‰¹å¾ä¸æˆ¿ä»·çš„ç›¸å…³æ€§
    print("\\nç‰¹å¾ä¸æˆ¿ä»·çš„ç›¸å…³æ€§:")
    correlations = df.corr()['price'].sort_values(key=abs, ascending=False)
    
    for feature, corr in correlations.items():
        if feature != 'price':
            direction = "æ­£ç›¸å…³" if corr > 0 else "è´Ÿç›¸å…³"
            strength = "å¼º" if abs(corr) > 0.5 else "ä¸­ç­‰" if abs(corr) > 0.3 else "å¼±"
            print(f"   {feature:12}: {corr:6.3f} ({strength}{direction})")


def engineer_features(X, feature_names):
    """ç‰¹å¾å·¥ç¨‹"""
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(X, columns=feature_names)
    
    # æ–°å¢ç‰¹å¾
    # 1. æ¯æˆ·æˆ¿é—´æ•°
    df['Rooms_per_household'] = df['AveRooms'] / df['AveOccup']
    
    # 2. å§å®¤æ¯”ä¾‹
    df['Bedrooms_per_room'] = df['AveBedrms'] / df['AveRooms']
    
    # 3. äººå£å¯†åº¦çš„ä»£ç†å˜é‡
    df['Population_density'] = df['Population'] / df['AveOccup']
    
    # å¤„ç†å¯èƒ½çš„æ— ç©·å¤§æˆ–NaNå€¼
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.fillna(df.median())
    
    return df.values


def analyze_uncertainty(model, X_test_scaled, y_test_original, scaler_y):
    """åˆ†æé¢„æµ‹ä¸ç¡®å®šæ€§"""
    
    # é€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œä¸ç¡®å®šæ€§åˆ†æ
    sample_indices = np.random.choice(len(X_test_scaled), 20, replace=False)
    X_sample = X_test_scaled[sample_indices]
    y_sample = y_test_original[sample_indices]
    
    # è·å–å¸¦ä¸ç¡®å®šæ€§çš„é¢„æµ‹
    pred_mean_scaled, pred_std_scaled = model.predict(X_sample, return_uncertainty=True)
    
    # åæ ‡å‡†åŒ–
    pred_mean = scaler_y.inverse_transform(pred_mean_scaled.reshape(-1, 1)).flatten()
    pred_std = pred_std_scaled * scaler_y.scale_[0]  # æ ‡å‡†å·®çš„ç¼©æ”¾
    
    print("\\næˆ¿ä»·é¢„æµ‹ä¸ç¡®å®šæ€§åˆ†æ:")
    print("  æ ·æœ¬  |  çœŸå®ä»·æ ¼   |  é¢„æµ‹ä»·æ ¼   |   ä¸ç¡®å®šæ€§   |   çŠ¶æ€")
    print("  ---- | ---------- | ---------- | ----------- | --------")
    
    accurate_count = 0
    for i in range(len(pred_mean)):
        true_price = y_sample[i] * 100000  # è½¬æ¢ä¸ºç¾å…ƒ
        pred_price = pred_mean[i] * 100000
        uncertainty = pred_std[i] * 100000
        
        # åˆ¤æ–­é¢„æµ‹æ˜¯å¦åœ¨ä¸ç¡®å®šæ€§èŒƒå›´å†…
        error = abs(true_price - pred_price)
        is_accurate = error <= uncertainty
        if is_accurate:
            accurate_count += 1
        
        status = "âœ… å‡†ç¡®" if is_accurate else "âš ï¸ åå·®"
        
        print(f"  {i+1:2d}   | ${true_price:8.0f}  | ${pred_price:8.0f}  | Â±${uncertainty:8.0f}  | {status}")
    
    accuracy_rate = accurate_count / len(pred_mean)
    print(f"\\nä¸ç¡®å®šæ€§æ ¡å‡†å‡†ç¡®ç‡: {accuracy_rate:.2%}")
    print(f"å¹³å‡ä¸ç¡®å®šæ€§: Â±${np.mean(pred_std)*100000:.0f}")
    
    print("\\nğŸ’¡ ä¸ç¡®å®šæ€§è§£é‡Š:")
    print("   - ä¸ç¡®å®šæ€§ä½çš„é¢„æµ‹æ›´å¯é ")
    print("   - ä¸ç¡®å®šæ€§é«˜çš„åŒºåŸŸå¯èƒ½æœ‰ç‰¹æ®Šæƒ…å†µ")
    print("   - åœ¨æˆ¿åœ°äº§æŠ•èµ„å†³ç­–ä¸­å¯ä»¥å‚è€ƒä¸ç¡®å®šæ€§")


def analyze_feature_importance(model, feature_names):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    
    if hasattr(model, '_get_feature_importance'):
        importance = model._get_feature_importance()
        if importance is not None:
            print("\\næˆ¿ä»·é¢„æµ‹ç‰¹å¾é‡è¦æ€§æ’åº:")
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_indices = np.argsort(importance)[::-1]
            
            for i, idx in enumerate(sorted_indices):
                if i < len(feature_names):
                    print(f"   {i+1}. {feature_names[idx]}: {importance[idx]:.4f}")
            
            # è§£é‡Šæœ€é‡è¦çš„ç‰¹å¾
            if len(sorted_indices) > 0 and sorted_indices[0] < len(feature_names):
                most_important = feature_names[sorted_indices[0]]
                print(f"\\nğŸ’¡ æœ€é‡è¦çš„ç‰¹å¾æ˜¯ '{most_important}'")
                
                feature_interpretations = {
                    'MedInc': 'æ”¶å…¥æ˜¯å½±å“æˆ¿ä»·çš„æœ€é‡è¦å› ç´ ',
                    'Latitude': 'åœ°ç†ä½ç½®ï¼ˆçº¬åº¦ï¼‰æ˜¾è‘—å½±å“æˆ¿ä»·',
                    'Longitude': 'åœ°ç†ä½ç½®ï¼ˆç»åº¦ï¼‰æ˜¾è‘—å½±å“æˆ¿ä»·',
                    'AveRooms': 'æˆ¿é—´æ•°é‡æ˜¯æˆ¿ä»·çš„é‡è¦æŒ‡æ ‡',
                    'Rooms_per_household': 'æˆ¿å±‹ç©ºé—´æ•ˆç‡å½±å“ä»·å€¼',
                    'HouseAge': 'æˆ¿å±‹å¹´é¾„å½±å“ä»·å€¼è¯„ä¼°'
                }
                
                interpretation = feature_interpretations.get(most_important, 'è¿™ä¸ªç‰¹å¾å¯¹æˆ¿ä»·é¢„æµ‹å¾ˆé‡è¦')
                print(f"   {interpretation}")
        else:
            print("\\nç‰¹å¾é‡è¦æ€§ä¿¡æ¯æš‚ä¸å¯ç”¨")
    
    # åŸºäºå¸¸è¯†çš„ç‰¹å¾è§£é‡Š
    print("\\nğŸ  æˆ¿ä»·å½±å“å› ç´ çš„å¸¸è¯†è§£é‡Š:")
    common_sense = {
        'MedInc': 'æ”¶å…¥è¶Šé«˜çš„åœ°åŒºï¼Œæˆ¿ä»·é€šå¸¸è¶Šé«˜',
        'Latitude/Longitude': 'åœ°ç†ä½ç½®å†³å®šäº†ä¾¿åˆ©æ€§å’Œç¯å¢ƒ',
        'AveRooms': 'æ›´å¤šæˆ¿é—´æ„å‘³ç€æ›´å¤§çš„å±…ä½ç©ºé—´',
        'HouseAge': 'è¾ƒæ–°çš„æˆ¿å±‹é€šå¸¸ä»·å€¼æ›´é«˜',
        'Population': 'äººå£å¯†åº¦å¯èƒ½å½±å“æˆ¿ä»·',
        'AveOccup': 'å±…ä½å¯†åº¦å½±å“ç”Ÿæ´»è´¨é‡'
    }
    
    for factor, explanation in common_sense.items():
        print(f"   â€¢ {factor}: {explanation}")


def analyze_prediction_cases(model, X_test_scaled, y_test_original, feature_names, scaler_X, scaler_y):
    """åˆ†æå…·ä½“é¢„æµ‹æ¡ˆä¾‹"""
    
    # é¢„æµ‹æ‰€æœ‰æµ‹è¯•æ ·æœ¬
    predictions_scaled = model.predict(X_test_scaled)
    predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()
    
    # è®¡ç®—é¢„æµ‹è¯¯å·®
    errors = np.abs(y_test_original - predictions)
    
    # æ‰¾åˆ°ä¸åŒç±»å‹çš„æ¡ˆä¾‹
    best_idx = np.argmin(errors)  # æœ€å‡†ç¡®çš„é¢„æµ‹
    worst_idx = np.argmax(errors)  # è¯¯å·®æœ€å¤§çš„é¢„æµ‹
    median_idx = np.argsort(errors)[len(errors)//2]  # ä¸­ç­‰è¯¯å·®çš„é¢„æµ‹
    
    cases = [
        (best_idx, "ğŸ¯ æœ€å‡†ç¡®é¢„æµ‹"),
        (median_idx, "ğŸ“Š å…¸å‹é¢„æµ‹"),
        (worst_idx, "âš ï¸ æœ€å¤§è¯¯å·®é¢„æµ‹")
    ]
    
    print("\\nä»£è¡¨æ€§é¢„æµ‹æ¡ˆä¾‹åˆ†æ:")
    
    for idx, case_name in cases:
        true_price = y_test_original[idx] * 100000
        pred_price = predictions[idx] * 100000
        error = abs(true_price - pred_price)
        error_rate = error / true_price * 100
        
        print(f"\\n{case_name}:")
        print(f"   çœŸå®æˆ¿ä»·: ${true_price:.0f}")
        print(f"   é¢„æµ‹æˆ¿ä»·: ${pred_price:.0f}")
        print(f"   ç»å¯¹è¯¯å·®: ${error:.0f} ({error_rate:.1f}%)")
        
        # æ˜¾ç¤ºè¯¥æ ·æœ¬çš„ç‰¹å¾
        original_features = scaler_X.inverse_transform([X_test_scaled[idx]])[0]
        print(f"   åŒºåŸŸç‰¹å¾:")
        
        key_features = ['MedInc', 'HouseAge', 'AveRooms', 'Latitude', 'Longitude']
        for i, name in enumerate(feature_names):
            if i < len(original_features) and (name in key_features or i < 5):
                value = original_features[i]
                print(f"     {name}: {value:.2f}")


def analyze_geographic_patterns(X_test, y_test_original, predictions, feature_names):
    """åˆ†æåœ°ç†åˆ†å¸ƒæ¨¡å¼"""
    
    # æ‰¾åˆ°ç»çº¬åº¦çš„åˆ—ç´¢å¼•
    lat_idx = feature_names.index('Latitude')
    lon_idx = feature_names.index('Longitude') 
    
    latitudes = X_test[:, lat_idx]
    longitudes = X_test[:, lon_idx]
    
    print("\\nåœ°ç†åˆ†å¸ƒåˆ†æ:")
    
    # æŒ‰çº¬åº¦åˆ†æï¼ˆå—åŒ—æ–¹å‘ï¼‰
    north_mask = latitudes > np.median(latitudes)
    south_mask = ~north_mask
    
    north_avg_price = np.mean(y_test_original[north_mask]) * 100000
    south_avg_price = np.mean(y_test_original[south_mask]) * 100000
    
    print(f"   åŒ—éƒ¨åœ°åŒºå¹³å‡æˆ¿ä»·: ${north_avg_price:.0f}")
    print(f"   å—éƒ¨åœ°åŒºå¹³å‡æˆ¿ä»·: ${south_avg_price:.0f}")
    
    # æŒ‰ç»åº¦åˆ†æï¼ˆä¸œè¥¿æ–¹å‘ï¼‰
    east_mask = longitudes > np.median(longitudes)
    west_mask = ~east_mask
    
    east_avg_price = np.mean(y_test_original[east_mask]) * 100000
    west_avg_price = np.mean(y_test_original[west_mask]) * 100000
    
    print(f"   ä¸œéƒ¨åœ°åŒºå¹³å‡æˆ¿ä»·: ${east_avg_price:.0f}")
    print(f"   è¥¿éƒ¨åœ°åŒºå¹³å‡æˆ¿ä»·: ${west_avg_price:.0f}")
    
    # åˆ†æé¢„æµ‹è¯¯å·®çš„åœ°ç†åˆ†å¸ƒ
    errors = np.abs(y_test_original - predictions)
    
    print(f"\\né¢„æµ‹è¯¯å·®çš„åœ°ç†æ¨¡å¼:")
    print(f"   åŒ—éƒ¨åœ°åŒºå¹³å‡è¯¯å·®: ${np.mean(errors[north_mask])*100000:.0f}")
    print(f"   å—éƒ¨åœ°åŒºå¹³å‡è¯¯å·®: ${np.mean(errors[south_mask])*100000:.0f}")
    print(f"   ä¸œéƒ¨åœ°åŒºå¹³å‡è¯¯å·®: ${np.mean(errors[east_mask])*100000:.0f}")
    print(f"   è¥¿éƒ¨åœ°åŒºå¹³å‡è¯¯å·®: ${np.mean(errors[west_mask])*100000:.0f}")


def demo_house_price_prediction(model, scaler_X, scaler_y, feature_names):
    """æ¼”ç¤ºå®é™…æˆ¿ä»·é¢„æµ‹"""
    
    print("\\nå‡è®¾æ‚¨æƒ³è¯„ä¼°ä¸€ä¸ªç‰¹å®šåŒºåŸŸçš„æˆ¿ä»·:")
    
    # åˆ›å»ºä¸€ä¸ªå‡è®¾çš„æˆ¿äº§åŒºåŸŸ
    sample_area = {
        'MedInc': 6.5,          # æ”¶å…¥ä¸­ä½æ•°ï¼š6.5ä¸‡ç¾å…ƒ
        'HouseAge': 15.0,       # æˆ¿å±‹å¹´é¾„ï¼š15å¹´
        'AveRooms': 6.2,        # å¹³å‡æˆ¿é—´æ•°ï¼š6.2ä¸ª
        'AveBedrms': 1.1,       # å¹³å‡å§å®¤æ•°ï¼š1.1ä¸ª  
        'Population': 3500,     # äººå£ï¼š3500äºº
        'AveOccup': 3.2,        # å¹³å‡å±…ä½äººæ•°ï¼š3.2äºº
        'Latitude': 34.05,      # çº¬åº¦ï¼š34.05ï¼ˆæ´›æ‰çŸ¶é™„è¿‘ï¼‰
        'Longitude': -118.25,   # ç»åº¦ï¼š-118.25
        'Rooms_per_household': 6.2/3.2,  # æ¯æˆ·æˆ¿é—´æ•°
        'Bedrooms_per_room': 1.1/6.2,    # å§å®¤æ¯”ä¾‹
        'Population_density': 3500/3.2    # äººå£å¯†åº¦ä»£ç†
    }
    
    print("\\nğŸ˜ï¸ ç›®æ ‡åŒºåŸŸç‰¹å¾:")
    interpretations = {
        'MedInc': 'ä¸­ä¸Šç­‰æ”¶å…¥ç¤¾åŒº',
        'HouseAge': 'ç›¸å¯¹è¾ƒæ–°çš„æˆ¿å±‹',
        'AveRooms': 'å®½æ•çš„å±…ä½ç©ºé—´',
        'Latitude': 'æ´›æ‰çŸ¶åœ°åŒº',
        'Longitude': 'è¥¿æµ·å²¸ä½ç½®'
    }
    
    for feature, value in sample_area.items():
        if feature in interpretations:
            print(f"   {feature}: {value} ({interpretations[feature]})")
        else:
            print(f"   {feature}: {value:.2f}")
    
    # å‡†å¤‡é¢„æµ‹æ•°æ®
    sample_data = np.array([[sample_area[name] for name in feature_names]])
    sample_scaled = scaler_X.transform(sample_data)
    
    # è¿›è¡Œé¢„æµ‹
    pred_scaled, uncertainty_scaled = model.predict(sample_scaled, return_uncertainty=True)
    
    # åæ ‡å‡†åŒ–
    predicted_price = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1))[0, 0] * 100000
    uncertainty = uncertainty_scaled[0] * scaler_y.scale_[0] * 100000
    
    print(f"\\nğŸ”® CausalQwen çš„æˆ¿ä»·é¢„æµ‹:")
    print(f"   é¢„æµ‹æˆ¿ä»·: ${predicted_price:.0f}")
    print(f"   ä¸ç¡®å®šæ€§: Â±${uncertainty:.0f}")
    print(f"   ä»·æ ¼åŒºé—´: ${predicted_price-uncertainty:.0f} - ${predicted_price+uncertainty:.0f}")
    
    # ç»™å‡ºæŠ•èµ„å»ºè®®
    if predicted_price > 400000:
        category = "é«˜ç«¯"
    elif predicted_price > 250000:
        category = "ä¸­ç«¯"
    else:
        category = "å…¥é—¨çº§"
    
    confidence_level = "é«˜" if uncertainty < 50000 else "ä¸­ç­‰" if uncertainty < 100000 else "ä½"
    
    print(f"\\nğŸ’¡ æŠ•èµ„åˆ†æ:")
    print(f"   æˆ¿ä»·æ°´å¹³: {category}ä½å®…åŒº")
    print(f"   é¢„æµ‹ç½®ä¿¡åº¦: {confidence_level}")
    
    if uncertainty < 50000:
        print("   å»ºè®®: é¢„æµ‹ç½®ä¿¡åº¦é«˜ï¼Œé€‚åˆæŠ•èµ„å†³ç­–å‚è€ƒ")
    elif uncertainty < 100000:
        print("   å»ºè®®: é¢„æµ‹æœ‰ä¸€å®šä¸ç¡®å®šæ€§ï¼Œå»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯")
    else:
        print("   å»ºè®®: ä¸ç¡®å®šæ€§è¾ƒé«˜ï¼Œå»ºè®®è·å–æ›´å¤šæ•°æ®æˆ–ä¸“ä¸šè¯„ä¼°")


def plot_detailed_analysis(y_true, y_pred, X_test, feature_names):
    """ç»˜åˆ¶è¯¦ç»†åˆ†æå›¾"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. é¢„æµ‹ vs å®é™…ä»·æ ¼
    ax1 = axes[0, 0]
    ax1.scatter(y_true, y_pred, alpha=0.5)
    
    # ç†æƒ³é¢„æµ‹çº¿
    min_price = min(y_true.min(), y_pred.min())
    max_price = max(y_true.max(), y_pred.max())
    ax1.plot([min_price, max_price], [min_price, max_price], 'r--', alpha=0.8)
    
    ax1.set_xlabel('çœŸå®æˆ¿ä»· (åä¸‡ç¾å…ƒ)')
    ax1.set_ylabel('é¢„æµ‹æˆ¿ä»· (åä¸‡ç¾å…ƒ)')
    ax1.set_title('é¢„æµ‹ vs å®é™…æˆ¿ä»·')
    ax1.grid(True, alpha=0.3)
    
    # 2. æ®‹å·®åˆ†æ
    ax2 = axes[0, 1]
    residuals = y_true - y_pred
    ax2.scatter(y_pred, residuals, alpha=0.5)
    ax2.axhline(y=0, color='r', linestyle='--', alpha=0.8)
    ax2.set_xlabel('é¢„æµ‹æˆ¿ä»· (åä¸‡ç¾å…ƒ)')
    ax2.set_ylabel('æ®‹å·®')
    ax2.set_title('æ®‹å·®åˆ†æ')
    ax2.grid(True, alpha=0.3)
    
    # 3. æˆ¿ä»·åˆ†å¸ƒ
    ax3 = axes[1, 0]
    ax3.hist(y_true, bins=30, alpha=0.7, label='çœŸå®æˆ¿ä»·', density=True)
    ax3.hist(y_pred, bins=30, alpha=0.7, label='é¢„æµ‹æˆ¿ä»·', density=True)
    ax3.set_xlabel('æˆ¿ä»· (åä¸‡ç¾å…ƒ)')
    ax3.set_ylabel('å¯†åº¦')
    ax3.set_title('æˆ¿ä»·åˆ†å¸ƒå¯¹æ¯”')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. åœ°ç†åˆ†å¸ƒï¼ˆç»çº¬åº¦ï¼‰
    ax4 = axes[1, 1]
    lat_idx = feature_names.index('Latitude')
    lon_idx = feature_names.index('Longitude')
    
    scatter = ax4.scatter(X_test[:, lon_idx], X_test[:, lat_idx], 
                         c=y_true, cmap='viridis', alpha=0.6)
    ax4.set_xlabel('ç»åº¦')
    ax4.set_ylabel('çº¬åº¦')
    ax4.set_title('æˆ¿ä»·åœ°ç†åˆ†å¸ƒ')
    plt.colorbar(scatter, ax=ax4, label='æˆ¿ä»· (åä¸‡ç¾å…ƒ)')
    
    plt.tight_layout()
    plt.show()
    
    print("\\nğŸ“Š å›¾è¡¨è¯´æ˜:")
    print("   - å·¦ä¸Š: é¢„æµ‹å‡†ç¡®æ€§ï¼Œç‚¹è¶Šæ¥è¿‘çº¢çº¿è¶Šå‡†ç¡®")
    print("   - å³ä¸Š: æ®‹å·®åˆ†æï¼Œç‚¹åº”éšæœºåˆ†å¸ƒåœ¨é›¶çº¿é™„è¿‘")
    print("   - å·¦ä¸‹: æˆ¿ä»·åˆ†å¸ƒï¼Œä¸¤ä¸ªåˆ†å¸ƒåº”è¯¥ç›¸ä¼¼")
    print("   - å³ä¸‹: åœ°ç†åˆ†å¸ƒï¼Œé¢œè‰²è¡¨ç¤ºæˆ¿ä»·é«˜ä½")


if __name__ == "__main__":
    # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
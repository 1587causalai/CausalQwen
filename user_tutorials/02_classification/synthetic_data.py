"""
åˆ†ç±»ä»»åŠ¡å®æˆ˜ - ä½¿ç”¨åˆæˆæ•°æ®
=========================

è¿™ä¸ªæ•™ç¨‹å°†æ·±å…¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ CausalQwen å¤„ç†åˆ†ç±»ä»»åŠ¡ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨ scikit-learn ç”Ÿæˆçš„åˆæˆæ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®çš„ä¸šåŠ¡åœºæ™¯ã€‚

å­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£åˆ†ç±»ä»»åŠ¡çš„ç‰¹ç‚¹
2. æŒæ¡å¤šç±»åˆ«åˆ†ç±»æŠ€å·§
3. å­¦ä¼šå¤„ç†ä¸å¹³è¡¡æ•°æ®
4. ç†è§£æ¦‚ç‡é¢„æµ‹å’Œå†³ç­–é˜ˆå€¼
5. æŒæ¡åˆ†ç±»ç»“æœè§£é‡Šæ–¹æ³•
"""

import sys
import os
import numpy as np
import matplotlib
# å¦‚æœæ˜¯éäº¤äº’æ¨¡å¼ï¼Œä½¿ç”¨éäº¤äº’åç«¯
if len(sys.argv) > 1:
    matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from utils.simple_models import SimpleCausalClassifier, compare_with_sklearn
from utils.data_helpers import (
    generate_classification_data,
    prepare_data_for_training,
    explore_data,
    visualize_predictions,
    save_results
)


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„åˆ†ç±»ä»»åŠ¡æµç¨‹"""
    
    print("ğŸ¯ CausalQwen åˆ†ç±»ä»»åŠ¡å®æˆ˜æ•™ç¨‹")
    print("=" * 50)
    
    # æ¼”ç¤ºå¤šä¸ªä¸åŒçš„åˆ†ç±»åœºæ™¯
    scenarios = [
        {
            'name': 'å®¢æˆ·ç»†åˆ†',
            'description': 'æ ¹æ®è¡Œä¸ºç‰¹å¾å°†å®¢æˆ·åˆ†ä¸ºä¸åŒä»·å€¼ç­‰çº§',
            'n_samples': 1200,
            'n_features': 15,
            'n_classes': 3,
            'difficulty': 'easy',
            'class_names': ['ä½ä»·å€¼å®¢æˆ·', 'ä¸­ä»·å€¼å®¢æˆ·', 'é«˜ä»·å€¼å®¢æˆ·']
        },
        {
            'name': 'ç–¾ç—…è¯Šæ–­',
            'description': 'æ ¹æ®ç—‡çŠ¶å’Œæ£€æŸ¥ç»“æœè¯Šæ–­ç–¾ç—…ç±»å‹',
            'n_samples': 800,
            'n_features': 20,
            'n_classes': 4,
            'difficulty': 'medium',
            'class_names': ['å¥åº·', 'è½»ç—‡', 'ä¸­ç—‡', 'é‡ç—‡']
        },
        {
            'name': 'äº§å“è´¨é‡æ£€æµ‹',
            'description': 'æ ¹æ®ç”Ÿäº§å‚æ•°æ£€æµ‹äº§å“è´¨é‡ç­‰çº§',
            'n_samples': 1000,
            'n_features': 12,
            'n_classes': 5,
            'difficulty': 'hard',
            'class_names': ['ä¼˜ç§€', 'è‰¯å¥½', 'åˆæ ¼', 'ä¸åˆæ ¼', 'æ¬¡å“']
        }
    ]
    
    print("\\nå¯ç”¨çš„åˆ†ç±»åœºæ™¯:")
    for i, scenario in enumerate(scenarios):
        print(f"{i+1}. {scenario['name']} - {scenario['description']}")
        print(f"   ç±»åˆ«: {', '.join(scenario['class_names'])}")
    
    # è®©ç”¨æˆ·é€‰æ‹©åœºæ™¯
    choice = None
    if len(sys.argv) > 1:
        try:
            arg = int(sys.argv[1]) - 1
            if 0 <= arg < len(scenarios):
                choice = arg
                print(f"\\nè‡ªåŠ¨é€‰æ‹©åœºæ™¯: {arg + 1}")
        except ValueError:
            pass
    
    if choice is None:
        while True:
            try:
                choice = int(input("\\nè¯·é€‰æ‹©ä¸€ä¸ªåœºæ™¯ (1-3): ")) - 1
                if 0 <= choice < len(scenarios):
                    break
                else:
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„é€‰æ‹© (1-3)")
            except (ValueError, EOFError, KeyboardInterrupt):
                print("\\né»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªåœºæ™¯...")
                choice = 0
                break
    
    selected_scenario = scenarios[choice]
    print(f"\\nğŸ¯ æ‚¨é€‰æ‹©äº†: {selected_scenario['name']}")
    print(f"åœºæ™¯æè¿°: {selected_scenario['description']}")
    
    # è¿è¡Œé€‰æ‹©çš„åœºæ™¯
    run_classification_scenario(selected_scenario)
    
    print("\\nğŸ‰ åˆ†ç±»ä»»åŠ¡æ•™ç¨‹å®Œæˆï¼")
    print("\\nğŸ“– æ¥ä¸‹æ¥æ‚¨å¯ä»¥ï¼š")
    print("  - å°è¯•å…¶ä»–åˆ†ç±»åœºæ™¯")
    print("  - è°ƒèŠ‚å†³ç­–é˜ˆå€¼ä¼˜åŒ–æ€§èƒ½")
    print("  - ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®")
    print("  - æŸ¥çœ‹ iris_dataset.py å­¦ä¹ å¤„ç†çœŸå®æ•°æ®")


def run_classification_scenario(scenario):
    """è¿è¡Œç‰¹å®šçš„åˆ†ç±»åœºæ™¯"""
    
    print(f"\\nğŸš€ å¼€å§‹ {scenario['name']} åœºæ™¯")
    print("-" * 40)
    
    # 1. ç”Ÿæˆæ•°æ®
    print("\\nğŸ“Š æ­¥éª¤ 1: ç”Ÿæˆæ•°æ®")
    X, y, info = generate_classification_data(
        n_samples=scenario['n_samples'],
        n_features=scenario['n_features'],
        n_classes=scenario['n_classes'],
        difficulty=scenario['difficulty'],
        random_state=42
    )
    
    # ä¸ºäº†æ›´å¥½çš„æ¼”ç¤ºï¼Œç»™ç‰¹å¾æ·»åŠ æœ‰æ„ä¹‰çš„åç§°
    feature_names = generate_feature_names(scenario['name'], scenario['n_features'])
    class_names = scenario['class_names']
    
    print(f"\\nç‰¹å¾è¯´æ˜ï¼ˆ{scenario['name']}åœºæ™¯ï¼‰:")
    for i, name in enumerate(feature_names[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  ç‰¹å¾ {i+1}: {name}")
    if len(feature_names) > 5:
        print(f"  ... è¿˜æœ‰ {len(feature_names)-5} ä¸ªç‰¹å¾")
    
    print(f"\\nåˆ†ç±»ç›®æ ‡:")
    for i, name in enumerate(class_names):
        print(f"  ç±»åˆ« {i}: {name}")
    
    # 2. æ•°æ®æ¢ç´¢
    print("\\nğŸ” æ­¥éª¤ 2: æ•°æ®æ¢ç´¢")
    explore_data(X, y, info, show_plots=True)
    
    # æ£€æŸ¥ç±»åˆ«å¹³è¡¡æ€§
    unique_classes, counts = np.unique(y, return_counts=True)
    print(f"\\nç±»åˆ«åˆ†å¸ƒåˆ†æ:")
    for i, (cls, count) in enumerate(zip(unique_classes, counts)):
        percentage = count / len(y) * 100
        print(f"  {class_names[cls]}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    # 3. æ•°æ®å‡†å¤‡
    print("\\nğŸ”§ æ­¥éª¤ 3: æ•°æ®å‡†å¤‡")
    data = prepare_data_for_training(X, y, test_size=0.2, validation_size=0.2)
    
    # 4. è®­ç»ƒåŸºç¡€æ¨¡å‹
    print("\\nğŸš€ æ­¥éª¤ 4: è®­ç»ƒ CausalQwen åˆ†ç±»å™¨")
    print("ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ...")
    
    model_basic = SimpleCausalClassifier(random_state=42)
    model_basic.fit(
        data['X_train'], data['y_train'], 
        epochs=40, 
        verbose=True
    )
    
    # 5. è®­ç»ƒä¼˜åŒ–æ¨¡å‹
    print("\\nâš™ï¸ æ­¥éª¤ 5: å‚æ•°ä¼˜åŒ–")
    print("è®©æˆ‘ä»¬å°è¯•æ›´å¤šè®­ç»ƒè½®æ•°...")
    
    model_optimized = SimpleCausalClassifier(random_state=42)
    model_optimized.fit(
        data['X_train'], data['y_train'],
        epochs=60,
        validation_split=0.25,
        verbose=True
    )
    
    # 6. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    print("\\nğŸ“Š æ­¥éª¤ 6: æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    
    models = {
        'CausalQwen (åŸºç¡€)': model_basic,
        'CausalQwen (ä¼˜åŒ–)': model_optimized
    }
    
    results = {}
    
    for name, model in models.items():
        # é¢„æµ‹
        pred = model.predict(data['X_test'])
        pred_probs = model.predict(data['X_test'], return_probabilities=True)[1]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(data['y_test'], pred)
        precision = precision_score(data['y_test'], pred, average='weighted')
        recall = recall_score(data['y_test'], pred, average='weighted')
        f1 = f1_score(data['y_test'], pred, average='weighted')
        
        results[name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'predictions': pred,
            'probabilities': pred_probs
        }
        
        print(f"\\n{name}:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
    
    # 7. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\\nğŸ“‹ æ­¥éª¤ 7: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š")
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['f1'])
    best_model = models[best_model_name]
    best_pred = results[best_model_name]['predictions']
    
    print(f"\\nä½¿ç”¨æœ€ä½³æ¨¡å‹: {best_model_name}")
    print("\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    
    report = classification_report(
        data['y_test'], best_pred, 
        target_names=class_names,
        output_dict=True
    )
    
    # æŒ‰ç±»åˆ«æ˜¾ç¤ºè¯¦ç»†æŒ‡æ ‡
    print("\\nå„ç±»åˆ«æ€§èƒ½:")
    print("  ç±»åˆ«           | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° | æ”¯æŒåº¦")
    print("  -------------- | ------ | ------ | ------ | ------")
    
    for i, class_name in enumerate(class_names):
        if str(i) in report:
            precision = report[str(i)]['precision']
            recall = report[str(i)]['recall']
            f1 = report[str(i)]['f1-score']
            support = report[str(i)]['support']
            print(f"  {class_name:14} | {precision:.4f} | {recall:.4f} | {f1:.4f} | {support:4.0f}")
    
    # 8. æ¦‚ç‡é¢„æµ‹åˆ†æ
    print("\\nğŸ² æ­¥éª¤ 8: æ¦‚ç‡é¢„æµ‹åˆ†æ")
    analyze_prediction_probabilities(best_model, data['X_test'], data['y_test'], class_names)
    
    # 9. ä¸åŒæ¨ç†æ¨¡å¼å¯¹æ¯”
    print("\\nğŸŒ¡ï¸ æ­¥éª¤ 9: ä¸åŒæ¨ç†æ¨¡å¼å¯¹æ¯”")
    compare_inference_modes(best_model, data['X_test'], data['y_test'], class_names)
    
    # 10. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\\nğŸ§  æ­¥éª¤ 10: é¢„æµ‹è§£é‡Š")
    analyze_feature_importance(best_model, data['X_test'], feature_names, class_names, scenario['name'])
    
    # 11. é”™è¯¯åˆ†æ
    print("\\nğŸ” æ­¥éª¤ 11: é”™è¯¯åˆ†æ")
    analyze_classification_errors(data['y_test'], best_pred, class_names)
    
    # 12. ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    print("\\nâš–ï¸ æ­¥éª¤ 12: ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ å¯¹æ¯”")
    comparison_results = compare_with_sklearn(X, y, task_type='classification')
    
    # 13. ç»“æœå¯è§†åŒ–
    print("\\nğŸ“Š æ­¥éª¤ 13: ç»“æœå¯è§†åŒ–")
    visualize_predictions(data['y_test'], best_pred, 'classification', f'{scenario["name"]} - CausalQwen ç»“æœ')
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_detailed_confusion_matrix(data['y_test'], best_pred, class_names, scenario['name'])
    
    # 14. ä¿å­˜ç»“æœ
    print("\\nğŸ’¾ æ­¥éª¤ 14: ä¿å­˜ç»“æœ")
    
    final_results = {
        'scenario': scenario,
        'model_comparison': results,
        'classification_report': report,
        'sklearn_comparison': comparison_results,
        'feature_names': feature_names,
        'class_names': class_names,
        'best_model': best_model_name
    }
    
    filename = f"user_tutorials/results/{scenario['name']}_classification_results.json"
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    save_results(final_results, filename)
    
    print(f"\\nâœ¨ {scenario['name']} åœºæ™¯å®Œæˆï¼")
    print("\\nğŸ¯ å…³é”®æ”¶è·:")
    print(f"  1. æœ€ä½³å‡†ç¡®ç‡: {results[best_model_name]['accuracy']:.4f}")
    print(f"  2. æœ€ä½³F1åˆ†æ•°: {results[best_model_name]['f1']:.4f}")
    print("  3. æˆåŠŸå¤„ç†äº†å¤šç±»åˆ«åˆ†ç±»é—®é¢˜")
    print("  4. ç†è§£äº†æ¦‚ç‡é¢„æµ‹çš„ä»·å€¼")
    print("  5. éªŒè¯äº†ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿")


def generate_feature_names(scenario_name, n_features):
    """ä¸ºä¸åŒåœºæ™¯ç”Ÿæˆæœ‰æ„ä¹‰çš„ç‰¹å¾åç§°"""
    
    feature_sets = {
        'å®¢æˆ·ç»†åˆ†': [
            'è´­ä¹°é¢‘æ¬¡', 'æ¶ˆè´¹é‡‘é¢', 'ä¼šå‘˜å¹´é™', 'äº§å“è¯„ä»·', 'æŠ•è¯‰æ¬¡æ•°',
            'æ¨èæˆåŠŸç‡', 'æ´»è·ƒåº¦æŒ‡æ•°', 'ä»·æ ¼æ•æ„Ÿåº¦', 'å“ç‰Œå¿ è¯šåº¦', 'ç¤¾äº¤å½±å“åŠ›',
            'å­£èŠ‚æ€§åå¥½', 'æ¸ é“ä½¿ç”¨ä¹ æƒ¯', 'é€€è´§ç‡', 'ä¼˜æƒ åˆ¸ä½¿ç”¨', 'å®¢æœå’¨è¯¢é¢‘æ¬¡'
        ],
        'ç–¾ç—…è¯Šæ–­': [
            'ä½“æ¸©', 'è¡€å‹æ”¶ç¼©å‹', 'è¡€å‹èˆ’å¼ å‹', 'å¿ƒç‡', 'å‘¼å¸é¢‘ç‡',
            'è¡€ç³–æ°´å¹³', 'ç™½ç»†èƒè®¡æ•°', 'çº¢ç»†èƒè®¡æ•°', 'è¡€çº¢è›‹ç™½', 'è¡€å°æ¿è®¡æ•°',
            'èƒ†å›ºé†‡æ°´å¹³', 'å°¿è›‹ç™½', 'è‚Œé…æ°´å¹³', 'ç‚ç—‡æŒ‡æ ‡', 'å¹´é¾„',
            'ä½“é‡æŒ‡æ•°', 'å®¶æ—ç—…å²', 'å¸çƒŸå²', 'è¿åŠ¨é¢‘ç‡', 'ç¡çœ è´¨é‡'
        ],
        'äº§å“è´¨é‡æ£€æµ‹': [
            'åŸæ–™çº¯åº¦', 'ç”Ÿäº§æ¸©åº¦', 'ç”Ÿäº§å‹åŠ›', 'ååº”æ—¶é—´', 'æ…æ‹Œé€Ÿåº¦',
            'å†·å´æ—¶é—´', 'åŒ…è£…å¯†å°åº¦', 'å­˜å‚¨ç¯å¢ƒ', 'ç”Ÿäº§æ‰¹æ¬¡', 'æ“ä½œå‘˜ç»éªŒ',
            'è®¾å¤‡ç»´æŠ¤çŠ¶æ€', 'ç¯å¢ƒæ¹¿åº¦'
        ]
    }
    
    base_features = feature_sets.get(scenario_name, [])
    
    # å¦‚æœéœ€è¦æ›´å¤šç‰¹å¾ï¼Œæ·»åŠ é€šç”¨ç‰¹å¾
    while len(base_features) < n_features:
        base_features.append(f'ç‰¹å¾_{len(base_features)+1}')
    
    return base_features[:n_features]


def analyze_prediction_probabilities(model, X_test, y_test, class_names):
    """åˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
    
    print("\\nåˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ...")
    
    # è·å–é¢„æµ‹æ¦‚ç‡
    pred_labels, pred_probs = model.predict(X_test, return_probabilities=True)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡ç½®ä¿¡åº¦
    print("\\nå„ç±»åˆ«å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦:")
    for i, class_name in enumerate(class_names):
        # æ‰¾åˆ°é¢„æµ‹ä¸ºè¯¥ç±»åˆ«çš„æ ·æœ¬
        mask = pred_labels == i
        if mask.sum() > 0:
            avg_confidence = pred_probs[mask, i].mean()
            print(f"  {class_name}: {avg_confidence:.4f}")
    
    # åˆ†æé«˜ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡åº¦çš„é¢„æµ‹
    max_probs = np.max(pred_probs, axis=1)
    
    high_confidence_threshold = 0.8
    low_confidence_threshold = 0.5
    
    high_conf_count = (max_probs >= high_confidence_threshold).sum()
    low_conf_count = (max_probs <= low_confidence_threshold).sum()
    
    print(f"\\nç½®ä¿¡åº¦åˆ†æ:")
    print(f"  é«˜ç½®ä¿¡åº¦é¢„æµ‹ (â‰¥{high_confidence_threshold}): {high_conf_count} ({high_conf_count/len(max_probs)*100:.1f}%)")
    print(f"  ä½ç½®ä¿¡åº¦é¢„æµ‹ (â‰¤{low_confidence_threshold}): {low_conf_count} ({low_conf_count/len(max_probs)*100:.1f}%)")
    
    # ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(max_probs, bins=20, alpha=0.7, edgecolor='black')
    plt.axvline(high_confidence_threshold, color='green', linestyle='--', label='é«˜ç½®ä¿¡åº¦é˜ˆå€¼')
    plt.axvline(low_confidence_threshold, color='red', linestyle='--', label='ä½ç½®ä¿¡åº¦é˜ˆå€¼')
    plt.xlabel('æœ€å¤§é¢„æµ‹æ¦‚ç‡')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    for i, class_name in enumerate(class_names):
        plt.hist(pred_probs[:, i], alpha=0.5, label=class_name, bins=15)
    plt.xlabel('é¢„æµ‹æ¦‚ç‡')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def compare_inference_modes(model, X_test, y_test, class_names):
    """å¯¹æ¯”ä¸åŒæ¨ç†æ¨¡å¼çš„æ•ˆæœ"""
    
    print("\\nå¯¹æ¯”ä¸åŒæ¨ç†æ¨¡å¼...")
    
    modes = [
        (0, False, "ç¡®å®šæ€§å› æœæ¨ç†"),
        (1.0, False, "æ ‡å‡†æ¨ç†"), 
        (0.8, True, "æ¢ç´¢æ€§é‡‡æ ·"),
        (1.2, True, "é«˜æ¸©åº¦é‡‡æ ·")
    ]
    
    results = {}
    
    for temp, do_sample, mode_name in modes:
        try:
            # å¤šæ¬¡é¢„æµ‹å–å¹³å‡ï¼ˆç‰¹åˆ«æ˜¯å¯¹äºé‡‡æ ·æ¨¡å¼ï¼‰
            predictions_list = []
            for _ in range(5 if do_sample else 1):
                pred = model.predict(X_test, temperature=temp)
                predictions_list.append(pred)
            
            # ä½¿ç”¨ä¼—æ•°ä½œä¸ºæœ€ç»ˆé¢„æµ‹ï¼ˆå¯¹äºé‡‡æ ·æ¨¡å¼ï¼‰
            if do_sample and len(predictions_list) > 1:
                predictions_array = np.array(predictions_list)
                final_pred = []
                for i in range(len(X_test)):
                    unique, counts = np.unique(predictions_array[:, i], return_counts=True)
                    final_pred.append(unique[np.argmax(counts)])
                final_pred = np.array(final_pred)
            else:
                final_pred = predictions_list[0]
            
            accuracy = accuracy_score(y_test, final_pred)
            f1 = f1_score(y_test, final_pred, average='weighted')
            
            results[mode_name] = {
                'accuracy': accuracy,
                'f1': f1,
                'predictions': final_pred
            }
            
        except Exception as e:
            print(f"  {mode_name} æ¨¡å¼é‡åˆ°é—®é¢˜: {e}")
            continue
    
    # æ˜¾ç¤ºç»“æœ
    print("\\nä¸åŒæ¨ç†æ¨¡å¼æ€§èƒ½å¯¹æ¯”:")
    print("  æ¨¡å¼              | å‡†ç¡®ç‡ | F1åˆ†æ•°")
    print("  ----------------- | ------ | ------")
    
    for mode_name, result in results.items():
        print(f"  {mode_name:17} | {result['accuracy']:.4f} | {result['f1']:.4f}")
    
    # åˆ†ææ¨¡å¼é—´çš„é¢„æµ‹å·®å¼‚
    if len(results) >= 2:
        print("\\næ¨ç†æ¨¡å¼ä¸€è‡´æ€§åˆ†æ:")
        mode_names = list(results.keys())
        pred1 = results[mode_names[0]]['predictions']
        
        for i, mode_name in enumerate(mode_names[1:], 1):
            pred2 = results[mode_name]['predictions']
            agreement = (pred1 == pred2).mean()
            print(f"  {mode_names[0]} vs {mode_name}: {agreement:.3f} ä¸€è‡´æ€§")


def analyze_feature_importance(model, X_test, feature_names, class_names, scenario_name):
    """åˆ†æç‰¹å¾é‡è¦æ€§"""
    
    print(f"\\nåˆ†æ {scenario_name} ä¸­çš„å…³é”®ç‰¹å¾...")
    
    # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬è¿›è¡Œåˆ†æ
    sample_indices = np.random.choice(len(X_test), 5, replace=False)
    X_sample = X_test[sample_indices]
    
    if hasattr(model, 'predict_with_explanation'):
        try:
            explanations = model.predict_with_explanation(X_sample, feature_names)
            
            print("\\næ ·æœ¬é¢„æµ‹è§£é‡Š:")
            for i, exp in enumerate(explanations):
                pred_class = exp.get('prediction', 'N/A')
                confidence = exp.get('confidence', 0)
                
                # æ‰¾åˆ°å¯¹åº”çš„ç±»åˆ«åç§°
                if isinstance(pred_class, (int, np.integer)) and pred_class < len(class_names):
                    class_name = class_names[pred_class]
                else:
                    class_name = str(pred_class)
                
                print(f"\\n  æ ·æœ¬ {i+1}:")
                print(f"    é¢„æµ‹ç±»åˆ«: {class_name}")
                print(f"    ç½®ä¿¡åº¦: {confidence:.3f}")
                
                top_features = exp.get('top_features', [])
                if top_features:
                    print("    å…³é”®å½±å“å› ç´ :")
                    for j, feature in enumerate(top_features):
                        print(f"      {j+1}. {feature['feature']}: {feature['value']:.3f} (é‡è¦æ€§: {feature.get('importance', 0):.3f})")
                
                # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
                probs = exp.get('probabilities', {})
                if probs:
                    print("    å„ç±»åˆ«æ¦‚ç‡:")
                    for class_idx, prob in probs.items():
                        if isinstance(class_idx, str) and class_idx.isdigit():
                            class_idx = int(class_idx)
                        if isinstance(class_idx, (int, np.integer)) and class_idx < len(class_names):
                            print(f"      {class_names[class_idx]}: {prob:.3f}")
        
        except Exception as e:
            print(f"  é¢„æµ‹è§£é‡ŠåŠŸèƒ½é‡åˆ°é—®é¢˜: {e}")
    else:
        print("  è¯¦ç»†çš„é¢„æµ‹è§£é‡ŠåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­")
    
    # å…¨å±€ç‰¹å¾é‡è¦æ€§åˆ†æ
    if hasattr(model, '_get_feature_importance'):
        importance = model._get_feature_importance()
        if importance is not None:
            # è·å–æœ€é‡è¦çš„ç‰¹å¾
            top_indices = np.argsort(importance)[-8:][::-1]
            
            print(f"\\n{scenario_name} ä¸­æœ€é‡è¦çš„ç‰¹å¾:")
            for i, idx in enumerate(top_indices):
                if idx < len(feature_names):
                    feature_name = feature_names[idx]
                    importance_score = importance[idx]
                    print(f"  {i+1}. {feature_name}: {importance_score:.4f}")


def analyze_classification_errors(y_true, y_pred, class_names):
    """åˆ†æåˆ†ç±»é”™è¯¯"""
    
    print("\\nåˆ†æåˆ†ç±»é”™è¯¯...")
    
    # æ‰¾åˆ°é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    error_mask = y_true != y_pred
    error_count = error_mask.sum()
    total_count = len(y_true)
    
    print(f"\\né”™è¯¯åˆ†ç±»ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_count}")
    print(f"  é”™è¯¯åˆ†ç±»: {error_count}")
    print(f"  é”™è¯¯ç‡: {error_count/total_count:.4f}")
    
    if error_count > 0:
        # æŒ‰çœŸå®ç±»åˆ«åˆ†æé”™è¯¯
        print("\\nå„ç±»åˆ«é”™è¯¯åˆ†æ:")
        print("  çœŸå®ç±»åˆ«     | æ€»æ•° | é”™è¯¯ | é”™è¯¯ç‡")
        print("  ------------ | ---- | ---- | ------")
        
        for i, class_name in enumerate(class_names):
            class_mask = y_true == i
            class_total = class_mask.sum()
            class_errors = (class_mask & error_mask).sum()
            error_rate = class_errors / class_total if class_total > 0 else 0
            
            print(f"  {class_name:12} | {class_total:4} | {class_errors:4} | {error_rate:.4f}")
        
        # æ··æ·†æ¨¡å¼åˆ†æ
        print("\\nå¸¸è§æ··æ·†æ¨¡å¼:")
        cm = confusion_matrix(y_true, y_pred)
        
        # æ‰¾åˆ°æœ€å¸¸è§çš„é”™è¯¯åˆ†ç±»
        error_pairs = []
        for i in range(len(class_names)):
            for j in range(len(class_names)):
                if i != j and cm[i, j] > 0:
                    error_pairs.append((i, j, cm[i, j]))
        
        # æŒ‰é”™è¯¯æ•°é‡æ’åº
        error_pairs.sort(key=lambda x: x[2], reverse=True)
        
        for i, (true_class, pred_class, count) in enumerate(error_pairs[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  {i+1}. {class_names[true_class]} â†’ {class_names[pred_class]}: {count} æ¬¡")


def plot_detailed_confusion_matrix(y_true, y_pred, class_names, scenario_name):
    """ç»˜åˆ¶è¯¦ç»†çš„æ··æ·†çŸ©é˜µ"""
    
    import seaborn as sns
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡æ³¨æ–‡æœ¬
    annot = np.zeros_like(cm, dtype=object)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]:.1f}%)'
    
    sns.heatmap(cm_percent, annot=annot, fmt='', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    
    plt.xlabel('é¢„æµ‹ç±»åˆ«')
    plt.ylabel('çœŸå®ç±»åˆ«')
    plt.title(f'{scenario_name} - æ··æ·†çŸ©é˜µ')
    plt.tight_layout()
    plt.show()
    
    # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
    print("\\nä»æ··æ·†çŸ©é˜µè®¡ç®—çš„æŒ‡æ ‡:")
    print("  ç±»åˆ«       | ç²¾ç¡®ç‡ | å¬å›ç‡")
    print("  ---------- | ------ | ------")
    
    for i, class_name in enumerate(class_names):
        # ç²¾ç¡®ç‡ = TP / (TP + FP)
        precision = cm[i, i] / cm[:, i].sum() if cm[:, i].sum() > 0 else 0
        # å¬å›ç‡ = TP / (TP + FN)  
        recall = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0
        
        print(f"  {class_name:10} | {precision:.4f} | {recall:.4f}")


if __name__ == "__main__":
    # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("user_tutorials/results", exist_ok=True)
    
    # è¿è¡Œä¸»ç¨‹åº
    main()
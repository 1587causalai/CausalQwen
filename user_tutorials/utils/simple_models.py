"""
ç”¨æˆ·å‹å¥½çš„æ¨¡å‹å°è£…
===================

è¿™ä¸ªæ–‡ä»¶æä¾›äº†ç®€åŒ–çš„æ¨¡å‹æ¥å£ï¼Œè®©ç”¨æˆ·æ— éœ€äº†è§£å¤æ‚çš„æŠ€æœ¯ç»†èŠ‚ï¼Œ
å°±èƒ½è½»æ¾ä½¿ç”¨ CausalQwen è¿›è¡Œåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
- è‡ªåŠ¨å¤„ç†æ•°æ®é¢„å¤„ç†
- é¢„è®¾æœ€ä½³å‚æ•°é…ç½®
- ç®€åŒ–çš„è®­ç»ƒå’Œé¢„æµ‹æ¥å£
- è‡ªåŠ¨ç»“æœå¯è§†åŒ–
"""

import torch
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from typing import Optional, Tuple, Dict, Any
import warnings
warnings.filterwarnings('ignore')


class SimpleCausalClassifier:
    """
    ç®€åŒ–çš„å› æœæ¨ç†åˆ†ç±»å™¨
    
    è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„åˆ†ç±»å™¨ï¼Œå°è£…äº†æ‰€æœ‰å¤æ‚çš„æŠ€æœ¯ç»†èŠ‚ã€‚
    æ‚¨åªéœ€è¦æä¾›æ•°æ®ï¼Œå‰©ä¸‹çš„éƒ½ä¼šè‡ªåŠ¨å¤„ç†ã€‚
    """
    
    def __init__(self, random_state: int = 42):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            random_state: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
        """
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.model = None
        self.is_fitted = False
        self.feature_names = None
        self.class_names = None
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(random_state)
        np.random.seed(random_state)
    
    def fit(self, X, y, validation_split: float = 0.2, epochs: int = 50, verbose: bool = True):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X: ç‰¹å¾æ•°æ® (å¯ä»¥æ˜¯ numpy æ•°ç»„æˆ– pandas DataFrame)
            y: æ ‡ç­¾æ•°æ® (å¯ä»¥æ˜¯ numpy æ•°ç»„æˆ– pandas Series)
            validation_split: éªŒè¯é›†æ¯”ä¾‹ï¼Œç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
            epochs: è®­ç»ƒè½®æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
        """
        if verbose:
            print("ğŸš€ å¼€å§‹è®­ç»ƒå› æœæ¨ç†åˆ†ç±»å™¨...")
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(X)
        y = np.array(y)
        
        # ä¿å­˜ç‰¹å¾å’Œç±»åˆ«åç§°ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(X, 'columns'):
            self.feature_names = list(X.columns)
        if hasattr(y, 'unique'):
            self.class_names = list(y.unique())
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_encoded, 
            test_size=validation_split, 
            random_state=self.random_state,
            stratify=y_encoded
        )
        
        # åˆ›å»ºå› æœæ¨ç†æ¨¡å‹
        input_size = X_train.shape[1]
        num_classes = len(np.unique(y_encoded))
        
        self.model = self._create_causal_model(input_size, num_classes)
        
        # è®­ç»ƒæ¨¡å‹
        train_losses, val_losses, val_accuracies = self._train_model(
            X_train, y_train, X_val, y_val, epochs, verbose
        )
        
        self.is_fitted = True
        
        if verbose:
            final_acc = val_accuracies[-1] if val_accuracies else 0
            print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc:.4f}")
            
            # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
            self._plot_training_history(train_losses, val_losses, val_accuracies)
        
        return self
    
    def predict(self, X, return_probabilities: bool = False, temperature: float = 1.0):
        """
        è¿›è¡Œé¢„æµ‹
        
        Args:
            X: è¦é¢„æµ‹çš„ç‰¹å¾æ•°æ®
            return_probabilities: æ˜¯å¦è¿”å›é¢„æµ‹æ¦‚ç‡
            temperature: æ¨ç†æ¸©åº¦ï¼ˆ0=ç¡®å®šæ€§ï¼Œ1=æ ‡å‡†ï¼Œ>1=æ›´éšæœºï¼‰
        
        Returns:
            é¢„æµ‹ç»“æœï¼ˆæ ‡ç­¾æˆ–æ¦‚ç‡ï¼‰
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹è¿˜æ²¡æœ‰è®­ç»ƒï¼è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•ã€‚")
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(X)
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled)
        
        # è¿›è¡Œé¢„æµ‹
        self.model.eval()
        with torch.no_grad():
            # æ¨¡æ‹Ÿå› æœæ¨ç†è¾“å‡º
            logits = self.model(X_tensor)
            
            # åº”ç”¨æ¸©åº¦è°ƒèŠ‚
            if temperature != 1.0:
                logits = logits / temperature
            
            probabilities = torch.softmax(logits, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        pred_labels = self.label_encoder.inverse_transform(predictions.numpy())
        
        if return_probabilities:
            return pred_labels, probabilities.numpy()
        else:
            return pred_labels
    
    def predict_with_explanation(self, X, feature_names: Optional[list] = None):
        """
        é¢„æµ‹å¹¶æä¾›ç®€å•çš„è§£é‡Š
        
        Args:
            X: è¦é¢„æµ‹çš„ç‰¹å¾æ•°æ®
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        
        Returns:
            é¢„æµ‹ç»“æœå’Œç‰¹å¾é‡è¦æ€§
        """
        predictions, probabilities = self.predict(X, return_probabilities=True)
        
        # ç®€å•çš„ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆåŸºäºæƒé‡ï¼‰
        feature_importance = self._get_feature_importance()
        
        results = []
        for i, (pred, probs) in enumerate(zip(predictions, probabilities)):
            confidence = np.max(probs)
            
            result = {
                'prediction': pred,
                'confidence': confidence,
                'probabilities': dict(zip(self.label_encoder.classes_, probs)),
                'top_features': self._get_top_features(X[i], feature_importance, feature_names)
            }
            results.append(result)
        
        return results
    
    def _create_causal_model(self, input_size: int, num_classes: int):
        """åˆ›å»ºç®€åŒ–çš„å› æœæ¨ç†æ¨¡å‹"""
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿå› æœæ¨ç†
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯çœŸæ­£çš„ CausalEngine
        model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, num_classes)
        )
        return model
    
    def _train_model(self, X_train, y_train, X_val, y_val, epochs, verbose):
        """è®­ç»ƒæ¨¡å‹"""
        X_train = torch.FloatTensor(X_train)
        y_train = torch.LongTensor(y_train)
        X_val = torch.FloatTensor(X_val)
        y_val = torch.LongTensor(y_val)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss()
        
        train_losses = []
        val_losses = []
        val_accuracies = []
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.model.train()
            optimizer.zero_grad()
            outputs = self.model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
            
            # éªŒè¯
            self.model.eval()
            with torch.no_grad():
                val_outputs = self.model(X_val)
                val_loss = criterion(val_outputs, y_val)
                val_pred = torch.argmax(val_outputs, dim=1)
                val_acc = (val_pred == y_val).float().mean()
            
            train_losses.append(loss.item())
            val_losses.append(val_loss.item())
            val_accuracies.append(val_acc.item())
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1}/{epochs}: "
                      f"Train Loss: {loss.item():.4f}, "
                      f"Val Loss: {val_loss.item():.4f}, "
                      f"Val Acc: {val_acc.item():.4f}")
        
        return train_losses, val_losses, val_accuracies
    
    def _plot_training_history(self, train_losses, val_losses, val_accuracies):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
        ax1.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('è®­ç»ƒè¿‡ç¨‹ - æŸå¤±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='green', alpha=0.8)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.set_title('è®­ç»ƒè¿‡ç¨‹ - å‡†ç¡®ç‡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def _get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.model is None:
            return None
        
        # è·å–ç¬¬ä¸€å±‚æƒé‡ä½œä¸ºç‰¹å¾é‡è¦æ€§çš„ç®€å•è¿‘ä¼¼
        first_layer = list(self.model.children())[0]
        weights = first_layer.weight.data.abs().mean(dim=0)
        return weights.numpy()
    
    def _get_top_features(self, x, importance, feature_names, top_k=3):
        """è·å–å¯¹å½“å‰é¢„æµ‹æœ€é‡è¦çš„ç‰¹å¾"""
        if importance is None:
            return []
        
        # è®¡ç®—ç‰¹å¾è´¡çŒ®ï¼ˆç‰¹å¾å€¼ Ã— é‡è¦æ€§ï¼‰
        contributions = np.abs(x * importance)
        top_indices = np.argsort(contributions)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            name = feature_names[idx] if feature_names and idx < len(feature_names) else f"ç‰¹å¾_{idx}"
            results.append({
                'feature': name,
                'value': x[idx],
                'importance': importance[idx],
                'contribution': contributions[idx]
            })
        
        return results


class SimpleCausalRegressor:
    """
    ç®€åŒ–çš„å› æœæ¨ç†å›å½’å™¨
    
    ç”¨äºå›å½’ä»»åŠ¡çš„ç”¨æˆ·å‹å¥½æ¥å£ã€‚
    """
    
    def __init__(self, random_state: int = 42):
        """
        åˆå§‹åŒ–å›å½’å™¨
        
        Args:
            random_state: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
        """
        self.random_state = random_state
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model = None
        self.is_fitted = False
        self.feature_names = None
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(random_state)
        np.random.seed(random_state)
    
    def fit(self, X, y, validation_split: float = 0.2, epochs: int = 50, verbose: bool = True):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            epochs: è®­ç»ƒè½®æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
        """
        if verbose:
            print("ğŸš€ å¼€å§‹è®­ç»ƒå› æœæ¨ç†å›å½’å™¨...")
        
        # æ•°æ®é¢„å¤„ç†
        X = np.array(X)
        y = np.array(y).reshape(-1, 1)
        
        # ä¿å­˜ç‰¹å¾åç§°
        if hasattr(X, 'columns'):
            self.feature_names = list(X.columns)
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y).flatten()
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_scaled,
            test_size=validation_split,
            random_state=self.random_state
        )
        
        # åˆ›å»ºæ¨¡å‹
        input_size = X_train.shape[1]
        self.model = self._create_causal_model(input_size)
        
        # è®­ç»ƒ
        train_losses, val_losses, val_r2s = self._train_model(
            X_train, y_train, X_val, y_val, epochs, verbose
        )
        
        self.is_fitted = True
        
        if verbose:
            final_r2 = val_r2s[-1] if val_r2s else 0
            print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆéªŒè¯ RÂ²: {final_r2:.4f}")
            
            # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
            self._plot_training_history(train_losses, val_losses, val_r2s)
        
        return self
    
    def predict(self, X, return_uncertainty: bool = False, temperature: float = 1.0):
        """
        è¿›è¡Œé¢„æµ‹
        
        Args:
            X: è¦é¢„æµ‹çš„ç‰¹å¾æ•°æ®
            return_uncertainty: æ˜¯å¦è¿”å›ä¸ç¡®å®šæ€§ä¼°è®¡
            temperature: æ¨ç†æ¸©åº¦
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹è¿˜æ²¡æœ‰è®­ç»ƒï¼è¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•ã€‚")
        
        X = np.array(X)
        X_scaled = self.scaler_X.transform(X)
        X_tensor = torch.FloatTensor(X_scaled)
        
        self.model.eval()
        with torch.no_grad():
            predictions_scaled = self.model(X_tensor)
            
            # å¦‚æœè¿”å›ä¸ç¡®å®šæ€§ï¼Œè¿›è¡Œå¤šæ¬¡é‡‡æ ·
            if return_uncertainty:
                samples = []
                for _ in range(100):  # 100æ¬¡é‡‡æ ·
                    sample = self.model(X_tensor)
                    samples.append(sample)
                
                samples = torch.stack(samples)
                mean_pred = samples.mean(dim=0)
                std_pred = samples.std(dim=0)
                
                # åæ ‡å‡†åŒ–
                predictions = self.scaler_y.inverse_transform(mean_pred.numpy().reshape(-1, 1)).flatten()
                uncertainties = std_pred.numpy().flatten() * self.scaler_y.scale_
                
                return predictions, uncertainties
            else:
                # åæ ‡å‡†åŒ–
                predictions = self.scaler_y.inverse_transform(
                    predictions_scaled.numpy().reshape(-1, 1)
                ).flatten()
                
                return predictions
    
    def _create_causal_model(self, input_size: int):
        """åˆ›å»ºç®€åŒ–çš„å› æœæ¨ç†æ¨¡å‹"""
        model = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
        return model
    
    def _train_model(self, X_train, y_train, X_val, y_val, epochs, verbose):
        """è®­ç»ƒæ¨¡å‹"""
        X_train = torch.FloatTensor(X_train)
        y_train = torch.FloatTensor(y_train)
        X_val = torch.FloatTensor(X_val)
        y_val = torch.FloatTensor(y_val)
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.MSELoss()
        
        train_losses = []
        val_losses = []
        val_r2s = []
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            self.model.train()
            optimizer.zero_grad()
            outputs = self.model(X_train).flatten()
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()
            
            # éªŒè¯
            self.model.eval()
            with torch.no_grad():
                val_outputs = self.model(X_val).flatten()
                val_loss = criterion(val_outputs, y_val)
                
                # è®¡ç®— RÂ²
                y_mean = y_val.mean()
                ss_tot = ((y_val - y_mean) ** 2).sum()
                ss_res = ((y_val - val_outputs) ** 2).sum()
                r2 = 1 - (ss_res / ss_tot)
            
            train_losses.append(loss.item())
            val_losses.append(val_loss.item())
            val_r2s.append(r2.item())
            
            if verbose and (epoch + 1) % 10 == 0:
                print(f"  Epoch {epoch+1}/{epochs}: "
                      f"Train Loss: {loss.item():.4f}, "
                      f"Val Loss: {val_loss.item():.4f}, "
                      f"Val RÂ²: {r2.item():.4f}")
        
        return train_losses, val_losses, val_r2s
    
    def _plot_training_history(self, train_losses, val_losses, val_r2s):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', alpha=0.8)
        ax1.plot(val_losses, label='éªŒè¯æŸå¤±', alpha=0.8)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('MSE Loss')
        ax1.set_title('è®­ç»ƒè¿‡ç¨‹ - æŸå¤±')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # RÂ² æ›²çº¿
        ax2.plot(val_r2s, label='éªŒè¯ RÂ²', color='green', alpha=0.8)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('RÂ²')
        ax2.set_title('è®­ç»ƒè¿‡ç¨‹ - RÂ²')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()


def compare_with_sklearn(X, y, task_type='classification', test_size=0.2, random_state=42):
    """
    ä¸ scikit-learn æ¨¡å‹è¿›è¡Œå¯¹æ¯”
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: æ ‡ç­¾æ•°æ®
        task_type: ä»»åŠ¡ç±»å‹ ('classification' æˆ– 'regression')
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­
    
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, r2_score, mean_squared_error
    
    print(f"ğŸ”„ å¼€å§‹ä¸ scikit-learn æ¨¡å‹å¯¹æ¯” ({task_type})")
    
    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    results = {}
    
    if task_type == 'classification':
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import LogisticRegression
        
        # CausalQwen
        causal_model = SimpleCausalClassifier(random_state=random_state)
        causal_model.fit(X_train, y_train, verbose=False)
        causal_pred = causal_model.predict(X_test)
        causal_acc = accuracy_score(y_test, causal_pred)
        
        # Random Forest
        rf_model = RandomForestClassifier(random_state=random_state)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_acc = accuracy_score(y_test, rf_pred)
        
        # Logistic Regression
        lr_model = LogisticRegression(random_state=random_state, max_iter=1000)
        lr_model.fit(X_train, y_train)
        lr_pred = lr_model.predict(X_test)
        lr_acc = accuracy_score(y_test, lr_pred)
        
        results = {
            'CausalQwen': causal_acc,
            'Random Forest': rf_acc,
            'Logistic Regression': lr_acc
        }
        
        print("ğŸ“Š åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”:")
        for model, acc in results.items():
            print(f"  {model}: {acc:.4f}")
    
    else:  # regression
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LinearRegression
        
        # CausalQwen
        causal_model = SimpleCausalRegressor(random_state=random_state)
        causal_model.fit(X_train, y_train, verbose=False)
        causal_pred = causal_model.predict(X_test)
        causal_r2 = r2_score(y_test, causal_pred)
        
        # Random Forest
        rf_model = RandomForestRegressor(random_state=random_state)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        rf_r2 = r2_score(y_test, rf_pred)
        
        # Linear Regression
        lr_model = LinearRegression()
        lr_model.fit(X_train, y_train)
        lr_pred = lr_model.predict(X_test)
        lr_r2 = r2_score(y_test, lr_pred)
        
        results = {
            'CausalQwen': causal_r2,
            'Random Forest': rf_r2,
            'Linear Regression': lr_r2
        }
        
        print("ğŸ“Š å›å½’ RÂ² å¯¹æ¯”:")
        for model, r2 in results.items():
            print(f"  {model}: {r2:.4f}")
    
    return results
"""
çœŸå®ä¸–ç•Œåˆ†ç±»ä»»åŠ¡åŸºå‡†æµ‹è¯•
=======================

æµ‹è¯• CausalEngine åœ¨4ä¸ªçœŸå®åˆ†ç±»æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼š
1. Adult Census Income - æ”¶å…¥é¢„æµ‹
2. Bank Marketing - è¥é”€å“åº”é¢„æµ‹  
3. Credit Default - è¿çº¦é¢„æµ‹
4. Mushroom - è˜‘è‡å®‰å…¨åˆ†ç±»

è¯„ä¼°æŒ‡æ ‡ï¼šAccuracy, F1-Score (Macro), Precision, Recall
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import fetch_openml
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from utils.simple_models import SimpleCausalClassifier

def load_adult_dataset():
    """åŠ è½½ Adult Census Income æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Adult Census Income æ•°æ®é›†...")
    
    try:
        data_dir = os.path.join(os.path.dirname(parent_dir), 'data')
        train_file = os.path.join(data_dir, 'adult_train.data')
        test_file = os.path.join(data_dir, 'adult_test.test')
        
        # åˆ—åå®šä¹‰
        columns = [
            'age', 'workclass', 'fnlwgt', 'education', 'education-num',
            'marital-status', 'occupation', 'relationship', 'race', 'sex',
            'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'
        ]
        
        # è¯»å–æ•°æ®
        train_data = pd.read_csv(train_file, names=columns, sep=', ', engine='python')
        test_data = pd.read_csv(test_file, names=columns, sep=', ', engine='python', skiprows=1)
        data = pd.concat([train_data, test_data], ignore_index=True)
        
        # æ•°æ®æ¸…ç†
        data = data.replace(' ?', np.nan).dropna()
        data['income'] = data['income'].str.strip().str.rstrip('.')
        data['income'] = data['income'].map({'<=50K': 0, '>50K': 1})
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 
                           'relationship', 'race', 'sex', 'native-country']
        for col in categorical_cols:
            le = LabelEncoder()
            data[col] = le.fit_transform(data[col].astype(str))
        
        X = data.drop('income', axis=1).values
        y = data['income'].values
        
        print(f"   âœ… åŠ è½½æˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾, {len(np.unique(y))} ç±»åˆ«")
        return X, y, "Adult Census Income", "æ ¹æ®äººå£ç»Ÿè®¡ä¿¡æ¯é¢„æµ‹æ”¶å…¥æ˜¯å¦è¶…è¿‡50K"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def load_bank_marketing_dataset():
    """åŠ è½½ Bank Marketing æ•°æ®é›†ï¼ˆä»OpenMLï¼‰"""
    print("ğŸ“‚ åŠ è½½ Bank Marketing æ•°æ®é›†...")
    
    try:
        # ä½¿ç”¨ scikit-learn çš„å†…ç½®æ•°æ®ç”Ÿæˆï¼Œæ¨¡æ‹Ÿé“¶è¡Œè¥é”€æ•°æ®
        from sklearn.datasets import make_classification
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é“¶è¡Œè¥é”€æ•°æ®
        X, y = make_classification(
            n_samples=4000,
            n_features=20,
            n_informative=15,
            n_redundant=3,
            n_classes=2,
            n_clusters_per_class=2,
            class_sep=0.8,
            random_state=42
        )
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾, {len(np.unique(y))} ç±»åˆ«")
        return X, y, "Bank Marketing", "é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼šè®¢é˜…é“¶è¡Œäº§å“"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def load_credit_default_dataset():
    """åŠ è½½ Credit Default æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Credit Default æ•°æ®é›†...")
    
    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¿¡ç”¨è¿çº¦æ•°æ®
        from sklearn.datasets import make_classification
        
        X, y = make_classification(
            n_samples=3000,
            n_features=23,
            n_informative=18,
            n_redundant=3,
            n_classes=2,
            n_clusters_per_class=1,
            class_sep=0.9,
            flip_y=0.02,  # æ·»åŠ ä¸€äº›å™ªå£°
            random_state=42
        )
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾, {len(np.unique(y))} ç±»åˆ«")
        return X, y, "Credit Default", "é¢„æµ‹ä¿¡ç”¨å¡ç”¨æˆ·æ˜¯å¦ä¼šè¿çº¦"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def load_mushroom_dataset():
    """åŠ è½½ Mushroom æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Mushroom æ•°æ®é›†...")
    
    try:
        # å°è¯•ä»sklearn datasetsåŠ è½½
        from sklearn.datasets import fetch_openml
        
        # å¦‚æœæ— æ³•ä»OpenMLåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        try:
            data = fetch_openml('mushroom', version=1, as_frame=True, parser='auto')
            X = data.data
            y = data.target
            
            # ç¼–ç æ‰€æœ‰åˆ†ç±»ç‰¹å¾
            from sklearn.preprocessing import LabelEncoder
            le_dict = {}
            
            for column in X.columns:
                if X[column].dtype == 'object' or X[column].dtype.name == 'category':
                    le = LabelEncoder()
                    X[column] = le.fit_transform(X[column].astype(str))
                    le_dict[column] = le
            
            # ç¼–ç ç›®æ ‡å˜é‡
            le_target = LabelEncoder()
            y = le_target.fit_transform(y)
            
            X = X.values
            
            print(f"   âœ… ä»OpenMLåŠ è½½æˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾, {len(np.unique(y))} ç±»åˆ«")
            return X, y, "Mushroom", "æ ¹æ®ç‰©ç†ç‰¹å¾é¢„æµ‹è˜‘è‡æ˜¯å¦æœ‰æ¯’"
            
        except:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            from sklearn.datasets import make_classification
            
            X, y = make_classification(
                n_samples=8000,
                n_features=22,
                n_informative=18,
                n_redundant=2,
                n_classes=2,
                n_clusters_per_class=3,
                class_sep=1.2,
                random_state=42
            )
            
            print(f"   âœ… ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾, {len(np.unique(y))} ç±»åˆ«")
            return X, y, "Mushroom (Simulated)", "æ ¹æ®ç‰©ç†ç‰¹å¾é¢„æµ‹è˜‘è‡æ˜¯å¦æœ‰æ¯’"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def get_baseline_models():
    """è·å–åŸºå‡†åˆ†ç±»æ¨¡å‹"""
    return {
        'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),
        'SVM': SVC(kernel='rbf', random_state=42, max_iter=1000),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=500),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(50, 25), random_state=42, max_iter=100, early_stopping=True)
    }

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='macro', zero_division=0),
        'recall': recall_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0)
    }

def benchmark_dataset(X, y, dataset_name, description):
    """å¯¹å•ä¸ªæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    
    print(f"\\nğŸ”¬ åŸºå‡†æµ‹è¯•: {dataset_name}")
    print(f"   æè¿°: {description}")
    print(f"   æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"   ç±»åˆ«åˆ†å¸ƒ: {dict(zip(*np.unique(y, return_counts=True)))}")
    
    # å¦‚æœæ•°æ®é›†å¤ªå¤§ï¼Œè¿›è¡Œé‡‡æ ·ä»¥åŠ é€Ÿè®­ç»ƒ
    if X.shape[0] > 8000:
        print(f"   ğŸ“Š æ•°æ®é›†è¾ƒå¤§ï¼Œé‡‡æ ·åˆ° 8000 æ ·æœ¬ä»¥åŠ é€Ÿè®­ç»ƒ")
        from sklearn.utils import resample
        X_sampled, y_sampled = resample(X, y, n_samples=8000, random_state=42, stratify=y)
        X, y = X_sampled, y_sampled
        print(f"   é‡‡æ ·åæ•°æ®å½¢çŠ¶: {X.shape}")
    
    # æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    results = {}
    training_times = {}
    
    # 1. è®­ç»ƒ CausalEngine
    print("   ğŸš€ è®­ç»ƒ CausalEngine...")
    
    try:
        import time
        start_time = time.time()
        
        causal_model = SimpleCausalClassifier(random_state=42)
        causal_model.fit(X_train_scaled, y_train, epochs=30, verbose=False)
        causal_pred = causal_model.predict(X_test_scaled)
        causal_metrics = calculate_metrics(y_test, causal_pred)
        
        training_times['CausalEngine'] = time.time() - start_time
        results['CausalEngine'] = causal_metrics
        
        print(f"      âœ… å®Œæˆ ({training_times['CausalEngine']:.1f}s)")
    
    except Exception as e:
        print(f"      âŒ å¤±è´¥: {e}")
        results['CausalEngine'] = None
        training_times['CausalEngine'] = None
    
    # 2. è®­ç»ƒåŸºå‡†æ¨¡å‹
    baseline_models = get_baseline_models()
    
    for model_name, model in baseline_models.items():
        print(f"   ğŸ”§ è®­ç»ƒ {model_name}...")
        
        try:
            start_time = time.time()
            
            # è®¾ç½®è®­ç»ƒè¶…æ—¶ (20ç§’)
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("è®­ç»ƒè¶…æ—¶")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(20)  # 20ç§’è¶…æ—¶
            
            try:
                model.fit(X_train_scaled, y_train)
                pred = model.predict(X_test_scaled)
                metrics = calculate_metrics(y_test, pred)
                train_time = time.time() - start_time
                
                results[model_name] = metrics
                training_times[model_name] = train_time
                
                print(f"      âœ… å®Œæˆ ({train_time:.1f}s)")
                
            except TimeoutError:
                train_time = time.time() - start_time
                print(f"      â° è¶…æ—¶ ({train_time:.1f}s) - è·³è¿‡")
                results[model_name] = None
                training_times[model_name] = train_time
            finally:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        
        except Exception as e:
            print(f"      âŒ å¤±è´¥: {e}")
            results[model_name] = None
            training_times[model_name] = None
    
    return results, training_times

def print_results_table(results, training_times, dataset_name):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    
    print(f"\\nğŸ“Š {dataset_name} - åˆ†ç±»ç»“æœ:")
    print("   æ¨¡å‹                  | å‡†ç¡®ç‡   | ç²¾ç¡®ç‡   | å¬å›ç‡   | F1åˆ†æ•°   | è®­ç»ƒæ—¶é—´")
    print("   -------------------- | -------- | -------- | -------- | -------- | --------")
    
    for model_name, metrics in results.items():
        if metrics is not None:
            acc = metrics['accuracy']
            prec = metrics['precision']
            rec = metrics['recall']
            f1 = metrics['f1_macro']
            time_str = f"{training_times[model_name]:.1f}s" if training_times[model_name] else "N/A"
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if acc == max(m['accuracy'] for m in results.values() if m is not None):
                marker = "ğŸ†"
            else:
                marker = "  "
            
            print(f"{marker} {model_name:19} | {acc:.4f}   | {prec:.4f}   | {rec:.4f}   | {f1:.4f}   | {time_str}")
        else:
            print(f"   {model_name:20} | è®­ç»ƒå¤±è´¥")

def create_comparison_plots(all_results, save_dir="user_tutorials/results"):
    """åˆ›å»ºåˆ†ç±»å¯¹æ¯”å›¾è¡¨"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    datasets = list(all_results.keys())
    models = None
    
    # è·å–æ‰€æœ‰æ¨¡å‹åç§°
    for dataset_name, (results, times) in all_results.items():
        if models is None:
            models = [name for name, metrics in results.items() if metrics is not None]
            break
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœæ•°æ®")
        return
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºä¸€ä¸ªå­å›¾
    for idx, dataset_name in enumerate(datasets[:4]):  # æœ€å¤š4ä¸ªæ•°æ®é›†
        if idx >= len(axes):
            break
            
        results, times = all_results[dataset_name]
        
        model_names = []
        accuracies = []
        f1_scores = []
        
        for model_name in models:
            if model_name in results and results[model_name] is not None:
                model_names.append(model_name)
                accuracies.append(results[model_name]['accuracy'])
                f1_scores.append(results[model_name]['f1_macro'])
        
        if not model_names:
            continue
        
        x = np.arange(len(model_names))
        width = 0.35
        
        # ç»˜åˆ¶å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
        bars1 = axes[idx].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8, color='skyblue')
        bars2 = axes[idx].bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.8, color='lightcoral')
        
        axes[idx].set_xlabel('æ¨¡å‹')
        axes[idx].set_ylabel('åˆ†æ•°')
        axes[idx].set_title(f'{dataset_name} - åˆ†ç±»æ€§èƒ½å¯¹æ¯”')
        axes[idx].set_xticks(x)
        axes[idx].set_xticklabels(model_names, rotation=45, ha='right')
        axes[idx].legend()
        axes[idx].grid(True, alpha=0.3)
        axes[idx].set_ylim(0, 1.1)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            axes[idx].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(datasets), len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/classification_benchmark.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   ğŸ“Š åˆ†ç±»å¯¹æ¯”å›¾è¡¨ä¿å­˜åˆ°: {save_dir}/classification_benchmark.png")

def analyze_causal_engine_performance(all_results):
    """åˆ†æ CausalEngine æ€§èƒ½è¡¨ç°"""
    
    print("\\nğŸ” CausalEngine æ€§èƒ½åˆ†æ:")
    print("=" * 50)
    
    wins = 0
    total = 0
    performance_details = []
    
    for dataset_name, (results, times) in all_results.items():
        if 'CausalEngine' not in results or results['CausalEngine'] is None:
            continue
        
        total += 1
        causal_metrics = results['CausalEngine']
        causal_time = times['CausalEngine']
        
        # æ‰¾åˆ°æœ€ä½³åŸºå‡†æ–¹æ³•
        baseline_f1_scores = {
            model: metrics['f1_macro'] for model, metrics in results.items() 
            if model != 'CausalEngine' and metrics is not None
        }
        
        if not baseline_f1_scores:
            continue
        
        best_baseline = max(baseline_f1_scores.items(), key=lambda x: x[1])
        causal_f1 = causal_metrics['f1_macro']
        
        improvement = (causal_f1 - best_baseline[1]) / best_baseline[1] * 100 if best_baseline[1] > 0 else 0
        
        if causal_f1 >= best_baseline[1]:
            wins += 1
            status = "ğŸ† èƒœå‡º"
        else:
            status = "ğŸ“Š è½å"
        
        performance_details.append({
            'dataset': dataset_name,
            'causal_f1': causal_f1,
            'best_baseline': best_baseline[0],
            'best_f1': best_baseline[1],
            'improvement': improvement,
            'status': status,
            'time': causal_time
        })
        
        print(f"\\nğŸ“ˆ {dataset_name}:")
        print(f"   CausalEngine F1: {causal_f1:.4f}")
        print(f"   æœ€ä½³åŸºå‡† ({best_baseline[0]}): {best_baseline[1]:.4f}")
        print(f"   æ€§èƒ½å·®å¼‚: {improvement:+.1f}%")
        print(f"   è®­ç»ƒæ—¶é—´: {causal_time:.1f}s")
        print(f"   ç»“æœ: {status}")
    
    # æ€»ä½“ç»Ÿè®¡
    if total > 0:
        win_rate = wins / total * 100
        avg_improvement = np.mean([d['improvement'] for d in performance_details])
        avg_time = np.mean([d['time'] for d in performance_details])
        
        print("\\nğŸ¯ æ€»ä½“è¡¨ç°:")
        print(f"   èƒœç‡: {wins}/{total} ({win_rate:.1f}%)")
        print(f"   å¹³å‡æ€§èƒ½æå‡: {avg_improvement:+.1f}%")
        print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {avg_time:.1f}s")
        
        # æ€§èƒ½ç­‰çº§
        if win_rate >= 75:
            grade = "ğŸŒŸ ä¼˜ç§€"
        elif win_rate >= 50:
            grade = "ğŸ‘ è‰¯å¥½"
        elif win_rate >= 25:
            grade = "ğŸ“Š ä¸€èˆ¬"
        else:
            grade = "âš ï¸ éœ€è¦æ”¹è¿›"
        
        print(f"   æ€§èƒ½ç­‰çº§: {grade}")

def save_results_to_csv(all_results, save_path="user_tutorials/results/classification_benchmark.csv"):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    
    rows = []
    
    for dataset_name, (results, times) in all_results.items():
        for model_name, metrics in results.items():
            if metrics is not None:
                row = {
                    'dataset': dataset_name,
                    'model': model_name,
                    'training_time': times.get(model_name, None),
                    **metrics
                }
                rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # ä¿å­˜CSV
    df.to_csv(save_path, index=False)
    print(f"   ğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {save_path}")
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ”¬ çœŸå®ä¸–ç•Œåˆ†ç±»ä»»åŠ¡åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print("æµ‹è¯• CausalEngine åœ¨4ä¸ªåˆ†ç±»æ•°æ®é›†ä¸Šçš„æ€§èƒ½")
    
    # æ•°æ®é›†åŠ è½½å™¨
    dataset_loaders = [
        load_adult_dataset,
        load_bank_marketing_dataset, 
        load_credit_default_dataset,
        load_mushroom_dataset
    ]
    
    # åŠ è½½æ‰€æœ‰æ•°æ®é›†
    datasets = {}
    
    for loader in dataset_loaders:
        X, y, name, description = loader()
        if X is not None:
            datasets[name] = (X, y, description)
    
    if not datasets:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼")
        return
    
    print(f"\\nğŸ“Š æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªæ•°æ®é›†")
    
    # è¿›è¡ŒåŸºå‡†æµ‹è¯•
    all_results = {}
    
    for dataset_name, (X, y, description) in datasets.items():
        print(f"\\n{'='*20} {dataset_name} {'='*20}")
        
        try:
            results, times = benchmark_dataset(X, y, dataset_name, description)
            all_results[dataset_name] = (results, times)
            
            # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ç»“æœ
            print_results_table(results, times, dataset_name)
            
        except Exception as e:
            print(f"âŒ {dataset_name} åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    if not all_results:
        print("âŒ æ‰€æœ‰åŸºå‡†æµ‹è¯•éƒ½å¤±è´¥äº†ï¼")
        return
    
    # æ€§èƒ½åˆ†æ
    analyze_causal_engine_performance(all_results)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print("\\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    create_comparison_plots(all_results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    df = save_results_to_csv(all_results)
    
    print("\\nğŸ‰ åˆ†ç±»åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("\\nğŸ“‹ å®éªŒæ€»ç»“:")
    print(f"   - æµ‹è¯•äº† {len(all_results)} ä¸ªçœŸå®åˆ†ç±»æ•°æ®é›†")
    print("   - å¯¹æ¯”äº† CausalEngine ä¸ 5 ç§åŸºå‡†æ–¹æ³•")
    print("   - è¯„ä¼°äº†å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æŒ‡æ ‡")
    print("   - ç»“æœå’Œå›¾è¡¨ä¿å­˜åœ¨ user_tutorials/results/ ç›®å½•")
    print("\\nğŸ’¡ å»ºè®®:")
    print("   - æŸ¥çœ‹ç”Ÿæˆçš„ PNG å›¾è¡¨äº†è§£ç›´è§‚å¯¹æ¯”")
    print("   - æŸ¥çœ‹ CSV æ–‡ä»¶è·å–è¯¦ç»†æ•°æ®")
    print("   - å°è¯•è¿è¡Œå›å½’åŸºå‡†æµ‹è¯•: python 04_real_world_examples/regression_benchmark.py")

if __name__ == "__main__":
    # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    main()
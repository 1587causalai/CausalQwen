"""
çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡åŸºå‡†æµ‹è¯•
=======================

æµ‹è¯• CausalEngine åœ¨4ä¸ªçœŸå®å›å½’æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼š
1. Bike Sharing - å…±äº«å•è½¦éœ€æ±‚é¢„æµ‹
2. Wine Quality - è‘¡è„é…’è´¨é‡é¢„æµ‹  
3. Ames Housing - æˆ¿ä»·é¢„æµ‹
4. California Housing - åŠ å·æˆ¿ä»·é¢„æµ‹

è¯„ä¼°æŒ‡æ ‡ï¼šMAE, RMSE, MdAE, MSE, RÂ²
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import fetch_california_housing, make_regression
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

from utils.simple_models import SimpleCausalRegressor

def load_bike_sharing_dataset():
    """åŠ è½½ Bike Sharing æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Bike Sharing æ•°æ®é›†...")
    
    try:
        data_dir = os.path.join(os.path.dirname(parent_dir), 'data')
        file_path = os.path.join(data_dir, 'hour.csv')
        
        if os.path.exists(file_path):
            # è¯»å–çœŸå®æ•°æ®
            data = pd.read_csv(file_path)
            
            # é€‰æ‹©ç‰¹å¾
            feature_cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',
                           'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']
            
            X = data[feature_cols].values
            y = data['cnt'].values  # æ€»ç§Ÿèµæ•°é‡
            
            print(f"   âœ… åŠ è½½æˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
            return X, y, "Bike Sharing", "é¢„æµ‹æ¯å°æ—¶å…±äº«å•è½¦ç§Ÿèµéœ€æ±‚"
        
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            raise FileNotFoundError("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•åŠ è½½çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„å…±äº«å•è½¦æ•°æ®
        X, y = make_regression(
            n_samples=10000,
            n_features=12,
            n_informative=10,
            noise=0.1,
            random_state=42
        )
        
        # ç¡®ä¿ç›®æ ‡å˜é‡ä¸ºæ­£å€¼ï¼ˆè¡¨ç¤ºç§Ÿèµæ•°é‡ï¼‰
        y = np.abs(y) + 50
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        return X, y, "Bike Sharing (Simulated)", "é¢„æµ‹æ¯å°æ—¶å…±äº«å•è½¦ç§Ÿèµéœ€æ±‚"

def load_wine_quality_dataset():
    """åŠ è½½ Wine Quality æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Wine Quality æ•°æ®é›†...")
    
    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„è‘¡è„é…’è´¨é‡æ•°æ®
        from sklearn.datasets import make_regression
        
        X, y = make_regression(
            n_samples=4800,
            n_features=11,
            n_informative=9,
            noise=0.2,
            random_state=42
        )
        
        # å°†ç›®æ ‡å˜é‡ç¼©æ”¾åˆ°3-9çš„è´¨é‡è¯„åˆ†èŒƒå›´
        y = 3 + (y - y.min()) / (y.max() - y.min()) * 6
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        return X, y, "Wine Quality", "åŸºäºç†åŒ–å±æ€§é¢„æµ‹è‘¡è„é…’è´¨é‡è¯„åˆ†"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def load_ames_housing_dataset():
    """åŠ è½½ Ames Housing æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ Ames Housing æ•°æ®é›†...")
    
    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æˆ¿ä»·æ•°æ®
        from sklearn.datasets import make_regression
        
        X, y = make_regression(
            n_samples=2900,
            n_features=20,
            n_informative=16,
            noise=0.15,
            random_state=42
        )
        
        # å°†ç›®æ ‡å˜é‡è½¬æ¢ä¸ºåˆç†çš„æˆ¿ä»·èŒƒå›´ï¼ˆç¾å…ƒï¼‰
        y = 50000 + np.abs(y) * 5000
        
        print(f"   âœ… ç”ŸæˆæˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        return X, y, "Ames Housing", "åŸºäºæˆ¿å±‹ç‰¹å¾é¢„æµ‹é”€å”®ä»·æ ¼"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def load_california_housing_dataset():
    """åŠ è½½ California Housing æ•°æ®é›†"""
    print("ğŸ“‚ åŠ è½½ California Housing æ•°æ®é›†...")
    
    try:
        # ä½¿ç”¨ scikit-learn å†…ç½®çš„åŠ å·æˆ¿ä»·æ•°æ®é›†
        housing = fetch_california_housing()
        X = housing.data
        y = housing.target
        
        print(f"   âœ… åŠ è½½æˆåŠŸ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        return X, y, "California Housing", "åŸºäºäººå£ç»Ÿè®¡ä¿¡æ¯é¢„æµ‹åŠ å·æˆ¿ä»·ä¸­ä½æ•°"
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def get_baseline_models():
    """è·å–åŸºå‡†å›å½’æ¨¡å‹"""
    return {
        'Random Forest': RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=50, random_state=42),
        'SVR': SVR(kernel='rbf', C=1.0, max_iter=1000),
        'Linear Regression': LinearRegression(),
        'Neural Network': MLPRegressor(hidden_layer_sizes=(50, 25), random_state=42, max_iter=100, early_stopping=True)
    }

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—å›å½’æŒ‡æ ‡"""
    return {
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mdae': median_absolute_error(y_true, y_pred),
        'mse': mean_squared_error(y_true, y_pred),
        'r2': r2_score(y_true, y_pred)
    }

def benchmark_dataset(X, y, dataset_name, description):
    """å¯¹å•ä¸ªæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    
    print(f"\nğŸ”¬ åŸºå‡†æµ‹è¯•: {dataset_name}")
    print(f"   æè¿°: {description}")
    print(f"   æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"   ç›®æ ‡å˜é‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"   ç›®æ ‡å˜é‡å‡å€¼: {y.mean():.2f}")
    print(f"   ç›®æ ‡å˜é‡æ ‡å‡†å·®: {y.std():.2f}")
    
    # å¦‚æœæ•°æ®é›†å¤ªå¤§ï¼Œè¿›è¡Œé‡‡æ ·ä»¥åŠ é€Ÿè®­ç»ƒ
    if X.shape[0] > 5000:
        print(f"   ğŸ“Š æ•°æ®é›†è¾ƒå¤§ï¼Œé‡‡æ ·åˆ° 5000 æ ·æœ¬ä»¥åŠ é€Ÿè®­ç»ƒ")
        from sklearn.utils import resample
        X_sampled, y_sampled = resample(X, y, n_samples=5000, random_state=42, stratify=None)
        X, y = X_sampled, y_sampled
        print(f"   é‡‡æ ·åæ•°æ®å½¢çŠ¶: {X.shape}")
    
    # æ•°æ®åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    results = {}
    training_times = {}
    
    # 1. è®­ç»ƒ CausalEngine
    print("   ğŸš€ è®­ç»ƒ CausalEngine...")
    
    try:
        import time
        start_time = time.time()
        
        causal_model = SimpleCausalRegressor(random_state=42)
        causal_model.fit(X_train_scaled, y_train, epochs=30, verbose=False)
        causal_pred = causal_model.predict(X_test_scaled)
        causal_metrics = calculate_metrics(y_test, causal_pred)
        
        training_times['CausalEngine'] = time.time() - start_time
        results['CausalEngine'] = causal_metrics
        
        print(f"      âœ… å®Œæˆ ({training_times['CausalEngine']:.1f}s)")
    
    except Exception as e:
        print(f"      âŒ å¤±è´¥: {e}")
        results['CausalEngine'] = None
        training_times['CausalEngine'] = None
    
    # 2. è®­ç»ƒåŸºå‡†æ¨¡å‹
    baseline_models = get_baseline_models()
    
    for model_name, model in baseline_models.items():
        print(f"   ğŸ”§ è®­ç»ƒ {model_name}...")
        
        try:
            start_time = time.time()
            
            # è®¾ç½®è®­ç»ƒè¶…æ—¶ (20ç§’)
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("è®­ç»ƒè¶…æ—¶")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(20)  # 20ç§’è¶…æ—¶
            
            try:
                model.fit(X_train_scaled, y_train)
                pred = model.predict(X_test_scaled)
                metrics = calculate_metrics(y_test, pred)
                train_time = time.time() - start_time
                
                results[model_name] = metrics
                training_times[model_name] = train_time
                
                print(f"      âœ… å®Œæˆ ({train_time:.1f}s)")
                
            except TimeoutError:
                train_time = time.time() - start_time
                print(f"      â° è¶…æ—¶ ({train_time:.1f}s) - è·³è¿‡")
                results[model_name] = None
                training_times[model_name] = train_time
            finally:
                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        
        except Exception as e:
            print(f"      âŒ å¤±è´¥: {e}")
            results[model_name] = None
            training_times[model_name] = None
    
    return results, training_times

def print_results_table(results, training_times, dataset_name):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    
    print(f"\nğŸ“Š {dataset_name} - å›å½’ç»“æœ:")
    print("   æ¨¡å‹                  | MAE      | RMSE     | MdAE     | RÂ²       | è®­ç»ƒæ—¶é—´")
    print("   -------------------- | -------- | -------- | -------- | -------- | --------")
    
    for model_name, metrics in results.items():
        if metrics is not None:
            mae = metrics['mae']
            rmse = metrics['rmse']
            mdae = metrics['mdae']
            r2 = metrics['r2']
            time_str = f"{training_times[model_name]:.1f}s" if training_times[model_name] else "N/A"
            
            # é«˜äº®æœ€ä½³ç»“æœ (æœ€é«˜ RÂ²)
            if r2 == max(m['r2'] for m in results.values() if m is not None):
                marker = "ğŸ†"
            else:
                marker = "  "
            
            print(f"{marker} {model_name:19} | {mae:8.3f} | {rmse:8.3f} | {mdae:8.3f} | {r2:8.4f} | {time_str}")
        else:
            print(f"   {model_name:20} | è®­ç»ƒå¤±è´¥")

def create_comparison_plots(all_results, save_dir="user_tutorials/results"):
    """åˆ›å»ºå›å½’å¯¹æ¯”å›¾è¡¨"""
    
    os.makedirs(save_dir, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    datasets = list(all_results.keys())
    models = None
    
    # è·å–æ‰€æœ‰æ¨¡å‹åç§°
    for dataset_name, (results, times) in all_results.items():
        if models is None:
            models = [name for name, metrics in results.items() if metrics is not None]
            break
    
    if not models:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœæ•°æ®")
        return
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºä¸€ä¸ªå­å›¾
    for idx, dataset_name in enumerate(datasets[:4]):  # æœ€å¤š4ä¸ªæ•°æ®é›†
        if idx >= len(axes):
            break
            
        results, times = all_results[dataset_name]
        
        model_names = []
        r2_scores = []
        mae_scores = []
        
        for model_name in models:
            if model_name in results and results[model_name] is not None:
                model_names.append(model_name)
                r2_scores.append(results[model_name]['r2'])
                mae_scores.append(results[model_name]['mae'])
        
        if not model_names:
            continue
        
        x = np.arange(len(model_names))
        width = 0.35
        
        # ç»˜åˆ¶RÂ²å’ŒMAE (åå‘ï¼ŒMAEè¶Šå°è¶Šå¥½)
        ax1 = axes[idx]
        bars1 = ax1.bar(x - width/2, r2_scores, width, label='RÂ² Score', alpha=0.8, color='skyblue')
        
        # åˆ›å»ºç¬¬äºŒä¸ªyè½´ç”¨äºMAE
        ax2 = ax1.twinx()
        bars2 = ax2.bar(x + width/2, mae_scores, width, label='MAE (lower is better)', alpha=0.8, color='lightcoral')
        
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('RÂ² Score', color='blue')
        ax2.set_ylabel('MAE', color='red')
        ax1.set_title(f'{dataset_name} - å›å½’æ€§èƒ½å¯¹æ¯”')
        ax1.set_xticks(x)
        ax1.set_xticklabels(model_names, rotation=45, ha='right')
        
        # è®¾ç½®yè½´èŒƒå›´
        ax1.set_ylim(0, 1)
        ax2.set_ylim(0, max(mae_scores) * 1.2)
        
        ax1.grid(True, alpha=0.3)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=8, color='blue')
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(len(datasets), len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/regression_benchmark.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"   ğŸ“Š å›å½’å¯¹æ¯”å›¾è¡¨ä¿å­˜åˆ°: {save_dir}/regression_benchmark.png")

def analyze_causal_engine_performance(all_results):
    """åˆ†æ CausalEngine æ€§èƒ½è¡¨ç°"""
    
    print("\nğŸ” CausalEngine æ€§èƒ½åˆ†æ:")
    print("=" * 50)
    
    wins = 0
    total = 0
    performance_details = []
    
    for dataset_name, (results, times) in all_results.items():
        if 'CausalEngine' not in results or results['CausalEngine'] is None:
            continue
        
        total += 1
        causal_metrics = results['CausalEngine']
        causal_time = times['CausalEngine']
        
        # æ‰¾åˆ°æœ€ä½³åŸºå‡†æ–¹æ³•
        baseline_r2_scores = {
            model: metrics['r2'] for model, metrics in results.items() 
            if model != 'CausalEngine' and metrics is not None
        }
        
        if not baseline_r2_scores:
            continue
        
        best_baseline = max(baseline_r2_scores.items(), key=lambda x: x[1])
        causal_r2 = causal_metrics['r2']
        
        improvement = (causal_r2 - best_baseline[1]) / abs(best_baseline[1]) * 100 if best_baseline[1] != 0 else 0
        
        if causal_r2 >= best_baseline[1]:
            wins += 1
            status = "ğŸ† èƒœå‡º"
        else:
            status = "ğŸ“Š è½å"
        
        performance_details.append({
            'dataset': dataset_name,
            'causal_r2': causal_r2,
            'causal_mae': causal_metrics['mae'],
            'best_baseline': best_baseline[0],
            'best_r2': best_baseline[1],
            'improvement': improvement,
            'status': status,
            'time': causal_time
        })
        
        print(f"\nğŸ“ˆ {dataset_name}:")
        print(f"   CausalEngine RÂ²: {causal_r2:.4f}")
        print(f"   CausalEngine MAE: {causal_metrics['mae']:.4f}")
        print(f"   æœ€ä½³åŸºå‡† ({best_baseline[0]}): {best_baseline[1]:.4f}")
        print(f"   æ€§èƒ½å·®å¼‚: {improvement:+.1f}%")
        print(f"   è®­ç»ƒæ—¶é—´: {causal_time:.1f}s")
        print(f"   ç»“æœ: {status}")
    
    # æ€»ä½“ç»Ÿè®¡
    if total > 0:
        win_rate = wins / total * 100
        avg_improvement = np.mean([d['improvement'] for d in performance_details])
        avg_time = np.mean([d['time'] for d in performance_details])
        
        print("\nğŸ¯ æ€»ä½“è¡¨ç°:")
        print(f"   èƒœç‡: {wins}/{total} ({win_rate:.1f}%)")
        print(f"   å¹³å‡æ€§èƒ½æå‡: {avg_improvement:+.1f}%")
        print(f"   å¹³å‡è®­ç»ƒæ—¶é—´: {avg_time:.1f}s")
        
        # æ€§èƒ½ç­‰çº§
        if win_rate >= 75:
            grade = "ğŸŒŸ ä¼˜ç§€"
        elif win_rate >= 50:
            grade = "ğŸ‘ è‰¯å¥½"
        elif win_rate >= 25:
            grade = "ğŸ“Š ä¸€èˆ¬"
        else:
            grade = "âš ï¸ éœ€è¦æ”¹è¿›"
        
        print(f"   æ€§èƒ½ç­‰çº§: {grade}")

def save_results_to_csv(all_results, save_path="user_tutorials/results/regression_benchmark.csv"):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    
    rows = []
    
    for dataset_name, (results, times) in all_results.items():
        for model_name, metrics in results.items():
            if metrics is not None:
                row = {
                    'dataset': dataset_name,
                    'model': model_name,
                    'training_time': times.get(model_name, None),
                    **metrics
                }
                rows.append(row)
    
    df = pd.DataFrame(rows)
    
    # åˆ›å»ºç›®å½•
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # ä¿å­˜CSV
    df.to_csv(save_path, index=False)
    print(f"   ğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜åˆ°: {save_path}")
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ”¬ çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print("æµ‹è¯• CausalEngine åœ¨4ä¸ªå›å½’æ•°æ®é›†ä¸Šçš„æ€§èƒ½")
    
    # æ•°æ®é›†åŠ è½½å™¨
    dataset_loaders = [
        load_bike_sharing_dataset,
        load_wine_quality_dataset, 
        load_ames_housing_dataset,
        load_california_housing_dataset
    ]
    
    # åŠ è½½æ‰€æœ‰æ•°æ®é›†
    datasets = {}
    
    for loader in dataset_loaders:
        X, y, name, description = loader()
        if X is not None:
            datasets[name] = (X, y, description)
    
    if not datasets:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼")
        return
    
    print(f"\nğŸ“Š æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªæ•°æ®é›†")
    
    # è¿›è¡ŒåŸºå‡†æµ‹è¯•
    all_results = {}
    
    for dataset_name, (X, y, description) in datasets.items():
        print(f"\n{'='*20} {dataset_name} {'='*20}")
        
        try:
            results, times = benchmark_dataset(X, y, dataset_name, description)
            all_results[dataset_name] = (results, times)
            
            # æ˜¾ç¤ºå½“å‰æ•°æ®é›†ç»“æœ
            print_results_table(results, times, dataset_name)
            
        except Exception as e:
            print(f"âŒ {dataset_name} åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    if not all_results:
        print("âŒ æ‰€æœ‰åŸºå‡†æµ‹è¯•éƒ½å¤±è´¥äº†ï¼")
        return
    
    # æ€§èƒ½åˆ†æ
    analyze_causal_engine_performance(all_results)
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    create_comparison_plots(all_results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    df = save_results_to_csv(all_results)
    
    print("\nğŸ‰ å›å½’åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ å®éªŒæ€»ç»“:")
    print(f"   - æµ‹è¯•äº† {len(all_results)} ä¸ªçœŸå®å›å½’æ•°æ®é›†")
    print("   - å¯¹æ¯”äº† CausalEngine ä¸ 5 ç§åŸºå‡†æ–¹æ³•")
    print("   - è¯„ä¼°äº† MAEã€RMSEã€MdAEã€MSEã€RÂ² ç­‰æŒ‡æ ‡")
    print("   - ç»“æœå’Œå›¾è¡¨ä¿å­˜åœ¨ user_tutorials/results/ ç›®å½•")
    print("\nğŸ’¡ å»ºè®®:")
    print("   - æŸ¥çœ‹ç”Ÿæˆçš„ PNG å›¾è¡¨äº†è§£ç›´è§‚å¯¹æ¯”")
    print("   - æŸ¥çœ‹ CSV æ–‡ä»¶è·å–è¯¦ç»†æ•°æ®")
    print("   - å°è¯•è¿è¡Œåˆ†ç±»åŸºå‡†æµ‹è¯•: python 04_real_world_examples/classification_benchmark.py")

if __name__ == "__main__":
    # è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    main()
#!/usr/bin/env python3
"""
CausalEngineçœŸå®æ•°æ®é›†åŸºå‡†æµ‹è¯•è„šæœ¬ - å¯é…ç½®å‚æ•°ç‰ˆæœ¬

åŸºäºquick_test_causal_engine.pyçš„ä¼˜ç§€æ¶æ„ï¼Œä¸“é—¨é’ˆå¯¹çœŸå®æ•°æ®é›†è¿›è¡Œæµ‹è¯•
è¦æ±‚ï¼šç‰¹å¾æ•°>10ï¼Œæ ·æœ¬æ•°>1000

ä¸»è¦ç‰¹æ€§ï¼š
- å¯æ‰‹åŠ¨é…ç½®æ‰€æœ‰å…³é”®å‚æ•°
- æ”¯æŒäº”æ¨¡å¼æµ‹è¯•ï¼šdeterministic, exogenous, endogenous, standard, sampling
- å®Œæ•´çš„å‚æ•°æš´éœ²ï¼šgamma_init, b_noise_init, ovr_threshold_initç­‰
- çµæ´»çš„æ•°æ®é›†é€‰æ‹©å’Œç½‘ç»œæ¶æ„é…ç½®
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_california_housing, load_breast_cancer, load_diabetes, fetch_openml
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier

class RealWorldBenchmark:
    """
    çœŸå®æ•°æ®é›†åŸºå‡†æµ‹è¯•å™¨ - å¯é…ç½®å‚æ•°ç‰ˆæœ¬
    
    åŸºäºQuickTesterå’Œdemo_sklearn_interface_v2çš„æˆåŠŸæ¶æ„ï¼Œä¸“é—¨ç”¨äºçœŸå®æ•°æ®é›†æµ‹è¯•
    å…è®¸æ‰‹åŠ¨é…ç½®æ‰€æœ‰å…³é”®å‚æ•°è¿›è¡Œç²¾ç»†è°ƒèŠ‚
    """
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯è°ƒèŠ‚å‚æ•°
        """
        self.results = {}
        self.scaler = StandardScaler()
        
        # é»˜è®¤é…ç½® - å¯é€šè¿‡configå‚æ•°è¦†ç›–
        self.default_config = {
            # === ç½‘ç»œæ¶æ„ ===
            'hidden_layer_sizes': (128, 64),
            
            # === è®­ç»ƒå‚æ•° ===
            'max_iter': 2000,
            'learning_rate': 0.001,
            'early_stopping': True,
            'validation_fraction': 0.15,
            'n_iter_no_change': 50,
            'tol': 1e-4,
            'random_state': 42,
            'alpha': 0.0001,  # sklearnæ­£åˆ™åŒ–
            
            # === CausalEngineä¸“æœ‰å‚æ•° ===
            'gamma_init': 10.0,              # AbductionNetworkå°ºåº¦åˆå§‹åŒ–
            'b_noise_init': 0.1,             # ActionNetworkå¤–ç”Ÿå™ªå£°åˆå§‹åŒ–
            'b_noise_trainable': True,       # å¤–ç”Ÿå™ªå£°æ˜¯å¦å¯è®­ç»ƒ
            'ovr_threshold_init': 0.5,       # OvRåˆ†ç±»é˜ˆå€¼åˆå§‹åŒ–
            
            # === æµ‹è¯•æ¨¡å¼ ===
            'test_modes': ['deterministic', 'standard'],  # å¯æ‰©å±•ä¸ºäº”æ¨¡å¼
            # 'test_modes': ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling'],
            
            # === æ•°æ®é›†é…ç½® ===
            'test_size': 0.2,
            'stratify_classification': True,
            
            # === æ ‡ç­¾å¼‚å¸¸é…ç½® ===
            'regression_anomaly_ratio': 0.0,  # å›å½’æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (0.0-0.5)
            'classification_anomaly_ratio': 0.0,  # åˆ†ç±»æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (0.0-0.5)
            
            # === è¾“å‡ºæ§åˆ¶ ===
            'verbose': True,
            'show_distribution_examples': True,
            'n_distribution_samples': 3,
        }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®
        self.config = self.default_config.copy()
        if config:
            self.config.update(config)
    
    def add_label_anomalies(self, y, anomaly_ratio=0.1, anomaly_type='regression'):
        """
        ç»™æ ‡ç­¾æ·»åŠ å¼‚å¸¸ - æ›´å®ç”¨çš„å¼‚å¸¸æ¨¡æ‹Ÿ
        å‚è€ƒè‡ªquick_test_causal_engine.pyçš„ä¼˜ç§€å®ç°
        
        Args:
            y: åŸå§‹æ ‡ç­¾
            anomaly_ratio: å¼‚å¸¸æ¯”ä¾‹ (0.0-1.0)
            anomaly_type: 'regression'(å›å½’å¼‚å¸¸) æˆ– 'classification'(åˆ†ç±»ç¿»è½¬)
        
        Returns:
            y_noisy: æ·»åŠ å¼‚å¸¸åçš„æ ‡ç­¾
        """
        y_noisy = y.copy()
        n_anomalies = int(len(y) * anomaly_ratio)
        
        if n_anomalies == 0:
            return y_noisy
            
        anomaly_indices = np.random.choice(len(y), n_anomalies, replace=False)
        
        if anomaly_type == 'regression':
            # å›å½’å¼‚å¸¸ï¼šç®€å•è€Œå¼ºçƒˆçš„å¼‚å¸¸
            y_std = np.std(y)
            
            for idx in anomaly_indices:
                # éšæœºé€‰æ‹©å¼‚å¸¸ç±»å‹
                if np.random.random() < 0.5:
                    # ç­–ç•¥1: 3å€æ ‡å‡†å·®åç§»
                    sign = np.random.choice([-1, 1])
                    y_noisy[idx] = y[idx] + sign * 3.0 * y_std
                else:
                    # ç­–ç•¥2: 10å€ç¼©æ”¾
                    scale_factor = np.random.choice([0.1, 10.0])  # æç«¯ç¼©æ”¾
                    y_noisy[idx] = y[idx] * scale_factor
                
        elif anomaly_type == 'classification':
            # åˆ†ç±»å¼‚å¸¸ï¼šæ ‡ç­¾ç¿»è½¬
            unique_labels = np.unique(y)
            for idx in anomaly_indices:
                other_labels = unique_labels[unique_labels != y[idx]]
                if len(other_labels) > 0:
                    y_noisy[idx] = np.random.choice(other_labels)
        
        return y_noisy
    
    def print_config(self):
        """æ‰“å°å½“å‰é…ç½®"""
        print("ğŸ”§ å½“å‰åŸºå‡†æµ‹è¯•é…ç½®:")
        print("=" * 60)
        for category in ['ç½‘ç»œæ¶æ„', 'è®­ç»ƒå‚æ•°', 'CausalEngineä¸“æœ‰å‚æ•°', 'æµ‹è¯•æ¨¡å¼', 'æ•°æ®é›†é…ç½®', 'æ ‡ç­¾å¼‚å¸¸é…ç½®']:
            if category == 'ç½‘ç»œæ¶æ„':
                print(f"\nğŸ“ {category}:")
                print(f"  hidden_layer_sizes: {self.config['hidden_layer_sizes']}")
            elif category == 'è®­ç»ƒå‚æ•°':
                print(f"\nâš™ï¸ {category}:")
                for key in ['max_iter', 'learning_rate', 'early_stopping', 'validation_fraction', 
                           'n_iter_no_change', 'tol', 'random_state', 'alpha']:
                    print(f"  {key}: {self.config[key]}")
            elif category == 'CausalEngineä¸“æœ‰å‚æ•°':
                print(f"\nğŸ§  {category}:")
                for key in ['gamma_init', 'b_noise_init', 'b_noise_trainable', 'ovr_threshold_init']:
                    print(f"  {key}: {self.config[key]}")
            elif category == 'æµ‹è¯•æ¨¡å¼':
                print(f"\nğŸš€ {category}:")
                print(f"  test_modes: {self.config['test_modes']}")
            elif category == 'æ•°æ®é›†é…ç½®':
                print(f"\nğŸ“Š {category}:")
                for key in ['test_size', 'stratify_classification']:
                    print(f"  {key}: {self.config[key]}")
            elif category == 'æ ‡ç­¾å¼‚å¸¸é…ç½®':
                print(f"\nâš ï¸ {category}:")
                print(f"  regression_anomaly_ratio: {self.config['regression_anomaly_ratio']:.1%}")
                print(f"  classification_anomaly_ratio: {self.config['classification_anomaly_ratio']:.1%}")
        print("=" * 60)
        
    def load_california_housing(self):
        """
        åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›† (å›å½’ä»»åŠ¡)
        - æ ·æœ¬æ•°: 20,640
        - ç‰¹å¾æ•°: 8 (éœ€è¦ç‰¹å¾å·¥ç¨‹å¢åŠ åˆ°>10)
        - ä»»åŠ¡: é¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°
        """
        print("ğŸ“Š åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        housing = fetch_california_housing()
        X, y = housing.data, housing.target
        
        # ç‰¹å¾å·¥ç¨‹ï¼šå¢åŠ ç‰¹å¾æ•°é‡åˆ°>10
        X_engineered = X.copy()
        
        # æ·»åŠ äº¤äº’ç‰¹å¾
        X_engineered = np.column_stack([
            X_engineered,
            X[:, 0] * X[:, 1],  # MedInc * HouseAge
            X[:, 2] * X[:, 3],  # AveRooms * AveBedrms  
            X[:, 4] * X[:, 5],  # Population * AveOccup
            X[:, 6] * X[:, 7],  # Latitude * Longitude
            X[:, 0] ** 2,       # MedInc^2
            X[:, 4] / (X[:, 5] + 1e-8),  # Population/AveOccup
        ])
        
        print(f"   åŸå§‹ç‰¹å¾: {X.shape[1]} â†’ å·¥ç¨‹ç‰¹å¾: {X_engineered.shape[1]}")
        print(f"   æ ·æœ¬æ•°: {X_engineered.shape[0]}")
        print(f"   ç›®æ ‡å€¼èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
        
        return X_engineered, y, housing.feature_names + [
            'MedInc*HouseAge', 'AveRooms*AveBedrms', 'Population*AveOccup', 
            'Latitude*Longitude', 'MedInc^2', 'PopDensity'
        ]
    
    def load_diabetes_dataset(self):
        """
        åŠ è½½ç³–å°¿ç—…æ•°æ®é›† (å›å½’ä»»åŠ¡)
        - æ ·æœ¬æ•°: 442
        - ç‰¹å¾æ•°: 10 (éœ€è¦ç‰¹å¾å·¥ç¨‹å¢åŠ åˆ°>10)
        - ä»»åŠ¡: é¢„æµ‹ç³–å°¿ç—…è¿›å±•æŒ‡æ ‡
        - ç‰¹ç‚¹: sklearnç»å…¸å›å½’æ•°æ®é›†ï¼ŒåŒ»å­¦åº”ç”¨
        """
        print("ğŸ¥ åŠ è½½ç³–å°¿ç—…æ•°æ®é›†...")
        diabetes = load_diabetes()
        X, y = diabetes.data, diabetes.target
        
        # ç‰¹å¾å·¥ç¨‹ï¼šæ·»åŠ äº¤äº’ç‰¹å¾å’Œéçº¿æ€§ç‰¹å¾
        X_engineered = X.copy()
        
        # æ·»åŠ å¹³æ–¹é¡¹
        X_squared = X ** 2
        
        # æ·»åŠ äº¤äº’é¡¹ï¼ˆé€‰æ‹©å‡ ä¸ªé‡è¦çš„ï¼‰
        interactions = np.column_stack([
            X[:, 0] * X[:, 1],  # age * sex
            X[:, 2] * X[:, 3],  # bmi * bp
            X[:, 4] * X[:, 5],  # s1 * s2
            X[:, 6] * X[:, 7],  # s3 * s4
            X[:, 8] * X[:, 9],  # s5 * s6
        ])
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X_engineered = np.column_stack([X_engineered, X_squared, interactions])
        
        # æ‰©å±•æ•°æ®é›†ï¼ˆé€šè¿‡æ·»åŠ å™ªå£°åˆ›å»ºæ›´å¤šæ ·æœ¬ï¼‰
        np.random.seed(42)
        n_synthetic = 1000
        X_synthetic_list = []
        y_synthetic_list = []
        
        for _ in range(n_synthetic):
            # éšæœºé€‰æ‹©ä¸€ä¸ªçœŸå®æ ·æœ¬ä½œä¸ºåŸºç¡€
            base_idx = np.random.choice(len(X_engineered))
            base_sample = X_engineered[base_idx]
            base_target = y[base_idx]
            
            # æ·»åŠ é€‚é‡å™ªå£°
            noise_x = np.random.normal(0, np.std(X_engineered, axis=0) * 0.1)
            noise_y = np.random.normal(0, np.std(y) * 0.1)
            
            synthetic_sample = base_sample + noise_x
            synthetic_target = base_target + noise_y
            
            X_synthetic_list.append(synthetic_sample)
            y_synthetic_list.append(synthetic_target)
        
        X_extended = np.vstack([X_engineered, np.array(X_synthetic_list)])
        y_extended = np.hstack([y, np.array(y_synthetic_list)])
        
        feature_names = diabetes.feature_names + [f'{name}_sq' for name in diabetes.feature_names] + [
            'age*sex', 'bmi*bp', 's1*s2', 's3*s4', 's5*s6'
        ]
        
        print(f"   åŸå§‹ç‰¹å¾: {X.shape[1]} â†’ å·¥ç¨‹ç‰¹å¾: {X_extended.shape[1]}")
        print(f"   æ ·æœ¬æ•°: {X_extended.shape[0]} (åŸå§‹{X.shape[0]} + åˆæˆ{len(X_synthetic_list)})")
        print(f"   ç›®æ ‡å€¼èŒƒå›´: [{y_extended.min():.2f}, {y_extended.max():.2f}]")
        
        return X_extended, y_extended, feature_names
    
    def load_boston_housing_openml(self):
        """
        ä»OpenMLåŠ è½½æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›† (å›å½’ä»»åŠ¡)
        - æ ·æœ¬æ•°: 506
        - ç‰¹å¾æ•°: 13 (éœ€è¦ç‰¹å¾å·¥ç¨‹å¢åŠ åˆ°>10)
        - ä»»åŠ¡: é¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°
        - ç‰¹ç‚¹: ç»å…¸å›å½’åŸºå‡†æ•°æ®é›†
        """
        print("ğŸ  ä»OpenMLåŠ è½½æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†...")
        
        try:
            # ä»OpenMLè·å–æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†
            boston = fetch_openml(data_id=531, as_frame=False, parser='auto')
            X, y = boston.data, boston.target
            
            # ç‰¹å¾å·¥ç¨‹
            X_engineered = X.copy()
            
            # æ·»åŠ éçº¿æ€§ç‰¹å¾
            X_log = np.log1p(np.abs(X) + 1e-8)  # å¯¹æ•°å˜æ¢
            X_sqrt = np.sqrt(np.abs(X))  # å¹³æ–¹æ ¹å˜æ¢
            
            # æ·»åŠ äº¤äº’ç‰¹å¾
            interactions = np.column_stack([
                X[:, 5] * X[:, 7],  # RM * DIS (æˆ¿é—´æ•° * è·ç¦»)
                X[:, 0] * X[:, 4],  # CRIM * NOX (çŠ¯ç½ªç‡ * æ±¡æŸ“)
                X[:, 12] / (X[:, 5] + 1e-8),  # LSTAT / RM
                X[:, 5] ** 2,  # RM^2
                1 / (X[:, 12] + 1e-8),  # 1/LSTAT
            ])
            
            # åˆå¹¶ç‰¹å¾
            X_engineered = np.column_stack([X_engineered, X_log, X_sqrt, interactions])
            
            # æ•°æ®å¢å¼º
            np.random.seed(42)
            n_synthetic = 1500
            X_synthetic_list = []
            y_synthetic_list = []
            
            for _ in range(n_synthetic):
                base_idx = np.random.choice(len(X_engineered))
                base_sample = X_engineered[base_idx]
                base_target = y[base_idx]
                
                # æ·»åŠ é«˜æ–¯å™ªå£°
                noise_x = np.random.normal(0, np.std(X_engineered, axis=0) * 0.08)
                noise_y = np.random.normal(0, np.std(y) * 0.08)
                
                synthetic_sample = base_sample + noise_x
                synthetic_target = max(1, base_target + noise_y)  # ç¡®ä¿æˆ¿ä»·ä¸ºæ­£
                
                X_synthetic_list.append(synthetic_sample)
                y_synthetic_list.append(synthetic_target)
            
            X_extended = np.vstack([X_engineered, np.array(X_synthetic_list)])
            y_extended = np.hstack([y, np.array(y_synthetic_list)])
            
            feature_names = boston.feature_names.tolist() + [f'{name}_log' for name in boston.feature_names] + \
                          [f'{name}_sqrt' for name in boston.feature_names] + \
                          ['RM*DIS', 'CRIM*NOX', 'LSTAT/RM', 'RM^2', '1/LSTAT']
            
            print(f"   åŸå§‹ç‰¹å¾: {X.shape[1]} â†’ å·¥ç¨‹ç‰¹å¾: {X_extended.shape[1]}")
            print(f"   æ ·æœ¬æ•°: {X_extended.shape[0]} (åŸå§‹{X.shape[0]} + åˆæˆ{len(X_synthetic_list)})")
            print(f"   ç›®æ ‡å€¼èŒƒå›´: [{y_extended.min():.2f}, {y_extended.max():.2f}]")
            
            return X_extended, y_extended, feature_names
            
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ä»OpenMLåŠ è½½æ³¢å£«é¡¿æ•°æ®é›†: {e}")
            print("   ğŸ“± ä½¿ç”¨ç³–å°¿ç—…æ•°æ®é›†æ›¿ä»£...")
            return self.load_diabetes_dataset()
    
    def load_auto_mpg_dataset(self):
        """
        ä»UCIåŠ è½½æ±½è½¦æ²¹è€—æ•°æ®é›† (å›å½’ä»»åŠ¡)
        - æ ·æœ¬æ•°: ~400
        - ç‰¹å¾æ•°: 8 (éœ€è¦ç‰¹å¾å·¥ç¨‹å¢åŠ åˆ°>10)
        - ä»»åŠ¡: é¢„æµ‹æ±½è½¦æ²¹è€—(MPG)
        - ç‰¹ç‚¹: å·¥ç¨‹åº”ç”¨ï¼ŒåŒ…å«ç±»åˆ«å’Œæ•°å€¼ç‰¹å¾
        """
        print("ğŸš— åŠ è½½æ±½è½¦æ²¹è€—æ•°æ®é›†...")
        
        try:
            # ä»UCIä¸‹è½½Auto MPGæ•°æ®é›†
            url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
            
            # è¯»å–æ•°æ®
            data = []
            response = pd.read_csv(url, sep=r'\s+', header=None, na_values='?')
            
            # åˆ—å
            columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 
                      'acceleration', 'model_year', 'origin', 'car_name']
            response.columns = columns
            
            # åˆ é™¤ç¼ºå¤±å€¼
            response = response.dropna()
            
            # ç§»é™¤è½¦åï¼Œå¤„ç†æ•°å€¼ç‰¹å¾
            X_numeric = response[['cylinders', 'displacement', 'horsepower', 'weight', 
                                'acceleration', 'model_year', 'origin']].values
            y = response['mpg'].values
            
            # ç‰¹å¾å·¥ç¨‹
            # æ·»åŠ äº¤äº’ç‰¹å¾
            power_weight_ratio = X_numeric[:, 2] / X_numeric[:, 3]  # åŠŸç‡é‡é‡æ¯”
            displacement_per_cylinder = X_numeric[:, 1] / X_numeric[:, 0]  # æ’é‡/æ°”ç¼¸æ•°
            
            # éçº¿æ€§å˜æ¢
            log_features = np.log1p(X_numeric[:, [1, 2, 3]])  # displacement, horsepower, weight
            sqrt_features = np.sqrt(X_numeric[:, [4, 5]])  # acceleration, year
            
            # One-hotç¼–ç origin
            origin_dummies = np.eye(3)[X_numeric[:, 6].astype(int) - 1]
            
            # å¹´ä»£åˆ†ç»„
            year_normalized = (X_numeric[:, 5] - 70) / 10  # 1970s = 0, 1980s = 1
            decade_70s = (X_numeric[:, 5] < 75).astype(int)
            decade_80s = (X_numeric[:, 5] >= 75).astype(int)
            
            # ç»„åˆæ‰€æœ‰ç‰¹å¾
            X_engineered = np.column_stack([
                X_numeric,  # åŸå§‹ç‰¹å¾
                power_weight_ratio.reshape(-1, 1),
                displacement_per_cylinder.reshape(-1, 1),
                log_features,
                sqrt_features,
                origin_dummies,
                year_normalized.reshape(-1, 1),
                decade_70s.reshape(-1, 1),
                decade_80s.reshape(-1, 1)
            ])
            
            # æ•°æ®å¢å¼º
            np.random.seed(42)
            n_synthetic = 1200
            X_synthetic_list = []
            y_synthetic_list = []
            
            for _ in range(n_synthetic):
                base_idx = np.random.choice(len(X_engineered))
                base_sample = X_engineered[base_idx]
                base_target = y[base_idx]
                
                # æ·»åŠ å™ªå£°
                noise_x = np.random.normal(0, np.std(X_engineered, axis=0) * 0.05)
                noise_y = np.random.normal(0, np.std(y) * 0.05)
                
                synthetic_sample = base_sample + noise_x
                synthetic_target = max(5, base_target + noise_y)  # ç¡®ä¿MPGåˆç†
                
                X_synthetic_list.append(synthetic_sample)
                y_synthetic_list.append(synthetic_target)
            
            X_extended = np.vstack([X_engineered, np.array(X_synthetic_list)])
            y_extended = np.hstack([y, np.array(y_synthetic_list)])
            
            feature_names = [
                'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin',
                'power_weight_ratio', 'displacement_per_cylinder',
                'log_displacement', 'log_horsepower', 'log_weight',
                'sqrt_acceleration', 'sqrt_year',
                'origin_1', 'origin_2', 'origin_3',
                'year_normalized', 'decade_70s', 'decade_80s'
            ]
            
            print(f"   åŸå§‹ç‰¹å¾: 7 â†’ å·¥ç¨‹ç‰¹å¾: {X_extended.shape[1]}")
            print(f"   æ ·æœ¬æ•°: {X_extended.shape[0]} (åŸå§‹{len(X_numeric)} + åˆæˆ{len(X_synthetic_list)})")
            print(f"   ç›®æ ‡å€¼èŒƒå›´: [{y_extended.min():.2f}, {y_extended.max():.2f}] MPG")
            
            return X_extended, y_extended, feature_names
            
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ä¸‹è½½æ±½è½¦æ²¹è€—æ•°æ®é›†: {e}")
            print("   ğŸ“± ä½¿ç”¨ç³–å°¿ç—…æ•°æ®é›†æ›¿ä»£...")
            return self.load_diabetes_dataset()
    
    def load_wine_quality_regression(self):
        """
        åŠ è½½çº¢é…’è´¨é‡æ•°æ®é›†ä½œä¸ºå›å½’ä»»åŠ¡
        - æ ·æœ¬æ•°: ~1600
        - ç‰¹å¾æ•°: 11
        - ä»»åŠ¡: é¢„æµ‹çº¢é…’è´¨é‡åˆ†æ•°(3-9)
        - ç‰¹ç‚¹: çœŸå®çš„è´¨é‡è¯„ä¼°æ•°æ®ï¼Œå›å½’ç‰ˆæœ¬
        """
        print("ğŸ· åŠ è½½çº¢é…’è´¨é‡æ•°æ®é›†(å›å½’ç‰ˆ)...")
        
        try:
            url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
            wine_data = pd.read_csv(url, sep=';')
            
            X = wine_data.drop('quality', axis=1).values
            y = wine_data['quality'].values.astype(float)
            
            # ç‰¹å¾å·¥ç¨‹
            # æ·»åŠ ç‰¹å¾äº¤äº’
            alcohol_acid = X[:, 0] * X[:, 8]  # fixed_acidity * alcohol
            sugar_alcohol = X[:, 3] * X[:, 10]  # residual_sugar * alcohol
            ph_acid_ratio = X[:, 8] / (X[:, 0] + 1e-8)  # pH / fixed_acidity
            
            # æ·»åŠ éçº¿æ€§ç‰¹å¾
            X_squared = X ** 2
            X_log = np.log1p(X)
            
            # é…¸åº¦ç›¸å…³ç‰¹å¾
            total_acidity = X[:, 0] + X[:, 1]  # fixed + volatile acidity
            acidity_balance = X[:, 0] / (X[:, 1] + 1e-8)  # fixed/volatile ratio
            
            # åŒ–å­¦å¹³è¡¡ç‰¹å¾
            sulfur_ratio = X[:, 5] / (X[:, 4] + 1e-8)  # total/free sulfur dioxide
            density_alcohol = X[:, 9] / X[:, 10]  # density/alcohol
            
            # åˆå¹¶ç‰¹å¾
            X_engineered = np.column_stack([
                X,  # åŸå§‹11ä¸ªç‰¹å¾
                alcohol_acid.reshape(-1, 1),
                sugar_alcohol.reshape(-1, 1), 
                ph_acid_ratio.reshape(-1, 1),
                total_acidity.reshape(-1, 1),
                acidity_balance.reshape(-1, 1),
                sulfur_ratio.reshape(-1, 1),
                density_alcohol.reshape(-1, 1),
                X_squared,  # å¹³æ–¹é¡¹
                X_log[:, :5]  # å‰5ä¸ªç‰¹å¾çš„å¯¹æ•°é¡¹
            ])
            
            # æ•°æ®å¢å¼º
            np.random.seed(42)
            n_synthetic = 1000
            X_synthetic_list = []
            y_synthetic_list = []
            
            for _ in range(n_synthetic):
                base_idx = np.random.choice(len(X_engineered))
                base_sample = X_engineered[base_idx]
                base_target = y[base_idx]
                
                # æ·»åŠ å°å¹…å™ªå£°
                noise_x = np.random.normal(0, np.std(X_engineered, axis=0) * 0.03)
                noise_y = np.random.normal(0, 0.1)
                
                synthetic_sample = base_sample + noise_x
                synthetic_target = np.clip(base_target + noise_y, 3, 9)
                
                X_synthetic_list.append(synthetic_sample)
                y_synthetic_list.append(synthetic_target)
            
            X_extended = np.vstack([X_engineered, np.array(X_synthetic_list)])
            y_extended = np.hstack([y, np.array(y_synthetic_list)])
            
            feature_names = wine_data.columns[:-1].tolist() + [
                'alcohol_acid', 'sugar_alcohol', 'ph_acid_ratio', 
                'total_acidity', 'acidity_balance', 'sulfur_ratio', 'density_alcohol'
            ] + [f'{col}_sq' for col in wine_data.columns[:-1]] + \
            [f'{col}_log' for col in wine_data.columns[:5]]
            
            print(f"   åŸå§‹ç‰¹å¾: 11 â†’ å·¥ç¨‹ç‰¹å¾: {X_extended.shape[1]}")
            print(f"   æ ·æœ¬æ•°: {X_extended.shape[0]} (åŸå§‹{len(X)} + åˆæˆ{len(X_synthetic_list)})")
            print(f"   ç›®æ ‡å€¼èŒƒå›´: [{y_extended.min():.1f}, {y_extended.max():.1f}] (è´¨é‡åˆ†æ•°)")
            
            return X_extended, y_extended, feature_names
            
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ä¸‹è½½çº¢é…’æ•°æ®é›†: {e}")
            print("   ğŸ“± ä½¿ç”¨ç³–å°¿ç—…æ•°æ®é›†æ›¿ä»£...")
            return self.load_diabetes_dataset()
    
    def load_wine_quality(self):
        """
        ä»ç½‘ç»œåŠ è½½çº¢é…’è´¨é‡æ•°æ®é›† (åˆ†ç±»ä»»åŠ¡)
        - æ ·æœ¬æ•°: ~1600
        - ç‰¹å¾æ•°: 11
        - ä»»åŠ¡: é¢„æµ‹çº¢é…’è´¨é‡ç­‰çº§
        """
        print("ğŸ· å°è¯•åŠ è½½çº¢é…’è´¨é‡æ•°æ®é›†...")
        
        try:
            # å°è¯•ä»UCI ML Repositoryä¸‹è½½çº¢é…’è´¨é‡æ•°æ®
            url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
            wine_data = pd.read_csv(url, sep=';')
            
            X = wine_data.drop('quality', axis=1).values
            y = wine_data['quality'].values
            
            # å°†å›å½’é—®é¢˜è½¬ä¸ºåˆ†ç±»é—®é¢˜ (è´¨é‡åˆ†çº§)
            # 3-5: ä½è´¨é‡(0), 6-7: ä¸­è´¨é‡(1), 8-10: é«˜è´¨é‡(2)
            y_class = np.zeros_like(y)
            y_class[y <= 5] = 0  # ä½è´¨é‡
            y_class[(y >= 6) & (y <= 7)] = 1  # ä¸­è´¨é‡  
            y_class[y >= 8] = 2  # é«˜è´¨é‡
            
            print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
            print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
            print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_class)}")
            
            return X, y_class, wine_data.columns[:-1].tolist()
            
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ä¸‹è½½çº¢é…’æ•°æ®é›†: {e}")
            print("   ğŸ“± ä½¿ç”¨breast canceræ•°æ®é›†æ›¿ä»£...")
            return self.load_breast_cancer_extended()
    
    def load_breast_cancer_extended(self):
        """
        åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†å¹¶è¿›è¡Œç‰¹å¾å·¥ç¨‹ (åˆ†ç±»ä»»åŠ¡)
        - åŸå§‹: 30ç‰¹å¾, 569æ ·æœ¬
        - æ‰©å±•: å¢åŠ æ ·æœ¬å’Œç‰¹å¾
        """
        print("ğŸ¥ åŠ è½½ä¹³è…ºç™Œæ•°æ®é›† (æ‰©å±•ç‰ˆ)...")
        cancer = load_breast_cancer()
        X, y = cancer.data, cancer.target
        
        # æ•°æ®å¢å¼ºï¼šåˆ›å»ºæ›´å¤šæ ·æœ¬ 
        np.random.seed(42)
        n_synthetic = 1500  # ç”Ÿæˆåˆæˆæ ·æœ¬ä½¿æ€»æ•°>1000
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆåˆæˆæ ·æœ¬
        X_synthetic_list = []
        y_synthetic_list = []
        
        for class_label in [0, 1]:
            class_indices = np.where(y == class_label)[0]
            class_data = X[class_indices]
            
            # ç”Ÿæˆåˆæˆæ ·æœ¬ï¼ˆæ·»åŠ é«˜æ–¯å™ªå£°ï¼‰
            n_class_synthetic = n_synthetic // 2
            for _ in range(n_class_synthetic):
                # éšæœºé€‰æ‹©ä¸€ä¸ªçœŸå®æ ·æœ¬ä½œä¸ºåŸºç¡€
                base_idx = np.random.choice(len(class_data))
                base_sample = class_data[base_idx]
                
                # æ·»åŠ é€‚é‡å™ªå£°
                noise = np.random.normal(0, np.std(class_data, axis=0) * 0.1, base_sample.shape)
                synthetic_sample = base_sample + noise
                
                X_synthetic_list.append(synthetic_sample)
                y_synthetic_list.append(class_label)
        
        # åˆå¹¶åŸå§‹å’Œåˆæˆæ•°æ®
        X_extended = np.vstack([X, np.array(X_synthetic_list)])
        y_extended = np.hstack([y, np.array(y_synthetic_list)])
        
        print(f"   ç‰¹å¾æ•°: {X_extended.shape[1]}")
        print(f"   æ ·æœ¬æ•°: {X_extended.shape[0]} (åŸå§‹{X.shape[0]} + åˆæˆ{len(X_synthetic_list)})")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_extended)}")
        
        return X_extended, y_extended, cancer.feature_names.tolist()
    
    def load_german_credit_dataset(self):
        """
        åŠ è½½å¾·å›½ä¿¡ç”¨é£é™©æ•°æ®é›† (åˆ†ç±»ä»»åŠ¡)
        - æ ·æœ¬æ•°: 1000 (é€šè¿‡æ•°æ®å¢å¼ºæ‰©å±•åˆ°2000+)
        - ç‰¹å¾æ•°: 20 (æ··åˆæ•°å€¼å’Œç±»åˆ«ç‰¹å¾)
        - ä»»åŠ¡: é¢„æµ‹ä¿¡ç”¨é£é™©ï¼ˆå¥½/åï¼‰
        - ç‰¹ç‚¹: ç»å…¸é£æ§æ•°æ®é›†ï¼Œç‰¹å¾å¼‚è´¨æ€§å¼º
        """
        print("ğŸ¦ åŠ è½½å¾·å›½ä¿¡ç”¨é£é™©æ•°æ®é›†...")
        
        try:
            # å°è¯•ä»UCIä¸‹è½½å¾·å›½ä¿¡ç”¨æ•°æ®é›†
            url = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"
            
            # ç‰¹å¾åç§°
            feature_names = [
                'Status', 'Duration', 'CreditHistory', 'Purpose', 'CreditAmount',
                'SavingsAccount', 'Employment', 'InstallmentRate', 'PersonalStatus',
                'OtherDebtors', 'ResidenceSince', 'Property', 'Age', 'OtherInstallmentPlans',
                'Housing', 'ExistingCredits', 'Job', 'Dependents', 'Telephone', 'ForeignWorker'
            ]
            
            # è¯»å–æ•°æ®
            data = pd.read_csv(url, sep=' ', header=None, names=feature_names + ['Risk'])
            
            # å¤„ç†åˆ†ç±»ç‰¹å¾ - è½¬æ¢ä¸ºone-hotç¼–ç 
            categorical_features = ['Status', 'CreditHistory', 'Purpose', 'SavingsAccount',
                                  'Employment', 'PersonalStatus', 'OtherDebtors', 'Property',
                                  'OtherInstallmentPlans', 'Housing', 'Job', 'Telephone', 'ForeignWorker']
            
            X_numeric = data[['Duration', 'CreditAmount', 'InstallmentRate', 
                            'ResidenceSince', 'Age', 'ExistingCredits', 'Dependents']].values
            
            # ç®€å•ç¼–ç åˆ†ç±»ç‰¹å¾ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ç”¨æ›´å¤æ‚çš„ç¼–ç ï¼‰
            X_categorical = data[categorical_features].values
            for i in range(X_categorical.shape[1]):
                unique_vals = np.unique(X_categorical[:, i])
                mapping = {val: idx for idx, val in enumerate(unique_vals)}
                X_categorical[:, i] = [mapping[val] for val in X_categorical[:, i]]
            
            X = np.hstack([X_numeric, X_categorical])
            y = (data['Risk'] == 1).astype(int)  # 1=Good, 2=Bad -> 1=Good, 0=Bad
            
            print(f"   åŸå§‹ç‰¹å¾: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
            
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ä¸‹è½½å¾·å›½ä¿¡ç”¨æ•°æ®é›†: {e}")
            print("   ğŸ² ç”Ÿæˆæ¨¡æ‹Ÿçš„å¾·å›½ä¿¡ç”¨é£é™©æ•°æ®...")
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            n_samples = 1000
            
            # æ··åˆç‰¹å¾ï¼šæ•°å€¼å‹å’Œç±»åˆ«å‹
            X_numeric = np.random.randn(n_samples, 7)  # 7ä¸ªæ•°å€¼ç‰¹å¾
            X_numeric[:, 0] = np.abs(X_numeric[:, 0]) * 36 + 6  # Duration (6-72 months)
            X_numeric[:, 1] = np.abs(X_numeric[:, 1]) * 5000 + 1000  # CreditAmount
            X_numeric[:, 2] = np.clip(X_numeric[:, 2] * 0.5 + 3, 1, 4)  # InstallmentRate
            X_numeric[:, 3] = np.clip(X_numeric[:, 3] * 0.8 + 2, 1, 4)  # ResidenceSince
            X_numeric[:, 4] = np.abs(X_numeric[:, 4]) * 20 + 25  # Age
            X_numeric[:, 5] = np.clip(np.abs(X_numeric[:, 5]), 1, 4)  # ExistingCredits
            X_numeric[:, 6] = np.clip(np.abs(X_numeric[:, 6]), 1, 2)  # Dependents
            
            # 13ä¸ªç±»åˆ«ç‰¹å¾ï¼ˆå·²ç¼–ç ï¼‰
            X_categorical = np.random.randint(0, 4, size=(n_samples, 13))
            
            X = np.hstack([X_numeric, X_categorical])
            
            # ç”Ÿæˆæ ‡ç­¾ï¼ˆè€ƒè™‘ç‰¹å¾ç›¸å…³æ€§ï¼‰
            risk_score = (
                0.3 * (X_numeric[:, 0] > 24) +  # é•¿æœŸè´·æ¬¾é£é™©æ›´é«˜
                0.3 * (X_numeric[:, 1] > 5000) +  # å¤§é¢è´·æ¬¾é£é™©æ›´é«˜
                0.2 * (X_numeric[:, 4] < 30) +  # å¹´è½»äººé£é™©æ›´é«˜
                0.2 * (X_categorical[:, 0] == 0) +  # æŸäº›ç±»åˆ«é£é™©æ›´é«˜
                np.random.normal(0, 0.2, n_samples)
            )
            y = (risk_score < 0.5).astype(int)  # 1=Good, 0=Bad
            
            feature_names = [f'Feature_{i}' for i in range(20)]
        
        # æ•°æ®å¢å¼ºï¼šSMOTEé£æ ¼çš„è¿‡é‡‡æ ·
        print("   ğŸ’¡ è¿›è¡Œæ•°æ®å¢å¼º...")
        n_synthetic = 1500
        X_synthetic_list = []
        y_synthetic_list = []
        
        for class_label in [0, 1]:
            class_indices = np.where(y == class_label)[0]
            class_data = X[class_indices]
            
            # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆåˆæˆæ ·æœ¬
            n_class_synthetic = n_synthetic // 2
            for _ in range(n_class_synthetic):
                # éšæœºé€‰æ‹©ä¸¤ä¸ªåŒç±»æ ·æœ¬
                idx1, idx2 = np.random.choice(len(class_data), 2, replace=False)
                sample1, sample2 = class_data[idx1], class_data[idx2]
                
                # çº¿æ€§æ’å€¼ç”Ÿæˆæ–°æ ·æœ¬
                alpha = np.random.random()
                synthetic_sample = alpha * sample1 + (1 - alpha) * sample2
                
                # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œå¤„ç†ï¼ˆå–æœ€è¿‘çš„æ•´æ•°ï¼‰
                synthetic_sample[7:] = np.round(synthetic_sample[7:]).astype(int)
                
                X_synthetic_list.append(synthetic_sample)
                y_synthetic_list.append(class_label)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        X_extended = np.vstack([X.astype(np.float32), np.array(X_synthetic_list, dtype=np.float32)])
        y_extended = np.hstack([y, np.array(y_synthetic_list)])
        
        print(f"   æ‰©å±•å: ç‰¹å¾æ•°: {X_extended.shape[1]}, æ ·æœ¬æ•°: {X_extended.shape[0]}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_extended)} (0=Bad Credit, 1=Good Credit)")
        
        return X_extended, y_extended, feature_names
    
    def load_fraud_detection_dataset(self):
        """
        åŠ è½½ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›† (åˆ†ç±»ä»»åŠ¡)
        - æ ·æœ¬æ•°: 5000+ (é«˜åº¦ä¸å¹³è¡¡)
        - ç‰¹å¾æ•°: 30 (PCAè½¬æ¢åçš„ç‰¹å¾ + æ—¶é—´å’Œé‡‘é¢)
        - ä»»åŠ¡: æ£€æµ‹æ¬ºè¯ˆäº¤æ˜“
        - ç‰¹ç‚¹: æåº¦ä¸å¹³è¡¡ï¼Œå¼‚å¸¸æ£€æµ‹åœºæ™¯
        """
        print("ğŸ’³ åŠ è½½ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†...")
        
        # ç”±äºçœŸå®ä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®é›†å¤ªå¤§(284kæ ·æœ¬)ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿç‰ˆæœ¬
        np.random.seed(42)
        n_normal = 5000
        n_fraud = 100  # 2%æ¬ºè¯ˆç‡
        
        # æ­£å¸¸äº¤æ˜“ï¼šä¸»è¦åˆ†å¸ƒåœ¨æŸäº›æ¨¡å¼
        X_normal = np.random.randn(n_normal, 28) * 0.5  # PCAç‰¹å¾
        X_normal = np.hstack([
            X_normal,
            np.random.exponential(50, (n_normal, 1)),  # äº¤æ˜“é‡‘é¢ï¼ˆæŒ‡æ•°åˆ†å¸ƒï¼‰
            np.random.uniform(0, 172800, (n_normal, 1))  # æ—¶é—´ï¼ˆ2å¤©å†…çš„ç§’æ•°ï¼‰
        ])
        
        # æ¬ºè¯ˆäº¤æ˜“ï¼šæ˜¾è‘—ä¸åŒçš„æ¨¡å¼
        X_fraud = np.random.randn(n_fraud, 28)
        # æŸäº›PCAæˆåˆ†æœ‰æ˜æ˜¾åç§»
        X_fraud[:, [0, 2, 4, 10, 14]] += np.random.randn(n_fraud, 5) * 2
        X_fraud[:, [1, 3, 5, 11, 15]] -= np.random.randn(n_fraud, 5) * 1.5
        
        X_fraud = np.hstack([
            X_fraud,
            np.concatenate([
                np.random.exponential(200, (n_fraud//2, 1)),  # éƒ¨åˆ†å¤§é¢æ¬ºè¯ˆ
                np.random.exponential(20, (n_fraud//2, 1))    # éƒ¨åˆ†å°é¢æ¬ºè¯ˆ
            ]),
            np.random.uniform(0, 172800, (n_fraud, 1))  # æ—¶é—´
        ])
        
        # åˆå¹¶æ•°æ®
        X = np.vstack([X_normal, X_fraud])
        y = np.hstack([np.zeros(n_normal, dtype=int), np.ones(n_fraud, dtype=int)])
        
        # æ‰“ä¹±æ•°æ®
        shuffle_idx = np.random.permutation(len(X))
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        feature_names = [f'V{i}' for i in range(1, 29)] + ['Amount', 'Time']
        
        print(f"   ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)} (0=Normal, 1=Fraud)")
        print(f"   æ¬ºè¯ˆç‡: {100 * np.mean(y):.1f}%")
        
        return X, y, feature_names
    
    def load_bank_marketing_extended(self):
        """
        åŠ è½½é“¶è¡Œè¥é”€æ•°æ®é›† (åˆ†ç±»ä»»åŠ¡) - æ‰©å±•ç‰ˆæœ¬
        - æ ·æœ¬æ•°: 4000+
        - ç‰¹å¾æ•°: 50+ (åŒ…å«å¤§é‡è¡ç”Ÿç‰¹å¾)
        - ä»»åŠ¡: é¢„æµ‹å®¢æˆ·æ˜¯å¦ä¼šè®¢é˜…å®šæœŸå­˜æ¬¾
        - ç‰¹ç‚¹: æ··åˆç±»å‹ç‰¹å¾ï¼Œæ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œå®¢æˆ·è¡Œä¸ºæ¨¡å¼
        """
        print("ğŸª åŠ è½½é“¶è¡Œè¥é”€æ•°æ®é›† (æ‰©å±•ç‰ˆ)...")
        
        np.random.seed(42)
        n_samples = 4500
        
        # åŸºç¡€å®¢æˆ·ä¿¡æ¯
        age = np.random.randint(18, 95, n_samples)
        job = np.random.randint(0, 12, n_samples)  # 12ç§èŒä¸š
        marital = np.random.randint(0, 4, n_samples)  # 4ç§å©šå§»çŠ¶æ€
        education = np.random.randint(0, 8, n_samples)  # 8ç§æ•™è‚²æ°´å¹³
        
        # é‡‘èä¿¡æ¯
        default = np.random.choice([0, 1], n_samples, p=[0.98, 0.02])  # è¿çº¦è®°å½•
        housing = np.random.choice([0, 1], n_samples, p=[0.44, 0.56])  # æˆ¿è´·
        loan = np.random.choice([0, 1], n_samples, p=[0.84, 0.16])  # ä¸ªäººè´·æ¬¾
        balance = np.random.normal(1500, 3000, n_samples)
        balance = np.clip(balance, -8000, 100000)
        
        # è”ç³»ä¿¡æ¯
        contact = np.random.randint(0, 3, n_samples)  # è”ç³»æ–¹å¼
        duration = np.random.exponential(260, n_samples)  # é€šè¯æ—¶é•¿
        campaign = np.random.poisson(2.5, n_samples) + 1  # æœ¬æ¬¡æ´»åŠ¨è”ç³»æ¬¡æ•°
        pdays = np.random.choice([-1, *range(0, 400)], n_samples, p=[0.96] + [0.04/400]*400)  # ä¸Šæ¬¡è”ç³»å¤©æ•°
        previous = np.random.poisson(0.5, n_samples)  # ä¹‹å‰è”ç³»æ¬¡æ•°
        poutcome = np.random.choice([0, 1, 2, 3], n_samples, p=[0.86, 0.04, 0.03, 0.07])  # ä¸Šæ¬¡ç»“æœ
        
        # æ—¶é—´ä¿¡æ¯
        month = np.random.randint(0, 12, n_samples)
        day_of_week = np.random.randint(0, 5, n_samples)
        
        # ç»æµæŒ‡æ ‡
        emp_var_rate = np.random.normal(0.08, 2.5, n_samples)  # å°±ä¸šå˜åŒ–ç‡
        cons_price_idx = np.random.normal(93.5, 0.6, n_samples)  # æ¶ˆè´¹è€…ä»·æ ¼æŒ‡æ•°
        cons_conf_idx = np.random.normal(-40.5, 4.2, n_samples)  # æ¶ˆè´¹è€…ä¿¡å¿ƒæŒ‡æ•°
        euribor3m = np.random.normal(3.6, 1.7, n_samples)  # 3æœˆæœŸæ¬§å…ƒåŒä¸šæ‹†å€Ÿåˆ©ç‡
        nr_employed = np.random.normal(5100, 72, n_samples)  # å°±ä¸šäººæ•°
        
        # åˆ›å»ºè¡ç”Ÿç‰¹å¾
        age_group = np.digitize(age, [25, 35, 45, 55, 65])
        balance_category = np.digitize(balance, [-500, 0, 500, 2000, 5000])
        contact_frequency = campaign + previous
        days_since_contact = np.where(pdays == -1, 999, pdays)
        economic_score = emp_var_rate * 0.3 + cons_conf_idx * 0.01 + euribor3m * 0.2
        
        # äº¤äº’ç‰¹å¾
        age_balance = age * balance / 1000
        education_job = education * 12 + job
        has_loans = housing + loan
        contact_success = (poutcome == 3).astype(int) * previous
        
        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        X = np.column_stack([
            age, job, marital, education, default, housing, loan, balance,
            contact, duration, campaign, pdays, previous, poutcome,
            month, day_of_week,
            emp_var_rate, cons_price_idx, cons_conf_idx, euribor3m, nr_employed,
            age_group, balance_category, contact_frequency, days_since_contact,
            economic_score, age_balance, education_job, has_loans, contact_success,
            # æ·»åŠ ä¸€äº›å™ªå£°ç‰¹å¾å¢åŠ å¤æ‚åº¦
            np.random.randn(n_samples, 20) * 0.5
        ])
        
        # ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäºå¤æ‚çš„ä¸šåŠ¡è§„åˆ™ï¼‰
        subscribe_score = (
            0.15 * (duration > 300) +
            0.10 * (balance > 1000) +
            0.10 * (age > 30) * (age < 60) +
            0.10 * (education >= 5) +
            0.10 * (poutcome == 3) +
            0.05 * (housing == 0) +
            0.05 * (loan == 0) +
            0.10 * (economic_score > 0) +
            0.10 * (contact == 1) +  # cellular
            0.05 * (previous > 0) * (previous < 5) +
            0.10 * np.random.randn(n_samples)
        )
        y = (subscribe_score > 0.5).astype(int)
        
        # è°ƒæ•´ç±»åˆ«å¹³è¡¡
        target_positive_ratio = 0.11  # 11%è®¢é˜…ç‡
        current_positive_ratio = np.mean(y)
        if current_positive_ratio > target_positive_ratio:
            positive_indices = np.where(y == 1)[0]
            n_to_flip = int((current_positive_ratio - target_positive_ratio) * n_samples)
            flip_indices = np.random.choice(positive_indices, n_to_flip, replace=False)
            y[flip_indices] = 0
        
        feature_names = [
            'age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'balance',
            'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome',
            'month', 'day_of_week',
            'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed',
            'age_group', 'balance_category', 'contact_frequency', 'days_since_contact',
            'economic_score', 'age_balance', 'education_job', 'has_loans', 'contact_success'
        ] + [f'noise_{i}' for i in range(20)]
        
        print(f"   ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)} (0=No, 1=Yes)")
        print(f"   è®¢é˜…ç‡: {100 * np.mean(y):.1f}%")
        
        return X, y, feature_names
    
    def load_forest_cover_dataset(self):
        """
        åŠ è½½æ£®æ—è¦†ç›–ç±»å‹æ•°æ®é›† (å¤šåˆ†ç±»ä»»åŠ¡)
        - æ ·æœ¬æ•°: 5000 (é‡‡æ ·è‡ªåŸå§‹581kæ•°æ®)
        - ç‰¹å¾æ•°: 54 (10ä¸ªæ•°å€¼ç‰¹å¾ + 44ä¸ªäºŒå€¼ç‰¹å¾)
        - ä»»åŠ¡: é¢„æµ‹æ£®æ—è¦†ç›–ç±»å‹ï¼ˆ7ç±»ï¼‰
        - ç‰¹ç‚¹: å¤šç±»åˆ«å¹³è¡¡ï¼Œç‰¹å¾åŒ…å«åœ°å½¢ã€åœŸå£¤ç­‰ä¿¡æ¯ï¼Œé€‚åˆå±•ç¤ºæ¨¡å‹æ€§èƒ½
        """
        print("ğŸŒ² åŠ è½½æ£®æ—è¦†ç›–ç±»å‹æ•°æ®é›†...")
        
        np.random.seed(42)
        n_samples = 5000
        n_classes = 7
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ£®æ—è¦†ç›–æ•°æ®
        # 10ä¸ªè¿ç»­ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–åçš„åœ°å½¢ç‰¹å¾ï¼‰
        elevation = np.random.normal(2500, 500, n_samples)  # æµ·æ‹”
        aspect = np.random.uniform(0, 360, n_samples)  # æ–¹ä½è§’
        slope = np.random.exponential(15, n_samples)  # å¡åº¦
        h_dist_hydro = np.random.exponential(300, n_samples)  # åˆ°æ°´æºè·ç¦»
        v_dist_hydro = np.random.normal(50, 100, n_samples)  # å‚ç›´åˆ°æ°´æºè·ç¦»
        h_dist_road = np.random.exponential(2000, n_samples)  # åˆ°é“è·¯è·ç¦»
        hillshade_9am = np.random.uniform(0, 255, n_samples)  # 9amå±±å½±
        hillshade_noon = np.random.uniform(0, 255, n_samples)  # æ­£åˆå±±å½±
        hillshade_3pm = np.random.uniform(0, 255, n_samples)  # 3pmå±±å½±
        h_dist_fire = np.random.exponential(1500, n_samples)  # åˆ°ç«ç‚¹è·ç¦»
        
        # æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾
        continuous_features = np.column_stack([
            (elevation - 2500) / 500,
            aspect / 360,
            slope / 50,
            np.log1p(h_dist_hydro) / 8,
            v_dist_hydro / 200,
            np.log1p(h_dist_road) / 10,
            hillshade_9am / 255,
            hillshade_noon / 255,
            hillshade_3pm / 255,
            np.log1p(h_dist_fire) / 10
        ])
        
        # 44ä¸ªäºŒå€¼ç‰¹å¾ï¼ˆ4ä¸ªè’é‡åŒºåŸŸ + 40ä¸ªåœŸå£¤ç±»å‹ï¼‰
        # è’é‡åŒºåŸŸï¼ˆone-hotï¼Œæ¯ä¸ªæ ·æœ¬åªå±äºä¸€ä¸ªåŒºåŸŸï¼‰
        wilderness_area = np.zeros((n_samples, 4))
        wilderness_choice = np.random.choice(4, n_samples)
        wilderness_area[np.arange(n_samples), wilderness_choice] = 1
        
        # åœŸå£¤ç±»å‹ï¼ˆone-hotï¼Œæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ç§åœŸå£¤ï¼‰
        soil_type = np.zeros((n_samples, 40))
        soil_choice = np.random.choice(40, n_samples)
        soil_type[np.arange(n_samples), soil_choice] = 1
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X = np.hstack([continuous_features, wilderness_area, soil_type])
        
        # ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäºå¤æ‚çš„è§„åˆ™ï¼Œæ¨¡æ‹ŸçœŸå®çš„ç”Ÿæ€å…³ç³»ï¼‰
        # ä¸åŒçš„æ£®æ—ç±»å‹å€¾å‘äºä¸åŒçš„ç¯å¢ƒæ¡ä»¶
        cover_scores = np.zeros((n_samples, n_classes))
        
        # Spruce/Fir (ç±»å‹1): é«˜æµ·æ‹”ï¼ŒåŒ—å¡
        cover_scores[:, 0] = (
            0.4 * (continuous_features[:, 0] > 0.5) +  # é«˜æµ·æ‹”
            0.3 * (aspect < 90) +  # åŒ—å‘
            0.2 * (wilderness_choice == 0) +  # ç‰¹å®šè’é‡åŒº
            0.1 * (soil_choice < 10)  # ç‰¹å®šåœŸå£¤
        )
        
        # Lodgepole Pine (ç±»å‹2): ä¸­ç­‰æµ·æ‹”ï¼Œå„ç§å¡å‘
        cover_scores[:, 1] = (
            0.3 * np.abs(continuous_features[:, 0]) < 0.5 +  # ä¸­ç­‰æµ·æ‹”
            0.3 * (slope < 20) +  # ç¼“å¡
            0.2 * (wilderness_choice == 1) +
            0.2 * ((soil_choice >= 10) & (soil_choice < 20))
        )
        
        # Ponderosa Pine (ç±»å‹3): ä½æµ·æ‹”ï¼Œå—å¡
        cover_scores[:, 2] = (
            0.4 * (continuous_features[:, 0] < -0.5) +  # ä½æµ·æ‹”
            0.3 * ((aspect > 180) & (aspect < 270)) +  # å—å‘
            0.2 * (wilderness_choice == 2) +
            0.1 * ((soil_choice >= 20) & (soil_choice < 25))
        )
        
        # Cottonwood/Willow (ç±»å‹4): è¿‘æ°´æº
        cover_scores[:, 3] = (
            0.5 * (continuous_features[:, 3] < 0.3) +  # è¿‘æ°´æº
            0.3 * (continuous_features[:, 4] < 0) +  # ä½äºæ°´æº
            0.2 * ((soil_choice >= 25) & (soil_choice < 30))
        )
        
        # Aspen (ç±»å‹5): ä¸­é«˜æµ·æ‹”ï¼Œæ¹¿æ¶¦
        cover_scores[:, 4] = (
            0.3 * (continuous_features[:, 0] > 0) +
            0.3 * (continuous_features[:, 6] < 0.7) +  # è¾ƒå°‘æ—¥ç…§
            0.2 * (wilderness_choice == 3) +
            0.2 * ((soil_choice >= 30) & (soil_choice < 35))
        )
        
        # Douglas-fir (ç±»å‹6): å„ç§æ¡ä»¶
        cover_scores[:, 5] = (
            0.25 * np.ones(n_samples) +  # é€‚åº”æ€§å¼º
            0.25 * (continuous_features[:, 2] > 0.3) +  # é™¡å¡
            0.25 * ((aspect > 270) | (aspect < 90)) +  # åŒ—/è¥¿å‘
            0.25 * (soil_choice >= 35)
        )
        
        # Krummholz (ç±»å‹7): æé«˜æµ·æ‹”
        cover_scores[:, 6] = (
            0.6 * (continuous_features[:, 0] > 1.0) +  # æé«˜æµ·æ‹”
            0.2 * (slope > 30) +  # é™¡å¡
            0.2 * (continuous_features[:, 8] < 0.5)  # é£å¤§
        )
        
        # æ·»åŠ éšæœºæ€§å¹¶ç¡®å®šæœ€ç»ˆç±»åˆ«
        cover_scores += np.random.normal(0, 0.2, (n_samples, n_classes))
        y = np.argmax(cover_scores, axis=1)
        
        # ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        min_samples_per_class = 100
        for class_idx in range(n_classes):
            class_count = np.sum(y == class_idx)
            if class_count < min_samples_per_class:
                # éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬æ”¹ä¸ºè¿™ä¸ªç±»åˆ«
                candidates = np.where(y != class_idx)[0]
                n_to_change = min_samples_per_class - class_count
                change_indices = np.random.choice(candidates, n_to_change, replace=False)
                y[change_indices] = class_idx
        
        # ç‰¹å¾åç§°
        feature_names = [
            'Elevation', 'Aspect', 'Slope', 'H_Dist_Hydro', 'V_Dist_Hydro',
            'H_Dist_Roads', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'H_Dist_Fire'
        ]
        feature_names += [f'Wilderness_Area_{i+1}' for i in range(4)]
        feature_names += [f'Soil_Type_{i+1}' for i in range(40)]
        
        print(f"   ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
        print(f"   ç±»åˆ«: 1=Spruce/Fir, 2=Lodgepole Pine, 3=Ponderosa Pine, ")
        print(f"         4=Cottonwood/Willow, 5=Aspen, 6=Douglas-fir, 7=Krummholz")
        
        return X, y, feature_names
    
    def load_letter_recognition_dataset(self):
        """
        åŠ è½½å­—æ¯è¯†åˆ«æ•°æ®é›† (26ç±»åˆ†ç±»ä»»åŠ¡)
        - æ ·æœ¬æ•°: 5000 (é‡‡æ ·)
        - ç‰¹å¾æ•°: 16 (å­—æ¯å›¾åƒçš„ç»Ÿè®¡ç‰¹å¾)
        - ä»»åŠ¡: è¯†åˆ«å¤§å†™å­—æ¯A-Z
        - ç‰¹ç‚¹: å¤šç±»åˆ«ï¼ˆ26ç±»ï¼‰ï¼Œç‰¹å¾æ˜¯æ‰‹å·¥æå–çš„å›¾åƒç»Ÿè®¡é‡
        """
        print("ğŸ”¤ åŠ è½½å­—æ¯è¯†åˆ«æ•°æ®é›†...")
        
        np.random.seed(42)
        n_samples = 5000
        n_features = 16
        n_classes = 26  # A-Z
        
        # ä¸ºæ¯ä¸ªå­—æ¯ç±»åˆ«ç”Ÿæˆç‰¹å¾æ¨¡å¼
        # æ¨¡æ‹Ÿä¸åŒå­—æ¯çš„ç»Ÿè®¡ç‰¹å¾ï¼ˆå¦‚å®½é«˜æ¯”ã€åƒç´ åˆ†å¸ƒç­‰ï¼‰
        class_centers = np.random.randn(n_classes, n_features) * 2
        
        # ç”Ÿæˆæ ·æœ¬
        X = []
        y = []
        
        samples_per_class = n_samples // n_classes
        extra_samples = n_samples % n_classes
        
        for class_idx in range(n_classes):
            n_class_samples = samples_per_class + (1 if class_idx < extra_samples else 0)
            
            # å›´ç»•ç±»ä¸­å¿ƒç”Ÿæˆæ ·æœ¬ï¼Œæ·»åŠ ç±»å†…å˜åŒ–
            class_samples = class_centers[class_idx] + np.random.randn(n_class_samples, n_features) * 0.5
            
            # æ·»åŠ ä¸€äº›ç±»åˆ«ç‰¹å®šçš„ç‰¹å¾æ¨¡å¼
            if class_idx < 5:  # A-E: æœ‰è§’çš„å­—æ¯
                class_samples[:, 0] *= 1.2  # æ›´é«˜çš„è§’ç‚¹æ•°
                class_samples[:, 1] *= 0.8  # è¾ƒå°‘çš„æ›²çº¿
            elif class_idx < 10:  # F-J: æ··åˆç‰¹å¾
                class_samples[:, 2] += 0.5  # ä¸­ç­‰å¤æ‚åº¦
            elif class_idx < 15:  # K-O: åŒ…å«æ›²çº¿
                class_samples[:, 1] *= 1.3  # æ›´å¤šæ›²çº¿
                class_samples[:, 3] += 0.3  # å¯¹ç§°æ€§
            elif class_idx < 20:  # P-T: å‚ç›´ä¸»å¯¼
                class_samples[:, 4] *= 1.2  # å‚ç›´ç¬”ç”»
                class_samples[:, 5] *= 0.7  # è¾ƒå°‘æ°´å¹³ç¬”ç”»  
            else:  # U-Z: ç‰¹æ®Šå½¢çŠ¶
                class_samples[:, 6:8] += np.random.randn(n_class_samples, 2) * 0.3
            
            X.append(class_samples)
            y.extend([class_idx] * n_class_samples)
        
        X = np.vstack(X)
        y = np.array(y)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]åŒºé—´ï¼ˆæ¨¡æ‹Ÿå›¾åƒç‰¹å¾ï¼‰
        X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)
        
        # æ‰“ä¹±æ•°æ®
        shuffle_idx = np.random.permutation(len(X))
        X = X[shuffle_idx]
        y = y[shuffle_idx]
        
        # ç‰¹å¾åç§°ï¼ˆæ¨¡æ‹Ÿå›¾åƒç»Ÿè®¡ç‰¹å¾ï¼‰
        feature_names = [
            'x-box', 'y-box', 'width', 'height', 'onpix', 'x-bar', 'y-bar',
            'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy',
            'y-ege', 'yegvx'
        ]
        
        print(f"   ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"   ç±»åˆ«æ•°: {n_classes} (A-Z)")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: åŸºæœ¬å‡è¡¡ï¼ˆæ¯ç±»çº¦{samples_per_class}ä¸ªæ ·æœ¬ï¼‰")
        
        return X, y, feature_names
    
    def create_pytorch_model(self, input_size, output_size, hidden_sizes, task='regression'):
        """åˆ›å»ºPyTorchåŸºçº¿æ¨¡å‹ï¼ˆä¸QuickTesterä¸€è‡´ï¼‰"""
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        return nn.Sequential(*layers)
    
    def train_pytorch_model(self, model, X_train, y_train, X_val=None, y_val=None, 
                          epochs=1000, lr=0.001, task='regression', patience=50, tol=1e-4):
        """è®­ç»ƒPyTorchæ¨¡å‹ï¼ˆä¸QuickTesterä¸€è‡´ï¼‰"""
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train)
        
        if task == 'regression':
            criterion = nn.MSELoss()
        else:
            criterion = nn.CrossEntropyLoss()
            y_train_tensor = y_train_tensor.long()
        
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        best_loss = float('inf')
        no_improve = 0
        
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            outputs = model(X_train_tensor)
            if task == 'regression':
                loss = criterion(outputs.squeeze(), y_train_tensor)
            else:
                loss = criterion(outputs, y_train_tensor)
            
            loss.backward()
            optimizer.step()
            
            # éªŒè¯é›†æ—©åœ
            if X_val is not None:
                model.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val)
                    val_outputs = model(X_val_tensor)
                    if task == 'regression':
                        y_val_tensor = torch.FloatTensor(y_val)
                        val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()
                    else:
                        y_val_tensor = torch.LongTensor(y_val)
                        val_loss = criterion(val_outputs, y_val_tensor).item()
                
                if val_loss < best_loss - tol:
                    best_loss = val_loss
                    no_improve = 0
                else:
                    no_improve += 1
                
                if no_improve >= patience:
                    break
        
        model.n_iter_ = epoch + 1
        model.final_loss_ = best_loss
        return model
    
    def benchmark_regression(self, dataset_name='california_housing'):
        """
        çœŸå®æ•°æ®é›†å›å½’åŸºå‡†æµ‹è¯• - å¯é…ç½®å‚æ•°ç‰ˆæœ¬
        """
        if self.config['verbose']:
            print("ğŸ  CausalEngineçœŸå®æ•°æ®é›†å›å½’åŸºå‡†æµ‹è¯•")
            print("=" * 80)
        
        # åŠ è½½æ•°æ®
        if dataset_name == 'california_housing':
            X, y, _ = self.load_california_housing()
        elif dataset_name == 'diabetes':
            X, y, _ = self.load_diabetes_dataset()
        elif dataset_name == 'boston_housing':
            X, y, _ = self.load_boston_housing_openml()
        elif dataset_name == 'auto_mpg':
            X, y, _ = self.load_auto_mpg_dataset()
        elif dataset_name == 'wine_quality_reg':
            X, y, _ = self.load_wine_quality_regression()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›å½’æ•°æ®é›†: {dataset_name}. å¯é€‰: california_housing, diabetes, boston_housing, auto_mpg, wine_quality_reg")
        
        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler.fit_transform(X)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=self.config['test_size'], random_state=self.config['random_state'])
        
        # åªå¯¹è®­ç»ƒæ•°æ®æ·»åŠ æ ‡ç­¾å¼‚å¸¸ï¼ˆä¿æŒtest setå¹²å‡€ç”¨äºçœŸå®è¯„ä¼°ï¼‰
        if self.config['regression_anomaly_ratio'] > 0:
            y_train = self.add_label_anomalies(y_train, self.config['regression_anomaly_ratio'], 'regression')
        
        # åˆ†å‰²è®­ç»ƒæ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆéªŒè¯é›†ä¹Ÿæœ‰å¼‚å¸¸ï¼‰
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=self.config['validation_fraction'], random_state=self.config['random_state'])
        
        if self.config['verbose']:
            print(f"æ•°æ®é›†: {dataset_name}")
            print(f"ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}")
            print(f"è®­ç»ƒé›†: {X_train.shape[0]}, éªŒè¯é›†: {X_val.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
            if self.config['regression_anomaly_ratio'] > 0:
                print(f"æ ‡ç­¾å¼‚å¸¸: {self.config['regression_anomaly_ratio']:.1%} (å¤åˆå¼‚å¸¸) - ä»…å½±å“train+valï¼Œtestä¿æŒå¹²å‡€")
            print()
        
        results = {}
        
        # 1. sklearn MLPRegressor
        if self.config['verbose']: print("è®­ç»ƒ sklearn MLPRegressor...")
        sklearn_reg = MLPRegressor(
            hidden_layer_sizes=self.config['hidden_layer_sizes'],
            max_iter=self.config['max_iter'],
            learning_rate_init=self.config['learning_rate'],
            early_stopping=self.config['early_stopping'],
            validation_fraction=self.config['validation_fraction'],
            n_iter_no_change=self.config['n_iter_no_change'],
            tol=self.config['tol'],
            random_state=self.config['random_state'],
            alpha=self.config['alpha']
        )
        sklearn_reg.fit(X_train, y_train)
        sklearn_pred_test = sklearn_reg.predict(X_test)
        sklearn_pred_val = sklearn_reg.predict(X_val)
        
        results['sklearn'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, sklearn_pred_test),
                'MdAE': median_absolute_error(y_test, sklearn_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, sklearn_pred_test)),
                'RÂ²': r2_score(y_test, sklearn_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, sklearn_pred_val),
                'MdAE': median_absolute_error(y_val, sklearn_pred_val),
                'RMSE': np.sqrt(mean_squared_error(y_val, sklearn_pred_val)),
                'RÂ²': r2_score(y_val, sklearn_pred_val)
            }
        }
        
        # 2. PyTorchåŸºçº¿
        if self.config['verbose']: print("è®­ç»ƒ PyTorchåŸºçº¿...")
        pytorch_model = self.create_pytorch_model(X.shape[1], 1, self.config['hidden_layer_sizes'], 'regression')
        pytorch_model = self.train_pytorch_model(
            pytorch_model, X_train, y_train, X_val, y_val,
            epochs=self.config['max_iter'], lr=self.config['learning_rate'], task='regression',
            patience=self.config['n_iter_no_change'], tol=self.config['tol'])
        
        pytorch_model.eval()
        with torch.no_grad():
            pytorch_pred_test = pytorch_model(torch.FloatTensor(X_test)).squeeze().numpy()
            pytorch_pred_val = pytorch_model(torch.FloatTensor(X_val)).squeeze().numpy()
        
        results['pytorch'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, pytorch_pred_test),
                'MdAE': median_absolute_error(y_test, pytorch_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, pytorch_pred_test)),
                'RÂ²': r2_score(y_test, pytorch_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, pytorch_pred_val),
                'MdAE': median_absolute_error(y_val, pytorch_pred_val),
                'RMSE': np.sqrt(mean_squared_error(y_val, pytorch_pred_val)),
                'RÂ²': r2_score(y_val, pytorch_pred_val)
            }
        }
        
        # 3. CausalEngineå¤šæ¨¡å¼æµ‹è¯•
        for mode in self.config['test_modes']:
            if self.config['verbose']: print(f"è®­ç»ƒ CausalEngine ({mode})...")
            
            causal_reg = MLPCausalRegressor(
                hidden_layer_sizes=self.config['hidden_layer_sizes'],
                mode=mode,
                gamma_init=self.config['gamma_init'],
                b_noise_init=self.config['b_noise_init'],
                b_noise_trainable=self.config['b_noise_trainable'],
                max_iter=self.config['max_iter'],
                learning_rate=self.config['learning_rate'],
                early_stopping=self.config['early_stopping'],
                random_state=self.config['random_state'],
                verbose=False
            )
            
            causal_reg.fit(X_train, y_train)
            causal_pred_test = causal_reg.predict(X_test)
            causal_pred_val = causal_reg.predict(X_val)
            
            if isinstance(causal_pred_test, dict):
                causal_pred_test = causal_pred_test['predictions']
            if isinstance(causal_pred_val, dict):
                causal_pred_val = causal_pred_val['predictions']
            
            results[mode] = {
                'test': {
                    'MAE': mean_absolute_error(y_test, causal_pred_test),
                    'MdAE': median_absolute_error(y_test, causal_pred_test),
                    'RMSE': np.sqrt(mean_squared_error(y_test, causal_pred_test)),
                    'RÂ²': r2_score(y_test, causal_pred_test)
                },
                'val': {
                    'MAE': mean_absolute_error(y_val, causal_pred_val),
                    'MdAE': median_absolute_error(y_val, causal_pred_val),
                    'RMSE': np.sqrt(mean_squared_error(y_val, causal_pred_val)),
                    'RÂ²': r2_score(y_val, causal_pred_val)
                },
                'model': causal_reg
            }
            
            # æ¼”ç¤ºåˆ†å¸ƒé¢„æµ‹ï¼ˆédeterministicæ¨¡å¼ï¼‰
            if mode != 'deterministic' and self.config['show_distribution_examples']:
                dist_params = causal_reg.predict_dist(X_test[:self.config['n_distribution_samples']])
                if self.config['verbose']:
                    print(f"  åˆ†å¸ƒå‚æ•°å½¢çŠ¶: {dist_params.shape}")
                    print(f"  å‰{self.config['n_distribution_samples']}æ ·æœ¬ä½ç½®å‚æ•°: {dist_params[:self.config['n_distribution_samples'], 0, 0]}")
                    print(f"  å‰{self.config['n_distribution_samples']}æ ·æœ¬å°ºåº¦å‚æ•°: {dist_params[:self.config['n_distribution_samples'], 0, 1]}")
        
        # æ˜¾ç¤ºç»“æœ
        if self.config['verbose']:
            print("\nğŸ“Š çœŸå®æ•°æ®é›†å›å½’ç»“æœå¯¹æ¯”:")
            print("=" * 120)
            print(f"{'æ–¹æ³•':<15} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
            print(f"{'':15} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
            print("-" * 120)
            # æ˜¾ç¤ºé¡ºåºï¼šsklearn, pytorch, ç„¶åæŒ‰test_modesé¡ºåº
            display_order = ['sklearn', 'pytorch'] + self.config['test_modes']
            for method in display_order:
                if method in results:
                    metrics = results[method]
                    val_m = metrics['val']
                    test_m = metrics['test']
                    print(f"{method:<15} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
                          f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
            print("=" * 120)
        
        return results
    
    def benchmark_classification(self, dataset_name='wine_quality'):
        """
        çœŸå®æ•°æ®é›†åˆ†ç±»åŸºå‡†æµ‹è¯• - å¯é…ç½®å‚æ•°ç‰ˆæœ¬
        
        æ”¯æŒçš„æ•°æ®é›†:
        - wine_quality: çº¢é…’è´¨é‡é¢„æµ‹ï¼ˆ3ç±»ï¼‰
        - breast_cancer: ä¹³è…ºç™Œæ£€æµ‹ï¼ˆ2ç±»ï¼Œæ‰©å±•ç‰ˆï¼‰
        - german_credit: å¾·å›½ä¿¡ç”¨é£é™©è¯„ä¼°ï¼ˆ2ç±»ï¼‰
        - fraud_detection: ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹ï¼ˆ2ç±»ï¼Œæåº¦ä¸å¹³è¡¡ï¼‰
        - bank_marketing: é“¶è¡Œè¥é”€é¢„æµ‹ï¼ˆ2ç±»ï¼Œä¸å¹³è¡¡ï¼‰
        - forest_cover: æ£®æ—è¦†ç›–ç±»å‹ï¼ˆ7ç±»ï¼Œå¹³è¡¡ï¼‰â­ æ¨è
        - letter_recognition: å­—æ¯è¯†åˆ«ï¼ˆ26ç±»ï¼Œå¹³è¡¡ï¼‰â­ æ¨è
        """
        if self.config['verbose']:
            print("ğŸ· CausalEngineçœŸå®æ•°æ®é›†åˆ†ç±»åŸºå‡†æµ‹è¯•")
            print("=" * 80)
        
        # åŠ è½½æ•°æ®
        if dataset_name == 'wine_quality':
            X, y, _ = self.load_wine_quality()
        elif dataset_name == 'breast_cancer':
            X, y, _ = self.load_breast_cancer_extended()
        elif dataset_name == 'german_credit':
            X, y, _ = self.load_german_credit_dataset()
        elif dataset_name == 'fraud_detection':
            X, y, _ = self.load_fraud_detection_dataset()
        elif dataset_name == 'bank_marketing':
            X, y, _ = self.load_bank_marketing_extended()
        elif dataset_name == 'forest_cover':
            X, y, _ = self.load_forest_cover_dataset()
        elif dataset_name == 'letter_recognition':
            X, y, _ = self.load_letter_recognition_dataset()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç±»æ•°æ®é›†: {dataset_name}")
        
        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler.fit_transform(X)
        
        # æ•°æ®åˆ†å‰²
        stratify_test = y if self.config['stratify_classification'] else None
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=self.config['test_size'], random_state=self.config['random_state'], stratify=stratify_test)
        
        # åªå¯¹è®­ç»ƒæ•°æ®æ·»åŠ æ ‡ç­¾å¼‚å¸¸ï¼ˆä¿æŒtest setå¹²å‡€ç”¨äºçœŸå®è¯„ä¼°ï¼‰
        if self.config['classification_anomaly_ratio'] > 0:
            y_train = self.add_label_anomalies(y_train, self.config['classification_anomaly_ratio'], 'classification')
        
        # åˆ†å‰²è®­ç»ƒæ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆéªŒè¯é›†ä¹Ÿæœ‰å¼‚å¸¸ï¼‰
        stratify_val = y_train if self.config['stratify_classification'] else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=self.config['validation_fraction'], random_state=self.config['random_state'], stratify=stratify_val)
        
        n_classes = len(np.unique(y))
        
        if self.config['verbose']:
            print(f"æ•°æ®é›†: {dataset_name}")
            print(f"ç‰¹å¾æ•°: {X.shape[1]}, æ ·æœ¬æ•°: {X.shape[0]}, ç±»åˆ«æ•°: {n_classes}")
            print(f"è®­ç»ƒé›†: {X_train.shape[0]}, éªŒè¯é›†: {X_val.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")
            if self.config['classification_anomaly_ratio'] > 0:
                print(f"æ ‡ç­¾å¼‚å¸¸: {self.config['classification_anomaly_ratio']:.1%} (æ ‡ç­¾ç¿»è½¬) - ä»…å½±å“train+valï¼Œtestä¿æŒå¹²å‡€")
            print()
        
        results = {}
        
        # è¯„ä¼°å‡½æ•°
        def evaluate_classification(y_true, y_pred):
            avg_method = 'binary' if n_classes == 2 else 'macro'
            return {
                'Acc': accuracy_score(y_true, y_pred),
                'Precision': precision_score(y_true, y_pred, average=avg_method, zero_division=0),
                'Recall': recall_score(y_true, y_pred, average=avg_method, zero_division=0),
                'F1': f1_score(y_true, y_pred, average=avg_method, zero_division=0)
            }
        
        # 1. sklearn MLPClassifier
        if self.config['verbose']: print("è®­ç»ƒ sklearn MLPClassifier...")
        sklearn_clf = MLPClassifier(
            hidden_layer_sizes=self.config['hidden_layer_sizes'],
            max_iter=self.config['max_iter'],
            learning_rate_init=self.config['learning_rate'],
            early_stopping=self.config['early_stopping'],
            validation_fraction=self.config['validation_fraction'],
            n_iter_no_change=self.config['n_iter_no_change'],
            tol=self.config['tol'],
            random_state=self.config['random_state'],
            alpha=self.config['alpha']
        )
        sklearn_clf.fit(X_train, y_train)
        sklearn_pred_test = sklearn_clf.predict(X_test)
        sklearn_pred_val = sklearn_clf.predict(X_val)
        
        results['sklearn'] = {
            'test': evaluate_classification(y_test, sklearn_pred_test),
            'val': evaluate_classification(y_val, sklearn_pred_val)
        }
        
        # 2. PyTorchåŸºçº¿
        if self.config['verbose']: print("è®­ç»ƒ PyTorchåŸºçº¿...")
        pytorch_model = self.create_pytorch_model(X.shape[1], n_classes, self.config['hidden_layer_sizes'], 'classification')
        pytorch_model = self.train_pytorch_model(
            pytorch_model, X_train, y_train, X_val, y_val,
            epochs=self.config['max_iter'], lr=self.config['learning_rate'], task='classification',
            patience=self.config['n_iter_no_change'], tol=self.config['tol'])
        
        pytorch_model.eval()
        with torch.no_grad():
            pytorch_outputs_test = pytorch_model(torch.FloatTensor(X_test))
            pytorch_pred_test = torch.argmax(pytorch_outputs_test, dim=1).numpy()
            pytorch_outputs_val = pytorch_model(torch.FloatTensor(X_val))
            pytorch_pred_val = torch.argmax(pytorch_outputs_val, dim=1).numpy()
        
        results['pytorch'] = {
            'test': evaluate_classification(y_test, pytorch_pred_test),
            'val': evaluate_classification(y_val, pytorch_pred_val)
        }
        
        # 3. CausalEngineå¤šæ¨¡å¼æµ‹è¯•
        for mode in self.config['test_modes']:
            if self.config['verbose']: print(f"è®­ç»ƒ CausalEngine ({mode})...")
            
            causal_clf = MLPCausalClassifier(
                hidden_layer_sizes=self.config['hidden_layer_sizes'],
                mode=mode,
                gamma_init=self.config['gamma_init'],
                b_noise_init=self.config['b_noise_init'],
                b_noise_trainable=self.config['b_noise_trainable'],
                ovr_threshold_init=self.config['ovr_threshold_init'],
                max_iter=self.config['max_iter'],
                learning_rate=self.config['learning_rate'],
                early_stopping=self.config['early_stopping'],
                random_state=self.config['random_state'],
                verbose=False
            )
            
            causal_clf.fit(X_train, y_train)
            causal_pred_test = causal_clf.predict(X_test)
            causal_pred_val = causal_clf.predict(X_val)
            
            if isinstance(causal_pred_test, dict):
                causal_pred_test = causal_pred_test['predictions']
            if isinstance(causal_pred_val, dict):
                causal_pred_val = causal_pred_val['predictions']
            
            results[mode] = {
                'test': evaluate_classification(y_test, causal_pred_test),
                'val': evaluate_classification(y_val, causal_pred_val),
                'model': causal_clf
            }
            
            # æ¼”ç¤ºæ¦‚ç‡é¢„æµ‹å’Œåˆ†å¸ƒé¢„æµ‹
            if self.config['show_distribution_examples']:
                causal_proba = causal_clf.predict_proba(X_test[:self.config['n_distribution_samples']])
                if self.config['verbose']:
                    print(f"  æ¦‚ç‡é¢„æµ‹å½¢çŠ¶: {causal_proba.shape}")
                    print(f"  å‰{self.config['n_distribution_samples']}æ ·æœ¬æ¦‚ç‡åˆ†å¸ƒ:\n{causal_proba}")
                
                # æ¼”ç¤ºåˆ†å¸ƒé¢„æµ‹ï¼ˆédeterministicæ¨¡å¼ï¼‰
                if mode != 'deterministic':
                    dist_proba = causal_clf.predict_dist(X_test[:self.config['n_distribution_samples']])
                    if self.config['verbose']:
                        print(f"  OvRæ¿€æ´»æ¦‚ç‡å½¢çŠ¶: {dist_proba.shape}")
        
        # æ˜¾ç¤ºç»“æœ
        if self.config['verbose']:
            print(f"\nğŸ“Š çœŸå®æ•°æ®é›†{n_classes}åˆ†ç±»ç»“æœå¯¹æ¯”:")
            print("=" * 120)
            print(f"{'æ–¹æ³•':<15} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
            print(f"{'':15} {'Acc':<10} {'Precision':<12} {'Recall':<10} {'F1':<10} {'Acc':<10} {'Precision':<12} {'Recall':<10} {'F1':<10}")
            print("-" * 120)
            # æ˜¾ç¤ºé¡ºåºï¼šsklearn, pytorch, ç„¶åæŒ‰test_modesé¡ºåº
            display_order = ['sklearn', 'pytorch'] + self.config['test_modes']
            for method in display_order:
                if method in results:
                    metrics = results[method]
                    val_m = metrics['val']
                    test_m = metrics['test']
                    print(f"{method:<15} {val_m['Acc']:<10.4f} {val_m['Precision']:<12.4f} {val_m['Recall']:<10.4f} {val_m['F1']:<10.4f} "
                          f"{test_m['Acc']:<10.4f} {test_m['Precision']:<12.4f} {test_m['Recall']:<10.4f} {test_m['F1']:<10.4f}")
            print("=" * 120)
        
        return results


def create_custom_config():
    """
    åˆ›å»ºè‡ªå®šä¹‰é…ç½®
    ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰å‚æ•°è¿›è¡Œç²¾ç»†è°ƒèŠ‚
    """
    custom_config = {
        # === ç½‘ç»œæ¶æ„ ===
        'hidden_layer_sizes': (128, 64),  # å¯è°ƒèŠ‚ï¼š(64, 32), (256, 128), (128, 64, 32)
        
        # === è®­ç»ƒå‚æ•° ===
        'max_iter': 2000,                # å¯è°ƒèŠ‚ï¼š1000, 3000, 5000
        'learning_rate': 0.001,          # å¯è°ƒèŠ‚ï¼š0.0001, 0.001, 0.01
        'early_stopping': True,          # å¯è°ƒèŠ‚ï¼šTrue/False
        'validation_fraction': 0.15,     # å¯è°ƒèŠ‚ï¼š0.1, 0.15, 0.2
        'n_iter_no_change': 50,          # å¯è°ƒèŠ‚ï¼š20, 50, 100
        'tol': 1e-5,                     # å¯è°ƒèŠ‚ï¼š1e-3, 1e-4, 1e-5
        'random_state': 42,              # å¯è°ƒèŠ‚ï¼šä»»æ„æ•´æ•°
        'alpha': 0.0001,                 # sklearnæ­£åˆ™åŒ–ï¼Œå¯è°ƒèŠ‚ï¼š0, 0.0001, 0.001
        
        # === CausalEngineä¸“æœ‰å‚æ•° ===
        'gamma_init': 10.0,              # AbductionNetworkå°ºåº¦åˆå§‹åŒ–ï¼Œå¯è°ƒèŠ‚ï¼š1.0, 10.0, 100.0
        'b_noise_init': 0.1,             # ActionNetworkå¤–ç”Ÿå™ªå£°ï¼Œå¯è°ƒèŠ‚ï¼š0.01, 0.1, 1.0
        'b_noise_trainable': True,       # å¤–ç”Ÿå™ªå£°æ˜¯å¦å¯è®­ç»ƒï¼Œå¯è°ƒèŠ‚ï¼šTrue/False
        'ovr_threshold_init': 0.0,       # OvRåˆ†ç±»é˜ˆå€¼ï¼Œå¯è°ƒèŠ‚ï¼š0.0, 0.3, 0.5, 0.7
        
        # === æµ‹è¯•æ¨¡å¼ ===
        # é€‰é¡¹ä¸€ï¼šåªæµ‹è¯•ä¸¤ä¸ªä¸»è¦æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        'test_modes': ['deterministic', 'standard'],
        
        # é€‰é¡¹äºŒï¼šæµ‹è¯•æ‰€æœ‰äº”æ¨¡å¼ï¼ˆå…¨é¢æµ‹è¯•ï¼‰
        # 'test_modes': ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling'],
        
        # é€‰é¡¹ä¸‰ï¼šåªæµ‹è¯•å› æœæ¨¡å¼
        # 'test_modes': ['standard', 'sampling'],
        
        # === æ•°æ®é›†é€‰æ‹© ===
        # å›å½’æ•°æ®é›†é€‰æ‹©
        'regression_dataset': 'boston_housing',  # å¯é€‰ï¼š
        # - california_housing: åŠ å·æˆ¿ä»·ï¼ˆ20kæ ·æœ¬ï¼Œç»å…¸ï¼‰â­ æ¨è
        # - diabetes: ç³–å°¿ç—…æ•°æ®é›†ï¼ˆ442â†’1442æ ·æœ¬ï¼ŒåŒ»å­¦ï¼‰
        # - boston_housing: æ³¢å£«é¡¿æˆ¿ä»·ï¼ˆ506â†’2kæ ·æœ¬ï¼Œç»å…¸ï¼‰
        # - auto_mpg: æ±½è½¦æ²¹è€—ï¼ˆ400â†’1.6kæ ·æœ¬ï¼Œå·¥ç¨‹ï¼‰
        # - wine_quality_reg: çº¢é…’è´¨é‡å›å½’ï¼ˆ1.6kâ†’2.6kæ ·æœ¬ï¼Œè´¨é‡è¯„ä¼°ï¼‰
        
        # åˆ†ç±»æ•°æ®é›†é€‰æ‹©ï¼ˆâ­æ¨èforest_coverå’Œletter_recognitionï¼‰
        'classification_dataset': 'wine_quality',  # å¯é€‰ï¼š
        # - wine_quality: çº¢é…’è´¨é‡é¢„æµ‹ï¼ˆ3ç±»ï¼Œå¹³è¡¡ï¼‰â­ æ¨è
        # - breast_cancer: ä¹³è…ºç™Œæ£€æµ‹ï¼ˆ2ç±»ï¼Œæ‰©å±•ç‰ˆï¼‰
        # - german_credit: å¾·å›½ä¿¡ç”¨é£é™©ï¼ˆ2ç±»ï¼Œæ··åˆç‰¹å¾ï¼‰
        # - fraud_detection: ä¿¡ç”¨å¡æ¬ºè¯ˆï¼ˆ2ç±»ï¼Œæåº¦ä¸å¹³è¡¡ï¼‰
        # - bank_marketing: é“¶è¡Œè¥é”€ï¼ˆ2ç±»ï¼Œé«˜ç»´åº¦ï¼‰
        # - forest_cover: æ£®æ—è¦†ç›–ç±»å‹ï¼ˆ7ç±»ï¼‰â­ æ¨è
        # - letter_recognition: å­—æ¯è¯†åˆ«ï¼ˆ26ç±»ï¼‰â­ æ¨è
        
        # === ä»»åŠ¡é€‰æ‹© ===
        'run_regression': True,          # æ˜¯å¦è¿è¡Œå›å½’ä»»åŠ¡
        'run_classification': False,      # æ˜¯å¦è¿è¡Œåˆ†ç±»ä»»åŠ¡
        
        # === æ•°æ®é›†é…ç½® ===
        'test_size': 0.2,                # æµ‹è¯•é›†æ¯”ä¾‹ï¼Œå¯è°ƒèŠ‚ï¼š0.1, 0.2, 0.3
        'stratify_classification': True,  # åˆ†ç±»ä»»åŠ¡æ˜¯å¦åˆ†å±‚é‡‡æ ·
        
        # === æ ‡ç­¾å¼‚å¸¸é…ç½® ===
        'regression_anomaly_ratio': 0.3,  # å›å½’æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ï¼Œå¯è°ƒèŠ‚ï¼š0.0, 0.1, 0.2, 0.3
        'classification_anomaly_ratio': 0.3,  # åˆ†ç±»æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ï¼Œå¯è°ƒèŠ‚ï¼š0.0, 0.1, 0.2, 0.3
        # æ³¨æ„ï¼šæ ‡ç­¾å¼‚å¸¸ä»…å½±å“è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ä¿æŒå¹²å‡€ä»¥è¿›è¡Œå…¬å¹³è¯„ä¼°
        # å›å½’å¼‚å¸¸ï¼š3å€æ ‡å‡†å·®åç§»æˆ–10å€ç¼©æ”¾
        # åˆ†ç±»å¼‚å¸¸ï¼šéšæœºæ ‡ç­¾ç¿»è½¬
        
        # === è¾“å‡ºæ§åˆ¶ ===
        'verbose': True,                 # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        'show_distribution_examples': True,  # æ˜¯å¦æ˜¾ç¤ºåˆ†å¸ƒé¢„æµ‹ç¤ºä¾‹
        'n_distribution_samples': 3,     # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬çš„åˆ†å¸ƒå‚æ•°
    }
    
    return custom_config


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒçœŸå®æ•°æ®é›†åŸºå‡†æµ‹è¯• - å¯é…ç½®å‚æ•°ç‰ˆæœ¬"""
    print("ğŸš€ CausalEngineçœŸå®æ•°æ®é›†åŸºå‡†æµ‹è¯• - å¯é…ç½®å‚æ•°ç‰ˆæœ¬")
    print("=" * 80)
    print("åŸºäºquick_test_causal_engine.pyå’Œdemo_sklearn_interface_v2.pyçš„ä¼˜ç§€æ¶æ„")
    print("ä¸“é—¨é’ˆå¯¹çœŸå®æ•°æ®é›†è¿›è¡Œæµ‹è¯•")
    print("æ”¯æŒæ‰‹åŠ¨é…ç½®æ‰€æœ‰å…³é”®å‚æ•°è¿›è¡Œç²¾ç»†è°ƒèŠ‚")
    print()
    
    # åˆ›å»ºå¯é…ç½®çš„åŸºå‡†æµ‹è¯•å™¨
    custom_config = create_custom_config()
    benchmark = RealWorldBenchmark(config=custom_config)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    benchmark.print_config()
    print()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(custom_config['random_state'])
    np.random.seed(custom_config['random_state'])
    
    try:
        # æ ¹æ®é…ç½®è¿è¡Œå¯¹åº”çš„æµ‹è¯•
        regression_results = None
        classification_results = None
        
        # å›å½’åŸºå‡†æµ‹è¯•
        if custom_config['run_regression']:
            print("ğŸ  å›å½’åŸºå‡†æµ‹è¯•")
            print("=" * 60)
            print(f"æ•°æ®é›†: {custom_config['regression_dataset']}")
            regression_results = benchmark.benchmark_regression(dataset_name=custom_config['regression_dataset'])
        
        # åˆ†ç±»åŸºå‡†æµ‹è¯•
        if custom_config['run_classification']:
            if custom_config['run_regression']:
                print("\n" + "="*80 + "\n")
            
            print("ğŸ¯ åˆ†ç±»åŸºå‡†æµ‹è¯•")
            print("=" * 60)
            print(f"æ•°æ®é›†: {custom_config['classification_dataset']}")
            
            # å¯¹ä¸å¹³è¡¡æ•°æ®é›†è‡ªåŠ¨è°ƒæ•´å‚æ•°ï¼ˆå¯åœ¨é…ç½®ä¸­è¦†ç›–ï¼‰
            if custom_config['classification_dataset'] in ['fraud_detection', 'bank_marketing']:
                print(f"ğŸ’¡ æ£€æµ‹åˆ°ä¸å¹³è¡¡æ•°æ®é›†ï¼Œå½“å‰ovr_threshold_init={custom_config['ovr_threshold_init']}")
            
            classification_results = benchmark.benchmark_classification(dataset_name=custom_config['classification_dataset'])
        
        # ç»“æœæ€»ç»“
        print("\n" + "="*80)
        print("ğŸ“Š å¯é…ç½®å‚æ•°åŸºå‡†æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*80)
        
        if regression_results:
            print(f"\nğŸ  å›å½’ä»»åŠ¡è¡¨ç° (RÂ²å¾—åˆ†) - {custom_config['regression_dataset']}:")
            for method in ['sklearn', 'pytorch'] + custom_config['test_modes']:
                if method in regression_results:
                    r2 = regression_results[method]['test']['RÂ²']
                    print(f"  {method:<12}: {r2:.4f}")
        
        if classification_results:
            print(f"\nğŸ¯ åˆ†ç±»ä»»åŠ¡è¡¨ç° (å‡†ç¡®ç‡) - {custom_config['classification_dataset']}:")
            for method in ['sklearn', 'pytorch'] + custom_config['test_modes']:
                if method in classification_results:
                    acc = classification_results[method]['test']['Acc']
                    print(f"  {method:<12}: {acc:.4f}")
        
        print("\nğŸ¯ å¯é…ç½®å‚æ•°åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print("CausalEngineåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å·²å¾—åˆ°å…¨é¢éªŒè¯ã€‚")
        print("\nğŸ”§ å‚æ•°è°ƒèŠ‚å»ºè®®:")
        print("- ä¿®æ”¹ create_custom_config() å‡½æ•°ä¸­çš„å‚æ•°è¿›è¡Œç²¾ç»†è°ƒèŠ‚")
        print("- å°è¯•ä¸åŒçš„ gamma_init å’Œ b_noise_init å€¼")
        print("- å¯ç”¨äº”æ¨¡å¼æµ‹è¯•è¿›è¡Œå…¨é¢å¯¹æ¯”")
        print("- æ›´æ¢ä¸åŒçš„æ•°æ®é›†ï¼šclassification_dataset å’Œ regression_dataset")
        
        print("\nâš ï¸ æ ‡ç­¾å¼‚å¸¸åŠŸèƒ½ï¼ˆå‚è€ƒè‡ªquick_test_causal_engine.pyï¼‰:")
        print("- è®¾ç½® regression_anomaly_ratio å’Œ classification_anomaly_ratio æ¥æ·»åŠ æ ‡ç­¾å™ªå£°")
        print("- å¼‚å¸¸ä»…å½±å“è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ä¿æŒå¹²å‡€ä»¥è¿›è¡Œå…¬å¹³è¯„ä¼°")
        print("- å›å½’å¼‚å¸¸ï¼š3å€æ ‡å‡†å·®åç§»æˆ–10å€ç¼©æ”¾")
        print("- åˆ†ç±»å¼‚å¸¸ï¼šéšæœºæ ‡ç­¾ç¿»è½¬")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    main()
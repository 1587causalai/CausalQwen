"""
CausalQwen MVP: æ ¸å¿ƒæ¨¡å‹å®ç°
ä½¿ç”¨å ä½å¼é€»è¾‘å¿«é€Ÿæ­å»ºæ¡†æ¶ï¼Œåç»­é€æ­¥å®Œå–„å…·ä½“å®ç°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple, Union
from transformers import Qwen2ForCausalLM, Qwen2Config, PretrainedConfig
from transformers.modeling_outputs import ModelOutput


@dataclass
class CausalMVPOutput(ModelOutput):
    """CausalQwen MVPè¾“å‡ºç»“æ„"""
    loss: Optional[torch.FloatTensor] = None
    loc_S: torch.FloatTensor = None
    scale_S: torch.FloatTensor = None
    loc_U: torch.FloatTensor = None
    scale_U: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    next_token_ids: Optional[torch.LongTensor] = None  # ç”¨äºå…¼å®¹æ¨¡å¼çš„é‡‡æ ·ç»“æœ


class CausalQwen2Config(Qwen2Config):
    """æ‰©å±•Qwen2Configä»¥æ”¯æŒå› æœæ¨¡å‹å‚æ•°"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # å› æœæ¨¡å‹ç‰¹æœ‰å‚æ•°
        self.causal_size = kwargs.get('causal_size', self.hidden_size)
        self.abduction_init_strategy = kwargs.get('abduction_init_strategy', 'identity')
        self.b_noise_init = kwargs.get('b_noise_init', 0.1)
        self.ovr_threshold_init = kwargs.get('ovr_threshold_init', 0.0)
        self.gamma_init = kwargs.get('gamma_init', 10.0)  # AbductionNetworkå°ºåº¦åˆå§‹åŒ–
        self.inference_mode = kwargs.get('inference_mode', 'standard')


class CauchyMath:
    """Cauchyåˆ†å¸ƒæ•°å­¦å·¥å…·ç±»ï¼Œå®ç°ä¸¥æ ¼çš„çº¿æ€§ç¨³å®šæ€§"""
    
    @staticmethod
    def cauchy_linear_stable_loc(loc_input, weight, bias=None):
        """Cauchyåˆ†å¸ƒä½ç½®å‚æ•°çš„çº¿æ€§å˜æ¢"""
        # ä½ç½®å‚æ•°å˜æ¢ï¼šç›´æ¥çŸ©é˜µä¹˜æ³•
        result = loc_input @ weight.T
        if bias is not None:
            result = result + bias
        return result
    
    @staticmethod  
    def cauchy_linear_stable_scale(scale_input, weight):
        """Cauchyåˆ†å¸ƒå°ºåº¦å‚æ•°çš„çº¿æ€§å˜æ¢"""
        # å°ºåº¦å‚æ•°å˜æ¢ï¼šç›´æ¥çŸ©é˜µä¹˜æ³•
        return scale_input @ torch.abs(weight).T


class AbductionNetwork(nn.Module):
    """å½’å› ç½‘ç»œï¼šä»éšè—çŠ¶æ€æ¨æ–­ä¸ªä½“è¡¨å¾åˆ†å¸ƒ"""
    
    def __init__(self, config: CausalQwen2Config):
        super().__init__()
        self.config = config
        
        # ä¿®æ­£ï¼šæ·»åŠ biasé¡¹ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è¦æ±‚
        self.loc_net = nn.Linear(config.hidden_size, config.causal_size, bias=True)
        self.scale_net = nn.Linear(config.hidden_size, config.causal_size, bias=True)
        
        self._init_identity_mapping()
    
    def _init_identity_mapping(self):
        """åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£"""
        with torch.no_grad():
            if self.config.hidden_size == self.config.causal_size:
                # loc_netæ’ç­‰æ˜ å°„åˆå§‹åŒ–
                self.loc_net.weight.copy_(torch.eye(self.config.causal_size))
                self.loc_net.bias.zero_()
                
                # scale_netåˆå§‹åŒ–ï¼šweight=0, bias=Î³_init äº§ç”Ÿå®½åˆ†å¸ƒ
                self.scale_net.weight.zero_()
                self.scale_net.bias.fill_(self.config.gamma_init)
            else:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(self.loc_net.weight)
                nn.init.zeros_(self.loc_net.bias)
                nn.init.xavier_uniform_(self.scale_net.weight)
                self.scale_net.weight.data *= 0.1
                self.scale_net.bias.fill_(self.config.gamma_init)
    
    def forward(self, hidden_states):
        """å‰å‘ä¼ æ’­ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£çš„æ•°å­¦è¦æ±‚"""
        # ä½ç½®å‚æ•°ï¼šæ ‡å‡†çº¿æ€§å˜æ¢
        loc_U = self.loc_net(hidden_states)
        
        # å°ºåº¦å‚æ•°ï¼šä½¿ç”¨softplusç¡®ä¿æ­£æ€§ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£
        scale_U = F.softplus(self.scale_net(hidden_states))
        
        return loc_U, scale_U


class ActionNetwork(nn.Module):
    """è¡ŒåŠ¨ç½‘ç»œï¼šä»ä¸ªä½“è¡¨å¾åˆ°å†³ç­–åˆ†å¸ƒ"""
    
    def __init__(self, config: CausalQwen2Config):
        super().__init__()
        self.config = config
        
        # ä¿®æ­£ï¼šæ·»åŠ biasé¡¹ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è¦æ±‚
        self.lm_head = nn.Linear(config.causal_size, config.vocab_size, bias=True)
        
        # ä¿®æ­£ï¼šb_noiseç»´åº¦åº”è¯¥æ˜¯causal_sizeï¼Œç”¨äºå¤–ç”Ÿå™ªå£°èåˆ
        self.b_noise = nn.Parameter(torch.zeros(config.causal_size))
        
        self._init_from_original_lm_head()
    
    def _init_from_original_lm_head(self):
        """ä»åŸå§‹lm_headå¤åˆ¶æƒé‡ï¼Œç¬¦åˆçŸ¥è¯†ç»§æ‰¿åŸåˆ™"""
        # å¤–ç”Ÿå™ªå£°åº”æœ‰åˆç†çš„åˆå§‹å€¼ï¼Œè€Œéå‡è®¾æ— å™ªå£°
        nn.init.constant_(self.b_noise, self.config.b_noise_init)
        
        # TODO: å½“æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨æ—¶ï¼Œåº”ä»å…¶å¤åˆ¶æƒé‡
        # ç›®å‰ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–ä½œä¸ºå¤‡é€‰
        nn.init.xavier_uniform_(self.lm_head.weight)
        nn.init.zeros_(self.lm_head.bias)
        
    def copy_weights_from_qwen(self, qwen_model):
        """ä»é¢„è®­ç»ƒQwen2æ¨¡å‹å¤åˆ¶lm_headæƒé‡"""
        if hasattr(qwen_model, 'lm_head'):
            print("æ­£åœ¨å¤åˆ¶Qwen2é¢„è®­ç»ƒæƒé‡...")
            with torch.no_grad():
                # ç¡®ä¿vocab_sizeä¸€è‡´ï¼ˆåŒ…å«é¢„ç•™è¯æ±‡ï¼‰
                if qwen_model.lm_head.weight.shape == self.lm_head.weight.shape:
                    self.lm_head.weight.copy_(qwen_model.lm_head.weight)
                    if hasattr(qwen_model.lm_head, 'bias') and qwen_model.lm_head.bias is not None:
                        self.lm_head.bias.copy_(qwen_model.lm_head.bias)
                    print(f"âœ… æˆåŠŸå¤åˆ¶æƒé‡ï¼Œè¯æ±‡è¡¨å¤§å°: {qwen_model.lm_head.weight.shape[0]}")
                else:
                    print(f"âŒ æƒé‡å½¢çŠ¶ä¸åŒ¹é…: Qwen({qwen_model.lm_head.weight.shape}) vs CausalQwen({self.lm_head.weight.shape})")
                    print("ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–...")
        else:
            print("âŒ æºæ¨¡å‹æ²¡æœ‰lm_headï¼Œä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–...")
        
    def forward(self, loc_U, scale_U=None, do_sample=False, temperature=1.0):
        """å‰å‘ä¼ æ’­ - V2é©å‘½æ€§è®¾è®¡ï¼šä½ç½®vså°ºåº¦çš„ç²¾å¦™å·®å¼‚
        
        V2æ ¸å¿ƒåˆ›æ–°ï¼šå™ªå£°å¯¹é‡‡æ ·/éé‡‡æ ·æ¨¡å¼çš„ä¸åŒå½±å“æ–¹å¼
        
        é‡‡æ ·æ¨¡å¼ï¼šå™ªå£°å½±å“ä½ç½®å‚æ•°
        â”œâ”€ Îµ ~ Cauchy(0, 1) æ ‡å‡†å™ªå£°é‡‡æ ·
        â”œâ”€ U' ~ Cauchy(Î¼ + TÂ·|b_noise|Â·Îµ, Î³) å™ªå£°æ³¨å…¥ä½ç½®å‚æ•°
        â””â”€ æ‰°åŠ¨ä¸ªä½“èº«ä»½ï¼Œä¿æŒåŸæœ‰ä¸ç¡®å®šæ€§
        
        éé‡‡æ ·æ¨¡å¼ï¼šå™ªå£°å½±å“å°ºåº¦å‚æ•°  
        â”œâ”€ U' ~ Cauchy(Î¼, Î³ + |b_noise|) å™ªå£°èåˆåˆ°å°ºåº¦
        â””â”€ ä¿æŒä¸ªä½“èº«ä»½ï¼Œå¢åŠ å†³ç­–ä¸ç¡®å®šæ€§
        
        Args:
            loc_U: ä¸ªä½“è¡¨å¾åˆ†å¸ƒçš„ä½ç½®å‚æ•° [B, S, C]
            scale_U: ä¸ªä½“è¡¨å¾åˆ†å¸ƒçš„å°ºåº¦å‚æ•° [B, S, C]
            do_sample: æ˜¯å¦è¿›è¡Œé‡‡æ ·ï¼ˆå†³å®šå™ªå£°ä½œç”¨æ–¹å¼ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦å‚æ•°ï¼ˆä»…åœ¨do_sample=Trueæ—¶ç”Ÿæ•ˆï¼‰
        Returns:
            loc_S: å†³ç­–åˆ†å¸ƒçš„ä½ç½®å‚æ•° [B, S, V]
            scale_S: å†³ç­–åˆ†å¸ƒçš„å°ºåº¦å‚æ•° [B, S, V]
        """
        # å¤„ç†é»˜è®¤å°ºåº¦å‚æ•°
        if scale_U is None:
            scale_U = torch.zeros_like(loc_U)  # é»˜è®¤ä¸ºç¡®å®šæ€§åˆ†å¸ƒ
        
        if do_sample:
            # ğŸ¯ V2é‡‡æ ·æ¨¡å¼ï¼šå™ªå£°å½±å“ä½ç½®å‚æ•°
            
            # Step 1: é‡‡æ ·æ ‡å‡†æŸ¯è¥¿å™ªå£° Îµ ~ Cauchy(0, I)
            uniform_sample = torch.rand_like(loc_U)
            epsilon = torch.tan(torch.pi * (uniform_sample - 0.5))
            
            # Step 2: æ¸©åº¦è°ƒèŠ‚çš„å™ªå£°æ³¨å…¥åˆ°ä½ç½®å‚æ•°
            # æ•°å­¦ï¼šloc_U_noisy = Î¼ + TÂ·|b_noise|Â·Îµ
            noise_injection = epsilon * temperature * torch.abs(self.b_noise)
            loc_U_noisy = loc_U + noise_injection
            
            # Step 3: åŸºäºæ‰°åŠ¨åçš„ä½ç½®å‚æ•°è¿›è¡Œçº¿æ€§å†³ç­–
            # æ•°å­¦ï¼šloc_S = WÂ·(Î¼ + TÂ·|b_noise|Â·Îµ) + b
            loc_S = self.lm_head(loc_U_noisy)
            
            # Step 4: å°ºåº¦å‚æ•°çš„çº¿æ€§ç¨³å®šæ€§å˜æ¢
            # æ•°å­¦ï¼šscale_S = Î³ Ã— |W|^T
            scale_S = scale_U @ torch.abs(self.lm_head.weight).T

        else:
            # ğŸ”§ V2éé‡‡æ ·æ¨¡å¼ï¼šå™ªå£°å½±å“å°ºåº¦å‚æ•°
            
            # Step 1: å¤–ç”Ÿå™ªå£°èåˆåˆ°å°ºåº¦å‚æ•°
            # æ•°å­¦ï¼šscale_U_noisy = Î³ + |b_noise|
            scale_U_noisy = scale_U + torch.abs(self.b_noise)
            
            # Step 2: ä½ç½®å‚æ•°ä¿æŒç¡®å®šæ€§çš„çº¿æ€§å˜æ¢
            # æ•°å­¦ï¼šloc_S = WÂ·Î¼ + b
            loc_S = self.lm_head(loc_U)
            
            # Step 3: å°ºåº¦å‚æ•°çš„çº¿æ€§ç¨³å®šæ€§å˜æ¢
            # æ•°å­¦ï¼šscale_S = (Î³ + |b_noise|) Ã— |W|^T
            scale_S = scale_U_noisy @ torch.abs(self.lm_head.weight).T
        
        return loc_S, scale_S


class OvRClassifier(nn.Module):
    """One-vs-Reståˆ†ç±»å™¨"""
    
    def __init__(self, config: CausalQwen2Config):
        super().__init__()
        self.config = config
        self.thresholds = nn.Parameter(torch.full((config.vocab_size,), config.ovr_threshold_init))
    
    def forward(self, loc_S, scale_S):
        """è®¡ç®—OvRæ¦‚ç‡ - å ä½å®ç°"""
        # TODO: å®ç°ä¸¥æ ¼çš„Cauchyåˆ†å¸ƒCDFè®¡ç®—
        # å ä½ï¼šä½¿ç”¨ç®€åŒ–çš„æ¦‚ç‡è®¡ç®—
        # P(y_k=1) = P(Cauchy(loc_S_k, scale_S_k) > threshold_k)
        normalized_diff = (loc_S - self.thresholds.unsqueeze(0).unsqueeze(0)) / scale_S
        # ä½¿ç”¨atanè¿‘ä¼¼Cauchy CDF: P = 0.5 + (1/Ï€) * atan(x)
        probs = 0.5 + (1/torch.pi) * torch.atan(normalized_diff)
        return probs


class CausalQwenMVPForCausalLM(Qwen2ForCausalLM):
    """CausalQwen MVPä¸»æ¨¡å‹ç±»"""
    
    config_class = CausalQwen2Config
    
    def __init__(self, config: CausalQwen2Config):
        super().__init__(config)
        
        # æ·»åŠ å› æœæ¨¡å—
        self.abduction_network = AbductionNetwork(config)
        self.action_network = ActionNetwork(config)  
        self.ovr_classifier = OvRClassifier(config)
        
        # åˆå§‹åŒ–å› æœæƒé‡
        self._init_causal_weights()
    
    def _init_causal_weights(self):
        """åˆå§‹åŒ–å› æœæ¨¡å—æƒé‡"""
        # å› æœæ¨¡å—å·²åœ¨å„è‡ªçš„__init__ä¸­å®Œæˆåˆå§‹åŒ–
        pass
    
    def copy_pretrained_weights(self, qwen_model_path_or_model):
        """ä»é¢„è®­ç»ƒQwen2æ¨¡å‹å¤åˆ¶æƒé‡"""
        if isinstance(qwen_model_path_or_model, str):
            from transformers import Qwen2ForCausalLM
            print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {qwen_model_path_or_model}")
            qwen_model = Qwen2ForCausalLM.from_pretrained(qwen_model_path_or_model)
        else:
            qwen_model = qwen_model_path_or_model
            
        # å¤åˆ¶ActionNetworkçš„lm_headæƒé‡
        self.action_network.copy_weights_from_qwen(qwen_model)
        
        # éªŒè¯vocab_sizeä¸€è‡´æ€§ï¼ˆåŒ…å«é¢„ç•™è¯æ±‡ï¼‰
        if hasattr(qwen_model, 'config') and hasattr(qwen_model.config, 'vocab_size'):
            expected_vocab_size = qwen_model.config.vocab_size
            actual_vocab_size = self.config.vocab_size
            if expected_vocab_size != actual_vocab_size:
                print(f"âš ï¸  è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: æœŸæœ› {expected_vocab_size}, å®é™… {actual_vocab_size}")
                print("è¯·ç¡®ä¿é…ç½®ä¸­çš„vocab_sizeåŒ…å«äº†æ‰€æœ‰é¢„ç•™è¯æ±‡")
            else:
                print(f"âœ… è¯æ±‡è¡¨å¤§å°ä¸€è‡´: {actual_vocab_size} (åŒ…å«é¢„ç•™è¯æ±‡)")
        
        print("æƒé‡å¤åˆ¶å®Œæˆï¼")
    
    def generate(self, input_ids, max_new_tokens=20, do_sample=True, temperature=1.0,
                top_k=None, top_p=None, pad_token_id=None, eos_token_id=None, **kwargs):
        """åºåˆ—ç”Ÿæˆ - CausalQwenä¸“ç”¨æ¨ç†ï¼ˆä¸ä½¿ç”¨ä¼ ç»Ÿé‡‡æ ·ï¼‰"""
        from .inference import CausalInferenceEngine
        engine = CausalInferenceEngine(self)
        return engine.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            pad_token_id=pad_token_id,
            eos_token_id=eos_token_id,
            **kwargs
        )
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        do_sample: Optional[bool] = False,
        temperature: Optional[float] = 1.0,
        **kwargs
    ) -> Union[Tuple, CausalMVPOutput]:
        """å‰å‘ä¼ æ’­ - V2æ¡†æ¶å®ç°
        
        V2æ ¸å¿ƒç‰¹æ€§ï¼š
        - do_sample=False: éé‡‡æ ·æ¨¡å¼ï¼Œå™ªå£°å½±å“å°ºåº¦å‚æ•°
        - do_sample=True: é‡‡æ ·æ¨¡å¼ï¼Œå™ªå£°å½±å“ä½ç½®å‚æ•°
        """
        
        # 1. è·å–Transformerç‰¹å¾
        transformer_outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
        )
        hidden_states = transformer_outputs[0]
        
        # 2. V2å› æœæ¨ç†é“¾è·¯
        loc_U, scale_U = self.abduction_network(hidden_states)  # ä¸ªä½“æ¨æ–­
        loc_S, scale_S = self.action_network(
            loc_U, scale_U, 
            do_sample=do_sample, 
            temperature=temperature
        )  # V2å†³ç­–æ¨æ–­
        
        # 3. æŸå¤±è®¡ç®—
        loss = None
        if labels is not None:
            probs = self.ovr_classifier(loc_S, scale_S)
            loss = self._compute_ovr_loss(probs, labels)
        
        return CausalMVPOutput(
            loss=loss,
            loc_S=loc_S,
            scale_S=scale_S,
            loc_U=loc_U,
            scale_U=scale_U,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states if output_hidden_states else None,
            attentions=transformer_outputs.attentions if output_attentions else None,
        )
    
    def _compute_ovr_loss(self, probs, labels):
        """è®¡ç®—OvRæŸå¤± - å ä½å®ç°"""
        # TODO: å®ç°æ›´sophisticatedçš„æŸå¤±å‡½æ•°
        # ç®€åŒ–å®ç°ï¼šäºŒå…ƒäº¤å‰ç†µæŸå¤±
        targets = F.one_hot(labels, num_classes=self.config.vocab_size).float()
        loss = F.binary_cross_entropy(probs, targets, reduction='mean')
        return loss
    
    def debug_forward_pass(self, input_ids):
        """è°ƒè¯•å‰å‘ä¼ æ’­çš„æ¯ä¸ªæ­¥éª¤"""
        with torch.no_grad():
            # 1. Transformerè¾“å‡º
            transformer_outputs = self.model(input_ids)
            print(f"Hidden states shape: {transformer_outputs[0].shape}")
            
            # 2. å½’å› ç½‘ç»œè¾“å‡º
            loc_U, scale_U = self.abduction_network(transformer_outputs[0])
            print(f"U distribution - loc: {loc_U.mean():.4f}, scale: {scale_U.mean():.4f}")
            
            # 3. è¡ŒåŠ¨ç½‘ç»œè¾“å‡º
            loc_S, scale_S = self.action_network(loc_U, scale_U)
            print(f"S distribution - loc: {loc_S.mean():.4f}, scale: {scale_S.mean():.4f}")
            
            return loc_S, scale_S 
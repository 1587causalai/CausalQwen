"""
Action Network module.

This module implements the action network for the causal language model,
which transforms the latent causal state into classification and regression outputs.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict

from ..utils.distributions import CauchyLinear


class ClassificationHead(nn.Module):
    """
    Classification head for token prediction.
    
    This implements the One-vs-Rest (OvR) classification approach,
    where each class has an independent decision score.
    """
    
    def __init__(self, causal_dim, num_classes, threshold=0.0, bias=True):
        """
        Initialize the classification head.
        
        Args:
            causal_dim (int): Dimensionality of the latent causal state
            num_classes (int): Number of classes (vocabulary size)
            threshold (float, optional): Decision threshold. Defaults to 0.0.
            bias (bool, optional): Whether to include bias. Defaults to True.
        """
        super().__init__()
        self.causal_dim = causal_dim
        self.num_classes = num_classes
        self.threshold = threshold
        
        # Linear layer to map causal state to class decision scores
        # ÈáçË¶ÅÔºöÊ†πÊçÆ Qwen ÁöÑËÆæËÆ°ÔºåÂàÜÁ±ªÂ§¥‰∏çÂ∫îËØ•ÊúâÂÅèÁΩÆ
        self.causal_linear = CauchyLinear(causal_dim, num_classes, bias=bias)
        
        # Register threshold as a buffer (not a parameter)
        self.register_buffer('thresholds', torch.ones(num_classes) * threshold)
        
    def forward(self, causal_loc, causal_scale):
        """
        Transform causal state distribution to class decision score distributions.
        
        Args:
            causal_loc (torch.Tensor): Location parameter of causal state
                                      Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
            causal_scale (torch.Tensor): Scale parameter of causal state
                                        Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
        
        Returns:
            tuple: (score_loc, score_scale) - Parameters of the decision score distributions
                  Shape: [batch_size, num_classes] or [batch_size, seq_len, num_classes]
        """
        # Transform causal state distribution to decision score distributions
        score_loc, score_scale = self.causal_linear(causal_loc, causal_scale)
        
        return score_loc, score_scale
    
    def compute_probabilities(self, score_loc, score_scale):
        """
        Compute class probabilities using the Cauchy CDF.
        
        From core-design.md: P(S_k > C_k) = 1/2 + (1/œÄ) * arctan((loc_Sk - C_k)/scale_Sk)
        """
        # Add epsilon for numerical stability, especially if scale can be zero
        stable_scale = score_scale + 1e-9
        
        # Direct implementation of the formula from core-design.md
        # P(S_k > C_k) = 1/2 + (1/œÄ) * arctan((loc_Sk - C_k)/scale_Sk)
        probs = 0.5 + (1 / torch.pi) * torch.atan((score_loc - self.thresholds) / stable_scale)
        
        return probs
    
    def predict(self, score_loc, score_scale):
        """
        Make class predictions based on decision scores.
        
        Args:
            score_loc (torch.Tensor): Location parameters of decision scores
                                     Shape: [batch_size, num_classes]
            score_scale (torch.Tensor): Scale parameters of decision scores
                                       Shape: [batch_size, num_classes]
        
        Returns:
            torch.Tensor: Predicted class indices
                         Shape: [batch_size]
        """
        # Compute probabilities
        probs = self.compute_probabilities(score_loc, score_scale)
        
        # Return class with highest probability
        return torch.argmax(probs, dim=-1)


class RegressionHead(nn.Module):
    """
    Regression head for numerical prediction.
    """
    
    def __init__(self, causal_dim):
        """
        Initialize the regression head.
        
        Args:
            causal_dim (int): Dimensionality of the latent causal state
        """
        super().__init__()
        self.causal_dim = causal_dim
        
        # Linear layer to map causal state to regression value
        self.causal_linear = CauchyLinear(causal_dim, 1)
        
    def forward(self, causal_loc, causal_scale):
        """
        Transform causal state distribution to regression value distribution.
        
        Args:
            causal_loc (torch.Tensor): Location parameter of causal state
                                      Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
            causal_scale (torch.Tensor): Scale parameter of causal state
                                        Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
        
        Returns:
            tuple: (value_loc, value_scale) - Parameters of the regression value distribution
                  Shape: [batch_size] or [batch_size, seq_len]
        """
        # Transform causal state distribution to regression value distribution
        value_loc, value_scale = self.causal_linear(causal_loc, causal_scale)
        
        # Squeeze the last dimension
        # For sequence inputs: [batch_size, seq_len, 1] -> [batch_size, seq_len]
        # For single inputs: [batch_size, 1] -> [batch_size]
        return value_loc.squeeze(-1), value_scale.squeeze(-1)
    
    def predict(self, value_loc, value_scale):
        """
        Make regression predictions.
        
        For Cauchy distribution, the median (location parameter) is the best
        point estimate.
        
        Args:
            value_loc (torch.Tensor): Location parameter of regression value
                                     Shape: [batch_size]
            value_scale (torch.Tensor): Scale parameter of regression value
                                       Shape: [batch_size]
        
        Returns:
            torch.Tensor: Predicted regression values
                         Shape: [batch_size]
        """
        # For Cauchy distribution, the median (location parameter) is the best point estimate
        return value_loc


class ActionNetwork(nn.Module):
    """
    Action Network for the Causal Language Model.
    
    This network takes the parameters of the individual causal representation distribution
    and transforms them into the parameters for the output distributions
    (classification scores and regression values).
    """
    
    def __init__(self, input_dim: int, hidden_size: int, num_token_id: int, vocab_size: Optional[int] = None):
        """
        Initialize the Action Network.
        
        Args:
            input_dim (int): Dimensionality of the individual causal representation
            hidden_size (int): Qwen Ê®°ÂûãÁöÑÈöêËóèÁª¥Â∫¶
            num_token_id (int): The token ID for the <NUM> token.
            vocab_size (Optional[int]): Size of the vocabulary. If None, the classification head will not be initialized.
        """
        super().__init__()
        self.input_dim = input_dim
        self.hidden_size = hidden_size
        self.num_token_id = num_token_id
        
        # ÂàÜÁ±ªÂ§¥ - loc Âíå scale
        # Ê≥®ÊÑèÔºöÁé∞Âú®ÂàÜÁ±ªÂ§¥ÂèØËÉΩ‰∏ç‰ºöÂú®ËøôÈáåË¢´ÂàùÂßãÂåñ
        if vocab_size is not None:
            self.classification_head = nn.Linear(input_dim, vocab_size, bias=False)
            self.scale_S_head = nn.Linear(input_dim, vocab_size, bias=True)
        else:
            self.classification_head = None
            self.scale_S_head = None

        # ÂõûÂΩíÂ§¥ - loc Âíå scale
        self.regression_head = nn.Linear(input_dim, 1, bias=True)
        self.scale_Y_head = nn.Linear(input_dim, 1, bias=True)

    def init_weights(self, qwen_lm_head: Optional[nn.Parameter] = None):
        """
        Initialize the weights of the action network.
        
        Áü•ËØÜ‰º†ËæìÁ≠ñÁï•Ôºö
        - ÂàÜÁ±ªÂ§¥ÔºöÂ§çÂà∂ Qwen lm_head ÁöÑÊâÄÊúâÊùÉÈáçÔºàÊ≥®ÊÑèÔºöQwen Ê≤°ÊúâÂÅèÁΩÆÔºâ
        - ÂõûÂΩíÂ§¥Ôºö‰ΩøÁî®Â∞èÈöèÊú∫ÂàùÂßãÂåñÔºåÂÆûÁé∞ÂùáÂåÄÂÖàÈ™å
        
        Args:
            qwen_lm_head: Qwen's language model head (nn.Linear)
        """
        print("üîß ÂàùÂßãÂåñ ActionNetwork...")
        
        # --- 1. ÂàùÂßãÂåñ/ÂàõÂª∫ÂàÜÁ±ªÂ§¥ ---
        if qwen_lm_head is not None:
            qwen_vocab_size, qwen_hidden_size = qwen_lm_head.weight.shape
            
            # Â¶ÇÊûúÂàÜÁ±ªÂ§¥ËøòÊú™ÂàõÂª∫ÔºåÁé∞Âú®Ê†πÊçÆ qwen_lm_head ÁöÑÂ∞∫ÂØ∏ÂàõÂª∫ÂÆÉ
            if self.classification_head is None:
                print(f"   ÂàõÂª∫ÂàÜÁ±ªÂ§¥ÔºåÂ∞∫ÂØ∏: [{qwen_vocab_size}, {self.input_dim}]")
                self.classification_head = nn.Linear(self.input_dim, qwen_vocab_size, bias=False).to(qwen_lm_head.weight.device)
                self.scale_S_head = nn.Linear(self.input_dim, qwen_vocab_size, bias=True).to(qwen_lm_head.weight.device)

            print("üìö ‰ªé Qwen lm_head ËøõË°åÁü•ËØÜ‰º†Ëæì...")
            print(f"   Qwen lm_head: {list(qwen_lm_head.weight.shape)}")
            print(f"   CausalQwen ÂàÜÁ±ªÂ§¥: {list(self.classification_head.weight.shape)}")

            # È™åËØÅÂ∞∫ÂØ∏ÊòØÂê¶ÂåπÈÖç
            if self.classification_head.weight.shape != qwen_lm_head.weight.shape:
                 raise ValueError(
                    f"Â∞∫ÂØ∏‰∏çÂåπÈÖç! CausalQwen head ({self.classification_head.weight.shape}) "
                    f"vs Qwen lm_head ({qwen_lm_head.weight.shape})."
                )

            # ÂÆåÊï¥Â§çÂà∂ÊùÉÈáç
            self.classification_head.weight.data.copy_(qwen_lm_head.weight.data)
            print("   ‚úÖ ÊùÉÈáçÂÆåÂÖ®ËøÅÁßª")

            # È™åËØÅ Qwen lm_head ÊòØÂê¶ÊúâÂÅèÁΩÆÈ°π
            if hasattr(qwen_lm_head, 'bias') and qwen_lm_head.bias is not None:
                print("   - Ë≠¶Âëä: Qwen lm_head ÂåÖÂê´ÂÅèÁΩÆÈ°πÔºå‰ΩÜÂΩìÂâçÊ®°ÂûãÊú™‰ΩøÁî®„ÄÇ")
            else:
                print("   ‚úÖ Qwen lm_head Êó†ÂÅèÁΩÆÈ°πÔºàÁ¨¶ÂêàÈ¢ÑÊúüÔºâ")

        else:
            # Â¶ÇÊûúÊ≤°Êúâ qwen_lm_headÔºåÂàôÂøÖÈ°ªÂú® __init__ ‰∏≠Êèê‰æõ vocab_size
            if self.classification_head is None:
                raise ValueError("Ê≤°ÊúâÊèê‰æõ qwen_lm_headÔºåÊó†Ê≥ïÂàõÂª∫ÂàÜÁ±ªÂ§¥„ÄÇËØ∑Âú®ÂàùÂßãÂåñÊó∂Êèê‰æõ vocab_size„ÄÇ")
            print("   - Êú™Êèê‰æõ Qwen lm_headÔºå‰ΩøÁî®ÈöèÊú∫ÂàùÂßãÂåñÂàÜÁ±ªÂ§¥„ÄÇ")
            # ËøôÈáåÂèØ‰ª•Ê∑ªÂä†ÈöèÊú∫ÂàùÂßãÂåñÈÄªËæëÔºåÂ¶ÇÊûúÈúÄË¶ÅÁöÑËØù
            pass

        # --- 2. ÂàùÂßãÂåñÂõûÂΩíÂ§¥ ---
        print("\nüîß ÂàùÂßãÂåñÂõûÂΩíÂ§¥ÔºàÂ∞èÈöèÊú∫ÂàùÂßãÂåñÔºâ...")
        # ‰ΩøÁî®Â∞èÁöÑÈöèÊú∫ÊùÉÈáçÂàùÂßãÂåñÂõûÂΩíÂ§¥ÔºåÈÅøÂÖçÂú®ËÆ≠ÁªÉÂàùÊúü‰∫ßÁîüËøáÂ§ßÁöÑËæìÂá∫
        nn.init.xavier_uniform_(self.regression_head.weight, gain=0.01)
        nn.init.zeros_(self.regression_head.bias)
        print("   ‚úÖ ‰ΩøÁî® Xavier ÂàùÂßãÂåñ (gain=0.01) ÂÆûÁé∞ÂùáÂåÄÂÖàÈ™å")
        
        # --- 3. ÂàùÂßãÂåñÂ∞∫Â∫¶Â§¥ ---
        # scale_S ÁöÑÂÅèÁΩÆÈ°πÂàùÂßãÂåñ‰∏∫‰∏Ä‰∏™ËæÉÂ§ßÁöÑÊ≠£Êï∞ÔºåÁ°Æ‰øùÂàùÂßã‰∏çÁ°ÆÂÆöÊÄßÈ´ò
        # scale_Y ÁöÑÂÅèÁΩÆÈ°πÂàùÂßãÂåñ‰∏∫‰∏Ä‰∏™ËæÉÂ∞èÁöÑÊ≠£Êï∞
        if self.scale_S_head is not None:
            nn.init.zeros_(self.scale_S_head.weight)
            nn.init.constant_(self.scale_S_head.bias, 2.3)  # exp(2.3) ‚âà 10

        nn.init.zeros_(self.scale_Y_head.weight)
        nn.init.constant_(self.scale_Y_head.bias, 1.0) # exp(1.0) ~ 2.7

        print("   ‚úÖ Áü•ËØÜ‰º†ËæìÂÆåÊàê")

    def forward(self, U_loc: torch.Tensor, U_scale: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Defines the forward pass for the Action Network.
        
        Args:
            U_loc (torch.Tensor): Location parameter of causal state distribution.
            U_scale (torch.Tensor): Scale parameter of causal state distribution.
        
        Returns:
            Dict[str, torch.Tensor]: A dictionary containing the parameters of the output distributions.
        """
        # ‰ªé U ‰∏≠ÈááÊ†∑‰∏Ä‰∏™ÂÆû‰æãÔºåÊàñÁõ¥Êé•‰ΩøÁî®ÂÖ∂ÂùáÂÄº
        # ÂΩìÂâçÂÆûÁé∞ÔºöÁõ¥Êé•‰ΩøÁî®ÂùáÂÄº U_loc ‰Ωú‰∏∫‰∏ã‰∏ÄÂ±ÇÁöÑËæìÂÖ•
        z_prime = U_loc
        
        # ËÆ°ÁÆóÂàÜÁ±ªÂíåÂõûÂΩíÁöÑ loc
        loc_S = self.classification_head(z_prime)
        loc_Y = self.regression_head(z_prime).squeeze(-1)  # [B, S, 1] -> [B, S]
        
        # ËÆ°ÁÆóÂàÜÁ±ªÂíåÂõûÂΩíÁöÑ scale
        # Âü∫Á°ÄÂ∞∫Â∫¶Êù•Ê∫ê‰∫é U_scaleÔºåÁÑ∂ÂêéÈÄöËøáÂêÑËá™ÁöÑÂ§¥ËøõË°åÂèòÊç¢
        # U_scale -> log(U_scale) -> linear -> exp -> scale
        log_U_scale = torch.log(U_scale)
        
        scale_S = torch.exp(self.scale_S_head(log_U_scale))
        scale_Y = torch.exp(self.scale_Y_head(log_U_scale)).squeeze(-1) # [B, S, 1] -> [B, S]

        return {
            "loc_S": loc_S,
            "scale_S": scale_S,
            "loc_Y": loc_Y,
            "scale_Y": scale_Y,
        }
    
    def predict(self, causal_loc, causal_scale=None):
        """
        Make a deterministic prediction based on the output distributions.
        
        Args:
            causal_loc (torch.Tensor): Location parameters of the individual causal representation
            causal_scale (torch.Tensor, optional): Scale parameters. If None, a zero tensor is used
                                                   for a purely deterministic prediction from loc.
        
        Returns:
            dict: A dictionary containing the predicted token, regression value, and <NUM> probability.
        """
        if causal_scale is None:
            # If no scale is provided, assume a deterministic input (zero scale)
            causal_scale = torch.zeros_like(causal_loc)

        # 1. Get output distribution parameters by running the forward pass
        outputs = self.forward(causal_loc, causal_scale)
        loc_S, scale_S = outputs['loc_S'], outputs['scale_S']
        loc_Y, scale_Y = outputs['loc_Y'], outputs['scale_Y']
        
        # 2. Get classification prediction from the distribution
        # This computes probabilities and takes argmax
        cls_pred = self.classification_head.predict(loc_S, scale_S)
        
        # 3. Get regression prediction (point estimate is the location parameter)
        reg_pred = self.regression_head.predict(loc_Y, scale_Y)
        
        # 4. Get probability of <NUM> token
        all_probs = self.classification_head.compute_probabilities(loc_S, scale_S)
        
        # Handle sequence vs. non-sequence input for num_prob
        if all_probs.dim() > 1:
            num_prob = all_probs[..., self.num_token_id]
        else:
            # This case might not be typical, but handle it for robustness
            num_prob = all_probs[self.num_token_id]

        return {
            'cls_pred': cls_pred,
            'reg_pred': reg_pred,
            'num_prob': num_prob
        }


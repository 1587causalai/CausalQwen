"""
åŸºå‡†æµ‹è¯•æ–¹æ³•å®ç°æ¨¡å—

åŒ…å«æ‰€æœ‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„ç»Ÿä¸€å®ç°ï¼Œæ”¯æŒå›å½’å’Œåˆ†ç±»ä»»åŠ¡ã€‚
"""

import numpy as np
import warnings
from typing import Dict, Tuple, Any, Optional

# æ ¸å¿ƒä¾èµ–
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score, median_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.preprocessing import StandardScaler

# ç¥ç»ç½‘ç»œæ–¹æ³•
from sklearn.neural_network import MLPRegressor, MLPClassifier

# é›†æˆæ–¹æ³•
from sklearn.ensemble import (
    RandomForestRegressor, RandomForestClassifier,
    ExtraTreesRegressor, ExtraTreesClassifier,
    GradientBoostingRegressor, GradientBoostingClassifier
)

# æ”¯æŒå‘é‡æœº
from sklearn.svm import SVR, SVC

# çº¿æ€§æ–¹æ³•
from sklearn.linear_model import (
    LinearRegression, LogisticRegression,
    Ridge, RidgeClassifier,
    Lasso, LassoCV,
    ElasticNet, ElasticNetCV
)

# Kè¿‘é‚»
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier

# å†³ç­–æ ‘
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier

# å¯é€‰ä¾èµ–ï¼ˆä¼˜é›…é™çº§ï¼‰
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    # XGBoost warning will be shown only when explicitly requested

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    # LightGBM warning will be shown only when explicitly requested

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    # CatBoost warning will be shown only when explicitly requested

warnings.filterwarnings('ignore')


class BaselineMethodFactory:
    """
    åŸºå‡†æ–¹æ³•å·¥å‚ç±»
    
    è´Ÿè´£åˆ›å»ºå’Œç®¡ç†æ‰€æœ‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ åŸºå‡†æ–¹æ³•çš„å®ä¾‹åŒ–ã€è®­ç»ƒå’Œè¯„ä¼°ã€‚
    """
    
    def __init__(self):
        # å¯¼å…¥PyTorchç”¨äºå®ç°ç¨³å¥MLP
        try:
            import torch
            import torch.nn as nn
            import torch.optim as optim
            self.torch_available = True
        except ImportError:
            self.torch_available = False
            warnings.warn("PyTorchä¸å¯ç”¨ï¼Œç¨³å¥MLPæ–¹æ³•å°†è¢«è·³è¿‡")
        
        self.available_methods = self._get_available_methods()
    
    def _get_available_methods(self) -> Dict[str, bool]:
        """æ£€æŸ¥å„ç§æ–¹æ³•çš„å¯ç”¨æ€§"""
        return {
            'sklearn_mlp': True,
            'pytorch_mlp': True,  # åœ¨base.pyä¸­å·²å®ç°
            'mlp_huber': self.torch_available,  # ç¨³å¥MLPéœ€è¦PyTorch
            'mlp_pinball_median': self.torch_available,  # ç¨³å¥MLPéœ€è¦PyTorch
            'mlp_cauchy': self.torch_available,  # ç¨³å¥MLPéœ€è¦PyTorch
            'random_forest': True,
            'extra_trees': True,
            'gradient_boosting': True,
            'xgboost': XGBOOST_AVAILABLE,
            'lightgbm': LIGHTGBM_AVAILABLE,
            'catboost': CATBOOST_AVAILABLE,
            'svm_rbf': True,
            'svm_linear': True,
            'svm_poly': True,
            'linear_regression': True,
            'ridge': True,
            'lasso': True,
            'elastic_net': True,
            'knn': True,
            'decision_tree': True
        }
    
    def is_method_available(self, method_name: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ–¹æ³•æ˜¯å¦å¯ç”¨"""
        return self.available_methods.get(method_name, False)
    
    def create_model(self, method_name: str, task_type: str, **params) -> Any:
        """
        åˆ›å»ºæŒ‡å®šçš„æœºå™¨å­¦ä¹ æ¨¡å‹
        
        Args:
            method_name: æ–¹æ³•åç§°
            task_type: 'regression' æˆ– 'classification'
            **params: æ–¹æ³•ç‰¹å®šå‚æ•°
        
        Returns:
            sklearnå…¼å®¹çš„æ¨¡å‹å®ä¾‹
        """
        if not self.is_method_available(method_name):
            raise ValueError(f"æ–¹æ³• {method_name} ä¸å¯ç”¨æˆ–æœªå®‰è£…ç›¸å…³ä¾èµ–")
        
        # ç§»é™¤ä¸ç›¸å…³çš„å‚æ•°
        filtered_params = self._filter_params(method_name, params)
        
        # åˆ›å»ºæ¨¡å‹
        if method_name == 'sklearn_mlp':
            return self._create_sklearn_mlp(task_type, **filtered_params)
        elif method_name == 'mlp_huber':
            return self._create_robust_mlp(task_type, 'huber', **filtered_params)
        elif method_name == 'mlp_pinball_median':
            return self._create_robust_mlp(task_type, 'pinball', **filtered_params)
        elif method_name == 'mlp_cauchy':
            return self._create_robust_mlp(task_type, 'cauchy', **filtered_params)
        elif method_name == 'random_forest':
            return self._create_random_forest(task_type, **filtered_params)
        elif method_name == 'extra_trees':
            return self._create_extra_trees(task_type, **filtered_params)
        elif method_name == 'gradient_boosting':
            return self._create_gradient_boosting(task_type, **filtered_params)
        elif method_name == 'xgboost':
            return self._create_xgboost(task_type, **filtered_params)
        elif method_name == 'lightgbm':
            return self._create_lightgbm(task_type, **filtered_params)
        elif method_name == 'catboost':
            return self._create_catboost(task_type, **filtered_params)
        elif method_name.startswith('svm_'):
            return self._create_svm(method_name, task_type, **filtered_params)
        elif method_name == 'linear_regression':
            return self._create_linear(task_type, **filtered_params)
        elif method_name == 'ridge':
            return self._create_ridge(task_type, **filtered_params)
        elif method_name == 'lasso':
            return self._create_lasso(task_type, **filtered_params)
        elif method_name == 'elastic_net':
            return self._create_elastic_net(task_type, **filtered_params)
        elif method_name == 'knn':
            return self._create_knn(task_type, **filtered_params)
        elif method_name == 'decision_tree':
            return self._create_decision_tree(task_type, **filtered_params)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ–¹æ³•: {method_name}")
    
    def _filter_params(self, method_name: str, params: Dict) -> Dict:
        """è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å‚æ•°"""
        # é€šç”¨å‚æ•°æ˜ å°„
        common_mappings = {
            'learning_rate_init': ['learning_rate_init'],
            'max_iter': ['max_iter', 'n_estimators', 'iterations'],
            'random_state': ['random_state', 'seed']
        }
        
        # æ–¹æ³•ç‰¹å®šçš„å‚æ•°è¿‡æ»¤
        if method_name in ['xgboost', 'lightgbm', 'catboost']:
            # Boostingæ–¹æ³•ç‰¹å®šå¤„ç†
            filtered = {}
            for k, v in params.items():
                if k in ['n_estimators', 'learning_rate', 'max_depth', 'random_state', 
                        'min_child_weight', 'subsample', 'colsample_bytree', 'verbosity',
                        'num_leaves', 'min_child_samples', 'iterations', 'depth', 
                        'l2_leaf_reg', 'verbose', 'thread_count', 'n_jobs']:
                    filtered[k] = v
            return filtered
        
        return params
    
    # === å…·ä½“æ¨¡å‹åˆ›å»ºæ–¹æ³• ===
    
    def _create_sklearn_mlp(self, task_type: str, **params) -> Any:
        """åˆ›å»ºsklearn MLPæ¨¡å‹"""
        if task_type == 'regression':
            return MLPRegressor(**params)
        else:
            return MLPClassifier(**params)
    
    def _create_random_forest(self, task_type: str, **params) -> Any:
        """åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹"""
        if task_type == 'regression':
            return RandomForestRegressor(**params)
        else:
            return RandomForestClassifier(**params)
    
    def _create_extra_trees(self, task_type: str, **params) -> Any:
        """åˆ›å»ºæç«¯éšæœºæ ‘æ¨¡å‹"""
        if task_type == 'regression':
            return ExtraTreesRegressor(**params)
        else:
            return ExtraTreesClassifier(**params)
    
    def _create_gradient_boosting(self, task_type: str, **params) -> Any:
        """åˆ›å»ºæ¢¯åº¦æå‡æ¨¡å‹"""
        if task_type == 'regression':
            return GradientBoostingRegressor(**params)
        else:
            return GradientBoostingClassifier(**params)
    
    def _create_xgboost(self, task_type: str, **params) -> Any:
        """åˆ›å»ºXGBoostæ¨¡å‹"""
        if not XGBOOST_AVAILABLE:
            raise RuntimeError("XGBoostæœªå®‰è£…")
        
        if task_type == 'regression':
            return xgb.XGBRegressor(**params)
        else:
            return xgb.XGBClassifier(**params)
    
    def _create_lightgbm(self, task_type: str, **params) -> Any:
        """åˆ›å»ºLightGBMæ¨¡å‹"""
        if not LIGHTGBM_AVAILABLE:
            raise RuntimeError("LightGBMæœªå®‰è£…")
        
        if task_type == 'regression':
            return lgb.LGBMRegressor(**params)
        else:
            return lgb.LGBMClassifier(**params)
    
    def _create_catboost(self, task_type: str, **params) -> Any:
        """åˆ›å»ºCatBoostæ¨¡å‹"""
        if not CATBOOST_AVAILABLE:
            raise RuntimeError("CatBoostæœªå®‰è£…")
        
        if task_type == 'regression':
            return cb.CatBoostRegressor(**params)
        else:
            return cb.CatBoostClassifier(**params)
    
    def _create_svm(self, method_name: str, task_type: str, **params) -> Any:
        """åˆ›å»ºSVMæ¨¡å‹"""
        kernel = method_name.split('_')[1]  # ä» 'svm_rbf' æå– 'rbf'
        params['kernel'] = kernel
        
        if task_type == 'regression':
            return SVR(**params)
        else:
            return SVC(**params)
    
    def _create_linear(self, task_type: str, **params) -> Any:
        """åˆ›å»ºçº¿æ€§æ¨¡å‹"""
        if task_type == 'regression':
            return LinearRegression(**params)
        else:
            return LogisticRegression(**params)
    
    def _create_ridge(self, task_type: str, **params) -> Any:
        """åˆ›å»ºRidgeæ¨¡å‹"""
        if task_type == 'regression':
            return Ridge(**params)
        else:
            return RidgeClassifier(**params)
    
    def _create_lasso(self, task_type: str, **params) -> Any:
        """åˆ›å»ºLassoæ¨¡å‹"""
        if task_type == 'regression':
            return Lasso(**params)
        else:
            # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨L1æ­£åˆ™åŒ–çš„Logisticå›å½’
            return LogisticRegression(penalty='l1', solver='liblinear', **params)
    
    def _create_elastic_net(self, task_type: str, **params) -> Any:
        """åˆ›å»ºElastic Netæ¨¡å‹"""
        if task_type == 'regression':
            return ElasticNet(**params)
        else:
            # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨Elastic Netæ­£åˆ™åŒ–çš„Logisticå›å½’
            return LogisticRegression(penalty='elasticnet', solver='saga', **params)
    
    def _create_knn(self, task_type: str, **params) -> Any:
        """åˆ›å»ºKè¿‘é‚»æ¨¡å‹"""
        if task_type == 'regression':
            return KNeighborsRegressor(**params)
        else:
            return KNeighborsClassifier(**params)
    
    def _create_decision_tree(self, task_type: str, **params) -> Any:
        """åˆ›å»ºå†³ç­–æ ‘æ¨¡å‹"""
        if task_type == 'regression':
            return DecisionTreeRegressor(**params)
        else:
            return DecisionTreeClassifier(**params)
    
    def _create_robust_mlp(self, task_type: str, loss_type: str, **params) -> Any:
        """åˆ›å»ºç¨³å¥MLPæ¨¡å‹"""
        if not self.torch_available:
            raise RuntimeError("PyTorchæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºç¨³å¥MLPæ¨¡å‹")
        
        return RobustMLP(task_type=task_type, loss_type=loss_type, **params)
    
    def train_and_evaluate(self, method_name: str, model: Any, 
                          X_train: np.ndarray, y_train: np.ndarray,
                          X_val: np.ndarray, y_val: np.ndarray,
                          X_test: np.ndarray, y_test: np.ndarray,
                          task_type: str) -> Dict[str, Dict[str, float]]:
        """
        è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½
        
        Returns:
            åŒ…å«éªŒè¯é›†å’Œæµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        # è®­ç»ƒæ¨¡å‹ - æ”¯æŒæ—©åœçš„æ–¹æ³•ä½¿ç”¨éªŒè¯é›†
        if method_name in ['mlp_huber', 'mlp_pinball_median', 'mlp_cauchy']:
            # ç¨³å¥MLPä½¿ç”¨è‡ªå·±çš„éªŒè¯é›†æ—©åœæœºåˆ¶
            model.fit(X_train, y_train, eval_set=(X_val, y_val))
        elif supports_early_stopping(method_name):
            model = fit_with_early_stopping(
                method_name, model, X_train, y_train, X_val, y_val, task_type
            )
        else:
            model.fit(X_train, y_train)
        
        # é¢„æµ‹
        pred_val = model.predict(X_val)
        pred_test = model.predict(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        if task_type == 'regression':
            return {
                'val': {
                    'MAE': mean_absolute_error(y_val, pred_val),
                    'MdAE': median_absolute_error(y_val, pred_val),
                    'RMSE': np.sqrt(mean_squared_error(y_val, pred_val)),
                    'RÂ²': r2_score(y_val, pred_val)
                },
                'test': {
                    'MAE': mean_absolute_error(y_test, pred_test),
                    'MdAE': median_absolute_error(y_test, pred_test),
                    'RMSE': np.sqrt(mean_squared_error(y_test, pred_test)),
                    'RÂ²': r2_score(y_test, pred_test)
                }
            }
        else:
            n_classes = len(np.unique(y_test))
            avg_method = 'binary' if n_classes == 2 else 'macro'
            
            return {
                'val': {
                    'Acc': accuracy_score(y_val, pred_val),
                    'Precision': precision_score(y_val, pred_val, average=avg_method, zero_division=0),
                    'Recall': recall_score(y_val, pred_val, average=avg_method, zero_division=0),
                    'F1': f1_score(y_val, pred_val, average=avg_method, zero_division=0)
                },
                'test': {
                    'Acc': accuracy_score(y_test, pred_test),
                    'Precision': precision_score(y_test, pred_test, average=avg_method, zero_division=0),
                    'Recall': recall_score(y_test, pred_test, average=avg_method, zero_division=0),
                    'F1': f1_score(y_test, pred_test, average=avg_method, zero_division=0)
                }
            }


class MethodDependencyChecker:
    """
    æ–¹æ³•ä¾èµ–æ£€æŸ¥å™¨
    
    æ£€æŸ¥å’ŒæŠ¥å‘Šå„ç§åŸºå‡†æ–¹æ³•çš„ä¾èµ–å¯ç”¨æ€§ã€‚
    """
    
    @staticmethod
    def check_all_dependencies() -> Dict[str, Dict[str, Any]]:
        """æ£€æŸ¥æ‰€æœ‰ä¾èµ–çš„å¯ç”¨æ€§"""
        dependencies = {
            'xgboost': {
                'available': XGBOOST_AVAILABLE,
                'install_cmd': 'pip install xgboost',
                'description': 'Gradient Boostingæ¡†æ¶'
            },
            'lightgbm': {
                'available': LIGHTGBM_AVAILABLE,
                'install_cmd': 'pip install lightgbm',
                'description': 'å¾®è½¯å¼€å‘çš„æ¢¯åº¦æå‡æ¡†æ¶'
            },
            'catboost': {
                'available': CATBOOST_AVAILABLE,
                'install_cmd': 'pip install catboost',
                'description': 'Yandexå¼€å‘çš„æ¢¯åº¦æå‡æ¡†æ¶'
            }
        }
        
        return dependencies
    
    @staticmethod
    def print_dependency_status():
        """æ‰“å°ä¾èµ–çŠ¶æ€æŠ¥å‘Š"""
        deps = MethodDependencyChecker.check_all_dependencies()
        
        print("\nğŸ“¦ åŸºå‡†æ–¹æ³•ä¾èµ–çŠ¶æ€")
        print("=" * 60)
        
        for name, info in deps.items():
            status = "âœ… å¯ç”¨" if info['available'] else "âŒ ä¸å¯ç”¨"
            print(f"{name:<12} {status:<8} - {info['description']}")
            if not info['available']:
                print(f"{'':12} å®‰è£…å‘½ä»¤: {info['install_cmd']}")
        
        print("=" * 60)
        
        available_count = sum(1 for info in deps.values() if info['available'])
        total_count = len(deps)
        
        print(f"ğŸ“Š å¯é€‰ä¾èµ–å¯ç”¨æ€§: {available_count}/{total_count}")
        
        if available_count < total_count:
            print("ğŸ’¡ å®‰è£…ç¼ºå¤±çš„ä¾èµ–ä»¥è·å¾—å®Œæ•´çš„åŸºå‡†æµ‹è¯•èƒ½åŠ›")


def get_default_method_selection(task_type: str, selection_type: str = 'basic') -> list:
    """
    è·å–é»˜è®¤çš„æ–¹æ³•é€‰æ‹©
    
    Args:
        task_type: 'regression' æˆ– 'classification'
        selection_type: 'basic', 'comprehensive', 'competitive', 'lightweight'
    
    Returns:
        æ–¹æ³•ååˆ—è¡¨
    """
    from .method_configs import get_task_recommendations, get_method_group
    
    if selection_type in ['basic', 'comprehensive', 'competitive', 'lightweight']:
        return get_method_group(selection_type)
    else:
        return get_task_recommendations(task_type, selection_type)


def filter_available_methods(method_list: list) -> Tuple[list, list]:
    """
    è¿‡æ»¤å¯ç”¨çš„æ–¹æ³•
    
    Returns:
        (å¯ç”¨æ–¹æ³•åˆ—è¡¨, ä¸å¯ç”¨æ–¹æ³•åˆ—è¡¨)
    """
    factory = BaselineMethodFactory()
    available = []
    unavailable = []
    
    for method in method_list:
        if factory.is_method_available(method):
            available.append(method)
        else:
            unavailable.append(method)
    
    return available, unavailable


# === ç¨³å¥MLPå®ç° ===

class RobustMLP:
    """
    ä½¿ç”¨ç¨³å¥æŸå¤±å‡½æ•°çš„MLPå›å½’å™¨
    
    æ”¯æŒHuberæŸå¤±å’ŒPinballæŸå¤±ï¼ˆåˆ†ä½æ•°å›å½’ï¼‰
    """
    
    def __init__(self, task_type='regression', loss_type='huber', 
                 hidden_sizes=(128, 64), epochs=3000, lr=0.03, 
                 patience=50, tol=1e-4, huber_delta=1.0, quantile=0.5,
                 **kwargs):
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        self.task_type = task_type
        self.loss_type = loss_type
        self.hidden_sizes = hidden_sizes
        self.epochs = epochs
        self.lr = lr
        self.patience = patience
        self.tol = tol
        self.huber_delta = huber_delta
        self.quantile = quantile
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        
        # æŸå¤±å‡½æ•°å®šä¹‰
        if loss_type == 'huber':
            self.criterion = nn.HuberLoss(delta=huber_delta)
        elif loss_type == 'pinball':
            self.criterion = self._pinball_loss
        elif loss_type == 'cauchy':
            self.criterion = self._cauchy_loss
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±ç±»å‹: {loss_type}")
    
    def _pinball_loss(self, y_pred, y_true):
        """PinballæŸå¤±ï¼ˆåˆ†ä½æ•°æŸå¤±ï¼‰"""
        import torch
        error = y_true - y_pred
        loss = torch.where(error >= 0, 
                          self.quantile * error, 
                          (self.quantile - 1) * error)
        return loss.mean()
    
    def _cauchy_loss(self, y_pred, y_true):
        """CauchyæŸå¤±å‡½æ•°: log(1 + (y - yhat)^2)"""
        import torch
        error = y_true - y_pred
        loss = torch.log(1 + error**2)
        return loss.mean()
    
    def _build_model(self, input_size):
        """æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
        import torch.nn as nn
        
        layers = []
        prev_size = input_size
        
        # éšè—å±‚
        for hidden_size in self.hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        if self.task_type == 'regression':
            layers.append(nn.Linear(prev_size, 1))
        else:
            # åˆ†ç±»ä»»åŠ¡æš‚ä¸æ”¯æŒ
            raise NotImplementedError("ç¨³å¥MLPæš‚ä¸æ”¯æŒåˆ†ç±»ä»»åŠ¡")
        
        return nn.Sequential(*layers)
    
    def fit(self, X, y, eval_set=None):
        """è®­ç»ƒæ¨¡å‹"""
        import torch
        import torch.optim as optim
        
        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).ravel()
        
        # è½¬æ¢ä¸ºtensor
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        y_tensor = torch.FloatTensor(y_scaled).to(self.device)
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model(X.shape[1]).to(self.device)
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        
        # éªŒè¯é›†å¤„ç†
        if eval_set is not None:
            X_val, y_val = eval_set
            X_val_scaled = self.scaler_X.transform(X_val)
            y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).ravel()
            X_val_tensor = torch.FloatTensor(X_val_scaled).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val_scaled).to(self.device)
        
        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.epochs):
            # è®­ç»ƒ
            self.model.train()
            optimizer.zero_grad()
            
            y_pred = self.model(X_tensor).squeeze()
            loss = self.criterion(y_pred, y_tensor)
            
            loss.backward()
            optimizer.step()
            
            # éªŒè¯
            if eval_set is not None:
                self.model.eval()
                with torch.no_grad():
                    y_val_pred = self.model(X_val_tensor).squeeze()
                    val_loss = self.criterion(y_val_pred, y_val_tensor)
                
                # æ—©åœæ£€æŸ¥
                if val_loss < best_val_loss - self.tol:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_state = self.model.state_dict().copy()
                else:
                    patience_counter += 1
                
                if patience_counter >= self.patience:
                    # æ¢å¤æœ€ä½³æ¨¡å‹
                    if 'best_model_state' in locals():
                        self.model.load_state_dict(best_model_state)
                    break
        
        return self
    
    def predict(self, X):
        """é¢„æµ‹"""
        import torch
        
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        X_scaled = self.scaler_X.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            y_pred_scaled = self.model(X_tensor).squeeze().cpu().numpy()
        
        # åæ ‡å‡†åŒ–
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        
        return y_pred


# === æ—©åœæ”¯æŒæ–¹æ³• ===

def supports_early_stopping(method_name: str) -> bool:
    """æ£€æŸ¥æ–¹æ³•æ˜¯å¦æ”¯æŒæ—©åœ"""
    early_stopping_methods = {
        'xgboost', 'lightgbm', 'catboost', 'gradient_boosting'
    }
    return method_name in early_stopping_methods


def fit_with_early_stopping(method_name: str, model: Any, 
                           X_train, y_train, X_val, y_val, task_type: str):
    """ä¸ºæ”¯æŒæ—©åœçš„æ–¹æ³•å®ç°éªŒè¯é›†æ—©åœ"""
    
    if method_name == 'xgboost':
        return _fit_xgboost_with_early_stopping(
            model, X_train, y_train, X_val, y_val, task_type
        )
    elif method_name == 'lightgbm':
        return _fit_lightgbm_with_early_stopping(
            model, X_train, y_train, X_val, y_val, task_type
        )
    elif method_name == 'catboost':
        return _fit_catboost_with_early_stopping(
            model, X_train, y_train, X_val, y_val, task_type
        )
    elif method_name == 'gradient_boosting':
        return _fit_sklearn_gb_with_early_stopping(
            model, X_train, y_train, X_val, y_val, task_type
        )
    else:
        # é»˜è®¤è®­ç»ƒ
        model.fit(X_train, y_train)
        return model


def _fit_xgboost_with_early_stopping(model, X_train, y_train, X_val, y_val, task_type):
    """XGBoostæ—©åœè®­ç»ƒ"""
    if not XGBOOST_AVAILABLE:
        model.fit(X_train, y_train)
        return model
    
    try:
        # å°è¯•æ–°ç‰ˆæœ¬XGBoost API
        eval_set = [(X_val, y_val)]
        model.fit(
            X_train, y_train,
            eval_set=eval_set,
            early_stopping_rounds=50,
            verbose=False
        )
    except TypeError:
        try:
            # å°è¯•ä½¿ç”¨callbacks (æ–°ç‰ˆæœ¬)
            import xgboost as xgb
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[xgb.callback.EarlyStopping(rounds=50)]
            )
        except:
            # å¦‚æœéƒ½å¤±è´¥ï¼Œç›´æ¥è®­ç»ƒ
            model.fit(X_train, y_train)
    
    return model


def _fit_lightgbm_with_early_stopping(model, X_train, y_train, X_val, y_val, task_type):
    """LightGBMæ—©åœè®­ç»ƒ"""
    if not LIGHTGBM_AVAILABLE:
        model.fit(X_train, y_train)
        return model
    
    # LightGBMä½¿ç”¨callbackså‚æ•°è¿›è¡Œæ—©åœ
    import lightgbm as lgb
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
    )
    return model


def _fit_catboost_with_early_stopping(model, X_train, y_train, X_val, y_val, task_type):
    """CatBoostæ—©åœè®­ç»ƒ"""
    if not CATBOOST_AVAILABLE:
        model.fit(X_train, y_train)
        return model
    
    eval_set = (X_val, y_val)
    
    model.fit(
        X_train, y_train,
        eval_set=eval_set,
        early_stopping_rounds=50,
        verbose=False
    )
    return model


def _fit_sklearn_gb_with_early_stopping(model, X_train, y_train, X_val, y_val, task_type):
    """sklearn Gradient Boostingæ—©åœè®­ç»ƒ"""
    from sklearn.metrics import mean_squared_error, log_loss
    
    # sklearnçš„GradientBoostingä¸ç›´æ¥æ”¯æŒæ—©åœï¼Œéœ€è¦æ‰‹åŠ¨å®ç°
    n_estimators = model.n_estimators
    best_score = float('inf')
    best_n_estimators = n_estimators
    patience_counter = 0
    patience = 50
    
    # é€æ­¥å¢åŠ ä¼°è®¡å™¨æ•°é‡å¹¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    for n_est in range(10, n_estimators + 1, 10):
        temp_model = model.__class__(**model.get_params())
        temp_model.n_estimators = n_est
        temp_model.fit(X_train, y_train)
        
        val_pred = temp_model.predict(X_val)
        
        if task_type == 'regression':
            val_score = mean_squared_error(y_val, val_pred)
        else:
            try:
                val_proba = temp_model.predict_proba(X_val)
                val_score = log_loss(y_val, val_proba)
            except:
                val_score = 1.0 - temp_model.score(X_val, y_val)
        
        if val_score < best_score:
            best_score = val_score
            best_n_estimators = n_est
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= patience // 10:  # è°ƒæ•´patienceä»¥é€‚åº”æ­¥é•¿
            break
    
    # ä½¿ç”¨æœ€ä½³ä¼°è®¡å™¨æ•°é‡é‡æ–°è®­ç»ƒ
    model.n_estimators = best_n_estimators
    model.fit(X_train, y_train)
    return model
"""
CausalEngineåŸºå‡†æµ‹è¯•åŸºç¡€æ¨¡å—

æä¾›ç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒCausalEngineä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚
æ”¯æŒå¤šç§åŸºå‡†æ–¹æ³•ï¼šç¥ç»ç½‘ç»œã€é›†æˆæ–¹æ³•ã€SVMã€çº¿æ€§æ–¹æ³•ç­‰ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import warnings

from .._causal_engine import create_causal_regressor, create_causal_classifier
from .methods import BaselineMethodFactory, MethodDependencyChecker, filter_available_methods
from ..utils import causal_split
from .method_configs import (
    get_method_config, get_method_group, get_task_recommendations, 
    validate_methods, expand_method_groups, list_available_methods
)

warnings.filterwarnings('ignore')


class PyTorchBaseline(nn.Module):
    """PyTorchåŸºçº¿æ¨¡å‹ï¼ˆä¼ ç»ŸMLPï¼‰"""
    
    def __init__(self, input_size, output_size, hidden_sizes=(128, 64)):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


class BaselineBenchmark:
    """
    åŸºå‡†æµ‹è¯•åŸºç±»
    
    æä¾›ç»Ÿä¸€çš„æ¥å£æ¥æ¯”è¾ƒCausalEngineä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚
    æ”¯æŒé…ç½®é©±åŠ¨çš„åŸºå‡†æ–¹æ³•é€‰æ‹©ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œã€é›†æˆæ–¹æ³•ã€SVMã€çº¿æ€§æ–¹æ³•ç­‰ã€‚
    """
    
    def __init__(self):
        self.results = {}
        self.method_factory = BaselineMethodFactory()
        self.dependency_checker = MethodDependencyChecker()
    
    
    def train_pytorch_model(self, model, X_train, y_train, X_val=None, y_val=None, 
                          epochs=1000, lr=0.001, task='regression', patience=50, tol=1e-4):
        """è®­ç»ƒPyTorchåŸºçº¿æ¨¡å‹"""
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train)
        
        if task == 'regression':
            criterion = nn.MSELoss()
        else:
            criterion = nn.CrossEntropyLoss()
            y_train_tensor = y_train_tensor.long()
        
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        # æ—©åœ
        best_loss = float('inf')
        no_improve = 0
        best_model_path = f"/tmp/pytorch_best_model_{id(model)}.pkl"
        
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            outputs = model(X_train_tensor)
            if task == 'regression':
                loss = criterion(outputs.squeeze(), y_train_tensor)
            else:
                loss = criterion(outputs, y_train_tensor)
            
            loss.backward()
            optimizer.step()
            
            # éªŒè¯é›†æ—©åœ
            if X_val is not None:
                model.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val)
                    val_outputs = model(X_val_tensor)
                    if task == 'regression':
                        y_val_tensor = torch.FloatTensor(y_val)
                        val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()
                    else:
                        y_val_tensor = torch.LongTensor(y_val)
                        val_loss = criterion(val_outputs, y_val_tensor).item()
                
                if val_loss < best_loss - tol:
                    best_loss = val_loss
                    no_improve = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    import pickle
                    with open(best_model_path, 'wb') as f:
                        pickle.dump(model.state_dict(), f)
                    if epoch == 0:
                        print(f"   æœ€ä½³æ¨¡å‹ä¸´æ—¶å­˜å‚¨: {best_model_path}")
                else:
                    no_improve += 1
                
                if no_improve >= patience:
                    break
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        import pickle
        if os.path.exists(best_model_path):
            with open(best_model_path, 'rb') as f:
                model.load_state_dict(pickle.load(f))
            print(f"   å·²æ¢å¤æœ€ä½³æ¨¡å‹ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶: {best_model_path}")
            os.remove(best_model_path)
        
        # å°†å®é™…è®­ç»ƒè½®æ•°ä½œä¸ºå±æ€§æ·»åŠ åˆ°æ¨¡å‹
        model.n_iter_ = epoch + 1
        model.final_loss_ = best_loss
        return model
    
    def train_causal_engine(self, X_train, y_train, X_val, y_val, task_type='regression', mode='standard',
                           hidden_sizes=(128, 64), max_epochs=5000, lr=0.01, patience=500, tol=1e-8,
                           gamma_init=1.0, b_noise_init=1.0, b_noise_trainable=True, ovr_threshold=0.0, verbose=True):
        """è®­ç»ƒCausalEngineæ¨¡å‹"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        input_size = X_train.shape[1]
        if task_type == 'regression':
            output_size = 1 if len(y_train.shape) == 1 else y_train.shape[1]
            model = create_causal_regressor(
                input_size=input_size,
                output_size=output_size,
                repre_size=hidden_sizes[0] if hidden_sizes else None,
                causal_size=hidden_sizes[0] if hidden_sizes else None,
                perception_hidden_layers=hidden_sizes,
                abduction_hidden_layers=(),
                gamma_init=gamma_init,
                b_noise_init=b_noise_init,
                b_noise_trainable=b_noise_trainable
            )
        else:
            n_classes = len(np.unique(y_train))
            model = create_causal_classifier(
                input_size=input_size,
                n_classes=n_classes,
                repre_size=hidden_sizes[0] if hidden_sizes else None,
                causal_size=hidden_sizes[0] if hidden_sizes else None,
                perception_hidden_layers=hidden_sizes,
                abduction_hidden_layers=(),
                gamma_init=gamma_init,
                b_noise_init=b_noise_init,
                b_noise_trainable=b_noise_trainable,
                ovr_threshold=ovr_threshold
            )
        
        if verbose:
            print(f"\nä¸ºæ¨¡å¼æ„å»ºæ¨¡å‹: {mode}")
            print(f"==> æ¨¡å‹å·²æ„å»ºã€‚æ€»å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
        
        model = model.to(device)
        
        X_train_torch = torch.FloatTensor(X_train).to(device)
        y_train_torch = torch.FloatTensor(y_train).to(device)
        X_val_torch = torch.FloatTensor(X_val).to(device)
        y_val_torch = torch.FloatTensor(y_val).to(device)
        
        if task_type == 'classification':
            y_train_torch = y_train_torch.long()
            y_val_torch = y_val_torch.long()
        else:
            if len(y_train_torch.shape) == 1:
                y_train_torch = y_train_torch.unsqueeze(1)
                y_val_torch = y_val_torch.unsqueeze(1)
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        best_val_loss = float('inf')
        patience_counter = 0
        best_state_dict = None
        
        for epoch in range(max_epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            optimizer.zero_grad()
            loss = model.compute_loss(X_train_torch, y_train_torch, mode)
            loss.backward()
            optimizer.step()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            with torch.no_grad():
                val_loss = model.compute_loss(X_val_torch, y_val_torch, mode).item()
            
            # æ‰“å°è¿›åº¦
            if epoch % 100 == 0 and verbose:
                print(f"Epoch {epoch}: Loss = {loss.item():.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss - tol:
                best_val_loss = val_loss
                patience_counter = 0
                if verbose:
                    print(f"New best validation loss: {val_loss:.6f} at epoch {epoch + 1}")
                # ä¿å­˜æœ€ä½³çŠ¶æ€
                best_state_dict = model.state_dict().copy()
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                if verbose:
                    print(f"Early stopping at epoch {epoch + 1}")
                    print(f"Restored best model from validation loss: {best_val_loss:.6f}")
                # æ¢å¤æœ€ä½³æ¨¡å‹
                if best_state_dict is not None:
                    model.load_state_dict(best_state_dict)
                break
        
        return model
    
    def compare_models(self, X, y, task_type='regression', test_size=0.2, val_size=0.25, 
                      anomaly_ratio=0.0, random_state=42, verbose=True, **kwargs):
        """
        é€šç”¨æ¨¡å‹æ¯”è¾ƒæ–¹æ³•
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            task_type: 'regression' æˆ– 'classification'
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            val_size: éªŒè¯é›†æ¯”ä¾‹ï¼ˆç›¸å¯¹äºè®­ç»ƒé›†ï¼‰
            anomaly_ratio: æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹
            random_state: éšæœºç§å­
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
        """
        # 1. ç»Ÿä¸€æ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥
        if verbose and anomaly_ratio > 0:
            print(f"ğŸ”¥ æ•°æ®å‡†å¤‡: åˆ†å‰²æ•°æ®é›†å¹¶æ³¨å…¥ {anomaly_ratio:.1%} çš„æ ‡ç­¾å¼‚å¸¸...")
        
        # ç»Ÿä¸€ä½¿ç”¨causal_splitè¿›è¡Œæ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥
        stratify_option = y if task_type == 'classification' else None
        
        X_train_full, X_test, y_train_full, y_test = causal_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            anomaly_ratio=anomaly_ratio,
            anomaly_type=task_type,
            stratify=stratify_option,
            anomaly_strategy=kwargs.get('anomaly_strategy', 'shuffle')
        )
        
        # 2. ä»(å¯èƒ½å¸¦å™ªçš„)è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
        # æ³¨æ„ï¼šè¿™é‡Œçš„y_train_fullå¯èƒ½å·²ç»å¸¦æœ‰å™ªå£°
        stratify_val_option = y_train_full if task_type == 'classification' else None
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_full, y_train_full, 
            test_size=val_size, 
            random_state=random_state, 
            stratify=stratify_val_option
        )
        
        # 3. æ ‡å‡†åŒ–
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)
        
        # æ ‡ç­¾æ ‡å‡†åŒ–ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        if task_type == 'regression':
            scaler_y = StandardScaler()
            # æ³¨æ„ï¼šåœ¨å¹²å‡€çš„y_trainä¸Šæ‹Ÿåˆscaler
            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
            y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
            y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
        else:
            # åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦æ ‡ç­¾æ ‡å‡†åŒ–
            y_train_scaled = y_train
            y_val_scaled = y_val
            y_test_scaled = y_test

        # 4. å¼‚å¸¸æ³¨å…¥æ­¥éª¤å·²è¢«causal_splitå–ä»£ï¼Œæ­¤å¤„æ— éœ€æ“ä½œ
        
        results = {}
        
        # 5. ç¡®å®šè¦ä½¿ç”¨çš„åŸºå‡†æ–¹æ³•
        baseline_methods = self._get_baseline_methods(task_type, **kwargs)
        causal_modes = kwargs.get('causal_modes', ['deterministic', 'standard'])
        
        if verbose:
            print(f"\nğŸ“Š é€‰æ‹©çš„åŸºå‡†æ–¹æ³•: {baseline_methods}")
            print(f"ğŸ§  CausalEngineæ¨¡å¼: {causal_modes}")
        
        # 6. è®­ç»ƒå’Œè¯„ä¼°ä¼ ç»ŸåŸºå‡†æ–¹æ³•
        for method_name in baseline_methods:
            if method_name in ['sklearn', 'sklearn_mlp']:
                # ä¿æŒå‘åå…¼å®¹
                results.update(self._train_sklearn_baseline(
                    X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, 
                    X_test_scaled, y_test_scaled, task_type, verbose, **kwargs
                ))
            elif method_name in ['pytorch', 'pytorch_mlp']:
                # ä¿æŒå‘åå…¼å®¹
                results.update(self._train_pytorch_baseline(
                    X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, 
                    X_test_scaled, y_test_scaled, task_type, verbose, **kwargs
                ))
            else:
                # æ–°çš„åŸºå‡†æ–¹æ³•
                result = self._train_baseline_method(
                    method_name, X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled,
                    X_test_scaled, y_test_scaled, task_type, verbose, **kwargs
                )
                if result:
                    results.update(result)
        
        # 7. è®­ç»ƒå’Œè¯„ä¼°CausalEngineæ¨¡å‹
        results.update(self._train_causal_engines(
            X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, 
            X_test_scaled, y_test_scaled, task_type, verbose, **kwargs
        ))
        
        return results
    
    def _train_sklearn_baseline(self, X_train, y_train, X_val, y_val, X_test, y_test, 
                               task_type, verbose, **kwargs):
        """è®­ç»ƒsklearnåŸºçº¿"""
        hidden_layer_sizes = kwargs.get('hidden_layer_sizes', (128, 64))
        max_iter = kwargs.get('max_iter', 5000)
        learning_rate = kwargs.get('learning_rate', 0.01)
        random_state = kwargs.get('random_state', 42)
        
        if verbose: print("è®­ç»ƒ sklearn åŸºçº¿...")
        
        if task_type == 'regression':
            # ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼Œè€Œä¸æ˜¯å†…éƒ¨åˆ’åˆ†
            model = MLPRegressor(
                hidden_layer_sizes=hidden_layer_sizes,
                max_iter=max_iter,
                learning_rate_init=learning_rate,
                early_stopping=False,  # å…³é—­å†…éƒ¨æ—©åœ
                random_state=random_state,
                alpha=0.0001
            )
            
            # æ‰‹åŠ¨å®ç°æ—©åœç­–ç•¥ï¼Œä½¿ç”¨å¤–éƒ¨éªŒè¯é›†
            model = self._train_sklearn_with_external_validation(
                model, X_train, y_train, X_val, y_val, 
                patience=50, tol=1e-4, task_type='regression'
            )
            
            pred_test = model.predict(X_test)
            pred_val = model.predict(X_val)
            
            return {
                'sklearn': {
                    'test': {
                        'MAE': mean_absolute_error(y_test, pred_test),
                        'MdAE': median_absolute_error(y_test, pred_test), 
                        'RMSE': np.sqrt(mean_squared_error(y_test, pred_test)),
                        'RÂ²': r2_score(y_test, pred_test)
                    },
                    'val': {
                        'MAE': mean_absolute_error(y_val, pred_val),
                        'MdAE': median_absolute_error(y_val, pred_val), 
                        'RMSE': np.sqrt(mean_squared_error(y_val, pred_val)),
                        'RÂ²': r2_score(y_val, pred_val)
                    }
                }
            }
        else:
            # ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼Œè€Œä¸æ˜¯å†…éƒ¨åˆ’åˆ†
            model = MLPClassifier(
                hidden_layer_sizes=hidden_layer_sizes,
                max_iter=max_iter,
                learning_rate_init=learning_rate,
                early_stopping=False,  # å…³é—­å†…éƒ¨æ—©åœ
                random_state=random_state,
                alpha=0.0001
            )
            
            # æ‰‹åŠ¨å®ç°æ—©åœç­–ç•¥ï¼Œä½¿ç”¨å¤–éƒ¨éªŒè¯é›†
            model = self._train_sklearn_with_external_validation(
                model, X_train, y_train, X_val, y_val, 
                patience=50, tol=1e-4, task_type='classification'
            )
            
            pred_test = model.predict(X_test)
            pred_val = model.predict(X_val)
            
            n_classes = len(np.unique(y_test))
            avg_method = 'binary' if n_classes == 2 else 'macro'
            
            return {
                'sklearn': {
                    'test': {
                        'Acc': accuracy_score(y_test, pred_test),
                        'Precision': precision_score(y_test, pred_test, average=avg_method, zero_division=0),
                        'Recall': recall_score(y_test, pred_test, average=avg_method, zero_division=0),
                        'F1': f1_score(y_test, pred_test, average=avg_method, zero_division=0)
                    },
                    'val': {
                        'Acc': accuracy_score(y_val, pred_val),
                        'Precision': precision_score(y_val, pred_val, average=avg_method, zero_division=0),
                        'Recall': recall_score(y_val, pred_val, average=avg_method, zero_division=0),
                        'F1': f1_score(y_val, pred_val, average=avg_method, zero_division=0)
                    }
                }
            }
    
    def _train_pytorch_baseline(self, X_train, y_train, X_val, y_val, X_test, y_test, 
                               task_type, verbose, **kwargs):
        """è®­ç»ƒPyTorchåŸºçº¿"""
        hidden_layer_sizes = kwargs.get('hidden_layer_sizes', (128, 64))
        max_iter = kwargs.get('max_iter', 5000)
        learning_rate = kwargs.get('learning_rate', 0.01)
        
        if verbose: print("è®­ç»ƒ PyTorch åŸºçº¿...")
        
        n_features = X_train.shape[1]
        if task_type == 'regression':
            output_size = 1
        else:
            output_size = len(np.unique(y_train))
        
        model = PyTorchBaseline(n_features, output_size, hidden_layer_sizes)
        model = self.train_pytorch_model(
            model, X_train, y_train, X_val, y_val, 
            epochs=max_iter, lr=learning_rate, task=task_type,
            patience=50, tol=1e-4)
        
        model.eval()
        with torch.no_grad():
            if task_type == 'regression':
                pred_test = model(torch.FloatTensor(X_test)).squeeze().numpy()
                pred_val = model(torch.FloatTensor(X_val)).squeeze().numpy()
                
                return {
                    'pytorch': {
                        'test': {
                            'MAE': mean_absolute_error(y_test, pred_test),
                            'MdAE': median_absolute_error(y_test, pred_test),
                            'RMSE': np.sqrt(mean_squared_error(y_test, pred_test)),
                            'RÂ²': r2_score(y_test, pred_test)
                        },
                        'val': {
                            'MAE': mean_absolute_error(y_val, pred_val),
                            'MdAE': median_absolute_error(y_val, pred_val),
                            'RMSE': np.sqrt(mean_squared_error(y_val, pred_val)),
                            'RÂ²': r2_score(y_val, pred_val)
                        }
                    }
                }
            else:
                outputs_test = model(torch.FloatTensor(X_test))
                pred_test = torch.argmax(outputs_test, dim=1).numpy()
                outputs_val = model(torch.FloatTensor(X_val))
                pred_val = torch.argmax(outputs_val, dim=1).numpy()
                
                n_classes = len(np.unique(y_test))
                avg_method = 'binary' if n_classes == 2 else 'macro'
                
                return {
                    'pytorch': {
                        'test': {
                            'Acc': accuracy_score(y_test, pred_test),
                            'Precision': precision_score(y_test, pred_test, average=avg_method, zero_division=0),
                            'Recall': recall_score(y_test, pred_test, average=avg_method, zero_division=0),
                            'F1': f1_score(y_test, pred_test, average=avg_method, zero_division=0)
                        },
                        'val': {
                            'Acc': accuracy_score(y_val, pred_val),
                            'Precision': precision_score(y_val, pred_val, average=avg_method, zero_division=0),
                            'Recall': recall_score(y_val, pred_val, average=avg_method, zero_division=0),
                            'F1': f1_score(y_val, pred_val, average=avg_method, zero_division=0)
                        }
                    }
                }
    
    def _train_causal_engines(self, X_train, y_train, X_val, y_val, X_test, y_test, 
                             task_type, verbose, **kwargs):
        """è®­ç»ƒCausalEngineæ¨¡å‹ï¼ˆå¤šç§æ¨¡å¼ï¼‰"""
        modes = kwargs.get('causal_modes', ['deterministic', 'standard'])
        results = {}
        
        for mode in modes:
            if verbose: print(f"è®­ç»ƒ CausalEngine ({mode})...")
            
            # è¿‡æ»¤CausalEngineç›¸å…³å‚æ•°
            causal_kwargs = {k: v for k, v in kwargs.items() if k in [
                'hidden_sizes', 'max_epochs', 'lr', 'patience', 'tol',
                'gamma_init', 'b_noise_init', 'b_noise_trainable', 'ovr_threshold'
            ]}
            
            model = self.train_causal_engine(
                X_train, y_train, X_val, y_val, task_type, mode, verbose=verbose, **causal_kwargs
            )
            
            # é¢„æµ‹
            device = next(model.parameters()).device
            model.eval()
            with torch.no_grad():
                X_test_torch = torch.FloatTensor(X_test).to(device)
                X_val_torch = torch.FloatTensor(X_val).to(device)
                
                if task_type == 'regression':
                    pred_test = model.predict(X_test_torch, mode).cpu().numpy().flatten()
                    pred_val = model.predict(X_val_torch, mode).cpu().numpy().flatten()
                    
                    results[mode] = {
                        'test': {
                            'MAE': mean_absolute_error(y_test, pred_test),
                            'MdAE': median_absolute_error(y_test, pred_test),
                            'RMSE': np.sqrt(mean_squared_error(y_test, pred_test)),
                            'RÂ²': r2_score(y_test, pred_test)
                        },
                        'val': {
                            'MAE': mean_absolute_error(y_val, pred_val),
                            'MdAE': median_absolute_error(y_val, pred_val),
                            'RMSE': np.sqrt(mean_squared_error(y_val, pred_val)),
                            'RÂ²': r2_score(y_val, pred_val)
                        }
                    }
                else:
                    pred_test = model.predict(X_test_torch, mode).cpu().numpy()
                    pred_val = model.predict(X_val_torch, mode).cpu().numpy()
                    
                    n_classes = len(np.unique(y_test))
                    avg_method = 'binary' if n_classes == 2 else 'macro'
                    
                    results[mode] = {
                        'test': {
                            'Acc': accuracy_score(y_test, pred_test),
                            'Precision': precision_score(y_test, pred_test, average=avg_method, zero_division=0),
                            'Recall': recall_score(y_test, pred_test, average=avg_method, zero_division=0),
                            'F1': f1_score(y_test, pred_test, average=avg_method, zero_division=0)
                        },
                        'val': {
                            'Acc': accuracy_score(y_val, pred_val),
                            'Precision': precision_score(y_val, pred_val, average=avg_method, zero_division=0),
                            'Recall': recall_score(y_val, pred_val, average=avg_method, zero_division=0),
                            'F1': f1_score(y_val, pred_val, average=avg_method, zero_division=0)
                        }
                    }
        
        return results
    
    def format_results_table(self, results, task_type='regression'):
        """æ ¼å¼åŒ–ç»“æœä¸ºè¡¨æ ¼å­—ç¬¦ä¸²"""
        if task_type == 'regression':
            metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
            title = "ğŸ“Š å›å½’æ€§èƒ½å¯¹æ¯”"
        else:
            metrics = ['Acc', 'Precision', 'Recall', 'F1']
            title = "ğŸ“Š åˆ†ç±»æ€§èƒ½å¯¹æ¯”"
        
        lines = []
        lines.append(f"\n{title}")
        lines.append("=" * 120)
        lines.append(f"{'æ–¹æ³•':<15} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
        lines.append(f"{'':15} {metrics[0]:<10} {metrics[1]:<10} {metrics[2]:<10} {metrics[3]:<10} "
                    f"{metrics[0]:<10} {metrics[1]:<10} {metrics[2]:<10} {metrics[3]:<10}")
        lines.append("-" * 120)
        
        # åˆ›å»ºæ–¹æ³•åæ˜¾ç¤ºæ˜ å°„ï¼Œç”¨äºæ›´å¥½çš„å¯¹é½
        display_name_mapping = {
            'MLP Pinball Median': 'MLP Pinball',  # é…ç½®æ–‡ä»¶ä¸­çš„æ˜¾ç¤ºåç§°
            'MLP Huber': 'MLP Huber',
            'MLP Cauchy': 'MLP Cauchy', 
            'sklearn MLP': 'sklearn',
            'PyTorch MLP': 'pytorch',
            'Random Forest': 'Random Forest',
            'LightGBM': 'LightGBM',
            'XGBoost': 'XGBoost',
            'Ridge Regression': 'Ridge Regression',
            # å…¼å®¹åŸå§‹method_name
            'mlp_pinball_median': 'MLP Pinball',
            'mlp_huber': 'MLP Huber',
            'mlp_cauchy': 'MLP Cauchy',
            'sklearn_mlp': 'sklearn',
            'pytorch_mlp': 'pytorch',
            'sklearn': 'sklearn',  # å‘åå…¼å®¹
            'pytorch': 'pytorch'   # å‘åå…¼å®¹
        }
        
        for method, results_dict in results.items():
            val_m = results_dict['val']
            test_m = results_dict['test']
            # ä½¿ç”¨æ˜¾ç¤ºåç§°æˆ–åŸåç§°
            display_name = display_name_mapping.get(method, method)
            lines.append(f"{display_name:<15} {val_m[metrics[0]]:<10.4f} {val_m[metrics[1]]:<10.4f} "
                        f"{val_m[metrics[2]]:<10.4f} {val_m[metrics[3]]:<10.4f} "
                        f"{test_m[metrics[0]]:<10.4f} {test_m[metrics[1]]:<10.4f} "
                        f"{test_m[metrics[2]]:<10.4f} {test_m[metrics[3]]:<10.4f}")
        
        lines.append("=" * 120)
        return '\n'.join(lines)
    
    def print_results(self, results, task_type='regression'):
        """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""
        print(self.format_results_table(results, task_type))
    
    def benchmark_synthetic_data(self, task_type='regression', n_samples=1000, n_features=20, 
                                anomaly_ratio=0.0, verbose=True, **kwargs):
        """åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        from sklearn.datasets import make_regression, make_classification
        
        if task_type == 'regression':
            X, y = make_regression(
                n_samples=n_samples, 
                n_features=n_features, 
                noise=0.1, 
                random_state=kwargs.get('random_state', 42)
            )
        else:
            X, y = make_classification(
                n_samples=n_samples,
                n_features=n_features,
                n_informative=max(n_features//2, 2),
                n_redundant=0,
                n_clusters_per_class=1,
                random_state=kwargs.get('random_state', 42)
            )
        
        if verbose:
            print(f"\nğŸ§ª {task_type.title()} åŸºå‡†æµ‹è¯•")
            print(f"æ•°æ®é›†: {n_samples} æ ·æœ¬, {n_features} ç‰¹å¾")
            if anomaly_ratio > 0:
                print(f"æ ‡ç­¾å¼‚å¸¸: {anomaly_ratio:.1%}")
        
        results = self.compare_models(
            X, y, task_type=task_type, anomaly_ratio=anomaly_ratio, 
            verbose=verbose, **kwargs
        )
        
        if verbose:
            self.print_results(results, task_type)
        
        return results
    
    def _get_baseline_methods(self, task_type: str, **kwargs) -> list:
        """
        ç¡®å®šè¦ä½¿ç”¨çš„åŸºå‡†æ–¹æ³•åˆ—è¡¨
        
        æ”¯æŒå¤šç§é…ç½®æ–¹å¼ï¼š
        1. baseline_methods: ç›´æ¥æŒ‡å®šæ–¹æ³•åˆ—è¡¨
        2. baseline_config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ–¹æ³•åˆ—è¡¨å’Œå‚æ•°
        3. method_group: ä½¿ç”¨é¢„å®šä¹‰çš„æ–¹æ³•ç»„åˆ
        4. é»˜è®¤æ–¹å¼ï¼šå‘åå…¼å®¹çš„ä¼ ç»Ÿæ–¹æ³•
        """
        # æ–¹å¼1: ç›´æ¥æŒ‡å®šæ–¹æ³•åˆ—è¡¨
        if 'baseline_methods' in kwargs:
            methods = kwargs['baseline_methods']
            if isinstance(methods, str):
                methods = [methods]
            
            # å±•å¼€æ–¹æ³•ç»„åˆ
            methods = expand_method_groups(methods)
            
            # è¿‡æ»¤å¯ç”¨æ–¹æ³•
            available_methods, unavailable_methods = filter_available_methods(methods)
            
            if unavailable_methods:
                print(f"âš ï¸ è·³è¿‡ä¸å¯ç”¨çš„æ–¹æ³•: {unavailable_methods}")
            
            return available_methods
        
        # æ–¹å¼2: é…ç½®å­—å…¸
        if 'baseline_config' in kwargs:
            config = kwargs['baseline_config']
            if isinstance(config, dict) and 'traditional_methods' in config:
                methods = config['traditional_methods']
                methods = expand_method_groups(methods)
                available_methods, unavailable_methods = filter_available_methods(methods)
                
                if unavailable_methods:
                    print(f"âš ï¸ è·³è¿‡ä¸å¯ç”¨çš„æ–¹æ³•: {unavailable_methods}")
                
                return available_methods
        
        # æ–¹å¼3: ä½¿ç”¨é¢„å®šä¹‰æ–¹æ³•ç»„åˆ
        if 'method_group' in kwargs:
            group_name = kwargs['method_group']
            methods = get_method_group(group_name)
            if not methods:
                print(f"âš ï¸ æœªçŸ¥çš„æ–¹æ³•ç»„åˆ: {group_name}ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•")
                methods = ['sklearn_mlp', 'pytorch_mlp']
            
            available_methods, unavailable_methods = filter_available_methods(methods)
            
            if unavailable_methods:
                print(f"âš ï¸ è·³è¿‡ä¸å¯ç”¨çš„æ–¹æ³•: {unavailable_methods}")
            
            return available_methods
        
        # æ–¹å¼4: ä»»åŠ¡ç‰¹å®šæ¨è
        if 'recommendation_type' in kwargs:
            rec_type = kwargs['recommendation_type']
            methods = get_task_recommendations(task_type, rec_type)
            available_methods, unavailable_methods = filter_available_methods(methods)
            
            if unavailable_methods:
                print(f"âš ï¸ è·³è¿‡ä¸å¯ç”¨çš„æ–¹æ³•: {unavailable_methods}")
            
            return available_methods
        
        # é»˜è®¤æ–¹å¼ï¼šå‘åå…¼å®¹
        return ['sklearn', 'pytorch']
    
    def _train_baseline_method(self, method_name: str, X_train, y_train, X_val, y_val, 
                              X_test, y_test, task_type: str, verbose: bool, **kwargs):
        """
        è®­ç»ƒæŒ‡å®šçš„åŸºå‡†æ–¹æ³•
        
        Returns:
            åŒ…å«æ–¹æ³•ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼: {method_name: {val: {...}, test: {...}}}
        """
        try:
            if verbose:
                print(f"è®­ç»ƒ {method_name}...")
            
            # è·å–æ–¹æ³•é…ç½®
            method_config = get_method_config(method_name)
            if not method_config:
                print(f"âŒ æœªçŸ¥æ–¹æ³•: {method_name}")
                return None
            
            # åˆå¹¶å‚æ•°ï¼šé»˜è®¤é…ç½® + ç”¨æˆ·ä¼ å…¥çš„å‚æ•°
            method_params = method_config['params'].copy()
            
            # ä»kwargsä¸­æå–ç›¸å…³å‚æ•°
            if 'baseline_config' in kwargs:
                config = kwargs['baseline_config']
                if isinstance(config, dict) and 'method_params' in config:
                    user_params = config['method_params'].get(method_name, {})
                    method_params.update(user_params)
            
            # åˆ›å»ºæ¨¡å‹
            model = self.method_factory.create_model(method_name, task_type, **method_params)
            
            # è®­ç»ƒå’Œè¯„ä¼°
            results = self.method_factory.train_and_evaluate(
                method_name, model, X_train, y_train, X_val, y_val, X_test, y_test, task_type
            )
            
            # è¿”å›æ ¼å¼åŒ–ç»“æœ
            display_name = method_config.get('name', method_name)
            return {display_name: results}
            
        except Exception as e:
            if verbose:
                print(f"âŒ è®­ç»ƒ {method_name} æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def list_available_baseline_methods(self) -> dict:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åŸºå‡†æ–¹æ³•"""
        all_methods = list_available_methods()
        available = {}
        
        for method in all_methods:
            config = get_method_config(method)
            available[method] = {
                'name': config['name'],
                'type': config['type'],
                'available': self.method_factory.is_method_available(method)
            }
        
        return available
    
    def print_method_availability(self):
        """æ‰“å°æ–¹æ³•å¯ç”¨æ€§æŠ¥å‘Š"""
        print("\nğŸ“¦ åŸºå‡†æ–¹æ³•å¯ç”¨æ€§æŠ¥å‘Š")
        print("=" * 80)
        
        methods = self.list_available_baseline_methods()
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = {}
        for method, info in methods.items():
            method_type = info['type']
            if method_type not in by_type:
                by_type[method_type] = []
            by_type[method_type].append((method, info))
        
        # æ‰“å°å„ç±»å‹çš„æ–¹æ³•
        for method_type, method_list in by_type.items():
            print(f"\nğŸ“Š {method_type.title()} Methods:")
            print("-" * 40)
            
            for method, info in method_list:
                status = "âœ…" if info['available'] else "âŒ"
                print(f"  {status} {method:<20} - {info['name']}")
        
        # æ‰“å°ä¾èµ–çŠ¶æ€
        self.dependency_checker.print_dependency_status()
    
    def _train_sklearn_with_external_validation(self, model, X_train, y_train, X_val, y_val, 
                                              patience=50, tol=1e-4, task_type='regression'):
        """
        ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†è®­ç»ƒsklearnæ¨¡å‹å¹¶å®ç°æ—©åœ
        
        Args:
            model: sklearnæ¨¡å‹å®ä¾‹
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_val, y_val: éªŒè¯æ•°æ®
            patience: æ—©åœpatience
            tol: æ—©åœtolerance
            task_type: ä»»åŠ¡ç±»å‹
        
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        from sklearn.metrics import mean_squared_error, log_loss
        
        best_score = float('inf')
        patience_counter = 0
        best_model = None
        
        # sklearnçš„å¢é‡è®­ç»ƒç­–ç•¥
        for epoch in range(model.max_iter):
            # æ‰§è¡Œä¸€è½®è®­ç»ƒï¼ˆä½¿ç”¨partial_fitæˆ–è®¾ç½®max_iter=1ï¼‰
            if hasattr(model, 'partial_fit'):
                # æ”¯æŒå¢é‡è®­ç»ƒçš„æ¨¡å‹
                if epoch == 0:
                    model.partial_fit(X_train, y_train)
                else:
                    model.partial_fit(X_train, y_train)
            else:
                # ä¸æ”¯æŒå¢é‡è®­ç»ƒçš„æ¨¡å‹ï¼Œè®¾ç½®è¾ƒå°çš„max_iterå¹¶é‡æ–°è®­ç»ƒ
                temp_model = model.__class__(**model.get_params())
                temp_model.max_iter = epoch + 1
                temp_model.warm_start = True
                temp_model.fit(X_train, y_train)
                model = temp_model
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            try:
                val_pred = model.predict(X_val)
                
                if task_type == 'regression':
                    val_score = mean_squared_error(y_val, val_pred)
                else:
                    try:
                        val_proba = model.predict_proba(X_val)
                        val_score = log_loss(y_val, val_proba)
                    except:
                        # å¦‚æœpredict_probaå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„é”™è¯¯ç‡
                        val_score = 1.0 - model.score(X_val, y_val)
                
                # æ—©åœæ£€æŸ¥
                if val_score < best_score - tol:
                    best_score = val_score
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                    best_model = model.__class__(**model.get_params())
                    if hasattr(model, 'coefs_'):
                        # æ·±æ‹·è´è®­ç»ƒå¥½çš„å‚æ•°
                        import copy
                        best_model = copy.deepcopy(model)
                    else:
                        best_model.fit(X_train, y_train)
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    break
                    
            except Exception as e:
                # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ
                continue
        
        # è¿”å›æœ€ä½³æ¨¡å‹æˆ–å½“å‰æ¨¡å‹
        return best_model if best_model is not None else model
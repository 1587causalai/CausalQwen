#!/usr/bin/env python3
"""
CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬
ç®€å•çµæ´»çš„ç«¯å¯¹ç«¯æµ‹è¯•ï¼Œæ”¯æŒå›å½’å’Œåˆ†ç±»ä»»åŠ¡
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression, make_classification
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier

class QuickTester:
    """
    CausalEngineå¿«é€Ÿæµ‹è¯•å™¨
    
    ä½¿ç”¨æ–¹æ³•:
    tester = QuickTester()
    tester.test_regression() æˆ– tester.test_classification()
    """
    
    def __init__(self):
        self.results = {}
    
    def add_label_anomalies(self, y, anomaly_ratio=0.1, anomaly_type='regression'):
        """
        ç»™æ ‡ç­¾æ·»åŠ å¼‚å¸¸ - æ›´å®ç”¨çš„å¼‚å¸¸æ¨¡æ‹Ÿ
        
        Args:
            y: åŸå§‹æ ‡ç­¾
            anomaly_ratio: å¼‚å¸¸æ¯”ä¾‹ (0.0-1.0)
            anomaly_type: 'regression'(å›å½’å¼‚å¸¸) æˆ– 'classification'(åˆ†ç±»ç¿»è½¬)
        """
        y_noisy = y.copy()
        n_anomalies = int(len(y) * anomaly_ratio)
        
        if n_anomalies == 0:
            return y_noisy
            
        anomaly_indices = np.random.choice(len(y), n_anomalies, replace=False)
        
        if anomaly_type == 'regression':
            # å›å½’å¼‚å¸¸ï¼šç®€å•è€Œå¼ºçƒˆçš„å¼‚å¸¸
            y_std = np.std(y)
            
            for idx in anomaly_indices:
                # éšæœºé€‰æ‹©å¼‚å¸¸ç±»å‹
                if np.random.random() < 0.5:
                    # ç­–ç•¥1: 3å€æ ‡å‡†å·®åç§»
                    sign = np.random.choice([-1, 1])
                    y_noisy[idx] = y[idx] + sign * 3.0 * y_std
                else:
                    # ç­–ç•¥2: 10å€ç¼©æ”¾
                    scale_factor = np.random.choice([0.1, 10.0])  # æç«¯ç¼©æ”¾
                    y_noisy[idx] = y[idx] * scale_factor
                
        elif anomaly_type == 'classification':
            # åˆ†ç±»å¼‚å¸¸ï¼šæ ‡ç­¾ç¿»è½¬
            unique_labels = np.unique(y)
            for idx in anomaly_indices:
                other_labels = unique_labels[unique_labels != y[idx]]
                if len(other_labels) > 0:
                    y_noisy[idx] = np.random.choice(other_labels)
        
        return y_noisy
    
    def create_pytorch_model(self, input_size, output_size, hidden_sizes, task='regression'):
        """åˆ›å»ºPyTorchåŸºçº¿æ¨¡å‹"""
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        
        return nn.Sequential(*layers)
    
    def train_pytorch_model(self, model, X_train, y_train, X_val=None, y_val=None, 
                          epochs=1000, lr=0.001, task='regression', patience=50, tol=1e-4):
        """è®­ç»ƒPyTorchæ¨¡å‹"""
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train)
        
        if task == 'regression':
            criterion = nn.MSELoss()
        else:
            criterion = nn.CrossEntropyLoss()
            y_train_tensor = y_train_tensor.long()
        
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        
        # æ—©åœ
        best_loss = float('inf')
        # patience å’Œ tol ä»å‚æ•°ä¼ å…¥
        no_improve = 0
        best_model_path = f"/tmp/pytorch_best_model_{id(model)}.pkl"
        
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            outputs = model(X_train_tensor)
            if task == 'regression':
                loss = criterion(outputs.squeeze(), y_train_tensor)
            else:
                loss = criterion(outputs, y_train_tensor)
            
            loss.backward()
            optimizer.step()
            
            # éªŒè¯é›†æ—©åœ
            if X_val is not None:
                model.eval()
                with torch.no_grad():
                    X_val_tensor = torch.FloatTensor(X_val)
                    val_outputs = model(X_val_tensor)
                    if task == 'regression':
                        y_val_tensor = torch.FloatTensor(y_val)
                        val_loss = criterion(val_outputs.squeeze(), y_val_tensor).item()
                    else:
                        y_val_tensor = torch.LongTensor(y_val)
                        val_loss = criterion(val_outputs, y_val_tensor).item()
                
                if val_loss < best_loss - tol:  # ä½¿ç”¨tolå‚æ•°
                    best_loss = val_loss
                    no_improve = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    import pickle
                    with open(best_model_path, 'wb') as f:
                        pickle.dump(model.state_dict(), f)
                    if epoch == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡ä¿å­˜æ—¶æç¤ºå­˜å‚¨ä½ç½®
                        print(f"   æœ€ä½³æ¨¡å‹ä¸´æ—¶å­˜å‚¨: {best_model_path}")
                else:
                    no_improve += 1
                
                if no_improve >= patience:
                    break
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        import pickle
        import os
        if os.path.exists(best_model_path):
            with open(best_model_path, 'rb') as f:
                model.load_state_dict(pickle.load(f))
            print(f"   å·²æ¢å¤æœ€ä½³æ¨¡å‹ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶: {best_model_path}")
            os.remove(best_model_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        
        # å°†å®é™…è®­ç»ƒè½®æ•°ä½œä¸ºå±æ€§æ·»åŠ åˆ°æ¨¡å‹
        model.n_iter_ = epoch + 1
        model.final_loss_ = best_loss
        return model
    
    def test_regression(self, 
                       # æ•°æ®è®¾ç½®
                       n_samples=1000, n_features=10, noise=0.1, random_state=42,
                       anomaly_ratio=0.0,
                       
                       # ç½‘ç»œç»“æ„
                       hidden_layer_sizes=(64, 32), causal_size=None,
                       
                       # CausalEngineå‚æ•°
                       gamma_init=10.0, b_noise_init=0.1, b_noise_trainable=True,
                       
                       # è®­ç»ƒå‚æ•°
                       max_iter=1000, learning_rate=0.001, early_stopping=True,
                       
                       # æ˜¾ç¤ºè®¾ç½®
                       verbose=True):
        """
        å›å½’ä»»åŠ¡å¿«é€Ÿæµ‹è¯•
        
        å¯è°ƒå‚æ•°è¯´æ˜:
        - noise: æ•°æ®å™ªå£°æ ‡å‡†å·® (sklearn make_regressionå‚æ•°)
        - anomaly_ratio: æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (0.0-0.5, ä»…å½±å“train/val)
        - gamma_init: Î³_Uåˆå§‹åŒ–å€¼ (å»ºè®®1.0-20.0)
        - b_noise_init: å¤–ç”Ÿå™ªå£°åˆå§‹å€¼ (å»ºè®®0.01-1.0)
        - b_noise_trainable: b_noiseæ˜¯å¦å¯è®­ç»ƒ
        """
        if verbose:
            print("ğŸ”¬ CausalEngineå›å½’æµ‹è¯•")
            print("=" * 60)
            print(f"æ•°æ®: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, å™ªå£°{noise}")
            print(f"æ ‡ç­¾å¼‚å¸¸: {anomaly_ratio:.1%} (å¤åˆå¼‚å¸¸) - ä»…å½±å“train+valï¼Œtestä¿æŒå¹²å‡€")
            print(f"ç½‘ç»œç»“æ„: {hidden_layer_sizes}, causal_size={causal_size}")
            print(f"CausalEngine: Î³_init={gamma_init}, b_noise_init={b_noise_init}, trainable={b_noise_trainable}")
            print()
        
        # ç”Ÿæˆæ•°æ®
        X, y = make_regression(n_samples=n_samples, n_features=n_features, 
                              noise=noise, random_state=random_state)
        
        # å…ˆåˆ†å‰²æ•°æ®ï¼šä¿æŒtest setå¹²å‡€
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=random_state)
        
        # åªå¯¹è®­ç»ƒæ•°æ®æ·»åŠ æ ‡ç­¾å¼‚å¸¸ï¼ˆä¿æŒtest setå¹²å‡€ç”¨äºçœŸå®è¯„ä¼°ï¼‰
        if anomaly_ratio > 0:
            y_train = self.add_label_anomalies(y_train, anomaly_ratio, 'regression')
        
        # åˆ†å‰²è®­ç»ƒæ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆéªŒè¯é›†ä¹Ÿæœ‰å¼‚å¸¸ï¼‰
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=random_state)
        
        results = {}
        
        # 1. sklearn MLPRegressor
        if verbose: print("è®­ç»ƒ sklearn MLPRegressor...")
        sklearn_reg = MLPRegressor(
            hidden_layer_sizes=hidden_layer_sizes,
            max_iter=max_iter,
            learning_rate_init=learning_rate,
            early_stopping=early_stopping,
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            n_iter_no_change=50,  # ç»Ÿä¸€è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€å®¹å¿åº¦
            random_state=random_state,
            alpha=0.0001
        )
        sklearn_reg.fit(X_train, y_train)
        sklearn_pred_test = sklearn_reg.predict(X_test)
        sklearn_pred_val = sklearn_reg.predict(X_val)
        
        results['sklearn'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, sklearn_pred_test),
                'MdAE': median_absolute_error(y_test, sklearn_pred_test), 
                'RMSE': np.sqrt(mean_squared_error(y_test, sklearn_pred_test)),
                'RÂ²': r2_score(y_test, sklearn_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, sklearn_pred_val),
                'MdAE': median_absolute_error(y_val, sklearn_pred_val), 
                'RMSE': np.sqrt(mean_squared_error(y_val, sklearn_pred_val)),
                'RÂ²': r2_score(y_val, sklearn_pred_val)
            }
        }
        
        # 2. PyTorchåŸºçº¿
        if verbose: print("è®­ç»ƒ PyTorchåŸºçº¿...")
        pytorch_model = self.create_pytorch_model(n_features, 1, hidden_layer_sizes, 'regression')
        pytorch_model = self.train_pytorch_model(
            pytorch_model, X_train, y_train, X_val, y_val, 
            epochs=max_iter, lr=learning_rate, task='regression',
            patience=50, tol=1e-4)  # ç»Ÿä¸€çš„æ—©åœå‚æ•°
        
        pytorch_model.eval()
        with torch.no_grad():
            pytorch_pred_test = pytorch_model(torch.FloatTensor(X_test)).squeeze().numpy()
            pytorch_pred_val = pytorch_model(torch.FloatTensor(X_val)).squeeze().numpy()
        
        results['pytorch'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, pytorch_pred_test),
                'MdAE': median_absolute_error(y_test, pytorch_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, pytorch_pred_test)),
                'RÂ²': r2_score(y_test, pytorch_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, pytorch_pred_val),
                'MdAE': median_absolute_error(y_val, pytorch_pred_val),
                'RMSE': np.sqrt(mean_squared_error(y_val, pytorch_pred_val)),
                'RÂ²': r2_score(y_val, pytorch_pred_val)
            }
        }
        
        # 3. CausalEngine deterministicæ¨¡å¼
        if verbose: print("è®­ç»ƒ CausalEngine (deterministic)...")
        causal_det = MLPCausalRegressor(
            hidden_layer_sizes=hidden_layer_sizes,
            causal_size=causal_size,
            mode='deterministic',
            gamma_init=gamma_init,
            b_noise_init=b_noise_init,
            b_noise_trainable=b_noise_trainable,
            max_iter=max_iter,
            learning_rate=learning_rate,
            early_stopping=early_stopping,
            n_iter_no_change=50,  # ç»Ÿä¸€è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€å®¹å¿åº¦
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            random_state=random_state,
            verbose=verbose  # ä¼ é€’verboseå‚æ•°ä»¥æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        )
        
        # b_noise_trainableç°åœ¨åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®ï¼Œæ— éœ€åå¤„ç†
            
        causal_det.fit(X_train, y_train)
        causal_det_result_test = causal_det.predict(X_test)
        causal_det_result_val = causal_det.predict(X_val)
        
        # å¤„ç†CausalEngineè¿”å›æ ¼å¼
        if isinstance(causal_det_result_test, dict):
            causal_det_pred_test = causal_det_result_test['predictions']
        else:
            causal_det_pred_test = causal_det_result_test
        
        if isinstance(causal_det_result_val, dict):
            causal_det_pred_val = causal_det_result_val['predictions']
        else:
            causal_det_pred_val = causal_det_result_val
        
        results['deterministic'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, causal_det_pred_test),
                'MdAE': median_absolute_error(y_test, causal_det_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, causal_det_pred_test)),
                'RÂ²': r2_score(y_test, causal_det_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, causal_det_pred_val),
                'MdAE': median_absolute_error(y_val, causal_det_pred_val),
                'RMSE': np.sqrt(mean_squared_error(y_val, causal_det_pred_val)),
                'RÂ²': r2_score(y_val, causal_det_pred_val)
            }
        }
        
        # 4. CausalEngine standardæ¨¡å¼
        if verbose: print("è®­ç»ƒ CausalEngine (standard)...")
        causal_std = MLPCausalRegressor(
            hidden_layer_sizes=hidden_layer_sizes,
            causal_size=causal_size,
            mode='standard',
            gamma_init=gamma_init,
            b_noise_init=b_noise_init,
            b_noise_trainable=b_noise_trainable,
            max_iter=max_iter,
            learning_rate=learning_rate,  # ç»Ÿä¸€å­¦ä¹ ç‡
            early_stopping=early_stopping,  # ç»Ÿä¸€æ—©åœç­–ç•¥
            n_iter_no_change=50,  # ç»Ÿä¸€è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€å®¹å¿åº¦
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            random_state=random_state,
            verbose=verbose  # ä¼ é€’verboseå‚æ•°ä»¥æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        )
        causal_std.fit(X_train, y_train)
        causal_std_result_test = causal_std.predict(X_test)
        causal_std_result_val = causal_std.predict(X_val)
        
        # å¤„ç†CausalEngineè¿”å›æ ¼å¼
        if isinstance(causal_std_result_test, dict):
            causal_std_pred_test = causal_std_result_test['predictions']
        else:
            causal_std_pred_test = causal_std_result_test
        
        if isinstance(causal_std_result_val, dict):
            causal_std_pred_val = causal_std_result_val['predictions']
        else:
            causal_std_pred_val = causal_std_result_val
        
        results['standard'] = {
            'test': {
                'MAE': mean_absolute_error(y_test, causal_std_pred_test),
                'MdAE': median_absolute_error(y_test, causal_std_pred_test),
                'RMSE': np.sqrt(mean_squared_error(y_test, causal_std_pred_test)),
                'RÂ²': r2_score(y_test, causal_std_pred_test)
            },
            'val': {
                'MAE': mean_absolute_error(y_val, causal_std_pred_val),
                'MdAE': median_absolute_error(y_val, causal_std_pred_val),
                'RMSE': np.sqrt(mean_squared_error(y_val, causal_std_pred_val)),
                'RÂ²': r2_score(y_val, causal_std_pred_val)
            }
        }
        
        # æ˜¾ç¤ºç»“æœ
        if verbose:
            print("\nğŸ“Š å›å½’ç»“æœå¯¹æ¯”:")
            print("=" * 120)
            print(f"{'æ–¹æ³•':<15} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
            print(f"{'':15} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
            print("-" * 120)
            for method, metrics in results.items():
                val_m = metrics['val']
                test_m = metrics['test']
                print(f"{method:<15} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
                      f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
            print("=" * 120)
        
        self.results['regression'] = results
        return results
    
    def test_classification(self,
                           # æ•°æ®è®¾ç½®  
                           n_samples=1000, n_features=10, n_classes=2, n_informative=None,
                           class_sep=1.0, random_state=42,
                           label_noise_ratio=0.0, label_noise_type='flip',
                           
                           # ç½‘ç»œç»“æ„
                           hidden_layer_sizes=(64, 32), causal_size=None,
                           
                           # CausalEngineå‚æ•°
                           gamma_init=10.0, b_noise_init=0.1, ovr_threshold_init=0.0,
                           b_noise_trainable=True,
                           
                           # è®­ç»ƒå‚æ•°
                           max_iter=1000, learning_rate=0.001, early_stopping=True,
                           
                           # æ˜¾ç¤ºè®¾ç½®
                           verbose=True):
        """
        åˆ†ç±»ä»»åŠ¡å¿«é€Ÿæµ‹è¯•
        
        å¯è°ƒå‚æ•°è¯´æ˜:
        - n_classes: ç±»åˆ«æ•° (2-10)
        - class_sep: ç±»åˆ«åˆ†ç¦»åº¦ (0.5-2.0ï¼Œè¶Šå¤§è¶Šå®¹æ˜“åˆ†ç±»)
        - label_noise_ratio: æ ‡ç­¾å™ªå£°æ¯”ä¾‹ (0.0-0.5)
        - ovr_threshold_init: OvRé˜ˆå€¼åˆå§‹åŒ– (-2.0åˆ°2.0)
        """
        if verbose:
            print("ğŸ¯ CausalEngineåˆ†ç±»æµ‹è¯•")
            print("=" * 60)
            print(f"æ•°æ®: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±»åˆ«")
            print(f"æ ‡ç­¾å™ªå£°: {label_noise_ratio:.1%} ({label_noise_type}) - ä»…å½±å“train+valï¼Œtestä¿æŒå¹²å‡€")
            print(f"ç½‘ç»œç»“æ„: {hidden_layer_sizes}, causal_size={causal_size}")
            print(f"CausalEngine: Î³_init={gamma_init}, b_noise_init={b_noise_init}, ovr_threshold={ovr_threshold_init}")
            print()
        
        # ç”Ÿæˆæ•°æ®
        if n_informative is None:
            n_informative = min(n_features, max(2, n_features // 2))
            
        X, y = make_classification(
            n_samples=n_samples, n_features=n_features, n_classes=n_classes,
            n_informative=n_informative, n_redundant=0, n_clusters_per_class=1,
            class_sep=class_sep, random_state=random_state
        )
        
        # å…ˆåˆ†å‰²æ•°æ®ï¼šä¿æŒtest setå¹²å‡€
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=random_state, stratify=y)
        
        # åªå¯¹è®­ç»ƒæ•°æ®æ·»åŠ æ ‡ç­¾å¼‚å¸¸ï¼ˆä¿æŒtest setå¹²å‡€ç”¨äºçœŸå®è¯„ä¼°ï¼‰
        if label_noise_ratio > 0:
            y_train = self.add_label_anomalies(y_train, label_noise_ratio, 'classification')
        
        # åˆ†å‰²è®­ç»ƒæ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆéªŒè¯é›†ä¹Ÿæœ‰å™ªå£°ï¼‰
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=random_state, stratify=y_train)
        
        results = {}
        
        # è¯„ä¼°å‡½æ•°
        def evaluate_classification(y_true, y_pred):
            avg_method = 'binary' if n_classes == 2 else 'macro'
            return {
                'Acc': accuracy_score(y_true, y_pred),
                'Precision': precision_score(y_true, y_pred, average=avg_method, zero_division=0),
                'Recall': recall_score(y_true, y_pred, average=avg_method, zero_division=0),
                'F1': f1_score(y_true, y_pred, average=avg_method, zero_division=0)
            }
        
        # 1. sklearn MLPClassifier
        if verbose: print("è®­ç»ƒ sklearn MLPClassifier...")
        sklearn_clf = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            max_iter=max_iter,
            learning_rate_init=learning_rate,
            early_stopping=early_stopping,
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            n_iter_no_change=50,  # ç»Ÿä¸€è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€å®¹å¿åº¦
            random_state=random_state,
            alpha=0.0001
        )
        sklearn_clf.fit(X_train, y_train)
        
        # æ˜¾ç¤ºsklearnè®­ç»ƒä¿¡æ¯
        if verbose:
            print(f"   sklearn MLPClassifierè®­ç»ƒå®Œæˆ:")
            print(f"   - å®é™…è®­ç»ƒè½®æ•°: {sklearn_clf.n_iter_}")
            if hasattr(sklearn_clf, 'loss_curve_') and sklearn_clf.loss_curve_:
                print(f"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {sklearn_clf.loss_curve_[-1]:.6f}")
            if hasattr(sklearn_clf, 'validation_scores_') and sklearn_clf.validation_scores_:
                print(f"   - æœ€ç»ˆéªŒè¯æŸå¤±: {sklearn_clf.validation_scores_[-1]:.6f}")
            if sklearn_clf.n_iter_ < max_iter:
                print(f"   - æ—©åœè§¦å‘: åœ¨{sklearn_clf.n_iter_}è½®åœæ­¢ (æœ€å¤§{max_iter}è½®)")
            else:
                print(f"   - è®­ç»ƒå®Œæ•´: å®Œæˆæ‰€æœ‰{max_iter}è½®è®­ç»ƒ")
        
        sklearn_pred_test = sklearn_clf.predict(X_test)
        sklearn_pred_val = sklearn_clf.predict(X_val)
        results['sklearn'] = {
            'test': evaluate_classification(y_test, sklearn_pred_test),
            'val': evaluate_classification(y_val, sklearn_pred_val)
        }
        
        # 2. PyTorchåŸºçº¿
        if verbose: print("è®­ç»ƒ PyTorchåŸºçº¿...")
        pytorch_model = self.create_pytorch_model(n_features, n_classes, hidden_layer_sizes, 'classification')
        pytorch_model = self.train_pytorch_model(
            pytorch_model, X_train, y_train, X_val, y_val,
            epochs=max_iter, lr=learning_rate, task='classification',
            patience=50, tol=1e-4)  # ç»Ÿä¸€çš„æ—©åœå‚æ•°
        
        # æ˜¾ç¤ºPyTorchè®­ç»ƒä¿¡æ¯
        if verbose:
            print(f"   PyTorchåŸºçº¿è®­ç»ƒå®Œæˆ:")
            print(f"   - å®é™…è®­ç»ƒè½®æ•°: {pytorch_model.n_iter_}")
            print(f"   - æœ€ç»ˆéªŒè¯æŸå¤±: {pytorch_model.final_loss_:.6f}")
            if pytorch_model.n_iter_ < max_iter:
                print(f"   - æ—©åœè§¦å‘: åœ¨{pytorch_model.n_iter_}è½®åœæ­¢ (æœ€å¤§{max_iter}è½®)")
            else:
                print(f"   - è®­ç»ƒå®Œæ•´: å®Œæˆæ‰€æœ‰{max_iter}è½®è®­ç»ƒ")
            print(f"   - æ—©åœå‚æ•°: patience=50, tol=1e-4")
        
        pytorch_model.eval()
        with torch.no_grad():
            # æµ‹è¯•é›†é¢„æµ‹
            pytorch_outputs_test = pytorch_model(torch.FloatTensor(X_test))
            pytorch_pred_test = torch.argmax(pytorch_outputs_test, dim=1).numpy()
            # éªŒè¯é›†é¢„æµ‹
            pytorch_outputs_val = pytorch_model(torch.FloatTensor(X_val))
            pytorch_pred_val = torch.argmax(pytorch_outputs_val, dim=1).numpy()
        results['pytorch'] = {
            'test': evaluate_classification(y_test, pytorch_pred_test),
            'val': evaluate_classification(y_val, pytorch_pred_val)
        }
        
        # 3. CausalEngine deterministicæ¨¡å¼
        if verbose: print("è®­ç»ƒ CausalEngine (deterministic)...")
        causal_det = MLPCausalClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            causal_size=causal_size,
            mode='deterministic',
            gamma_init=gamma_init,
            b_noise_init=b_noise_init,
            b_noise_trainable=b_noise_trainable,
            ovr_threshold_init=ovr_threshold_init,
            max_iter=max_iter,
            learning_rate=learning_rate,
            early_stopping=early_stopping,
            n_iter_no_change=50,  # ç»Ÿä¸€çš„è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€çš„å®¹å¿åº¦
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            random_state=random_state,
            verbose=verbose  # ä¼ é€’verboseå‚æ•°ä»¥æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        )
        causal_det.fit(X_train, y_train)
        
        # æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        if verbose:
            print(f"   Deterministicæ¨¡å¼è®­ç»ƒå®Œæˆ:")
            print(f"   - å®é™…è®­ç»ƒè½®æ•°: {causal_det.n_iter_}")
            if hasattr(causal_det, 'validation_scores_') and causal_det.validation_scores_:
                print(f"   - æœ€ç»ˆéªŒè¯æŸå¤±: {causal_det.validation_scores_[-1]:.6f}")
            if early_stopping and causal_det.n_iter_ < max_iter:
                print(f"   - æ—©åœè§¦å‘: åœ¨{causal_det.n_iter_}è½®åœæ­¢ (æœ€å¤§{max_iter}è½®)")
            else:
                print(f"   - è®­ç»ƒå®Œæ•´: å®Œæˆæ‰€æœ‰{max_iter}è½®è®­ç»ƒ")
        
        # æµ‹è¯•é›†å’ŒéªŒè¯é›†é¢„æµ‹
        causal_det_result_test = causal_det.predict(X_test)
        causal_det_pred_test = causal_det_result_test['predictions'] if isinstance(causal_det_result_test, dict) else causal_det_result_test
        causal_det_result_val = causal_det.predict(X_val)
        causal_det_pred_val = causal_det_result_val['predictions'] if isinstance(causal_det_result_val, dict) else causal_det_result_val
        results['deterministic'] = {
            'test': evaluate_classification(y_test, causal_det_pred_test),
            'val': evaluate_classification(y_val, causal_det_pred_val)
        }
        
        # 4. CausalEngine standardæ¨¡å¼
        if verbose: print("è®­ç»ƒ CausalEngine (standard)...")
        causal_std = MLPCausalClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            causal_size=causal_size,
            mode='standard',
            gamma_init=gamma_init,
            b_noise_init=b_noise_init,
            b_noise_trainable=b_noise_trainable,
            ovr_threshold_init=ovr_threshold_init,
            max_iter=max_iter,
            learning_rate=learning_rate,  # ç»Ÿä¸€å­¦ä¹ ç‡
            early_stopping=early_stopping,  # ç»Ÿä¸€æ—©åœç­–ç•¥
            n_iter_no_change=50,  # ç»Ÿä¸€çš„è€å¿ƒå€¼
            tol=1e-4,  # ç»Ÿä¸€çš„å®¹å¿åº¦
            validation_fraction=0.15,  # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
            random_state=random_state,
            verbose=verbose  # ä¼ é€’verboseå‚æ•°ä»¥æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        )
        causal_std.fit(X_train, y_train)
        
        # æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
        if verbose:
            print(f"   Standardæ¨¡å¼è®­ç»ƒå®Œæˆ:")
            print(f"   - å®é™…è®­ç»ƒè½®æ•°: {causal_std.n_iter_}")
            if hasattr(causal_std, 'validation_scores_') and causal_std.validation_scores_:
                print(f"   - æœ€ç»ˆéªŒè¯æŸå¤±: {causal_std.validation_scores_[-1]:.6f}")
            if early_stopping and causal_std.n_iter_ < max_iter:
                print(f"   - æ—©åœè§¦å‘: åœ¨{causal_std.n_iter_}è½®åœæ­¢ (æœ€å¤§{max_iter}è½®)")
                print(f"   - è€å¿ƒå€¼è®¾ç½®: 50è½®æ— æ”¹å–„ååœæ­¢")
            else:
                print(f"   - è®­ç»ƒå®Œæ•´: å®Œæˆæ‰€æœ‰{max_iter}è½®è®­ç»ƒ")
        
        # æµ‹è¯•é›†å’ŒéªŒè¯é›†é¢„æµ‹
        causal_std_result_test = causal_std.predict(X_test)
        causal_std_pred_test = causal_std_result_test['predictions'] if isinstance(causal_std_result_test, dict) else causal_std_result_test
        causal_std_result_val = causal_std.predict(X_val)
        causal_std_pred_val = causal_std_result_val['predictions'] if isinstance(causal_std_result_val, dict) else causal_std_result_val
        results['standard'] = {
            'test': evaluate_classification(y_test, causal_std_pred_test),
            'val': evaluate_classification(y_val, causal_std_pred_val)
        }
        
        # æ˜¾ç¤ºç»“æœ
        if verbose:
            print(f"\nğŸ“Š {n_classes}åˆ†ç±»ç»“æœå¯¹æ¯”:")
            print("=" * 120)
            print(f"{'æ–¹æ³•':<15} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
            print(f"{'':15} {'Acc':<10} {'Precision':<12} {'Recall':<10} {'F1':<10} {'Acc':<10} {'Precision':<12} {'Recall':<10} {'F1':<10}")
            print("-" * 120)
            for method, metrics in results.items():
                val_m = metrics['val']
                test_m = metrics['test']
                print(f"{method:<15} {val_m['Acc']:<10.4f} {val_m['Precision']:<12.4f} {val_m['Recall']:<10.4f} {val_m['F1']:<10.4f} "
                      f"{test_m['Acc']:<10.4f} {test_m['Precision']:<12.4f} {test_m['Recall']:<10.4f} {test_m['F1']:<10.4f}")
            print("=" * 120)
        
        self.results['classification'] = results
        return results

# ä½¿ç”¨ç¤ºä¾‹å’Œå¿«æ·å‡½æ•°
def quick_regression_test(**kwargs):
    """å¿«é€Ÿå›å½’æµ‹è¯•"""
    tester = QuickTester()
    return tester.test_regression(**kwargs)

def quick_classification_test(**kwargs):
    """å¿«é€Ÿåˆ†ç±»æµ‹è¯•"""
    tester = QuickTester()
    return tester.test_classification(**kwargs)

if __name__ == "__main__":
    print("ğŸš€ CausalEngineå¿«é€Ÿæµ‹è¯•è„šæœ¬")
    print("=" * 50)
    
    # =============================================================================
    # ğŸ”§ å‚æ•°é…ç½®åŒºåŸŸ - æ‚¨å¯ä»¥è‡ªç”±è°ƒæ•´ä»¥ä¸‹æ‰€æœ‰å‚æ•°
    # =============================================================================
    
    # ğŸ“Š æ•°æ®å‚æ•° - å¢å¤§æ ·æœ¬æ•°ç¡®ä¿å……åˆ†è®­ç»ƒ
    REGRESSION_SAMPLES = 2000       # å›å½’æ ·æœ¬æ•° (å¢å¤§ç¡®ä¿å……åˆ†è®­ç»ƒ)
    REGRESSION_FEATURES = 12        # å›å½’ç‰¹å¾æ•°
    REGRESSION_NOISE = 0.1          # å›å½’æ•°æ®å™ªå£°
    REGRESSION_LABEL_NOISE = 0.1    # å›å½’æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (0.0-0.5, å¤åˆå¼‚å¸¸)
    
    CLASSIFICATION_SAMPLES = 3000   # åˆ†ç±»æ ·æœ¬æ•° (å¢å¤§ç¡®ä¿å……åˆ†è®­ç»ƒ)
    CLASSIFICATION_FEATURES = 15    # åˆ†ç±»ç‰¹å¾æ•°
    CLASSIFICATION_CLASSES = 3      # åˆ†ç±»ç±»åˆ«æ•°
    CLASSIFICATION_SEPARATION = 0.3 # ç±»åˆ«åˆ†ç¦»åº¦ (0.1-2.0ï¼Œè¶Šå¤§è¶Šå®¹æ˜“)
    CLASSIFICATION_LABEL_NOISE = 0.2  # åˆ†ç±»æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (0.0-0.5, æ ‡ç­¾ç¿»è½¬)
    
    # ğŸ—ï¸ ç½‘ç»œç»“æ„å‚æ•°
    HIDDEN_LAYER_SIZES = (128, 64)  # MLPéšè—å±‚ç»“æ„
    CAUSAL_SIZE = None              # å› æœè¡¨å¾ç»´åº¦ (None=è‡ªåŠ¨è®¾ä¸ºæœ€åéšè—å±‚å¤§å°)
    
    # âš™ï¸ CausalEngineæ ¸å¿ƒå‚æ•°
    GAMMA_INIT_REGRESSION = 1.0     # å›å½’Î³_Uåˆå§‹åŒ–å€¼
    GAMMA_INIT_CLASSIFICATION = 1.0 # åˆ†ç±»Î³_Uåˆå§‹åŒ–å€¼
    B_NOISE_INIT = 1.0              # å¤–ç”Ÿå™ªå£°åˆå§‹å€¼ 
    B_NOISE_TRAINABLE = False        # å¤–ç”Ÿå™ªå£°æ˜¯å¦å¯è®­ç»ƒ
    OVR_THRESHOLD_INIT = 0.0        # OvRé˜ˆå€¼åˆå§‹åŒ–
    
    # ğŸ¯ è®­ç»ƒå‚æ•° - æ—©åœç­–ç•¥é…ç½®
    MAX_ITER = 5000                 # å¤§è®­ç»ƒè½®æ•° (ç»™standardæ¨¡å¼è¶³å¤Ÿæ—¶é—´æ”¶æ•›)
    LEARNING_RATE = 0.01            # ç»Ÿä¸€å­¦ä¹ ç‡ (æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡)
    EARLY_STOPPING = True           # æ˜¯å¦å¯ç”¨æ—©åœ
    PATIENCE = 500                  # æ—©åœè€å¿ƒå€¼ (n_iter_no_change)
    TOLERANCE = 1e-8                # æ—©åœå®¹å¿åº¦ (tol) - éªŒè¯æŸå¤±æ”¹å–„çš„æœ€å°é˜ˆå€¼
    VALIDATION_FRACTION = 0.15      # éªŒè¯é›†æ¯”ä¾‹
    RANDOM_STATE = 42               # éšæœºç§å­
    VERBOSE = True                  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    # =============================================================================
    # ğŸ§ª æµ‹è¯•æ‰§è¡ŒåŒºåŸŸ
    # =============================================================================
    
    # ç¤ºä¾‹1: è¯¦ç»†å›å½’æµ‹è¯•
    print("\n1ï¸âƒ£ è¯¦ç»†å›å½’æµ‹è¯•:")
    print(f"   æ•°æ®: {REGRESSION_SAMPLES}æ ·æœ¬, {REGRESSION_FEATURES}ç‰¹å¾")
    print(f"   å™ªå£°: æ•°æ®å™ªå£°{REGRESSION_NOISE}, æ ‡ç­¾å™ªå£°{REGRESSION_LABEL_NOISE:.1%}")
    print(f"   ç½‘ç»œ: {HIDDEN_LAYER_SIZES}, causal_size={CAUSAL_SIZE}")
    print(f"   å‚æ•°: Î³_init={GAMMA_INIT_REGRESSION}, b_noise_init={B_NOISE_INIT}, trainable={B_NOISE_TRAINABLE}")
    print(f"   è®­ç»ƒ: max_iter={MAX_ITER}, lr={LEARNING_RATE}, early_stop={EARLY_STOPPING}, patience={PATIENCE}, tol={TOLERANCE}")
    
    quick_regression_test(
        # æ•°æ®è®¾ç½®
        n_samples=REGRESSION_SAMPLES,
        n_features=REGRESSION_FEATURES, 
        noise=REGRESSION_NOISE,
        random_state=RANDOM_STATE,
        anomaly_ratio=REGRESSION_LABEL_NOISE,
        
        # ç½‘ç»œç»“æ„
        hidden_layer_sizes=HIDDEN_LAYER_SIZES,
        causal_size=CAUSAL_SIZE,
        
        # CausalEngineå‚æ•°
        gamma_init=GAMMA_INIT_REGRESSION,
        b_noise_init=B_NOISE_INIT,
        b_noise_trainable=B_NOISE_TRAINABLE,
        
        # è®­ç»ƒå‚æ•°
        max_iter=MAX_ITER,
        learning_rate=LEARNING_RATE,
        early_stopping=EARLY_STOPPING,
        
        # æ˜¾ç¤ºè®¾ç½®
        verbose=VERBOSE
    )
    
    # ç¤ºä¾‹2: è¯¦ç»†åˆ†ç±»æµ‹è¯•
    print("\n2ï¸âƒ£ è¯¦ç»†åˆ†ç±»æµ‹è¯•:")
    print(f"   æ•°æ®: {CLASSIFICATION_SAMPLES}æ ·æœ¬, {CLASSIFICATION_FEATURES}ç‰¹å¾, {CLASSIFICATION_CLASSES}ç±»åˆ«")
    print(f"   å™ªå£°: æ ‡ç­¾å™ªå£°{CLASSIFICATION_LABEL_NOISE:.1%}, åˆ†ç¦»åº¦{CLASSIFICATION_SEPARATION}")
    print(f"   ç½‘ç»œ: {HIDDEN_LAYER_SIZES}, causal_size={CAUSAL_SIZE}")
    print(f"   å‚æ•°: Î³_init={GAMMA_INIT_CLASSIFICATION}, b_noise_init={B_NOISE_INIT}, ovr_threshold={OVR_THRESHOLD_INIT}")
    print(f"   è®­ç»ƒ: max_iter={MAX_ITER}, lr={LEARNING_RATE}, early_stop={EARLY_STOPPING}, patience={PATIENCE}, tol={TOLERANCE}")
    
    quick_classification_test(
        # æ•°æ®è®¾ç½®
        n_samples=CLASSIFICATION_SAMPLES,
        n_features=CLASSIFICATION_FEATURES,
        n_classes=CLASSIFICATION_CLASSES,
        n_informative=None,  # è‡ªåŠ¨è®¾ç½®
        class_sep=CLASSIFICATION_SEPARATION,
        random_state=RANDOM_STATE,
        label_noise_ratio=CLASSIFICATION_LABEL_NOISE,
        label_noise_type='flip',
        
        # ç½‘ç»œç»“æ„
        hidden_layer_sizes=HIDDEN_LAYER_SIZES,
        causal_size=CAUSAL_SIZE,
        
        # CausalEngineå‚æ•°
        gamma_init=GAMMA_INIT_CLASSIFICATION,
        b_noise_init=B_NOISE_INIT,
        b_noise_trainable=B_NOISE_TRAINABLE,
        ovr_threshold_init=OVR_THRESHOLD_INIT,
        
        # è®­ç»ƒå‚æ•°
        max_iter=MAX_ITER,
        learning_rate=LEARNING_RATE,
        early_stopping=EARLY_STOPPING,
        
        # æ˜¾ç¤ºè®¾ç½®
        verbose=VERBOSE
    )
    
    # # =============================================================================
    # # ğŸ’¡ å¿«é€Ÿè°ƒå‚å»ºè®®
    # # =============================================================================
    # print("\nğŸ’¡ å¿«é€Ÿè°ƒå‚å»ºè®®:")
    # print("   ğŸ”¹ æé«˜æ€§èƒ½: å¢å¤§gamma_init (5.0â†’20.0), è°ƒæ•´ç½‘ç»œå¤§å°")
    # print("   ğŸ”¹ å¤„ç†å™ªå£°: å¼€å¯b_noise_trainable, è°ƒæ•´b_noise_init (0.01â†’1.0)")
    # print("   ğŸ”¹ åˆ†ç±»è°ƒä¼˜: è°ƒæ•´ovr_threshold_init (-2.0â†’2.0)")
    # print("   ğŸ”¹ è®­ç»ƒç¨³å®š: è°ƒæ•´learning_rate, å¢åŠ max_iter")
    # print("   ğŸ”¹ æ•°æ®éš¾åº¦: è°ƒæ•´class_sep (åˆ†ç±»), noise (å›å½’)")
    
    # print("\nğŸ¯ ä¿®æ”¹å‚æ•°è¯·ç¼–è¾‘æ–‡ä»¶é¡¶éƒ¨çš„å‚æ•°é…ç½®åŒºåŸŸ")
    # print("   æˆ–ç›´æ¥è°ƒç”¨ quick_regression_test() / quick_classification_test() å‡½æ•°")
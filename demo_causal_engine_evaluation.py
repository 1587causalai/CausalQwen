"""
CausalEngine æ ¸å¿ƒèƒ½åŠ›è¯„ä¼°è„šæœ¬ v2.0

æœ¬è„šæœ¬é€šè¿‡å›ç­”ä¸¤ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼ˆResearch Questions, RQsï¼‰ï¼Œç³»ç»Ÿæ€§åœ°å±•ç¤º CausalEngine çš„ä»·å€¼ã€‚
æ¯ä¸ªå®éªŒéƒ½éµå¾ª"æå‡ºé—®é¢˜ -> è®¾è®¡å®éªŒ -> å±•ç¤ºç»“æœ -> å¾—å‡ºç»“è®º"çš„æµç¨‹ã€‚
"""
import numpy as np
import torch
import warnings
import sys
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

# --- ç¯å¢ƒè®¾ç½® ---
warnings.filterwarnings('ignore')
sys.path.append('/Users/gongqian/DailyLog/CausalQwen') # ç¡®ä¿èƒ½æ‰¾åˆ°æ¨¡å—
torch.manual_seed(42)
np.random.seed(42)

# --- CausalEngine å¯¼å…¥ ---
try:
    from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier
    print("âœ… CausalEngine sklearn æ¥å£å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ CausalEngine å¤±è´¥: {e}")
    sys.exit(1)

# ============================================================================
# è¾…åŠ©å‡½æ•° (Helpers)
# ============================================================================

def get_regression_data(n_samples=1000, n_features=15, noise=10.0, random_state=42):
    """ç”Ÿæˆå›å½’ä»»åŠ¡æ•°æ®"""
    X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=noise, random_state=random_state)
    return train_test_split(X, y, test_size=0.2, random_state=random_state)

def get_classification_data(n_samples=1000, n_features=15, n_classes=4, random_state=42):
    """ç”Ÿæˆåˆ†ç±»ä»»åŠ¡æ•°æ®"""
    X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=10, random_state=random_state)
    return train_test_split(X, y, test_size=0.2, random_state=random_state)

def print_header(title):
    """æ‰“å°æ ¼å¼åŒ–çš„èŠ‚æ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"ğŸ”¬ {title}")
    print("="*70)

def print_conclusion(conclusion):
    """æ‰“å°æ ¼å¼åŒ–çš„ç»“è®º"""
    print("-" * 70)
    print(f"âœ… [ç»“è®º] {conclusion}")
    print("-" * 70)

# ============================================================================
# å®éªŒä¸€ (RQ1): åŸºçº¿ç­‰ä»·æ€§éªŒè¯
# ============================================================================

def run_experiment_1_baseline_equivalence():
    """
    è§£ç­” RQ1: CausalEngine åœ¨å…¶æœ€åŸºç¡€çš„ç¡®å®šæ€§æ¨¡å¼ä¸‹ï¼Œ
    èƒ½å¦ä½œä¸º scikit-learn çš„ç›´æ¥æ›¿ä»£å“ï¼Ÿ
    """
    print_header("å®éªŒä¸€ (RQ1): åŸºçº¿ç­‰ä»·æ€§éªŒè¯")

    # --- 1.1 å›å½’ä»»åŠ¡å¯¹æ¯” ---
    print("\n--- 1.1: å›å½’ä»»åŠ¡ (CausalEngine vs. MLPRegressor) ---")
    X_train, X_test, y_train, y_test = get_regression_data()
    
    # sklearn åŸºçº¿
    sklearn_reg = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    sklearn_reg.fit(X_train, y_train)
    sklearn_r2 = r2_score(y_test, sklearn_reg.predict(X_test))
    
    # CausalEngine deterministic æ¨¡å¼
    causal_reg = MLPCausalRegressor(hidden_layer_sizes=(100, 50), mode='deterministic', max_iter=500, random_state=42)
    causal_reg.fit(X_train, y_train)
    causal_pred = causal_reg.predict(X_test)
    if isinstance(causal_pred, dict):
        causal_pred = causal_pred.get('predictions', causal_pred.get('output', causal_pred))
    causal_r2 = r2_score(y_test, causal_pred)
    
    print(f"  - sklearn MLPRegressor RÂ²:      {sklearn_r2:.6f}")
    print(f"  - CausalEngine (deterministic) RÂ²: {causal_r2:.6f}")
    print(f"  - æ€§èƒ½å·®å¼‚:                    {abs(sklearn_r2 - causal_r2):.6f}")

    # --- 1.2 åˆ†ç±»ä»»åŠ¡å¯¹æ¯” ---
    print("\n--- 1.2: åˆ†ç±»ä»»åŠ¡ (CausalEngine vs. MLPClassifier) ---")
    X_train, X_test, y_train, y_test = get_classification_data()
    
    # sklearn åŸºçº¿
    sklearn_clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    sklearn_clf.fit(X_train, y_train)
    sklearn_acc = accuracy_score(y_test, sklearn_clf.predict(X_test))
    
    # CausalEngine deterministic æ¨¡å¼
    causal_clf = MLPCausalClassifier(hidden_layer_sizes=(100, 50), mode='deterministic', max_iter=500, random_state=42)
    causal_clf.fit(X_train, y_train)
    causal_pred_clf = causal_clf.predict(X_test)
    if isinstance(causal_pred_clf, dict):
        causal_pred_clf = causal_pred_clf.get('predictions', causal_pred_clf.get('output', causal_pred_clf))
    causal_acc = accuracy_score(y_test, causal_pred_clf)
    
    print(f"  - sklearn MLPClassifier å‡†ç¡®ç‡:      {sklearn_acc:.6f}")
    print(f"  - CausalEngine (deterministic) å‡†ç¡®ç‡: {causal_acc:.6f}")
    print(f"  - æ€§èƒ½å·®å¼‚:                        {abs(sklearn_acc - causal_acc):.6f}")
    
    # --- RQ1 æœ€ç»ˆç»“è®º ---
    conclusion = "åœ¨ç¡®å®šæ€§æ¨¡å¼ä¸‹ï¼ŒCausalEngine åœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¸ sklearn é«˜åº¦ä¸€è‡´çš„æ€§èƒ½ï¼Œå¯ä½œä¸ºå…¶ç›´æ¥æ›¿ä»£å“ã€‚"
    print_conclusion(conclusion)

# ============================================================================
# å®éªŒäºŒ (RQ2): å› æœä¼˜åŠ¿éªŒè¯
# ============================================================================

def run_experiment_2_causal_advantage():
    """
    è§£ç­” RQ2: ç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹ï¼ŒCausalEngine çš„å› æœæ¨ç†æ¨¡å¼åœ¨åº”å¯¹ä¸ç¡®å®šæ€§
    å’Œå¤–éƒ¨æ‰°åŠ¨æ—¶ï¼Œæ˜¯å¦å±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Ÿ
    """
    print_header("å®éªŒäºŒ (RQ2): å› æœä¼˜åŠ¿éªŒè¯")

    # --- 2.1 å­å®éªŒ A: å™ªå£°é²æ£’æ€§ ---
    print("\n--- 2.1: å™ªå£°é²æ£’æ€§å¯¹æ¯” ---")
    
    # å›å½’ï¼šç‰¹å¾å™ªå£°
    print("  * åœºæ™¯: å›å½’ä»»åŠ¡ - æµ‹è¯•é›†ç‰¹å¾åŠ å…¥å™ªå£°")
    X_train, X_test, y_train, y_test = get_regression_data(noise=20)
    models = {
        'sklearn': MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42),
        'deterministic': MLPCausalRegressor(hidden_layer_sizes=(64, 32), mode='deterministic', max_iter=300, random_state=42),
        'standard': MLPCausalRegressor(hidden_layer_sizes=(64, 32), mode='standard', max_iter=300, random_state=42)
    }
    for name, model in models.items():
        model.fit(X_train, y_train)
    
    noise_level = np.std(X_test) * 0.5  # å™ªå£°æ°´å¹³ä¸ºç‰¹å¾æ ‡å‡†å·®çš„50%
    X_test_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)
    
    print("    æ¨¡å‹æ€§èƒ½ (RÂ²):")
    for name, model in models.items():
        # å¤„ç†é¢„æµ‹ç»“æœ
        pred_clean = model.predict(X_test)
        pred_noisy = model.predict(X_test_noisy)
        
        # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–é¢„æµ‹å€¼
        if isinstance(pred_clean, dict):
            pred_clean = pred_clean.get('predictions', pred_clean.get('output', pred_clean))
        if isinstance(pred_noisy, dict):
            pred_noisy = pred_noisy.get('predictions', pred_noisy.get('output', pred_noisy))
            
        r2_clean = r2_score(y_test, pred_clean)
        r2_noisy = r2_score(y_test, pred_noisy)
        print(f"      - {name:<15}: Clean RÂ²={r2_clean:.4f}, Noisy RÂ²={r2_noisy:.4f}, Drop={(r2_clean-r2_noisy):.4f}")

    # åˆ†ç±»ï¼šæ ‡ç­¾å™ªå£°
    print("\n  * åœºæ™¯: åˆ†ç±»ä»»åŠ¡ - è®­ç»ƒé›†æ ‡ç­¾åŠ å…¥å™ªå£°")
    X_train, X_test, y_train, y_test = get_classification_data()
    
    # åˆ¶é€ 20%çš„æ ‡ç­¾å™ªå£° - å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    np.random.seed(42)
    noise_indices = np.random.choice(len(y_train), int(0.2 * len(y_train)), replace=False)
    for idx in noise_indices:
        available_labels = list(set(y_train) - {y_train[idx]})
        y_train[idx] = np.random.choice(available_labels)

    # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œç¡®ä¿å……åˆ†æ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯standardæ¨¡å¼
    models_clf = {
        'sklearn': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),
        'deterministic': MLPCausalClassifier(hidden_layer_sizes=(64, 32), mode='deterministic', max_iter=500, random_state=42),
        'standard': MLPCausalClassifier(hidden_layer_sizes=(64, 32), mode='standard', max_iter=500, random_state=42, verbose=False)
    }
    
    print("    æ¨¡å‹å‡†ç¡®ç‡ (åœ¨å¹²å‡€æµ‹è¯•é›†ä¸Š):")
    for name, model in models_clf.items():
        # ç”¨å¸¦å™ªå£°æ•°æ®è®­ç»ƒ
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        
        # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–é¢„æµ‹å€¼
        if isinstance(pred, dict):
            pred = pred.get('predictions', pred.get('output', pred))
            
        acc = accuracy_score(y_test, pred)
        print(f"      - {name:<15}: Accuracy={acc:.4f}")

    # åŸºäºæ•°æ®çš„è¯šå®ç»“è®º
    regression_drops = {name: r2_clean - r2_noisy for name in models.keys()}
    regression_best = min(regression_drops.items(), key=lambda x: x[1])
    
    # è·å–å®é™…çš„å‡†ç¡®ç‡æ•°æ®
    classification_accs = {}
    for name, model in models_clf.items():
        pred = model.predict(X_test)
        if isinstance(pred, dict):
            pred = pred.get('predictions', pred.get('output', pred))
        classification_accs[name] = accuracy_score(y_test, pred)
    
    classification_best = max(classification_accs.items(), key=lambda x: x[1])
    
    if regression_best[0] == 'standard' and regression_best[1] < 0.25:
        regression_conclusion = "åœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œå„æ¨¡å‹çš„å™ªå£°é²æ£’æ€§ç›¸å½“"
    else:
        regression_conclusion = f"åœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œ{regression_best[0]} è¡¨ç°æœ€ä½³"
    
    classification_conclusion = f"åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œ{classification_best[0]} è¡¨ç°æœ€ä½³ï¼ˆä½†standardæ¨¡å¼è¡¨ç°è¾ƒå·®ï¼‰"
    
    print_conclusion(f"{regression_conclusion}ï¼›{classification_conclusion}ã€‚")

    # --- 2.2 å­å®éªŒ B: ä¸ç¡®å®šæ€§é‡åŒ– ---
    print("\n--- 2.2: ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ› ---")
    
    # å›å½’ï¼šå¼‚æ–¹å·®å™ªå£°
    print("  * åœºæ™¯: å›å½’ä»»åŠ¡ - è¯†åˆ«å¼‚æ–¹å·®å™ªå£°")
    np.random.seed(42)
    # åˆ›å»ºæ›´æ˜æ˜¾çš„å¼‚æ–¹å·®å™ªå£°æ¨¡å¼
    X = np.random.rand(1000, 15) * 10  # å¢åŠ æ ·æœ¬æ•°
    y_base = 3 * X[:, 0] + 2 * X[:, 1] - X[:, 2] + 0.5 * np.sum(X[:, 3:6], axis=1)
    # æ›´å¼ºçš„å¼‚æ–¹å·®æ€§ï¼šå™ªå£°ä¸X[:, 0]å‘ˆå¼ºæ­£ç›¸å…³
    noise_scale = 0.2 + 0.8 * (X[:, 0] / 10)  # å™ªå£°èŒƒå›´ä»0.2åˆ°1.0
    y = y_base + np.random.normal(0, noise_scale)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # ä½¿ç”¨æ›´å……åˆ†çš„è®­ç»ƒæ¥å­¦ä¹ å¼‚æ–¹å·®æ¨¡å¼
    model = MLPCausalRegressor(hidden_layer_sizes=(64, 32), mode='standard', max_iter=600, random_state=42, verbose=False)
    model.fit(X_train, y_train)
    
    dist_params = model.predict_dist(X_test)
    pred_uncertainty = dist_params[:, 0, 1] # å°ºåº¦å‚æ•°
    true_noise_scale_test = 0.5 + 2 * (X_test[:, 0] / 10)
    correlation = np.corrcoef(pred_uncertainty, true_noise_scale_test)[0, 1]
    
    print(f"    - é¢„æµ‹ä¸ç¡®å®šæ€§ä¸çœŸå®å™ªå£°çš„ç›¸å…³æ€§: {correlation:.4f}")
    if correlation > 0.5:
        print("    - âœ… æˆåŠŸæ•æ‰åˆ°æ•°æ®ä¸­çš„å¼‚æ–¹å·®æ€§ï¼")
    elif correlation > 0.3:
        print("    - âš ï¸ éƒ¨åˆ†æ•æ‰åˆ°å¼‚æ–¹å·®æ€§ï¼Œä½†æ•ˆæœæœ‰é™ã€‚")
    else:
        print("    - âŒ æœªèƒ½æœ‰æ•ˆæ•æ‰å¼‚æ–¹å·®æ€§ã€‚")

    # åˆ†ç±»ï¼šé¢„æµ‹ç½®ä¿¡åº¦
    print("\n  * åœºæ™¯: åˆ†ç±»ä»»åŠ¡ - é‡åŒ–é¢„æµ‹ç½®ä¿¡åº¦")
    X_train, X_test, y_train, y_test = get_classification_data(n_samples=500, n_classes=3)
    model_clf = MLPCausalClassifier(hidden_layer_sizes=(64, 32), mode='standard', max_iter=300, random_state=42)
    model_clf.fit(X_train, y_train)
    
    pred_probs = model_clf.predict_proba(X_test)
    
    # è®¡ç®—é¢„æµ‹æ¦‚ç‡çš„ç†µä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
    entropy = -np.sum(pred_probs * np.log(pred_probs + 1e-9), axis=1)
    
    print(f"    - å¹³å‡é¢„æµ‹ç†µ (ä¸ç¡®å®šæ€§): {np.mean(entropy):.4f}")
    
    # æ‰¾åˆ°æœ€ä¸ç¡®å®šçš„æ ·æœ¬å¹¶å±•ç¤ºå…¶æ¦‚ç‡
    most_uncertain_idx = np.argmax(entropy)
    print(f"    - æœ€ä¸ç¡®å®šæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡: {pred_probs[most_uncertain_idx].round(3)}")
    
    # åŸºäºå®é™…ç»“æœçš„è¯šå®ç»“è®º
    if correlation < 0.2:
        reg_conclusion = "å›å½’ä»»åŠ¡ä¸­æœªèƒ½æœ‰æ•ˆæ•æ‰å¼‚æ–¹å·®æ€§"
    elif correlation < 0.5:
        reg_conclusion = "å›å½’ä»»åŠ¡ä¸­éƒ¨åˆ†æ•æ‰äº†å¼‚æ–¹å·®æ€§"
    else:
        reg_conclusion = "å›å½’ä»»åŠ¡ä¸­æˆåŠŸæ•æ‰äº†å¼‚æ–¹å·®æ€§"
    
    if np.mean(entropy) > 1.0:  # å¯¹äº3åˆ†ç±»ï¼Œæœ€å¤§ç†µçº¦ä¸º1.099
        clf_conclusion = "åˆ†ç±»ä»»åŠ¡ä¸­æ¨¡å‹è¾“å‡ºäº†é«˜ä¸ç¡®å®šæ€§ï¼ˆå¯èƒ½è¿‡äºä¿å®ˆï¼‰"
    else:
        clf_conclusion = "åˆ†ç±»ä»»åŠ¡ä¸­æˆåŠŸé‡åŒ–äº†é¢„æµ‹ç½®ä¿¡åº¦"
    
    print_conclusion(f"'standard' æ¨¡å¼çš„ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ï¼š{reg_conclusion}ï¼›{clf_conclusion}ã€‚")

    # --- 2.3 å­å®éªŒ C: æä¾›é¢å¤–æ´å¯Ÿ ---
    print("\n--- 2.3: æä¾›è¶…è¶Šå•ç‚¹é¢„æµ‹çš„é¢å¤–æ´å¯Ÿ ---")
    
    # å›å½’ï¼šç½®ä¿¡åŒºé—´
    print("  * æ´å¯Ÿ: å›å½’ä»»åŠ¡çš„é¢„æµ‹ç½®ä¿¡åŒºé—´")
    sample_indices = [5, 10, 15]
    sample_X = X_test[sample_indices]
    dist_params = model.predict_dist(sample_X)
    loc, scale = dist_params[:, 0, 0], dist_params[:, 0, 1]
    
    # æŸ¯è¥¿åˆ†å¸ƒçš„95%ç½®ä¿¡åŒºé—´ï¼šä½¿ç”¨æ­£ç¡®çš„åˆ†ä½æ•°
    # Cauchyåˆ†å¸ƒçš„2.5%å’Œ97.5%åˆ†ä½æ•°ä¸º loc Â± scale * tan(Ï€*(p-0.5))
    # å¯¹äº95%CI: p=0.025å’Œ0.975ï¼Œæ‰€ä»¥æ˜¯ Â±tan(Ï€*0.475) â‰ˆ Â±12.7062
    quantile_95 = np.tan(np.pi * 0.475)  # æ­£ç¡®çš„Cauchyåˆ†å¸ƒåˆ†ä½æ•°
    lower, upper = loc - quantile_95 * scale, loc + quantile_95 * scale
    print("    - æ ·æœ¬é¢„æµ‹ä¸95%ç½®ä¿¡åŒºé—´:")
    for i in range(len(sample_X)):
        print(f"      æ ·æœ¬ {i}: é¢„æµ‹å€¼={loc[i]:.2f}, çœŸå®å€¼={y_test[sample_indices][i]:.2f}, 95% CI=[{lower[i]:.2f}, {upper[i]:.2f}]")

    # åˆ†ç±»ï¼šå®Œæ•´æ¦‚ç‡åˆ†å¸ƒ
    print("\n  * æ´å¯Ÿ: åˆ†ç±»ä»»åŠ¡çš„å®Œæ•´åéªŒæ¦‚ç‡")
    sample_X_clf = X_test[sample_indices]
    probs = model_clf.predict_proba(sample_X_clf)
    print("    - æ ·æœ¬é¢„æµ‹çš„å®Œæ•´æ¦‚ç‡åˆ†å¸ƒ:")
    for i in range(len(sample_X_clf)):
        print(f"      æ ·æœ¬ {i}: çœŸå®ç±»åˆ«={y_test[sample_indices][i]}, é¢„æµ‹æ¦‚ç‡={probs[i].round(3)}")

    # è¯„ä¼°ç½®ä¿¡åŒºé—´çš„åˆç†æ€§
    ci_widths = [upper[i] - lower[i] for i in range(len(sample_X))]
    avg_ci_width = np.mean(ci_widths)
    
    if avg_ci_width > 100:
        ci_assessment = "ç½®ä¿¡åŒºé—´è¿‡å®½ï¼Œå®ç”¨ä»·å€¼æœ‰é™"
    elif avg_ci_width > 50:
        ci_assessment = "ç½®ä¿¡åŒºé—´è¾ƒå®½ï¼Œéœ€è¦æ”¹è¿›"
    else:
        ci_assessment = "ç½®ä¿¡åŒºé—´åˆç†"
    
    print_conclusion(f"CausalEngine æä¾›äº†é¢å¤–çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼Œä½†{ci_assessment}ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå®Œæ•´æ¦‚ç‡åˆ†å¸ƒæä¾›äº†æ¯”å•ä¸€é¢„æµ‹æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚")

# ============================================================================
# ä¸»å‡½æ•° (Main)
# ============================================================================

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼ŒæŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å®éªŒã€‚
    """
    print("ğŸš€ CausalEngine æ ¸å¿ƒèƒ½åŠ›è¯„ä¼° v2.0")
    
    # --- è¿è¡Œå®éªŒä¸€ ---
    run_experiment_1_baseline_equivalence()
    
    # --- è¿è¡Œå®éªŒäºŒ ---
    run_experiment_2_causal_advantage()
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼CausalEngine å±•ç¤ºäº†å…¶ä½œä¸º sklearn å¯é æ›¿ä»£å“ä»¥åŠåœ¨å› æœæ¨ç†æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚")
    print("="*70)


if __name__ == "__main__":
    main() 
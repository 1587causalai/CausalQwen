"""
CausalEngine å®é™…åº”ç”¨æ¼”ç¤º v3.0

åŸºäºå·²éªŒè¯çš„æ•°å­¦ç­‰ä»·æ€§ï¼Œå±•ç¤ºCausalEngineåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨ä»·å€¼ï¼š
1. sklearnæ›¿ä»£èƒ½åŠ›éªŒè¯  
2. äº”æ¨¡å¼å› æœæ¨ç†å…‰è°±
3. ä¸ç¡®å®šæ€§é‡åŒ–ä¸é²æ£’æ€§
4. å®é™…æ•°æ®é›†åº”ç”¨æ¡ˆä¾‹
5. é«˜çº§åŠŸèƒ½æ¼”ç¤º

æ ¸å¿ƒç‰¹æ€§ï¼š
- åŸºäºä¼˜åŒ–æ¶æ„ï¼ˆæ™ºèƒ½ç»´åº¦å¯¹é½ + å‰å‘ä¼ æ’­bypassï¼‰
- æ— éœ€å¤æ‚é…ç½®ï¼Œå¼€ç®±å³ç”¨
- å®Œæ•´çš„sklearnå…¼å®¹æ€§
- ä¸°å¯Œçš„å› æœæ¨ç†èƒ½åŠ›
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression, make_classification, load_diabetes, load_wine
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, classification_report
from sklearn.preprocessing import StandardScaler
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

# å¯¼å…¥CausalEngine sklearnæ¥å£
try:
    from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier
    print("âœ… CausalEngine sklearnæ¥å£å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def demo_sklearn_replacement():
    """æ¼”ç¤ºCausalEngineä½œä¸ºsklearnçš„ç›´æ¥æ›¿ä»£å“"""
    print("\n" + "="*60)
    print("ğŸ”„ sklearnç›´æ¥æ›¿ä»£æ¼”ç¤º")
    print("="*60)
    
    # å›å½’ä»»åŠ¡å¯¹æ¯”
    print("\nğŸ“Š å›å½’ä»»åŠ¡å¯¹æ¯”:")
    X_reg, y_reg = make_regression(n_samples=1000, n_features=15, noise=0.1, random_state=42)
    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)
    
    # sklearnåŸºçº¿
    sklearn_reg = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    sklearn_reg.fit(X_train_reg, y_train_reg)
    sklearn_pred = sklearn_reg.predict(X_test_reg)
    sklearn_r2 = r2_score(y_test_reg, sklearn_pred)
    
    # CausalEngine deterministicæ¨¡å¼ï¼ˆç›´æ¥æ›¿ä»£ï¼‰
    causal_reg = MLPCausalRegressor(
        hidden_layer_sizes=(100, 50), 
        mode='deterministic',  # ç­‰ä»·äºsklearn
        max_iter=500, 
        random_state=42
    )
    causal_reg.fit(X_train_reg, y_train_reg)
    causal_pred = causal_reg.predict(X_test_reg)
    if isinstance(causal_pred, dict):
        causal_pred = causal_pred['predictions']
    causal_r2 = r2_score(y_test_reg, causal_pred)
    
    print(f"  sklearn MLPRegressor:      RÂ² = {sklearn_r2:.6f}")
    print(f"  CausalEngine deterministic: RÂ² = {causal_r2:.6f}")
    print(f"  å·®å¼‚: {abs(causal_r2 - sklearn_r2):.6f} (åº”è¯¥å¾ˆå°)")
    
    # åˆ†ç±»ä»»åŠ¡å¯¹æ¯”
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡å¯¹æ¯”:")
    X_clf, y_clf = make_classification(n_samples=1000, n_features=15, n_classes=4, 
                                      n_informative=10, random_state=42)
    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)
    
    # sklearnåŸºçº¿
    sklearn_clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    sklearn_clf.fit(X_train_clf, y_train_clf)
    sklearn_acc = accuracy_score(y_test_clf, sklearn_clf.predict(X_test_clf))
    
    # CausalEngine deterministicæ¨¡å¼
    causal_clf = MLPCausalClassifier(
        hidden_layer_sizes=(100, 50),
        mode='deterministic',
        max_iter=500,
        random_state=42
    )
    causal_clf.fit(X_train_clf, y_train_clf)
    causal_pred_clf = causal_clf.predict(X_test_clf)
    if isinstance(causal_pred_clf, dict):
        causal_pred_clf = causal_pred_clf['predictions']
    causal_acc = accuracy_score(y_test_clf, causal_pred_clf)
    
    print(f"  sklearn MLPClassifier:      å‡†ç¡®ç‡ = {sklearn_acc:.6f}")
    print(f"  CausalEngine deterministic: å‡†ç¡®ç‡ = {causal_acc:.6f}")
    print(f"  å·®å¼‚: {abs(causal_acc - sklearn_acc):.6f} (åº”è¯¥å¾ˆå°)")
    
    print("\nâœ… CausalEngineå¯ä»¥ä½œä¸ºsklearnçš„ç›´æ¥æ›¿ä»£å“ï¼")
    return {
        'regression': {'sklearn': sklearn_r2, 'causal': causal_r2},
        'classification': {'sklearn': sklearn_acc, 'causal': causal_acc}
    }


def demo_five_modes_spectrum():
    """æ¼”ç¤ºäº”æ¨¡å¼å› æœæ¨ç†å…‰è°±"""
    print("\n" + "="*60)
    print("ğŸŒˆ äº”æ¨¡å¼å› æœæ¨ç†å…‰è°±æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆæœ‰å™ªå£°çš„æ•°æ®
    X, y = make_regression(n_samples=800, n_features=10, noise=0.2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    modes = {
        'deterministic': 'ç¡®å®šæ€§é¢„æµ‹ï¼ˆç­‰ä»·sklearnï¼‰',
        'exogenous': 'å¤–ç”Ÿå™ªå£°æ¨ç†',
        'endogenous': 'å†…ç”Ÿå› æœæ¨ç†', 
        'standard': 'æ ‡å‡†å› æœæ¨ç†',
        'sampling': 'æ¢ç´¢æ€§å› æœæ¨ç†'
    }
    
    print(f"\nğŸš€ äº”æ¨¡å¼æ€§èƒ½è¡¨æ ¼:")
    print("+" + "-"*70 + "+")
    print(f"| {'æ¨¡å¼':<12} | {'RÂ²':<8} | {'MSE':<10} | {'æè¿°':<20} |")
    print("+" + "-"*70 + "+")
    
    results = {}
    for mode, description in modes.items():
        # è®­ç»ƒæ¨¡å‹
        reg = MLPCausalRegressor(
            hidden_layer_sizes=(64, 32),
            mode=mode,
            max_iter=300,
            random_state=42,
            verbose=False
        )
        
        reg.fit(X_train, y_train)
        pred = reg.predict(X_test)
        
        if isinstance(pred, dict):
            pred = pred['predictions']
            
        r2 = r2_score(y_test, pred)
        mse = mean_squared_error(y_test, pred)
        
        results[mode] = {'r2': r2, 'mse': mse, 'model': reg}
        print(f"| {mode:<12} | {r2:<8.4f} | {mse:<10.1f} | {description:<20} |")
    
    print("+" + "-"*70 + "+")
    
    # æ¼”ç¤ºæ¨¡å¼ç‰¹æœ‰åŠŸèƒ½
    print(f"\nğŸ” æ¨¡å¼ç‰¹æœ‰åŠŸèƒ½æ¼”ç¤º:")
    
    # ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆédeterministicæ¨¡å¼ï¼‰
    for mode in ['endogenous', 'standard', 'sampling']:
        model = results[mode]['model']
        
        if hasattr(model, 'predict_dist'):
            dist_params = model.predict_dist(X_test[:3])
            if dist_params is not None:
                uncertainty = dist_params[:, 0, 1]  # å°ºåº¦å‚æ•°
                print(f"  {mode}: å¹³å‡ä¸ç¡®å®šæ€§ = {np.mean(uncertainty):.4f}")
    
    return results


def demo_uncertainty_quantification():
    """æ¼”ç¤ºä¸ç¡®å®šæ€§é‡åŒ–çš„å®é™…ä»·å€¼"""
    print("\n" + "="*60)
    print("ğŸŒ¡ï¸ ä¸ç¡®å®šæ€§é‡åŒ–å®é™…åº”ç”¨")
    print("="*60)
    
    # åˆ›å»ºå¼‚è´¨å™ªå£°æ•°æ®ï¼ˆä¸åŒåŒºåŸŸå™ªå£°ä¸åŒï¼‰
    np.random.seed(42)
    X = np.random.randn(600, 8)
    
    # åŸºç¡€ä¿¡å·
    y_base = 2 * X[:, 0] + 1.5 * X[:, 1] - X[:, 2] + 0.5 * np.sum(X[:, 3:6], axis=1)
    
    # å¼‚è´¨å™ªå£°ï¼šæ ¹æ®X[:, 0]çš„å€¼å†³å®šå™ªå£°å¤§å°
    noise_scale = 0.1 + 0.4 * np.abs(X[:, 0])  # å™ªå£°éšX[:, 0]å˜åŒ–
    noise = np.random.normal(0, noise_scale)
    y = y_base + noise
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    print("ğŸ“Š å¼‚è´¨å™ªå£°å›å½’ä»»åŠ¡ï¼šå™ªå£°éšè¾“å…¥ç‰¹å¾å˜åŒ–")
    
    # å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ˆåªæœ‰ç‚¹ä¼°è®¡ï¼‰vs å› æœæ–¹æ³•ï¼ˆå®Œæ•´åˆ†å¸ƒï¼‰
    models = {
        'sklearn': MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42),
        'deterministic': MLPCausalRegressor(hidden_layer_sizes=(64, 32), mode='deterministic', max_iter=300, random_state=42),
        'standard': MLPCausalRegressor(hidden_layer_sizes=(64, 32), mode='standard', max_iter=300, random_state=42)
    }
    
    print(f"\nğŸ¯ é¢„æµ‹æ€§èƒ½å¯¹æ¯”:")
    for name, model in models.items():
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        
        if isinstance(pred, dict):
            pred = pred['predictions']
            
        r2 = r2_score(y_test, pred)
        mse = mean_squared_error(y_test, pred)
        print(f"  {name:<12}: RÂ² = {r2:.4f}, MSE = {mse:.4f}")
    
    # ä¸ç¡®å®šæ€§åˆ†æ
    print(f"\nğŸŒ¡ï¸ ä¸ç¡®å®šæ€§é‡åŒ–åˆ†æ:")
    standard_model = models['standard']
    
    # è·å–é¢„æµ‹åˆ†å¸ƒ
    dist_params = standard_model.predict_dist(X_test)
    if dist_params is not None:
        pred_mean = dist_params[:, 0, 0]  # ä½ç½®å‚æ•°
        pred_uncertainty = dist_params[:, 0, 1]  # å°ºåº¦å‚æ•°
        
        # åˆ†æä¸ç¡®å®šæ€§ä¸çœŸå®å™ªå£°çš„ç›¸å…³æ€§
        true_noise_scale = 0.1 + 0.4 * np.abs(X_test[:, 0])
        correlation = np.corrcoef(pred_uncertainty, true_noise_scale)[0, 1]
        
        print(f"  é¢„æµ‹ä¸ç¡®å®šæ€§èŒƒå›´: [{np.min(pred_uncertainty):.3f}, {np.max(pred_uncertainty):.3f}]")
        print(f"  çœŸå®å™ªå£°èŒƒå›´: [{np.min(true_noise_scale):.3f}, {np.max(true_noise_scale):.3f}]")
        print(f"  ä¸ç¡®å®šæ€§-çœŸå®å™ªå£°ç›¸å…³æ€§: {correlation:.4f}")
        
        # é«˜ä¸ç¡®å®šæ€§æ ·æœ¬åˆ†æ
        high_uncertainty_mask = pred_uncertainty > np.percentile(pred_uncertainty, 80)
        high_uncertainty_error = np.abs(pred_mean[high_uncertainty_mask] - y_test[high_uncertainty_mask])
        low_uncertainty_error = np.abs(pred_mean[~high_uncertainty_mask] - y_test[~high_uncertainty_mask])
        
        print(f"  é«˜ä¸ç¡®å®šæ€§æ ·æœ¬å¹³å‡è¯¯å·®: {np.mean(high_uncertainty_error):.4f}")
        print(f"  ä½ä¸ç¡®å®šæ€§æ ·æœ¬å¹³å‡è¯¯å·®: {np.mean(low_uncertainty_error):.4f}")
        
        if np.mean(high_uncertainty_error) > np.mean(low_uncertainty_error):
            print("  âœ… ä¸ç¡®å®šæ€§æ­£ç¡®æŒ‡ç¤ºäº†é¢„æµ‹è¯¯å·®ï¼")
        else:
            print("  âš ï¸ ä¸ç¡®å®šæ€§æŒ‡ç¤ºéœ€è¦æ”¹è¿›")


def demo_noise_robustness():
    """æ¼”ç¤ºå™ªå£°é²æ£’æ€§çš„å®é™…ä»·å€¼"""
    print("\n" + "="*60)
    print("ğŸ›¡ï¸ å™ªå£°é²æ£’æ€§å®é™…åº”ç”¨")
    print("="*60)
    
    # åˆ†ç±»ä»»åŠ¡ï¼šæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ ‡ç­¾å™ªå£°
    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, 
                              n_informative=15, n_redundant=2, random_state=42)
    X_train, X_test, y_train_clean, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # æ·»åŠ ä¸åŒç¨‹åº¦çš„æ ‡ç­¾å™ªå£°
    noise_levels = [0.0, 0.1, 0.2, 0.3]
    results = {}
    
    print("ğŸ“Š ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„æ€§èƒ½å¯¹æ¯”:")
    print("+" + "-"*65 + "+")
    print(f"| {'å™ªå£°æ°´å¹³':<8} | {'sklearn':<12} | {'deterministic':<12} | {'standard':<12} |")
    print("+" + "-"*65 + "+")
    
    for noise_level in noise_levels:
        # åˆ›å»ºå™ªå£°æ ‡ç­¾
        y_train_noisy = y_train_clean.copy()
        if noise_level > 0:
            n_noise = int(noise_level * len(y_train_noisy))
            noise_indices = np.random.choice(len(y_train_noisy), n_noise, replace=False)
            for idx in noise_indices:
                available_labels = [l for l in np.unique(y) if l != y_train_noisy[idx]]
                y_train_noisy[idx] = np.random.choice(available_labels)
        
        # æµ‹è¯•ä¸‰ç§æ–¹æ³•
        methods = {
            'sklearn': MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42),
            'deterministic': MLPCausalClassifier(hidden_layer_sizes=(64, 32), mode='deterministic', max_iter=300, random_state=42),
            'standard': MLPCausalClassifier(hidden_layer_sizes=(64, 32), mode='standard', max_iter=300, random_state=42)
        }
        
        noise_results = {}
        for name, model in methods.items():
            model.fit(X_train, y_train_noisy)
            pred = model.predict(X_test)
            
            if isinstance(pred, dict):
                pred = pred['predictions']
                
            acc = accuracy_score(y_test, pred)
            noise_results[name] = acc
        
        results[noise_level] = noise_results
        print(f"| {noise_level:<8.1f} | {noise_results['sklearn']:<12.4f} | {noise_results['deterministic']:<12.4f} | {noise_results['standard']:<12.4f} |")
    
    print("+" + "-"*65 + "+")
    
    # åˆ†æé²æ£’æ€§
    print(f"\nğŸ” é²æ£’æ€§åˆ†æ:")
    clean_performance = results[0.0]
    noisy_performance = results[0.3]  # 30%å™ªå£°
    
    for method in ['sklearn', 'deterministic', 'standard']:
        degradation = clean_performance[method] - noisy_performance[method]
        print(f"  {method}: æ€§èƒ½ä¸‹é™ = {degradation:.4f} (30%å™ªå£°)")
    
    # æ‰¾å‡ºæœ€é²æ£’çš„æ–¹æ³•
    degradations = {method: clean_performance[method] - noisy_performance[method] 
                   for method in ['sklearn', 'deterministic', 'standard']}
    most_robust = min(degradations, key=degradations.get)
    print(f"  ğŸ† æœ€é²æ£’æ–¹æ³•: {most_robust} (æ€§èƒ½ä¸‹é™æœ€å°)")
    
    return results


def demo_real_world_datasets():
    """åœ¨çœŸå®æ•°æ®é›†ä¸Šæ¼”ç¤ºCausalEngine"""
    print("\n" + "="*60)
    print("ğŸŒ çœŸå®æ•°æ®é›†åº”ç”¨æ¼”ç¤º")
    print("="*60)
    
    # ç³–å°¿ç—…å›å½’æ•°æ®é›†
    print("\nğŸ“Š ç³–å°¿ç—…å›å½’ä»»åŠ¡ (sklearnå†…ç½®æ•°æ®é›†):")
    diabetes = load_diabetes()
    X_diabetes, y_diabetes = diabetes.data, diabetes.target
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_diabetes = scaler.fit_transform(X_diabetes)
    
    X_train, X_test, y_train, y_test = train_test_split(X_diabetes, y_diabetes, test_size=0.3, random_state=42)
    
    # å¯¹æ¯”ä¸åŒæ–¹æ³•
    models = {
        'sklearn': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),
        'deterministic': MLPCausalRegressor(hidden_layer_sizes=(100, 50), mode='deterministic', max_iter=500, random_state=42),
        'standard': MLPCausalRegressor(hidden_layer_sizes=(100, 50), mode='standard', max_iter=500, random_state=42),
        'sampling': MLPCausalRegressor(hidden_layer_sizes=(100, 50), mode='sampling', max_iter=500, random_state=42)
    }
    
    print("  æ–¹æ³•å¯¹æ¯”:")
    diabetes_results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        
        if isinstance(pred, dict):
            pred = pred['predictions']
            
        r2 = r2_score(y_test, pred)
        mse = mean_squared_error(y_test, pred)
        diabetes_results[name] = {'r2': r2, 'mse': mse}
        print(f"    {name:<12}: RÂ² = {r2:.4f}, MSE = {mse:.1f}")
    
    # çº¢é…’åˆ†ç±»æ•°æ®é›†
    print("\nğŸ· çº¢é…’åˆ†ç±»ä»»åŠ¡ (sklearnå†…ç½®æ•°æ®é›†):")
    wine = load_wine()
    X_wine, y_wine = wine.data, wine.target
    
    # æ ‡å‡†åŒ–
    X_wine = scaler.fit_transform(X_wine)
    X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=42)
    
    clf_models = {
        'sklearn': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),
        'deterministic': MLPCausalClassifier(hidden_layer_sizes=(100, 50), mode='deterministic', max_iter=500, random_state=42),
        'standard': MLPCausalClassifier(hidden_layer_sizes=(100, 50), mode='standard', max_iter=500, random_state=42),
        'endogenous': MLPCausalClassifier(hidden_layer_sizes=(100, 50), mode='endogenous', max_iter=500, random_state=42)
    }
    
    print("  æ–¹æ³•å¯¹æ¯”:")
    wine_results = {}
    for name, model in clf_models.items():
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        
        if isinstance(pred, dict):
            pred = pred['predictions']
            
        acc = accuracy_score(y_test, pred)
        wine_results[name] = acc
        print(f"    {name:<12}: å‡†ç¡®ç‡ = {acc:.4f}")
    
    return {'diabetes': diabetes_results, 'wine': wine_results}


def demo_advanced_features():
    """æ¼”ç¤ºé«˜çº§åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸš€ é«˜çº§åŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒä¸€ä¸ªæ ‡å‡†æ¨¡å¼çš„æ¨¡å‹
    model = MLPCausalRegressor(
        hidden_layer_sizes=(64, 32),
        mode='standard',
        max_iter=300,
        random_state=42
    )
    model.fit(X_train, y_train)
    
    print("\nğŸ”„ åŠ¨æ€æ¨¡å¼åˆ‡æ¢:")
    # åŒä¸€æ¨¡å‹ï¼Œä¸åŒé¢„æµ‹æ¨¡å¼
    sample_X = X_test[:3]
    modes = ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    
    for mode in modes:
        pred = model.predict(sample_X, mode=mode)
        if isinstance(pred, dict):
            values = pred['predictions']
        else:
            values = pred
        print(f"  {mode:<12}: {values}")
    
    print("\nğŸ“Š åˆ†å¸ƒå‚æ•°é¢„æµ‹:")
    # è·å–å®Œæ•´çš„åˆ†å¸ƒå‚æ•°
    if hasattr(model, 'predict_dist'):
        dist_params = model.predict_dist(sample_X)
        if dist_params is not None:
            print(f"  åˆ†å¸ƒå‚æ•°å½¢çŠ¶: {dist_params.shape}")
            print(f"  ä½ç½®å‚æ•°: {dist_params[:, 0, 0]}")
            print(f"  å°ºåº¦å‚æ•°: {dist_params[:, 0, 1]}")
    
    print("\nğŸ¯ ç½®ä¿¡åŒºé—´ä¼°è®¡:")
    # åŸºäºåˆ†å¸ƒå‚æ•°æ„å»ºç½®ä¿¡åŒºé—´
    if dist_params is not None:
        loc = dist_params[:, 0, 0]
        scale = dist_params[:, 0, 1]
        
        # æŸ¯è¥¿åˆ†å¸ƒçš„95%ç½®ä¿¡åŒºé—´ï¼ˆè¿‘ä¼¼ï¼‰
        lower = loc - 12.7 * scale  # è¿‘ä¼¼95%ç½®ä¿¡åŒºé—´
        upper = loc + 12.7 * scale
        
        print("  æ ·æœ¬é¢„æµ‹ä¸95%ç½®ä¿¡åŒºé—´:")
        for i in range(len(sample_X)):
            print(f"    æ ·æœ¬{i}: {loc[i]:.2f} [{lower[i]:.2f}, {upper[i]:.2f}]")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ CausalEngine å®é™…åº”ç”¨æ¼”ç¤º v3.0")
    print("="*70)
    print("åŸºäºå·²éªŒè¯çš„æ•°å­¦ç­‰ä»·æ€§ï¼Œå±•ç¤ºCausalEngineçš„å®é™…åº”ç”¨ä»·å€¼")
    
    results = {}
    
    try:
        # 1. sklearnæ›¿ä»£èƒ½åŠ›éªŒè¯
        results['sklearn_replacement'] = demo_sklearn_replacement()
        
        # 2. äº”æ¨¡å¼å› æœæ¨ç†å…‰è°±
        results['five_modes'] = demo_five_modes_spectrum()
        
        # 3. ä¸ç¡®å®šæ€§é‡åŒ–
        demo_uncertainty_quantification()
        
        # 4. å™ªå£°é²æ£’æ€§
        results['noise_robustness'] = demo_noise_robustness()
        
        # 5. çœŸå®æ•°æ®é›†åº”ç”¨
        results['real_world'] = demo_real_world_datasets()
        
        # 6. é«˜çº§åŠŸèƒ½
        demo_advanced_features()
        
        # æ€»ç»“
        print("\n" + "="*70)
        print("ğŸ“Š åº”ç”¨æ¼”ç¤ºæ€»ç»“")
        print("="*70)
        
        print("âœ… æ ¸å¿ƒéªŒè¯:")
        sklearn_replacement = results['sklearn_replacement']
        print(f"  - å›å½’æ›¿ä»£: CausalEngine vs sklearnå·®å¼‚ {abs(sklearn_replacement['regression']['causal'] - sklearn_replacement['regression']['sklearn']):.6f}")
        print(f"  - åˆ†ç±»æ›¿ä»£: CausalEngine vs sklearnå·®å¼‚ {abs(sklearn_replacement['classification']['causal'] - sklearn_replacement['classification']['sklearn']):.6f}")
        
        print("\nğŸŒˆ äº”æ¨¡å¼èƒ½åŠ›:")
        five_modes = results['five_modes']
        best_mode = max(five_modes.keys(), key=lambda k: five_modes[k]['r2'])
        print(f"  - æœ€ä½³æ¨¡å¼: {best_mode} (RÂ² = {five_modes[best_mode]['r2']:.4f})")
        print(f"  - æ¨¡å¼èŒƒå›´: {len(five_modes)} ç§ä¸åŒçš„å› æœæ¨ç†ç­–ç•¥")
        
        print("\nğŸ›¡ï¸ é²æ£’æ€§ä¼˜åŠ¿:")
        noise_clean = results['noise_robustness'][0.0]
        noise_heavy = results['noise_robustness'][0.3]
        for method in ['sklearn', 'standard']:
            degradation = noise_clean[method] - noise_heavy[method]
            print(f"  - {method}: 30%å™ªå£°ä¸‹æ€§èƒ½ä¸‹é™ {degradation:.4f}")
        
        print("\nğŸŒ çœŸå®æ•°æ®è¡¨ç°:")
        real_world = results['real_world']
        print(f"  - ç³–å°¿ç—…æ•°æ®: æœ€ä½³RÂ² = {max(real_world['diabetes'][m]['r2'] for m in real_world['diabetes']):.4f}")
        print(f"  - çº¢é…’æ•°æ®: æœ€ä½³å‡†ç¡®ç‡ = {max(real_world['wine'].values()):.4f}")
        
        print("\nğŸ‰ CausalEngineä¸ºæœºå™¨å­¦ä¹ å¸¦æ¥äº†ä»ç¡®å®šæ€§åˆ°å› æœæ¨ç†çš„å®Œæ•´èƒ½åŠ›å…‰è°±ï¼")
        print("ğŸš€ ä»sklearnå…¼å®¹æ€§åˆ°é«˜çº§å› æœæ¨ç†ï¼Œä¸€ä¸ªæ¡†æ¶æ»¡è¶³æ‰€æœ‰éœ€æ±‚ï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import torch
    torch.manual_seed(42)
    np.random.seed(42)
    
    success = main()
    exit(0 if success else 1)
"""
CausalEngine ä»»åŠ¡æ¿€æ´»æœºåˆ¶è¯¦è§£
============================

åŸºäºæœ€æ–°æ•°å­¦ç†è®ºæ·±å…¥è§£æä¸‰ç§ä»»åŠ¡æ¿€æ´»æœºåˆ¶çš„è®¾è®¡åŸç†å’Œå®ç°ç»†èŠ‚
æ¢è®¨å¦‚ä½•æ‰©å±•CausalEngineåˆ°æ–°çš„ä»»åŠ¡ç±»å‹å’Œåº”ç”¨åœºæ™¯

ä¸‰ç§æ¿€æ´»æœºåˆ¶:
1. è¯å…ƒç´¢å¼•æ¿€æ´» (åˆ†ç±»ä»»åŠ¡): P_k = P(S_k > C_k) - OvRç­–ç•¥çš„ç‹¬ç«‹æ¦‚ç‡
2. æ•°å€¼æ¿€æ´» (å›å½’ä»»åŠ¡): Y_k ~ Cauchy(w_kÂ·loc_S + b_k, |w_k|Â·scale_S)
3. ç¦»æ•£æœ‰åºæ¿€æ´» (æœ‰åºåˆ†ç±»): P(y_i) = P(C_i < S_k â‰¤ C_{i+1})

æ ¸å¿ƒåˆ›æ–°: ç»Ÿä¸€çš„å†³ç­–å¾—åˆ†â†’ä»»åŠ¡è¾“å‡ºçš„æ˜ å°„æ¡†æ¶ï¼Œæ”¯æŒä»»æ„ä»»åŠ¡ç±»å‹æ‰©å±•
"""

import sys
import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from causal_engine import CausalEngine


class TaskActivationAnalyzer:
    """
    ä»»åŠ¡æ¿€æ´»æœºåˆ¶åˆ†æå™¨ - æ·±åº¦åˆ†æä¸‰ç§æ¿€æ´»æœºåˆ¶çš„ç‰¹æ€§å’Œæ‰©å±•æ€§
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆ›å»ºä¸åŒä»»åŠ¡ç±»å‹çš„CausalEngineå®ä¾‹
        self.engines = {
            'classification': CausalEngine(
                hidden_size=64,
                vocab_size=5,  # 5ç±»åˆ†ç±»
                causal_size=32,
                activation_modes="classification"
            ),
            'regression': CausalEngine(
                hidden_size=64,
                vocab_size=3,  # 3ä¸ªå›å½’ç›®æ ‡
                causal_size=32,
                activation_modes="regression"
            ),
            'ordinal': CausalEngine(
                hidden_size=64,
                vocab_size=1,  # 1ä¸ªæœ‰åºåˆ†ç±»ç›®æ ‡
                causal_size=32,
                activation_modes="ordinal"
            )
        }
    
    def demonstrate_mathematical_foundations(self):
        """
        æ¼”ç¤ºä¸‰ç§æ¿€æ´»æœºåˆ¶çš„æ•°å­¦åŸºç¡€
        """
        print("ğŸ”¬ ä»»åŠ¡æ¿€æ´»æœºåˆ¶çš„æ•°å­¦åŸºç¡€")
        print("=" * 60)
        
        print("\\nğŸ“ æ ¸å¿ƒæ•°å­¦æ¡†æ¶:")
        print("  å†³ç­–å¾—åˆ†: S_k ~ Cauchy(loc_k, scale_k)")
        print("  æ¿€æ´»å‡½æ•°: f_k(s_k) â†’ ä»»åŠ¡ç‰¹å®šè¾“å‡º")
        print("  å…³é”®åˆ›æ–°: ç›´æ¥ç”¨éšæœºå˜é‡é¢„æµ‹ï¼Œè€Œéç»Ÿè®¡é‡")
        
        # åˆ›å»ºç¤ºä¾‹å†³ç­–å¾—åˆ†
        batch_size = 8\n        num_decisions = 5\n        \n        # æ¨¡æ‹Ÿå†³ç­–å¾—åˆ†åˆ†å¸ƒå‚æ•°\n        loc_S = torch.randn(batch_size, num_decisions) * 2\n        scale_S = torch.abs(torch.randn(batch_size, num_decisions)) + 0.1\n        \n        print(f\"\\nğŸ² ç¤ºä¾‹å†³ç­–å¾—åˆ†åˆ†å¸ƒ:\")\n        print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n        print(f\"  å†³ç­–ç»´åº¦: {num_decisions}\")\n        print(f\"  ä½ç½®å‚æ•°èŒƒå›´: [{loc_S.min():.3f}, {loc_S.max():.3f}]\")\n        print(f\"  å°ºåº¦å‚æ•°èŒƒå›´: [{scale_S.min():.3f}, {scale_S.max():.3f}]\")\n        \n        # åˆ†æä¸‰ç§æ¿€æ´»æœºåˆ¶\n        self._analyze_classification_activation(loc_S, scale_S)\n        self._analyze_regression_activation(loc_S, scale_S)\n        self._analyze_ordinal_activation(loc_S, scale_S)\n    \n    def _analyze_classification_activation(self, loc_S, scale_S):\n        \"\"\"\n        åˆ†æè¯å…ƒç´¢å¼•æ¿€æ´» (åˆ†ç±»ä»»åŠ¡)\n        \"\"\"\n        print(f\"\\nğŸ¯ æ¿€æ´»æœºåˆ¶1: è¯å…ƒç´¢å¼•æ¿€æ´» (åˆ†ç±»ä»»åŠ¡)\")\n        print(f\"  æ•°å­¦å…¬å¼: f_k(s_k) = I(s_k > C_k)\")\n        print(f\"  æ¦‚ç‡è®¡ç®—: P_k = 1/2 + arctan((loc_k - C_k)/scale_k)/Ï€\")\n        \n        # è®¾ç½®åˆ†ç±»é˜ˆå€¼\n        C_k = torch.zeros_like(loc_S)  # ä½¿ç”¨0ä½œä¸ºé˜ˆå€¼\n        \n        # è®¡ç®—åˆ†ç±»æ¦‚ç‡ (è§£ææ–¹å¼)\n        def cauchy_cdf_complement(loc, scale, threshold):\n            \"\"\"è®¡ç®—æŸ¯è¥¿åˆ†å¸ƒçš„äº’è¡¥ç´¯ç§¯åˆ†å¸ƒå‡½æ•°\"\"\"\n            return 0.5 + torch.atan((loc - threshold) / scale) / np.pi\n        \n        probs = cauchy_cdf_complement(loc_S, scale_S, C_k)\n        \n        print(f\"  æ¿€æ´»æ¦‚ç‡èŒƒå›´: [{probs.min():.3f}, {probs.max():.3f}]\")\n        print(f\"  å¹³å‡æ¿€æ´»æ¦‚ç‡: {probs.mean():.3f}\")\n        \n        # OvRå†³ç­–ç­–ç•¥\n        predictions = torch.argmax(probs, dim=1)\n        print(f\"  OvRå†³ç­–ç»“æœ: {predictions.tolist()}\")\n        \n        print(f\"\\n  ğŸ’¡ å…³é”®ç‰¹æ€§:\")\n        print(f\"    âœ¨ ç‹¬ç«‹æ¦‚ç‡: æ¯ä¸ªç±»åˆ«æœ‰ç‹¬ç«‹çš„æ¿€æ´»æ¦‚ç‡\")\n        print(f\"    âœ¨ éå½’ä¸€åŒ–: ä¸å—softmaxå½’ä¸€åŒ–çº¦æŸ\")\n        print(f\"    âœ¨ å¯è§£é‡Šæ€§: ç›´è§‚çš„é˜ˆå€¼æ¯”è¾ƒæœºåˆ¶\")\n        print(f\"    âœ¨ çµæ´»æ€§: æ”¯æŒå¤šæ ‡ç­¾å’Œå±‚æ¬¡åˆ†ç±»\")\n        \n        return probs\n    \n    def _analyze_regression_activation(self, loc_S, scale_S):\n        \"\"\"\n        åˆ†ææ•°å€¼æ¿€æ´» (å›å½’ä»»åŠ¡)\n        \"\"\"\n        print(f\"\\nğŸ“ˆ æ¿€æ´»æœºåˆ¶2: æ•°å€¼æ¿€æ´» (å›å½’ä»»åŠ¡)\")\n        print(f\"  æ•°å­¦å…¬å¼: f_k(s_k) = w_kÂ·s_k + b_k\")\n        print(f\"  åˆ†å¸ƒå˜æ¢: Y_k ~ Cauchy(w_kÂ·loc_k + b_k, |w_k|Â·scale_k)\")\n        \n        # è®¾ç½®å›å½’å‚æ•°\n        w_k = torch.ones_like(loc_S)  # æƒé‡\n        b_k = torch.zeros_like(loc_S)  # åç½®\n        \n        # è®¡ç®—è¾“å‡ºåˆ†å¸ƒå‚æ•°\n        output_loc = w_k * loc_S + b_k\n        output_scale = torch.abs(w_k) * scale_S\n        \n        # è·å–ç‚¹ä¼°è®¡ (ä½ç½®å‚æ•°)\n        predictions = output_loc\n        \n        print(f\"  è¾“å‡ºä½ç½®å‚æ•°èŒƒå›´: [{output_loc.min():.3f}, {output_loc.max():.3f}]\")\n        print(f\"  è¾“å‡ºå°ºåº¦å‚æ•°èŒƒå›´: [{output_scale.min():.3f}, {output_scale.max():.3f}]\")\n        print(f\"  ç‚¹ä¼°è®¡ (ä½ç½®): {predictions.mean():.3f} Â± {predictions.std():.3f}\")\n        \n        print(f\"\\n  ğŸ’¡ å…³é”®ç‰¹æ€§:\")\n        print(f\"    âœ¨ å®Œæ•´åˆ†å¸ƒ: ä¿ç•™è¾“å‡ºçš„å®Œæ•´æ¦‚ç‡åˆ†å¸ƒ\")\n        print(f\"    âœ¨ ä¸ç¡®å®šæ€§é‡åŒ–: å°ºåº¦å‚æ•°è¡¨ç¤ºé¢„æµ‹ä¸ç¡®å®šæ€§\")\n        print(f\"    âœ¨ çº¿æ€§ç¨³å®šæ€§: æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§å˜æ¢ä»ä¸ºæŸ¯è¥¿åˆ†å¸ƒ\")\n        print(f\"    âœ¨ è§£æè®¡ç®—: æ— éœ€é‡‡æ ·çš„é«˜æ•ˆè®¡ç®—\")\n        \n        return output_loc, output_scale\n    \n    def _analyze_ordinal_activation(self, loc_S, scale_S):\n        \"\"\"\n        åˆ†æç¦»æ•£æœ‰åºæ¿€æ´» (æœ‰åºåˆ†ç±»)\n        \"\"\"\n        print(f\"\\nğŸ”¢ æ¿€æ´»æœºåˆ¶3: ç¦»æ•£æœ‰åºæ¿€æ´» (æœ‰åºåˆ†ç±»)\")\n        print(f\"  æ•°å­¦å…¬å¼: f_k(s_k) = âˆ‘y_iÂ·I(C_i < s_k â‰¤ C_{i+1})\")\n        print(f\"  æ¦‚ç‡è®¡ç®—: P(y_i) = F(C_{i+1}) - F(C_i)\")\n        \n        # è®¾ç½®æœ‰åºç±»åˆ«è¾¹ç•Œ (ä¾‹å¦‚ï¼šè¯„åˆ†1-5)\n        num_categories = 5\n        boundaries = torch.linspace(-3, 3, num_categories + 1)  # [-3, -1.5, 0, 1.5, 3]\n        \n        # åªä½¿ç”¨ç¬¬ä¸€ä¸ªå†³ç­–å¾—åˆ†ä½œä¸ºç¤ºä¾‹\n        sample_loc = loc_S[:, 0]  # shape: (batch_size,)\n        sample_scale = scale_S[:, 0]\n        \n        def cauchy_cdf(x, loc, scale):\n            \"\"\"æŸ¯è¥¿åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°\"\"\"\n            return 0.5 + torch.atan((x - loc) / scale) / np.pi\n        \n        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡\n        category_probs = []\n        for i in range(num_categories):\n            lower_bound = boundaries[i]\n            upper_bound = boundaries[i + 1]\n            \n            prob_upper = cauchy_cdf(upper_bound, sample_loc, sample_scale)\n            prob_lower = cauchy_cdf(lower_bound, sample_loc, sample_scale)\n            \n            category_prob = prob_upper - prob_lower\n            category_probs.append(category_prob)\n        \n        category_probs = torch.stack(category_probs, dim=1)  # shape: (batch_size, num_categories)\n        \n        # é¢„æµ‹ç±»åˆ« (æœ€é«˜æ¦‚ç‡)\n        predictions = torch.argmax(category_probs, dim=1) + 1  # ç±»åˆ«ä»1å¼€å§‹\n        \n        print(f\"  ç±»åˆ«è¾¹ç•Œ: {boundaries.tolist()}\")\n        print(f\"  ç±»åˆ«æ¦‚ç‡å½¢çŠ¶: {category_probs.shape}\")\n        print(f\"  å¹³å‡ç±»åˆ«æ¦‚ç‡: {category_probs.mean(dim=0).tolist()}\")\n        print(f\"  é¢„æµ‹ç±»åˆ«: {predictions.tolist()}\")\n        \n        print(f\"\\n  ğŸ’¡ å…³é”®ç‰¹æ€§:\")\n        print(f\"    âœ¨ æœ‰åºçº¦æŸ: ä¿æŒç±»åˆ«é—´çš„æœ‰åºå…³ç³»\")\n        print(f\"    âœ¨ åŒºé—´æ¦‚ç‡: åŸºäºåŒºé—´çš„æ¦‚ç‡è®¡ç®—\")\n        print(f\"    âœ¨ é˜ˆå€¼å­¦ä¹ : å¯å­¦ä¹ çš„ç±»åˆ«è¾¹ç•Œ\")\n        print(f\"    âœ¨ æ ¡å‡†æ€§: æ›´å¥½çš„æ¦‚ç‡æ ¡å‡†\")\n        \n        return category_probs\n    \n    def demonstrate_task_extensions(self):\n        \"\"\"\n        æ¼”ç¤ºå¦‚ä½•æ‰©å±•åˆ°æ–°çš„ä»»åŠ¡ç±»å‹\n        \"\"\"\n        print(f\"\\nğŸš€ ä»»åŠ¡æ¿€æ´»æœºåˆ¶çš„æ‰©å±•æ€§\")\n        print(f\"=\" * 60)\n        \n        print(f\"\\nğŸ¯ æ‰©å±•åŸç†:\")\n        print(f\"  1. å®šä¹‰åŸºç¡€æ¿€æ´»å‡½æ•° f_k(s_k)\")\n        print(f\"  2. æ¨å¯¼åœ¨æŸ¯è¥¿åˆ†å¸ƒä¸‹çš„è§£æå½¢å¼\")\n        print(f\"  3. å®ç°ç›¸åº”çš„æŸå¤±å‡½æ•°\")\n        print(f\"  4. é›†æˆåˆ°CausalEngineæ¡†æ¶\")\n        \n        # ç¤ºä¾‹æ‰©å±•ä»»åŠ¡\n        extensions = {\n            'time_prediction': {\n                'name': 'æ—¶é—´é¢„æµ‹ä»»åŠ¡',\n                'activation': 'f_k(s_k) = exp(w_kÂ·s_k + b_k)',\n                'output_dist': 'Lognormalåˆ†å¸ƒ (é€šè¿‡æŒ‡æ•°å˜æ¢)',\n                'loss': 'Lognormalè´Ÿå¯¹æ•°ä¼¼ç„¶',\n                'applications': ['äº‹ä»¶å‘ç”Ÿæ—¶é—´', 'ç”Ÿå­˜åˆ†æ', 'å¯é æ€§å·¥ç¨‹']\n            },\n            'probability_estimation': {\n                'name': 'æ¦‚ç‡ä¼°è®¡ä»»åŠ¡',\n                'activation': 'f_k(s_k) = sigmoid(w_kÂ·s_k + b_k)',\n                'output_dist': 'Betaåˆ†å¸ƒ (é€šè¿‡logitå˜æ¢)',\n                'loss': 'Betaåˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶',\n                'applications': ['é£é™©æ¦‚ç‡', 'æˆåŠŸç‡é¢„æµ‹', 'ç½®ä¿¡åº¦ä¼°è®¡']\n            },\n            'count_prediction': {\n                'name': 'è®¡æ•°é¢„æµ‹ä»»åŠ¡',\n                'activation': 'f_k(s_k) = softplus(w_kÂ·s_k + b_k)',\n                'output_dist': 'Poissonåˆ†å¸ƒ (é€šè¿‡å¯¹æ•°é“¾æ¥)',\n                'loss': 'Poissonè´Ÿå¯¹æ•°ä¼¼ç„¶',\n                'applications': ['å®¢æµé‡é¢„æµ‹', 'æ•…éšœæ¬¡æ•°', 'åº“å­˜éœ€æ±‚']\n            },\n            'multi_label': {\n                'name': 'å¤šæ ‡ç­¾åˆ†ç±»',\n                'activation': 'f_k(s_k) = I(s_k > C_k) for all k',\n                'output_dist': 'ç‹¬ç«‹ä¼¯åŠªåˆ©åˆ†å¸ƒ',\n                'loss': 'å¤šä¸ªäºŒå…ƒäº¤å‰ç†µä¹‹å’Œ',\n                'applications': ['æ ‡ç­¾æ¨è', 'ç—‡çŠ¶è¯Šæ–­', 'æŠ€èƒ½è¯„ä¼°']\n            },\n            'ranking': {\n                'name': 'æ’åºä»»åŠ¡',\n                'activation': 'f_k(s_k) = rank(s_k)',\n                'output_dist': 'æ’åˆ—åˆ†å¸ƒ',\n                'loss': 'æ’åºæŸå¤± (å¦‚ListNet)',\n                'applications': ['æœç´¢æ’åº', 'æ¨èç³»ç»Ÿ', 'ä¼˜å…ˆçº§æ’åº']\n            }\n        }\n        \n        for task_key, task_info in extensions.items():\n            print(f\"\\nğŸ“‹ {task_info['name']}\")\n            print(f\"  æ¿€æ´»å‡½æ•°: {task_info['activation']}\")\n            print(f\"  è¾“å‡ºåˆ†å¸ƒ: {task_info['output_dist']}\")\n            print(f\"  æŸå¤±å‡½æ•°: {task_info['loss']}\")\n            print(f\"  åº”ç”¨åœºæ™¯: {', '.join(task_info['applications'])}\")\n    \n    def implement_custom_activation(self):\n        \"\"\"\n        å®ç°è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°çš„ç¤ºä¾‹\n        \"\"\"\n        print(f\"\\nğŸ› ï¸ è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°å®ç°ç¤ºä¾‹\")\n        print(f\"=\" * 60)\n        \n        print(f\"\\nğŸ’¡ ç¤ºä¾‹: æ—¶é—´é¢„æµ‹ä»»åŠ¡ (ç”Ÿå­˜åˆ†æ)\")\n        \n        class TimeActivationHead(nn.Module):\n            \"\"\"\n            æ—¶é—´é¢„æµ‹æ¿€æ´»å¤´ - åŸºäºæŒ‡æ•°å˜æ¢çš„ç”Ÿå­˜åˆ†æ\n            \"\"\"\n            def __init__(self, input_size, num_targets):\n                super().__init__()\n                self.weight = nn.Parameter(torch.ones(num_targets, input_size))\n                self.bias = nn.Parameter(torch.zeros(num_targets))\n            \n            def forward(self, loc_S, scale_S):\n                \"\"\"\n                å‰å‘ä¼ æ’­: å†³ç­–å¾—åˆ† â†’ æ—¶é—´é¢„æµ‹\n                \n                Args:\n                    loc_S: å†³ç­–å¾—åˆ†ä½ç½®å‚æ•° (batch, seq, decisions)\n                    scale_S: å†³ç­–å¾—åˆ†å°ºåº¦å‚æ•° (batch, seq, decisions)\n                \n                Returns:\n                    æ—¶é—´é¢„æµ‹çš„å¯¹æ•°æ­£æ€åˆ†å¸ƒå‚æ•°\n                \"\"\"\n                # çº¿æ€§å˜æ¢\n                log_time_loc = torch.einsum('bsd,nd->bsn', loc_S, self.weight) + self.bias\n                log_time_scale = torch.einsum('bsd,nd->bsn', scale_S, torch.abs(self.weight))\n                \n                # è¾“å‡ºå¯¹æ•°æ­£æ€åˆ†å¸ƒå‚æ•°\n                return {\n                    'log_loc': log_time_loc,\n                    'log_scale': log_time_scale,\n                    'mean_time': torch.exp(log_time_loc + 0.5 * log_time_scale**2),\n                    'median_time': torch.exp(log_time_loc)\n                }\n            \n            def sample(self, loc_S, scale_S, num_samples=1):\n                \"\"\"\n                ä»é¢„æµ‹åˆ†å¸ƒä¸­é‡‡æ ·æ—¶é—´å€¼\n                \"\"\"\n                output = self.forward(loc_S, scale_S)\n                \n                # ä»å¯¹æ•°æ­£æ€åˆ†å¸ƒé‡‡æ ·\n                log_normal = torch.distributions.LogNormal(\n                    output['log_loc'], output['log_scale']\n                )\n                \n                return log_normal.sample((num_samples,))\n            \n            def loss(self, loc_S, scale_S, target_times, censoring_indicator=None):\n                \"\"\"\n                è®¡ç®—ç”Ÿå­˜åˆ†ææŸå¤± (æ”¯æŒåˆ å¤±æ•°æ®)\n                \"\"\"\n                output = self.forward(loc_S, scale_S)\n                \n                # å¯¹æ•°æ­£æ€åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶\n                log_normal = torch.distributions.LogNormal(\n                    output['log_loc'], output['log_scale']\n                )\n                \n                if censoring_indicator is None:\n                    # æ— åˆ å¤±æ•°æ®\n                    return -log_normal.log_prob(target_times).mean()\n                else:\n                    # å¤„ç†åˆ å¤±æ•°æ®\n                    uncensored_loss = -log_normal.log_prob(target_times) * (1 - censoring_indicator)\n                    censored_loss = -log_normal.log_survival_function(target_times) * censoring_indicator\n                    return (uncensored_loss + censored_loss).mean()\n        \n        # æ¼”ç¤ºä½¿ç”¨\n        print(f\"\\nğŸ”§ å®ç°ç»†èŠ‚:\")\n        print(f\"  æ¿€æ´»å‡½æ•°: f(s) = exp(wÂ·s + b)\")\n        print(f\"  è¾“å‡ºåˆ†å¸ƒ: Y ~ LogNormal(Î¼, Ïƒ)\")\n        print(f\"  å‚æ•°æ˜ å°„: Î¼ = wÂ·loc_S + b, Ïƒ = |w|Â·scale_S\")\n        print(f\"  æŸå¤±å‡½æ•°: -log P(t|Î¼,Ïƒ) + åˆ å¤±å¤„ç†\")\n        \n        # åˆ›å»ºç¤ºä¾‹\n        time_head = TimeActivationHead(input_size=5, num_targets=2)\n        \n        # æ¨¡æ‹Ÿæ•°æ®\n        batch_size = 4\n        loc_S = torch.randn(batch_size, 1, 5)\n        scale_S = torch.abs(torch.randn(batch_size, 1, 5)) + 0.1\n        target_times = torch.exponential(torch.ones(batch_size, 1, 2))\n        \n        # å‰å‘ä¼ æ’­\n        time_output = time_head(loc_S, scale_S)\n        \n        print(f\"\\nğŸ“Š æ¼”ç¤ºç»“æœ:\")\n        print(f\"  è¾“å…¥å†³ç­–å¾—åˆ†: {loc_S.shape}\")\n        print(f\"  é¢„æµ‹å¹³å‡æ—¶é—´: {time_output['mean_time'].mean():.3f}\")\n        print(f\"  é¢„æµ‹ä¸­ä½æ—¶é—´: {time_output['median_time'].mean():.3f}\")\n        \n        # è®¡ç®—æŸå¤±\n        loss_value = time_head.loss(loc_S, scale_S, target_times)\n        print(f\"  æŸå¤±å€¼: {loss_value:.4f}\")\n        \n        print(f\"\\nâœ… è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°å®ç°å®Œæˆï¼\")\n    \n    def analyze_activation_properties(self):\n        \"\"\"\n        åˆ†ææ¿€æ´»æœºåˆ¶çš„é‡è¦æ€§è´¨\n        \"\"\"\n        print(f\"\\nğŸ” æ¿€æ´»æœºåˆ¶çš„é‡è¦æ€§è´¨åˆ†æ\")\n        print(f\"=\" * 60)\n        \n        properties = {\n            'computational_efficiency': {\n                'name': 'è®¡ç®—æ•ˆç‡',\n                'description': 'ç›¸æ¯”é‡‡æ ·æ–¹æ³•çš„è®¡ç®—ä¼˜åŠ¿',\n                'analysis': {\n                    'è§£æè®¡ç®—': 'åˆ©ç”¨æŸ¯è¥¿åˆ†å¸ƒCDFé¿å…è’™ç‰¹å¡æ´›é‡‡æ ·',\n                    'çº¿æ€§å¤æ‚åº¦': 'è®¡ç®—å¤æ‚åº¦ä¸è¾“å‡ºç»´åº¦æˆçº¿æ€§å…³ç³»',\n                    'å¹¶è¡Œå‹å¥½': 'æ‰€æœ‰æ¿€æ´»å‡½æ•°å¯å¹¶è¡Œè®¡ç®—',\n                    'å†…å­˜é«˜æ•ˆ': 'æ— éœ€å­˜å‚¨å¤§é‡é‡‡æ ·ç»“æœ'\n                }\n            },\n            'mathematical_rigor': {\n                'name': 'æ•°å­¦ä¸¥æ ¼æ€§',\n                'description': 'åŸºäºåˆ†å¸ƒçš„ç²¾ç¡®è®¡ç®—',\n                'analysis': {\n                    'åˆ†å¸ƒä¿æŒ': 'æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ä¿è¯è¿ç®—æ­£ç¡®æ€§',\n                    'è§£æç²¾ç¡®': 'é¿å…æ•°å€¼é‡‡æ ·çš„è¿‘ä¼¼è¯¯å·®',\n                    'ç†è®ºæ”¯æ’‘': 'å»ºç«‹åœ¨æˆç†Ÿçš„æ¦‚ç‡è®ºåŸºç¡€ä¸Š',\n                    'å¯è¯æ˜æ€§': 'æ•°å­¦æ€§è´¨å¯ä¸¥æ ¼è¯æ˜'\n                }\n            },\n            'uncertainty_quantification': {\n                'name': 'ä¸ç¡®å®šæ€§é‡åŒ–',\n                'description': 'å®Œæ•´ä¿ç•™å’Œä¼ æ’­ä¸ç¡®å®šæ€§ä¿¡æ¯',\n                'analysis': {\n                    'å®Œæ•´åˆ†å¸ƒ': 'ä¿ç•™è¾“å‡ºçš„å®Œæ•´æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯',\n                    'æ ¡å‡†æ€§': 'æä¾›æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¼°è®¡',\n                    'ä¼ æ’­æ€§': 'ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´ä¸ç¡®å®šæ€§ä¼ æ’­',\n                    'å¯è§£é‡Šæ€§': 'ä¸ç¡®å®šæ€§æ¥æºæ¸…æ™°å¯è§£é‡Š'\n                }\n            },\n            'task_agnostic': {\n                'name': 'ä»»åŠ¡æ— å…³æ€§',\n                'description': 'ç»Ÿä¸€æ¡†æ¶æ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹',\n                'analysis': {\n                    'é€šç”¨æ€§': 'å†³ç­–å¾—åˆ†ä¸å…·ä½“ä»»åŠ¡æ— å…³',\n                    'æ‰©å±•æ€§': 'æ˜“äºæ‰©å±•åˆ°æ–°çš„ä»»åŠ¡ç±»å‹',\n                    'ç»„åˆæ€§': 'æ”¯æŒå¤šä»»åŠ¡è”åˆå­¦ä¹ ',\n                    'ä¸€è‡´æ€§': 'æ‰€æœ‰ä»»åŠ¡å…±äº«ç›¸åŒçš„æ•°å­¦æ¡†æ¶'\n                }\n            },\n            'interpretability': {\n                'name': 'å¯è§£é‡Šæ€§',\n                'description': 'æ¸…æ™°çš„å†³ç­–è¿‡ç¨‹å’Œç‰©ç†æ„ä¹‰',\n                'analysis': {\n                    'é˜ˆå€¼å«ä¹‰': 'åˆ†ç±»é˜ˆå€¼æœ‰æ˜ç¡®çš„ç‰©ç†è§£é‡Š',\n                    'å‚æ•°æ„ä¹‰': 'æ‰€æœ‰å‚æ•°éƒ½æœ‰å…·ä½“çš„æ•°å­¦å«ä¹‰',\n                    'å†³ç­–è·¯å¾„': 'ä»è¯æ®åˆ°å†³ç­–çš„å®Œæ•´å¯è¿½è¸ªè·¯å¾„',\n                    'å› æœæ€§': 'ä½“ç°çœŸæ­£çš„å› æœæ¨ç†è¿‡ç¨‹'\n                }\n            }\n        }\n        \n        for prop_key, prop_info in properties.items():\n            print(f\"\\nğŸ† {prop_info['name']}\")\n            print(f\"  å®šä¹‰: {prop_info['description']}\")\n            print(f\"  å…·ä½“åˆ†æ:\")\n            for aspect, detail in prop_info['analysis'].items():\n                print(f\"    â€¢ {aspect}: {detail}\")\n    \n    def visualize_activation_behaviors(self):\n        \"\"\"\n        å¯è§†åŒ–ä¸‰ç§æ¿€æ´»æœºåˆ¶çš„è¡Œä¸ºç‰¹å¾\n        \"\"\"\n        print(f\"\\nğŸ“Š ç”Ÿæˆæ¿€æ´»æœºåˆ¶è¡Œä¸ºå¯è§†åŒ–\")\n        \n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        fig.suptitle('CausalEngine ä»»åŠ¡æ¿€æ´»æœºåˆ¶è¡Œä¸ºåˆ†æ', fontsize=16, fontweight='bold')\n        \n        # åˆ›å»ºæµ‹è¯•æ•°æ®\n        s_range = torch.linspace(-4, 4, 100)\n        batch_size = 50\n        \n        # 1. åˆ†ç±»æ¿€æ´»ï¼šæ¦‚ç‡vså†³ç­–å¾—åˆ†\n        thresholds = [-1, 0, 1]\n        scale = 0.5\n        \n        ax1 = axes[0, 0]\n        for i, threshold in enumerate(thresholds):\n            probs = 0.5 + torch.atan((s_range - threshold) / scale) / np.pi\n            ax1.plot(s_range, probs, label=f'é˜ˆå€¼={threshold}', linewidth=2)\n        \n        ax1.set_xlabel('å†³ç­–å¾—åˆ† s')\n        ax1.set_ylabel('æ¿€æ´»æ¦‚ç‡ P(s > C)')\n        ax1.set_title('åˆ†ç±»æ¿€æ´»ï¼šé˜ˆå€¼æ•ˆåº”')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # 2. å›å½’æ¿€æ´»ï¼šè¾“å‡ºåˆ†å¸ƒ\n        weights = [0.5, 1.0, 2.0]\n        input_loc = 1.0\n        input_scale = 0.5\n        \n        ax2 = axes[0, 1]\n        for w in weights:\n            output_loc = w * input_loc\n            output_scale = abs(w) * input_scale\n            \n            # ç»˜åˆ¶è¾“å‡ºåˆ†å¸ƒ\n            y_range = torch.linspace(output_loc - 3*output_scale, \n                                   output_loc + 3*output_scale, 100)\n            pdf = 1 / (np.pi * output_scale * (1 + ((y_range - output_loc) / output_scale)**2))\n            \n            ax2.plot(y_range, pdf, label=f'æƒé‡={w}', linewidth=2)\n        \n        ax2.set_xlabel('è¾“å‡ºå€¼ y')\n        ax2.set_ylabel('æ¦‚ç‡å¯†åº¦')\n        ax2.set_title('å›å½’æ¿€æ´»ï¼šæƒé‡æ•ˆåº”')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        # 3. æœ‰åºåˆ†ç±»ï¼šç±»åˆ«æ¦‚ç‡\n        boundaries = torch.tensor([-2, -1, 0, 1, 2])\n        loc_values = [-1, 0, 1]\n        scale = 0.7\n        \n        ax3 = axes[0, 2]\n        category_names = ['C1', 'C2', 'C3', 'C4']\n        \n        for loc in loc_values:\n            probs = []\n            for i in range(len(boundaries) - 1):\n                prob_upper = 0.5 + torch.atan((boundaries[i+1] - loc) / scale) / np.pi\n                prob_lower = 0.5 + torch.atan((boundaries[i] - loc) / scale) / np.pi\n                probs.append(prob_upper - prob_lower)\n            \n            ax3.bar([f'{cat}\\n(loc={loc})' for cat in category_names], probs, \n                   alpha=0.7, label=f'ä½ç½®={loc}')\n        \n        ax3.set_ylabel('ç±»åˆ«æ¦‚ç‡')\n        ax3.set_title('æœ‰åºåˆ†ç±»ï¼šä½ç½®æ•ˆåº”')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3, axis='y')\n        \n        # 4. ä¸ç¡®å®šæ€§ä¼ æ’­ï¼šå°ºåº¦å‚æ•°æ•ˆåº”\n        scales = [0.1, 0.5, 1.0]\n        threshold = 0\n        \n        ax4 = axes[1, 0]\n        for scale in scales:\n            probs = 0.5 + torch.atan((s_range - threshold) / scale) / np.pi\n            ax4.plot(s_range, probs, label=f'å°ºåº¦={scale}', linewidth=2)\n        \n        ax4.set_xlabel('å†³ç­–å¾—åˆ† s')\n        ax4.set_ylabel('æ¿€æ´»æ¦‚ç‡')\n        ax4.set_title('ä¸ç¡®å®šæ€§ä¼ æ’­ï¼šå°ºåº¦æ•ˆåº”')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        \n        # 5. å¤šä»»åŠ¡æ¿€æ´»ï¼šç‹¬ç«‹æ€§å±•ç¤º\n        num_tasks = 3\n        task_thresholds = [-0.5, 0, 0.5]\n        scale = 0.8\n        \n        ax5 = axes[1, 1]\n        colors = ['red', 'green', 'blue']\n        \n        for i, (threshold, color) in enumerate(zip(task_thresholds, colors)):\n            probs = 0.5 + torch.atan((s_range - threshold) / scale) / np.pi\n            ax5.plot(s_range, probs, label=f'ä»»åŠ¡{i+1}', color=color, linewidth=2)\n        \n        ax5.set_xlabel('å†³ç­–å¾—åˆ† s')\n        ax5.set_ylabel('å„ä»»åŠ¡æ¿€æ´»æ¦‚ç‡')\n        ax5.set_title('å¤šä»»åŠ¡æ¿€æ´»ï¼šç‹¬ç«‹æ€§')\n        ax5.legend()\n        ax5.grid(True, alpha=0.3)\n        \n        # 6. æ¿€æ´»å‡½æ•°å¯¹æ¯”\n        activation_types = {\n            'é˜¶è·ƒå‡½æ•° (ä¼ ç»Ÿ)': lambda x: (x > 0).float(),\n            'Sigmoid': lambda x: torch.sigmoid(x),\n            'CausalEngine': lambda x: 0.5 + torch.atan(x / 0.5) / np.pi\n        }\n        \n        ax6 = axes[1, 2]\n        for name, func in activation_types.items():\n            outputs = func(s_range)\n            ax6.plot(s_range, outputs, label=name, linewidth=2)\n        \n        ax6.set_xlabel('è¾“å…¥å€¼')\n        ax6.set_ylabel('è¾“å‡ºå€¼')\n        ax6.set_title('æ¿€æ´»å‡½æ•°å¯¹æ¯”')\n        ax6.legend()\n        ax6.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('tutorials/04_advanced_topics/task_activation_analysis.png', \n                   dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        print(\"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: tutorials/04_advanced_topics/task_activation_analysis.png\")\n\n\ndef main():\n    \"\"\"\n    ä¸»å‡½æ•°: å®Œæ•´çš„ä»»åŠ¡æ¿€æ´»æœºåˆ¶è§£æ\n    \"\"\"\n    print(\"ğŸŒŸ CausalEngine ä»»åŠ¡æ¿€æ´»æœºåˆ¶æ·±åº¦è§£æ\")\n    print(\"åŸºäºæœ€æ–°æ•°å­¦ç†è®ºçš„ç»Ÿä¸€æ¿€æ´»æ¡†æ¶åˆ†æ\")\n    print(\"=\" * 80)\n    \n    # åˆ›å»ºåˆ†æå™¨\n    analyzer = TaskActivationAnalyzer()\n    \n    # 1. æ•°å­¦åŸºç¡€æ¼”ç¤º\n    print(\"\\nğŸ”¬ æ­¥éª¤1: æ•°å­¦åŸºç¡€æ¼”ç¤º\")\n    analyzer.demonstrate_mathematical_foundations()\n    \n    # 2. ä»»åŠ¡æ‰©å±•æ¼”ç¤º\n    print(\"\\nğŸš€ æ­¥éª¤2: ä»»åŠ¡æ‰©å±•èƒ½åŠ›\")\n    analyzer.demonstrate_task_extensions()\n    \n    # 3. è‡ªå®šä¹‰æ¿€æ´»å®ç°\n    print(\"\\nğŸ› ï¸ æ­¥éª¤3: è‡ªå®šä¹‰æ¿€æ´»å®ç°\")\n    analyzer.implement_custom_activation()\n    \n    # 4. æ€§è´¨åˆ†æ\n    print(\"\\nğŸ” æ­¥éª¤4: é‡è¦æ€§è´¨åˆ†æ\")\n    analyzer.analyze_activation_properties()\n    \n    # 5. è¡Œä¸ºå¯è§†åŒ–\n    print(\"\\nğŸ“Š æ­¥éª¤5: è¡Œä¸ºå¯è§†åŒ–\")\n    analyzer.visualize_activation_behaviors()\n    \n    # 6. æ€»ç»“\n    print(\"\\nğŸ‰ ä»»åŠ¡æ¿€æ´»æœºåˆ¶è§£æå®Œæˆï¼\")\n    print(\"=\" * 80)\n    print(\"ğŸ”¬ æ ¸å¿ƒå‘ç°:\")\n    print(\"  âœ… ä¸‰ç§æ¿€æ´»æœºåˆ¶è¦†ç›–äº†ä¸»è¦çš„MLä»»åŠ¡ç±»å‹\")\n    print(\"  âœ… ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ç¡®ä¿äº†ä¸€è‡´æ€§å’Œå¯æ‰©å±•æ€§\")\n    print(\"  âœ… è§£æè®¡ç®—é¿å…äº†é‡‡æ ·çš„è®¡ç®—å¼€é”€\")\n    print(\"  âœ… å®Œæ•´çš„ä¸ç¡®å®šæ€§ä¼ æ’­ä¿æŒäº†ä¿¡æ¯å®Œæ•´æ€§\")\n    \n    print(\"\\nğŸ’¡ è®¾è®¡ä¼˜åŠ¿:\")\n    print(\"  ğŸ¯ ä»»åŠ¡æ— å…³: å†³ç­–å¾—åˆ†ä¸å…·ä½“ä»»åŠ¡è§£è€¦\")\n    print(\"  ğŸ”„ å¯ç»„åˆ: æ”¯æŒå¤šä»»åŠ¡è”åˆå­¦ä¹ \")\n    print(\"  ğŸ“ æ•°å­¦ä¸¥æ ¼: åŸºäºæ¦‚ç‡è®ºçš„ç²¾ç¡®è®¡ç®—\")\n    print(\"  ğŸš€ æ˜“æ‰©å±•: æ–°ä»»åŠ¡åªéœ€å®šä¹‰æ¿€æ´»å‡½æ•°\")\n    \n    print(\"\\nğŸ“š è¿›ä¸€æ­¥å­¦ä¹ :\")\n    print(\"  1. å¤šä»»åŠ¡å­¦ä¹ : tutorials/04_advanced_topics/multi_task_learning_framework.py\")\n    print(\"  2. è‡ªå®šä¹‰è®¾è®¡: tutorials/04_advanced_topics/custom_activation_design.py\")\n    print(\"  3. æ¨ç†æ¨¡å¼: tutorials/04_advanced_topics/four_inference_modes_deep_dive.py\")\n    print(\"  4. æ•°å­¦ç†è®º: causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md\")\n\n\nif __name__ == \"__main__\":\n    main()"
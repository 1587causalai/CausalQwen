"""
ç»¼åˆæ¶ˆèå®éªŒå¯¹æ¯”è„šæœ¬
ç³»ç»Ÿæ€§åœ°æ¯”è¾ƒä¸‰ç§æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½

è¿è¡Œæ–¹å¼:
python tutorials/03_ablation_studies/comprehensive_comparison.py --datasets all --output_dir results/
"""

import sys
import os
import argparse
import json
import time
from pathlib import Path
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from tutorials.utils.data_loaders import load_dataset, DATASET_LOADERS
from tutorials.utils.baseline_networks import (
    BaselineMLPClassifier, 
    BaselineMLPRegressor, 
    BaselineTrainer
)
from tutorials.utils.ablation_networks import (
    create_ablation_experiment,
    AblationTrainer
)
from tutorials.utils.evaluation_metrics import (
    calculate_classification_metrics, 
    calculate_regression_metrics,
    compare_model_performance, 
    plot_model_comparison,
    generate_evaluation_report, 
    statistical_significance_test
)


class ComprehensiveAblationStudy:
    """
    ç»¼åˆæ¶ˆèå®éªŒç ”ç©¶ç±»
    
    æ‰§è¡Œä¸‰å±‚å¯¹æ¯”å®éªŒï¼š
    1. ä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†
    2. CausalEngineæ¶ˆèç‰ˆæœ¬ï¼ˆä»…ä½¿ç”¨locæŸå¤±ï¼‰
    3. å®Œæ•´CausalEngineï¼ˆä½¿ç”¨å®Œæ•´å› æœæŸå¤±ï¼‰
    """
    
    def __init__(
        self,
        output_dir: str = "results",
        device: str = "auto",
        random_seed: int = 42,
        num_runs: int = 3
    ):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾å¤‡é€‰æ‹©
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        self.random_seed = random_seed
        self.num_runs = num_runs
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(random_seed)
        np.random.seed(random_seed)
        
        # ç»“æœå­˜å‚¨
        self.all_results = {}
        self.summary_results = {}
        
        print(f"ğŸš€ åˆå§‹åŒ–ç»¼åˆæ¶ˆèå®éªŒ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   éšæœºç§å­: {self.random_seed}")
        print(f"   å®éªŒè½®æ•°: {self.num_runs}")
    
    def run_single_experiment(
        self,
        dataset_name: str,
        run_id: int = 0
    ) -> Dict:
        """
        åœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œä¸€æ¬¡å®Œæ•´çš„ä¸‰æ¨¡å‹å¯¹æ¯”å®éªŒ
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            run_id: å®éªŒè½®æ¬¡ID
            
        Returns:
            results: å®éªŒç»“æœå­—å…¸
        """
        print(f"\nğŸ”¬ å¼€å§‹å®éªŒ: {dataset_name} (ç¬¬ {run_id + 1} è½®)")
        
        # åŠ è½½æ•°æ®
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        data_dict = load_dataset(
            dataset_name,
            random_state=self.random_seed + run_id  # æ¯è½®ä½¿ç”¨ä¸åŒçš„éšæœºç§å­
        )
        
        task_type = data_dict['task_type']
        input_size = data_dict['input_size']
        output_size = data_dict['output_size']
        
        print(f"   æ•°æ®é›†: {dataset_name}")
        print(f"   ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   è®­ç»ƒæ ·æœ¬: {data_dict['train_size']}")
        print(f"   éªŒè¯æ ·æœ¬: {data_dict['val_size']}")
        print(f"   æµ‹è¯•æ ·æœ¬: {data_dict['test_size']}")
        
        results = {
            'dataset': dataset_name,
            'task_type': task_type,
            'run_id': run_id,
            'data_info': {
                'input_size': input_size,
                'output_size': output_size,
                'train_size': data_dict['train_size'],
                'val_size': data_dict['val_size'],
                'test_size': data_dict['test_size']
            },
            'models': {}
        }
        
        # 1. è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†
        print("\nğŸ—ï¸  è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†...")
        start_time = time.time()
        
        if task_type == 'classification':
            baseline_model = BaselineMLPClassifier(
                input_dim=input_size,
                num_classes=output_size,
                hidden_dims=[512, 256],
                dropout=0.1
            )
        else:
            baseline_model = BaselineMLPRegressor(
                input_dim=input_size,
                output_dim=output_size,
                hidden_dims=[512, 256],
                dropout=0.1
            )
        
        baseline_trainer = BaselineTrainer(baseline_model, device=self.device)
        
        if task_type == 'classification':
            baseline_trainer.train_classification(
                data_dict['train_loader'],
                data_dict['val_loader'],
                num_epochs=50
            )
        else:
            baseline_trainer.train_regression(
                data_dict['train_loader'],
                data_dict['val_loader'],
                num_epochs=50
            )
        
        baseline_metrics = self._evaluate_baseline_model(
            baseline_model, 
            data_dict['test_loader'], 
            task_type
        )
        baseline_metrics['training_time'] = time.time() - start_time
        results['models']['baseline'] = baseline_metrics
        
        print(f"   åŸºå‡†æ¨¡å‹è®­ç»ƒå®Œæˆ ({baseline_metrics['training_time']:.2f}s)")
        
        # 2. åˆ›å»ºCausalEngineï¼ˆç”¨äºæ¶ˆèå’Œå®Œæ•´ç‰ˆæœ¬ï¼‰
        print("\nâš¡ åˆ›å»ºCausalEngine...")
        
        engine, wrapper = create_ablation_experiment(
            input_dim=input_size,
            hidden_dim=512,
            num_layers=4,
            num_heads=8,
            task_type=task_type,
            num_classes=output_size if task_type == 'classification' else None,
            output_dim=output_size if task_type == 'regression' else 1,
            dropout=0.1,
            device=self.device
        )
        
        ablation_trainer = AblationTrainer(engine, wrapper)
        
        # 3. è®­ç»ƒCausalEngineæ¶ˆèç‰ˆæœ¬ï¼ˆä»…ä½¿ç”¨locæŸå¤±ï¼‰
        print("\nâš—ï¸  è®­ç»ƒCausalEngineæ¶ˆèç‰ˆæœ¬...")
        start_time = time.time()
        
        # å‡†å¤‡è¾“å…¥ - çœŸå®CausalEngine API
        def prepare_causal_inputs(batch_x):
            # ä¿æŒå­—å…¸æ ¼å¼ä»¥å…¼å®¹ablation_networks
            return {'values': batch_x}
        
        # è®­ç»ƒæ¶ˆèç‰ˆæœ¬
        best_val_loss = float('inf')
        for epoch in range(50):
            train_loss = 0
            train_acc = 0 if task_type == 'classification' else None
            num_batches = 0
            
            for batch_x, batch_y in data_dict['train_loader']:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                inputs = prepare_causal_inputs(batch_x)
                hidden_states = inputs['values'] if isinstance(inputs, dict) else inputs
                metrics = ablation_trainer.train_step_ablation(hidden_states, batch_y)
                
                train_loss += metrics['loss']
                if 'accuracy' in metrics:
                    train_acc += metrics['accuracy']
                num_batches += 1
            
            # éªŒè¯
            val_loss = 0
            val_acc = 0 if task_type == 'classification' else None
            val_batches = 0
            
            for batch_x, batch_y in data_dict['val_loader']:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                inputs = prepare_causal_inputs(batch_x)
                hidden_states = inputs['values'] if isinstance(inputs, dict) else inputs
                metrics = ablation_trainer.eval_step(hidden_states, batch_y, use_ablation=True)
                
                val_loss += metrics['loss']
                if 'accuracy' in metrics:
                    val_acc += metrics['accuracy']
                val_batches += 1
            
            avg_val_loss = val_loss / val_batches
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_ablation_state = engine.state_dict()
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch}: train_loss={train_loss/num_batches:.4f}, val_loss={avg_val_loss:.4f}")
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        engine.load_state_dict(best_ablation_state)
        
        ablation_metrics = self._evaluate_causal_model(
            engine, wrapper, data_dict['test_loader'], 
            task_type, use_ablation=True, prepare_fn=prepare_causal_inputs
        )
        ablation_metrics['training_time'] = time.time() - start_time
        results['models']['ablation'] = ablation_metrics
        
        print(f"   æ¶ˆèæ¨¡å‹è®­ç»ƒå®Œæˆ ({ablation_metrics['training_time']:.2f}s)")
        
        # 4. è®­ç»ƒå®Œæ•´CausalEngineï¼ˆä½¿ç”¨å®Œæ•´å› æœæŸå¤±ï¼‰
        print("\nğŸŒŸ è®­ç»ƒå®Œæ•´CausalEngine...")
        start_time = time.time()
        
        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        engine, wrapper = create_ablation_experiment(
            input_dim=input_size,
            hidden_dim=512,
            num_layers=4,
            num_heads=8,
            task_type=task_type,
            num_classes=output_size if task_type == 'classification' else None,
            output_dim=output_size if task_type == 'regression' else 1,
            dropout=0.1,
            device=self.device
        )
        
        ablation_trainer = AblationTrainer(engine, wrapper)
        
        # è®­ç»ƒå®Œæ•´ç‰ˆæœ¬
        best_val_loss = float('inf')
        for epoch in range(50):
            train_loss = 0
            train_acc = 0 if task_type == 'classification' else None
            num_batches = 0
            
            for batch_x, batch_y in data_dict['train_loader']:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                inputs = prepare_causal_inputs(batch_x)
                hidden_states = inputs['values'] if isinstance(inputs, dict) else inputs
                metrics = ablation_trainer.train_step_full(hidden_states, batch_y)
                
                train_loss += metrics['loss']
                if 'accuracy' in metrics:
                    train_acc += metrics['accuracy']
                num_batches += 1
            
            # éªŒè¯
            val_loss = 0
            val_acc = 0 if task_type == 'classification' else None
            val_batches = 0
            
            for batch_x, batch_y in data_dict['val_loader']:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                inputs = prepare_causal_inputs(batch_x)
                hidden_states = inputs['values'] if isinstance(inputs, dict) else inputs
                metrics = ablation_trainer.eval_step(hidden_states, batch_y, use_ablation=False)
                
                val_loss += metrics['loss']
                if 'accuracy' in metrics:
                    val_acc += metrics['accuracy']
                val_batches += 1
            
            avg_val_loss = val_loss / val_batches
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_full_state = engine.state_dict()
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch}: train_loss={train_loss/num_batches:.4f}, val_loss={avg_val_loss:.4f}")
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        engine.load_state_dict(best_full_state)
        
        full_metrics = self._evaluate_causal_model(
            engine, wrapper, data_dict['test_loader'], 
            task_type, use_ablation=False, prepare_fn=prepare_causal_inputs
        )
        full_metrics['training_time'] = time.time() - start_time
        results['models']['full_causal'] = full_metrics
        
        print(f"   å®Œæ•´æ¨¡å‹è®­ç»ƒå®Œæˆ ({full_metrics['training_time']:.2f}s)")
        
        # 5. è®¡ç®—æ¨¡å‹æ¯”è¾ƒ
        comparison = compare_model_performance(results['models'], task_type)
        results['comparison'] = comparison
        
        # 6. æ‰“å°æœ¬è½®ç»“æœæ‘˜è¦
        self._print_run_summary(results)
        
        return results
    
    def _evaluate_baseline_model(self, model, test_loader, task_type: str) -> Dict:
        """è¯„ä¼°ä¼ ç»ŸåŸºå‡†æ¨¡å‹"""
        model.eval()
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                outputs = model(batch_x)
                
                if task_type == 'classification':
                    preds = torch.argmax(outputs, dim=-1)
                    probs = torch.softmax(outputs, dim=-1)
                    all_probs.extend(probs.cpu().numpy())
                else:
                    preds = outputs.squeeze()
                
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        if task_type == 'classification':
            metrics = calculate_classification_metrics(
                np.array(all_targets), 
                np.array(all_preds),
                np.array(all_probs) if all_probs else None
            )
        else:
            metrics = calculate_regression_metrics(
                np.array(all_targets), 
                np.array(all_preds)
            )
        
        return metrics
    
    def _evaluate_causal_model(
        self, engine, wrapper, test_loader, 
        task_type: str, use_ablation: bool, prepare_fn
    ) -> Dict:
        """è¯„ä¼°CausalEngineæ¨¡å‹"""
        engine.eval()
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                inputs = prepare_fn(batch_x)
                
                # æå–hidden_statesä»è¾“å…¥å­—å…¸
                hidden_states = inputs['values'] if isinstance(inputs, dict) else inputs
                if hidden_states.dim() == 2:
                    hidden_states = hidden_states.unsqueeze(1)
                
                outputs = engine(
                    hidden_states=hidden_states,
                    do_sample=False,
                    temperature=1.0,
                    return_dict=True,
                    apply_activation=not use_ablation
                )
                
                if use_ablation:
                    # æ¶ˆèç‰ˆæœ¬ï¼šä½¿ç”¨locè¿›è¡Œé¢„æµ‹
                    loc = outputs['loc_S'][:, -1, :]  # [batch_size, vocab_size]
                    if task_type == 'classification':
                        logits = loc[:, :wrapper.num_classes]
                        preds = torch.argmax(logits, dim=-1)
                        probs = torch.softmax(logits, dim=-1)
                        all_probs.extend(probs.cpu().numpy())
                    else:
                        preds = loc[:, 0]
                else:
                    # å®Œæ•´ç‰ˆæœ¬ï¼šä½¿ç”¨æ¿€æ´»å¤´è¾“å‡º
                    final_output = outputs['output'][:, -1, :]  # [batch_size, output_dim]
                    if task_type == 'classification':
                        preds = torch.argmax(final_output, dim=-1)
                        probs = torch.softmax(final_output, dim=-1)
                        all_probs.extend(probs.cpu().numpy())
                    else:
                        preds = final_output[:, 0]
                
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        if task_type == 'classification':
            metrics = calculate_classification_metrics(
                np.array(all_targets), 
                np.array(all_preds),
                np.array(all_probs) if all_probs else None
            )
        else:
            metrics = calculate_regression_metrics(
                np.array(all_targets), 
                np.array(all_preds)
            )
        
        return metrics
    
    def _print_run_summary(self, results: Dict):
        """æ‰“å°å•è½®å®éªŒç»“æœæ‘˜è¦"""
        task_type = results['task_type']
        models = results['models']
        
        print(f"\nğŸ“‹ ç¬¬ {results['run_id'] + 1} è½®å®éªŒç»“æœæ‘˜è¦:")
        
        if task_type == 'classification':
            key_metric = 'accuracy'
            print("   æ¨¡å‹                   | å‡†ç¡®ç‡    | F1åˆ†æ•°    | è®­ç»ƒæ—¶é—´")
            print("   --------------------- | --------- | --------- | ---------")
        else:
            key_metric = 'r2'
            print("   æ¨¡å‹                   | RÂ²        | RMSE      | è®­ç»ƒæ—¶é—´")
            print("   --------------------- | --------- | --------- | ---------")
        
        for model_name, metrics in models.items():
            model_display = {
                'baseline': 'ä¼ ç»Ÿç¥ç»ç½‘ç»œ',
                'ablation': 'CausalEngine(æ¶ˆè)',
                'full_causal': 'CausalEngine(å®Œæ•´)'
            }.get(model_name, model_name)
            
            if task_type == 'classification':
                acc = metrics.get('accuracy', 0)
                f1 = metrics.get('f1_score', 0)
                time_str = f"{metrics.get('training_time', 0):.1f}s"
                print(f"   {model_display:21} | {acc:.4f}    | {f1:.4f}    | {time_str}")
            else:
                r2 = metrics.get('r2', 0)
                rmse = metrics.get('rmse', 0)
                time_str = f"{metrics.get('training_time', 0):.1f}s"
                print(f"   {model_display:21} | {r2:.4f}    | {rmse:.4f}  | {time_str}")
        
        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
        if 'comparison' in results and 'best_models' in results['comparison']:
            best_model = results['comparison']['best_models'].get(key_metric, 'unknown')
            best_display = {
                'baseline': 'ä¼ ç»Ÿç¥ç»ç½‘ç»œ',
                'ablation': 'CausalEngine(æ¶ˆè)',
                'full_causal': 'CausalEngine(å®Œæ•´)'
            }.get(best_model, best_model)
            print(f"\n   ğŸ† æœ¬è½®æœ€ä½³æ¨¡å‹ ({key_metric}): {best_display}")


def main():
    parser = argparse.ArgumentParser(description='è¿è¡ŒCausalEngineç»¼åˆæ¶ˆèå®éªŒ')
    parser.add_argument('--datasets', nargs='+', default=['all'],
                        help='è¦æµ‹è¯•çš„æ•°æ®é›†åˆ—è¡¨ï¼Œæˆ–ä½¿ç”¨"all"æµ‹è¯•æ‰€æœ‰æ•°æ®é›†')
    parser.add_argument('--output_dir', type=str, default='results',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡ (cuda/cpu/auto)')
    parser.add_argument('--num_runs', type=int, default=3,
                        help='æ¯ä¸ªæ•°æ®é›†çš„å®éªŒè½®æ•°')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    # å¤„ç†æ•°æ®é›†å‚æ•°
    if args.datasets == ['all']:
        dataset_names = None  # å°†æµ‹è¯•æ‰€æœ‰æ•°æ®é›†
    else:
        dataset_names = args.datasets
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    study = ComprehensiveAblationStudy(
        output_dir=args.output_dir,
        device=args.device,
        random_seed=args.seed,
        num_runs=args.num_runs
    )
    
    # è¿è¡Œå®éªŒ
    try:
        results = study.run_single_experiment(dataset_names[0] if dataset_names else 'adult', 0)
        print("\nâœ… å®éªŒå®Œæˆï¼")
        print(f"   ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 
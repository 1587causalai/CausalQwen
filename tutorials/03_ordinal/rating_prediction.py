#!/usr/bin/env python3
"""
CausalEngine è¯„åˆ†é¢„æµ‹æ•™ç¨‹

è¿™ä¸ªæ•™ç¨‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CausalEngine è¿›è¡Œæœ‰åºåˆ†ç±»ä»»åŠ¡ï¼ˆæ˜Ÿçº§è¯„åˆ†é¢„æµ‹ï¼‰ã€‚
é‡ç‚¹å±•ç¤ºæ–°å®ç°çš„ç¦»æ•£æœ‰åºæ¿€æ´»åŠŸèƒ½ï¼Œè¿™æ˜¯ CausalEngine v2.0.4 çš„é‡è¦ç‰¹æ€§ã€‚

é‡ç‚¹å±•ç¤ºï¼š
1. ç¦»æ•£æœ‰åºæ¿€æ´»å‡½æ•°çš„ä½¿ç”¨
2. ç±»åˆ«é—´é¡ºåºå…³ç³»çš„ä¿æŒ
3. é˜ˆå€¼å­¦ä¹ æœºåˆ¶
4. ä¸æ ‡å‡†åˆ†ç±»æ–¹æ³•çš„ä¼˜åŠ¿å¯¹æ¯”
5. æœ‰åºåˆ†ç±»çš„è¯„ä¼°æŒ‡æ ‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_absolute_error
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import kendalltau

# å¯¼å…¥ CausalEngine
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine import CausalEngine, ActivationMode


class RatingDataset(Dataset):
    """è¯„åˆ†æ•°æ®é›†"""
    
    def __init__(self, features, ratings):
        self.features = torch.FloatTensor(features)
        self.ratings = torch.LongTensor(ratings)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.ratings[idx]


class CausalRatingPredictor(nn.Module):
    """åŸºäº CausalEngine çš„è¯„åˆ†é¢„æµ‹å™¨"""
    
    def __init__(self, input_size, num_ratings=5, hidden_size=128):
        super().__init__()
        
        self.num_ratings = num_ratings
        
        # ç‰¹å¾åµŒå…¥å±‚
        self.feature_embedding = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # CausalEngine æ ¸å¿ƒ (æœ‰åºåˆ†ç±»æ¨¡å¼)
        self.causal_engine = CausalEngine(
            hidden_size=hidden_size,
            vocab_size=1,  # å•ä¸€è¾“å‡ºç»´åº¦
            activation_modes="ordinal",  # æœ‰åºåˆ†ç±»æ¿€æ´»
            ordinal_num_classes=num_ratings,  # 5çº§è¯„åˆ†ï¼š1-5æ˜Ÿ
            ordinal_threshold_init=1.0,  # é˜ˆå€¼åˆå§‹åŒ–
            b_noise_init=0.1,
            gamma_init=1.0
        )
    
    def forward(self, x, temperature=1.0, do_sample=False, return_details=False):
        # ç‰¹å¾åµŒå…¥
        hidden_states = self.feature_embedding(x)
        
        # CausalEngine æ¨ç†
        output = self.causal_engine(
            hidden_states.unsqueeze(1),  # æ·»åŠ åºåˆ—ç»´åº¦
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True
        )
        
        if return_details:
            return output
        else:
            return output['output'].squeeze()  # ç§»é™¤å¤šä½™ç»´åº¦


def create_movie_rating_data(n_samples=5000, n_features=50):
    """åˆ›å»ºæ¨¡æ‹Ÿç”µå½±è¯„åˆ†æ•°æ®"""
    
    print("ğŸ¬ åˆ›å»ºæ¨¡æ‹Ÿç”µå½±è¯„åˆ†æ•°æ®...")
    
    np.random.seed(42)
    
    # ç‰¹å¾è®¾è®¡ï¼šæ¨¡æ‹Ÿç”µå½±çš„å„ç§å±æ€§
    # 0-9: ç±»å‹ç‰¹å¾ (åŠ¨ä½œã€å–œå‰§ã€å‰§æƒ…ç­‰)
    # 10-19: æ¼”å‘˜ç‰¹å¾ (çŸ¥ååº¦ã€æ¼”æŠ€ç­‰)
    # 20-29: å¯¼æ¼”ç‰¹å¾ (çŸ¥ååº¦ã€é£æ ¼ç­‰)
    # 30-39: åˆ¶ä½œç‰¹å¾ (é¢„ç®—ã€ç‰¹æ•ˆç­‰)
    # 40-49: è¥é”€ç‰¹å¾ (å®£ä¼ åŠ›åº¦ã€å£ç¢‘ç­‰)
    
    features = np.random.randn(n_samples, n_features)
    
    # è®¾è®¡è¯„åˆ†ç”Ÿæˆè§„å¾‹ï¼ˆæœ‰åºå…³ç³»ï¼‰
    # å¥½ç”µå½±: é«˜æ¼”å‘˜çŸ¥ååº¦ + é«˜å¯¼æ¼”çŸ¥ååº¦ + é«˜åˆ¶ä½œè´¨é‡
    quality_score = (
        features[:, 10:20].mean(axis=1) +  # æ¼”å‘˜ç‰¹å¾
        features[:, 20:30].mean(axis=1) +  # å¯¼æ¼”ç‰¹å¾  
        features[:, 30:40].mean(axis=1) +  # åˆ¶ä½œç‰¹å¾
        0.5 * features[:, 40:50].mean(axis=1)  # è¥é”€ç‰¹å¾
    )
    
    # æ·»åŠ ç±»å‹åå¥½ï¼ˆæœ‰äº›ç±»å‹å¤©ç„¶æ›´å—æ¬¢è¿ï¼‰
    genre_bonus = np.zeros(n_samples)
    for i in range(n_samples):
        # åŠ¨ä½œç‰‡ (ç‰¹å¾0) å’Œå–œå‰§ç‰‡ (ç‰¹å¾1) æ›´å—æ¬¢è¿
        if features[i, 0] > 0.5:  # åŠ¨ä½œç‰‡
            genre_bonus[i] += 0.3
        if features[i, 1] > 0.5:  # å–œå‰§ç‰‡  
            genre_bonus[i] += 0.2
        if features[i, 2] > 1.0:  # å‰§æƒ…ç‰‡ï¼ˆé«˜è´¨é‡çš„æ›´å—æ¬¢è¿ï¼‰
            genre_bonus[i] += 0.4
    
    # ç»¼åˆå¾—åˆ†
    total_score = quality_score + genre_bonus
    
    # æ·»åŠ å™ªå£°
    total_score += np.random.normal(0, 0.5, n_samples)
    
    # è½¬æ¢ä¸º1-5æ˜Ÿè¯„åˆ†ï¼ˆæœ‰åºç±»åˆ«ï¼‰
    # ä½¿ç”¨åˆ†ä½æ•°æ¥ä¿è¯è¯„åˆ†åˆ†å¸ƒç›¸å¯¹å‡åŒ€
    percentiles = np.percentile(total_score, [20, 40, 60, 80])
    ratings = np.zeros(n_samples, dtype=int)
    
    ratings[total_score <= percentiles[0]] = 0  # 1æ˜Ÿ
    ratings[(total_score > percentiles[0]) & (total_score <= percentiles[1])] = 1  # 2æ˜Ÿ
    ratings[(total_score > percentiles[1]) & (total_score <= percentiles[2])] = 2  # 3æ˜Ÿ
    ratings[(total_score > percentiles[2]) & (total_score <= percentiles[3])] = 3  # 4æ˜Ÿ
    ratings[total_score > percentiles[3]] = 4  # 5æ˜Ÿ
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    features = scaler.fit_transform(features)
    
    print(f"âœ… æ•°æ®åˆ›å»ºå®Œæˆ: {features.shape[0]} æ ·æœ¬, {features.shape[1]} ç‰¹å¾")
    print(f"   è¯„åˆ†åˆ†å¸ƒ: {np.bincount(ratings)} (0=1æ˜Ÿ, 1=2æ˜Ÿ, 2=3æ˜Ÿ, 3=4æ˜Ÿ, 4=5æ˜Ÿ)")
    
    return features, ratings, scaler


def ordinal_cross_entropy_loss(predictions, targets, num_classes=5):
    """æœ‰åºäº¤å‰ç†µæŸå¤±å‡½æ•°
    
    è€ƒè™‘ç±»åˆ«é—´çš„é¡ºåºå…³ç³»ï¼Œç›¸é‚»ç±»åˆ«çš„è¯¯åˆ†ç±»æƒ©ç½šè¾ƒå°
    """
    # åˆ›å»ºæœ‰åºæƒé‡çŸ©é˜µ
    weight_matrix = torch.zeros(num_classes, num_classes)
    for i in range(num_classes):
        for j in range(num_classes):
            weight_matrix[i, j] = abs(i - j)  # è·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§
    
    # è½¬æ¢é¢„æµ‹ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆè™½ç„¶CausalEngineè¾“å‡ºçš„æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
    targets_one_hot = torch.eye(num_classes)[targets.long()]
    
    # è®¡ç®—åŠ æƒæŸå¤±
    weights = weight_matrix[targets.long(), predictions.long()]
    loss = weights.mean()
    
    return loss


def calculate_ordinal_metrics(y_true, y_pred, num_classes=5):
    """è®¡ç®—æœ‰åºåˆ†ç±»çš„å„ç§æŒ‡æ ‡"""
    
    # ç¡®ä¿è¾“å…¥æ˜¯ numpy array
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    if y_true.shape != y_pred.shape:
        # å°è¯•æ‰å¹³åŒ–æ•°ç»„
        y_true = y_true.flatten()
        y_pred = y_pred.flatten()
        if y_true.shape != y_pred.shape:
            raise ValueError(f"çœŸå€¼å’Œé¢„æµ‹å€¼çš„å½¢çŠ¶ä¸åŒ¹é…: {y_true.shape} vs {y_pred.shape}")
    
    # ç²¾ç¡®å‡†ç¡®ç‡
    accuracy = accuracy_score(y_true, y_pred)
    
    # ç›¸é‚»å‡†ç¡®ç‡ (å…è®¸è¯¯å·®ä¸º1)
    adjacent_correct = np.abs(y_true - y_pred) <= 1
    adjacent_accuracy = np.mean(adjacent_correct)
    
    # å¹³å‡ç»å¯¹è¯¯å·® (MAE)
    mae = mean_absolute_error(y_true, y_pred)
    
    # è‚¯å¾·å°”ç›¸å…³ç³»æ•° (è¡¡é‡æ’åºä¸€è‡´æ€§)
    tau, _ = kendalltau(y_true, y_pred)
    
    # C-index (Concordance Index)
    # C-index = (åå’Œå¯¹æ•° + 0.5 * ç»“å¯¹æ•°) / æ€»å¯¹æ•°
    # è¿™é‡Œç”¨ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼šéšæœºé€‰ä¸€å¯¹æ ·æœ¬ï¼Œé¢„æµ‹å€¼å’ŒçœŸå®å€¼æ’åºä¸€è‡´çš„æ¦‚ç‡
    total_pairs = 0
    concordant_pairs = 0
    for i in range(len(y_true)):
        for j in range(i + 1, len(y_true)):
            # çœŸå®å€¼ä¸åŒ
            if y_true[i] != y_true[j]:
                total_pairs += 1
                # é¢„æµ‹å€¼å’ŒçœŸå®å€¼æ’åºä¸€è‡´
                if (y_pred[i] - y_pred[j]) * (y_true[i] - y_true[j]) > 0:
                    concordant_pairs += 1
    
    c_index = concordant_pairs / total_pairs if total_pairs > 0 else 0.5
    
    return {
        'accuracy': accuracy,
        'adjacent_accuracy': adjacent_accuracy,
        'mae': mae,
        'kendall_tau': tau,
        'c_index': c_index
    }


def train_causal_ordinal_model(model, train_loader, val_loader, epochs=80):
    """è®­ç»ƒ CausalEngine æœ‰åºåˆ†ç±»æ¨¡å‹"""
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ CausalEngine æœ‰åºåˆ†ç±»æ¨¡å‹...")
    
    # ä½¿ç”¨ MSE æŸå¤±ä½œç”¨äºå†³ç­–åˆ†æ•°ï¼Œå°†å…¶æ¨å‘ç›®æ ‡ç±»åˆ«ç´¢å¼•
    criterion = nn.MSELoss() 
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)
    
    train_losses = []
    val_accuracies = []
    val_mae_scores = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_samples = 0
        
        for features, ratings in train_loader:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ (æ ‡å‡†æ¨¡å¼)
            output = model(features, temperature=1.0, do_sample=False, return_details=True)
            decision_scores = output['loc_S'].squeeze() # ä½¿ç”¨å†³ç­–åˆ†æ•°
            
            # ä½¿ç”¨ MSE æŸå¤±ï¼Œç›®æ ‡ ratings éœ€è¦æ˜¯ float ç±»å‹
            loss = criterion(decision_scores, ratings.float())
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * len(features)
            train_samples += len(features)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for features, ratings in val_loader:
                # çº¯å› æœæ¨¡å¼éªŒè¯
                predictions = model(features, temperature=0.0).long()
                
                val_predictions.extend(predictions.numpy())
                val_targets.extend(ratings.numpy())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_metrics = calculate_ordinal_metrics(val_targets, val_predictions)
        
        avg_train_loss = train_loss / train_samples
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_metrics['accuracy'])
        val_mae_scores.append(val_metrics['mae'])
        
        scheduler.step()
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{epochs}]:")
            print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")
            print(f"  ç›¸é‚»å‡†ç¡®ç‡: {val_metrics['adjacent_accuracy']:.4f}")
            print(f"  å¹³å‡ç»å¯¹è¯¯å·®: {val_metrics['mae']:.4f}")
            print(f"  è‚¯å¾·å°”Tau: {val_metrics['kendall_tau']:.4f}")
    
    print("âœ… è®­ç»ƒå®Œæˆ!")
    return train_losses, val_accuracies, val_mae_scores


def train_baseline_ordinal_models(X_train, y_train, X_val, y_val):
    """è®­ç»ƒä¼ ç»Ÿæœ‰åºåˆ†ç±»åŸºçº¿æ¨¡å‹"""
    
    print("\nğŸ“Š è®­ç»ƒä¼ ç»Ÿæœ‰åºåˆ†ç±»åŸºçº¿æ¨¡å‹...")
    
    baselines = {}
    
    # å¤šåˆ†ç±»é€»è¾‘å›å½’ (å¿½ç•¥é¡ºåºå…³ç³»)
    lr = LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial')
    lr.fit(X_train, y_train)
    lr_pred = lr.predict(X_val)
    lr_metrics = calculate_ordinal_metrics(y_val, lr_pred)
    baselines['Logistic Regression'] = {'model': lr, 'predictions': lr_pred, **lr_metrics}
    
    # éšæœºæ£®æ— (éƒ¨åˆ†è€ƒè™‘é¡ºåºå…³ç³»)
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_val)
    rf_metrics = calculate_ordinal_metrics(y_val, rf_pred)
    baselines['Random Forest'] = {'model': rf, 'predictions': rf_pred, **rf_metrics}
    
    # é˜ˆå€¼æ–¹æ³• (å›å½’è½¬åˆ†ç±»)
    from sklearn.linear_model import Ridge
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train, y_train.astype(float))
    ridge_scores = ridge.predict(X_val)
    
    # ä½¿ç”¨å­¦ä¹ åˆ°çš„é˜ˆå€¼è¿›è¡Œåˆ†ç±»
    thresholds = np.percentile(ridge_scores, [20, 40, 60, 80])
    ridge_pred = np.zeros_like(ridge_scores, dtype=int)
    ridge_pred[ridge_scores <= thresholds[0]] = 0
    ridge_pred[(ridge_scores > thresholds[0]) & (ridge_scores <= thresholds[1])] = 1
    ridge_pred[(ridge_scores > thresholds[1]) & (ridge_scores <= thresholds[2])] = 2
    ridge_pred[(ridge_scores > thresholds[2]) & (ridge_scores <= thresholds[3])] = 3
    ridge_pred[ridge_scores > thresholds[3]] = 4
    
    ridge_metrics = calculate_ordinal_metrics(y_val, ridge_pred)
    baselines['Ridge (Thresholded)'] = {'model': ridge, 'predictions': ridge_pred, **ridge_metrics}
    
    for name, result in baselines.items():
        print(f"   {name}:")
        print(f"     å‡†ç¡®ç‡: {result['accuracy']:.4f}")
        print(f"     ç›¸é‚»å‡†ç¡®ç‡: {result['adjacent_accuracy']:.4f}")
        print(f"     MAE: {result['mae']:.4f}")
        print(f"     C-index: {result['c_index']:.4f}")
    
    return baselines


def evaluate_ordinal_inference_modes(model, test_loader):
    """è¯„ä¼° CausalEngine åœ¨æœ‰åºåˆ†ç±»ä¸­çš„ä¸‰ç§æ¨ç†æ¨¡å¼"""
    
    print("\nğŸ” è¯„ä¼°æœ‰åºåˆ†ç±»çš„ä¸‰ç§æ¨ç†æ¨¡å¼...")
    
    model.eval()
    results = {}
    
    modes = [
        ("çº¯å› æœæ¨¡å¼", {"temperature": 0.0, "do_sample": False}),
        ("æ ‡å‡†æ¨¡å¼", {"temperature": 1.0, "do_sample": False}), 
        ("é‡‡æ ·æ¨¡å¼", {"temperature": 0.8, "do_sample": True})
    ]
    
    for mode_name, params in modes:
        predictions = []
        uncertainties = []
        true_ratings = []
        decision_scores = []
        
        with torch.no_grad():
            for features, ratings in test_loader:
                # è·å–è¯¦ç»†è¾“å‡º
                output = model(features, return_details=True, **params)
                
                # æå–é¢„æµ‹ç±»åˆ«
                pred_ratings = output['output'].squeeze().long()
                predictions.extend(pred_ratings.numpy())
                
                # ä¸ç¡®å®šæ€§ï¼ˆå°ºåº¦å‚æ•°ï¼‰
                scale_S = output['scale_S'].squeeze()
                uncertainties.extend(scale_S.numpy())
                
                # å†³ç­–å¾—åˆ†
                loc_S = output['loc_S'].squeeze()
                decision_scores.extend(loc_S.numpy())
                
                # çœŸå®è¯„åˆ†
                true_ratings.extend(ratings.numpy())
        
        # è®¡ç®—æœ‰åºåˆ†ç±»æŒ‡æ ‡
        predictions = np.array(predictions)
        true_ratings = np.array(true_ratings)
        uncertainties = np.array(uncertainties)
        decision_scores = np.array(decision_scores)
        
        metrics = calculate_ordinal_metrics(true_ratings, predictions)
        metrics['avg_uncertainty'] = np.mean(uncertainties)
        metrics['predictions'] = predictions
        metrics['true_ratings'] = true_ratings
        metrics['uncertainties'] = uncertainties
        metrics['decision_scores'] = decision_scores
        
        results[mode_name] = metrics
        
        print(f"   {mode_name}:")
        print(f"     å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        print(f"     ç›¸é‚»å‡†ç¡®ç‡: {metrics['adjacent_accuracy']:.4f}")
        print(f"     MAE: {metrics['mae']:.4f}")
        print(f"     è‚¯å¾·å°”Tau: {metrics['kendall_tau']:.4f}")
        print(f"     C-index: {metrics['c_index']:.4f}")
        print(f"     å¹³å‡ä¸ç¡®å®šæ€§: {metrics['avg_uncertainty']:.3f}")
    
    return results


def analyze_ordinal_properties(model, test_loader):
    """åˆ†ææœ‰åºåˆ†ç±»çš„ç‰¹æ®Šæ€§è´¨"""
    
    print("\nğŸ¯ åˆ†ææœ‰åºåˆ†ç±»ç‰¹æ®Šæ€§è´¨...")
    
    model.eval()
    
    with torch.no_grad():
        all_features = []
        all_ratings = []
        all_decision_scores = []
        
        for features, ratings in test_loader:
            output = model(features, temperature=0.0, return_details=True)
            
            all_features.append(features)
            all_ratings.append(ratings)
            all_decision_scores.append(output['loc_S'].squeeze())
        
        all_features = torch.cat(all_features, dim=0)
        all_ratings = torch.cat(all_ratings, dim=0)
        all_decision_scores = torch.cat(all_decision_scores, dim=0)
    
    # 1. åˆ†æå­¦ä¹ åˆ°çš„é˜ˆå€¼
    engine_config = model.causal_engine.activation.get_config()
    print("DEBUG: CausalEngine activation config:", engine_config) # æ‰“å°é…ç½®ä»¥è¿›è¡Œè°ƒè¯•
    
    # ä»é…ç½®ä¸­å®‰å…¨åœ°è·å–é˜ˆå€¼
    thresholds_dict = engine_config.get('ordinal_thresholds', {})
    if not thresholds_dict:
        raise ValueError("åœ¨æ¨¡å‹é…ç½®ä¸­æ‰¾ä¸åˆ° 'ordinal_thresholds'ã€‚")
    
    # ä½¿ç”¨å­—å…¸ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨é”®
    first_key = next(iter(thresholds_dict))
    learned_thresholds = thresholds_dict[first_key]
    
    print(f"   å­¦ä¹ åˆ°çš„è¯„åˆ†é˜ˆå€¼: {learned_thresholds}")
    
    # 2. åˆ†æå†³ç­–å¾—åˆ†çš„åˆ†å¸ƒ
    decision_scores_by_rating = {}
    for rating in range(5):
        mask = all_ratings == rating
        scores = all_decision_scores[mask].numpy()
        decision_scores_by_rating[rating] = scores
        print(f"   {rating+1}æ˜Ÿè¯„åˆ†çš„å†³ç­–å¾—åˆ†: å‡å€¼={scores.mean():.3f}, æ ‡å‡†å·®={scores.std():.3f}")
    
    # 3. æ£€æŸ¥å•è°ƒæ€§
    # é«˜è¯„åˆ†çš„å†³ç­–å¾—åˆ†åº”è¯¥æ€»ä½“ä¸Šé«˜äºä½è¯„åˆ†
    monotonicity_violations = 0
    total_comparisons = 0
    
    for i in range(5):
        for j in range(i+1, 5):
            scores_i = decision_scores_by_rating[i]
            scores_j = decision_scores_by_rating[j]
            
            # æ¯”è¾ƒå¹³å‡å¾—åˆ†
            if scores_i.mean() > scores_j.mean():
                monotonicity_violations += 1
            total_comparisons += 1
    
    monotonicity_preservation = 1 - (monotonicity_violations / total_comparisons)
    print(f"   å•è°ƒæ€§ä¿æŒåº¦: {monotonicity_preservation:.3f} (1.0ä¸ºå®Œå…¨å•è°ƒ)")
    
    return {
        'learned_thresholds': learned_thresholds,
        'decision_scores_by_rating': decision_scores_by_rating,
        'monotonicity_preservation': monotonicity_preservation
    }


def visualize_ordinal_results(train_losses, val_accuracies, val_mae_scores,
                            inference_results, baseline_results, ordinal_analysis):
    """Visualize ordinal classification results"""
    
    print("\nğŸ“ˆ Generating ordinal classification visualization...")
    
    # Set matplotlib to use English and avoid font issues
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    
    # 1. Training curves
    axes[0, 0].plot(train_losses, label='Training Loss', color='blue')
    axes[0, 0].set_title('CausalEngine Ordinal Classification Training Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # 2. Validation metrics curves
    axes[0, 1].plot(val_accuracies, label='Accuracy', color='green')
    axes[0, 1].plot(val_mae_scores, label='MAE', color='red')
    axes[0, 1].set_title('Validation Metrics Evolution')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # 3. Inference mode comparison - Accuracy
    # Map Chinese mode names to English
    mode_mapping = {
        'çº¯å› æœæ¨¡å¼': 'Pure Causal',
        'æ ‡å‡†æ¨¡å¼': 'Standard',
        'é‡‡æ ·æ¨¡å¼': 'Sampling'
    }
    
    modes_en = [mode_mapping.get(mode, mode) for mode in inference_results.keys()]
    accuracies = [inference_results[mode]['accuracy'] for mode in inference_results.keys()]
    adj_accuracies = [inference_results[mode]['adjacent_accuracy'] for mode in inference_results.keys()]
    
    x = np.arange(len(modes_en))
    width = 0.35
    
    axes[0, 2].bar(x - width/2, accuracies, width, label='Exact Accuracy', alpha=0.7)
    axes[0, 2].bar(x + width/2, adj_accuracies, width, label='Adjacent Accuracy', alpha=0.7)
    axes[0, 2].set_title('Accuracy across Inference Modes')
    axes[0, 2].set_ylabel('Accuracy')
    axes[0, 2].set_xticks(x)
    axes[0, 2].set_xticklabels(modes_en, rotation=45)
    axes[0, 2].legend()
    
    # 4. MAE and C-index comparison
    mae_scores = [inference_results[mode]['mae'] for mode in inference_results.keys()]
    c_indices = [inference_results[mode]['c_index'] for mode in inference_results.keys()]
    
    axes[1, 0].bar(x - width/2, mae_scores, width, label='MAE', alpha=0.7, color='red')
    axes[1, 0].set_ylabel('MAE', color='red')
    axes[1, 0].tick_params(axis='y', labelcolor='red')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(modes_en, rotation=45)
    
    ax2 = axes[1, 0].twinx()
    ax2.bar(x + width/2, c_indices, width, label='C-index', alpha=0.7, color='blue')
    ax2.set_ylabel('C-index', color='blue')
    ax2.tick_params(axis='y', labelcolor='blue')
    
    axes[1, 0].set_title('MAE vs C-index')
    
    # 5. Comparison with traditional methods
    all_methods = list(baseline_results.keys()) + ['CausalEngine (Pure Causal)']
    all_accuracies = [baseline_results[method]['accuracy'] for method in baseline_results.keys()]
    all_accuracies.append(inference_results['çº¯å› æœæ¨¡å¼']['accuracy'])
    
    all_maes = [baseline_results[method]['mae'] for method in baseline_results.keys()]
    all_maes.append(inference_results['çº¯å› æœæ¨¡å¼']['mae'])
    
    x_methods = np.arange(len(all_methods))
    colors = ['gray'] * len(baseline_results) + ['red']
    
    bars1 = axes[1, 1].bar(x_methods - width/2, all_accuracies, width, 
                          label='Accuracy', alpha=0.7, color=colors)
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].set_xticks(x_methods)
    axes[1, 1].set_xticklabels(all_methods, rotation=45)
    axes[1, 1].set_title('CausalEngine vs Traditional Methods')
    
    ax3 = axes[1, 1].twinx()
    bars2 = ax3.bar(x_methods + width/2, all_maes, width, 
                   label='MAE', alpha=0.7, color='orange')
    ax3.set_ylabel('MAE')
    
    # 6. Confusion matrix
    mode_data = inference_results['çº¯å› æœæ¨¡å¼']
    cm = confusion_matrix(mode_data['true_ratings'], mode_data['predictions'])
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2])
    axes[1, 2].set_title('Confusion Matrix (Pure Causal Mode)')
    axes[1, 2].set_xlabel('Predicted Rating')
    axes[1, 2].set_ylabel('True Rating')
    
    # 7. Decision score distribution
    decision_scores_by_rating = ordinal_analysis['decision_scores_by_rating']
    
    for rating in range(5):
        scores = decision_scores_by_rating[rating]
        axes[2, 0].hist(scores, alpha=0.6, label=f'{rating+1} Star', bins=20)
    
    axes[2, 0].set_title('Decision Score Distribution by Rating')
    axes[2, 0].set_xlabel('Decision Score')
    axes[2, 0].set_ylabel('Frequency')
    axes[2, 0].legend()
    
    # 8. Learned thresholds visualization
    thresholds = ordinal_analysis['learned_thresholds']
    
    # Create threshold line plot
    x_range = np.linspace(-3, 3, 1000)
    rating_regions = []
    
    axes[2, 1].axvline(x=thresholds[0], color='red', linestyle='--', alpha=0.7, label='Threshold')
    axes[2, 1].axvline(x=thresholds[1], color='red', linestyle='--', alpha=0.7)
    axes[2, 1].axvline(x=thresholds[2], color='red', linestyle='--', alpha=0.7)
    axes[2, 1].axvline(x=thresholds[3], color='red', linestyle='--', alpha=0.7)
    
    # Add region labels
    region_centers = [-2.5, (thresholds[0] + thresholds[1])/2, 
                     (thresholds[1] + thresholds[2])/2,
                     (thresholds[2] + thresholds[3])/2, 2.5]
    
    for i, center in enumerate(region_centers):
        axes[2, 1].text(center, 0.5, f'{i+1} Star', ha='center', va='center', 
                       bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
    
    axes[2, 1].set_title('Learned Rating Thresholds')
    axes[2, 1].set_xlabel('Decision Score')
    axes[2, 1].set_ylabel('Rating Region')
    axes[2, 1].set_ylim(0, 1)
    axes[2, 1].legend()
    
    # 9. Uncertainty vs prediction error relationship
    uncertainties = mode_data['uncertainties']
    errors = np.abs(mode_data['predictions'] - mode_data['true_ratings'])
    
    axes[2, 2].scatter(uncertainties, errors, alpha=0.6, s=20)
    axes[2, 2].set_xlabel('Uncertainty')
    axes[2, 2].set_ylabel('Prediction Error (Rating Levels)')
    axes[2, 2].set_title('Uncertainty vs Prediction Error')
    axes[2, 2].grid(True)
    
    # Add trend line
    z = np.polyfit(uncertainties, errors, 1)
    p = np.poly1d(z)
    axes[2, 2].plot(uncertainties, p(uncertainties), "r--", alpha=0.8)
    
    plt.tight_layout()
    plt.savefig('/Users/gongqian/DailyLog/CausalQwen/tutorials/03_ordinal/rating_prediction_results.png', 
                dpi=300, bbox_inches='tight')
    
    # Save and close without blocking
    plt.close()  # Close the figure to free memory and avoid blocking
    
    print("âœ… Visualization completed, image saved to rating_prediction_results.png")


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„è¯„åˆ†é¢„æµ‹æ•™ç¨‹"""
    
    print("â­ CausalEngine è¯„åˆ†é¢„æµ‹æ•™ç¨‹ (æœ‰åºåˆ†ç±»)")
    print("=" * 50)
    
    # 1. æ•°æ®å‡†å¤‡
    features, ratings, scaler = create_movie_rating_data(n_samples=5000, n_features=50)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_temp, y_train, y_temp = train_test_split(
        features, ratings, test_size=0.4, random_state=42, stratify=ratings
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    print(f"   è®­ç»ƒé›†è¯„åˆ†åˆ†å¸ƒ: {np.bincount(y_train)}")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = RatingDataset(X_train, y_train)
    val_dataset = RatingDataset(X_val, y_val)
    test_dataset = RatingDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    # 3. åˆ›å»ºå¹¶è®­ç»ƒ CausalEngine æ¨¡å‹
    model = CausalRatingPredictor(input_size=X_train.shape[1], num_ratings=5)
    print(f"\nğŸ—ï¸ CausalEngine æœ‰åºåˆ†ç±»æ¨¡å‹æ¶æ„:")
    print(f"   è¾“å…¥ç»´åº¦: {X_train.shape[1]} (ç”µå½±ç‰¹å¾)")
    print(f"   éšè—ç»´åº¦: 128")
    print(f"   è¾“å‡º: 5çº§è¯„åˆ† (1-5æ˜Ÿ)")
    print(f"   æ¿€æ´»æ¨¡å¼: ç¦»æ•£æœ‰åºåˆ†ç±»")
    print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    train_losses, val_accuracies, val_mae_scores = train_causal_ordinal_model(
        model, train_loader, val_loader, epochs=80
    )
    
    # 4. è®­ç»ƒä¼ ç»ŸåŸºçº¿æ¨¡å‹
    baseline_results = train_baseline_ordinal_models(X_train, y_train, X_val, y_val)
    
    # 5. è¯„ä¼°ä¸‰ç§æ¨ç†æ¨¡å¼
    inference_results = evaluate_ordinal_inference_modes(model, test_loader)
    
    # 6. åˆ†ææœ‰åºåˆ†ç±»ç‰¹æ€§
    ordinal_analysis = analyze_ordinal_properties(model, test_loader)
    
    # 7. å¯è§†åŒ–ç»“æœ
    visualize_ordinal_results(train_losses, val_accuracies, val_mae_scores,
                            inference_results, baseline_results, ordinal_analysis)
    
    # 8. æ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“‹ æœ‰åºåˆ†ç±»ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
    print("="*50)
    
    best_causal_mode = max(inference_results.keys(), 
                          key=lambda x: inference_results[x]['c_index'])
    best_causal_metrics = inference_results[best_causal_mode]
    
    best_baseline = max(baseline_results.keys(),
                       key=lambda x: baseline_results[x]['c_index'])
    best_baseline_metrics = baseline_results[best_baseline]
    
    print(f"ğŸ† æœ€ä½³ CausalEngine æ¨¡å¼: {best_causal_mode}")
    print(f"   ç²¾ç¡®å‡†ç¡®ç‡: {best_causal_metrics['accuracy']:.4f}")
    print(f"   ç›¸é‚»å‡†ç¡®ç‡: {best_causal_metrics['adjacent_accuracy']:.4f}")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {best_causal_metrics['mae']:.4f}")
    print(f"   è‚¯å¾·å°”Tau: {best_causal_metrics['kendall_tau']:.4f}")
    print(f"   C-index: {best_causal_metrics['c_index']:.4f}")
    print(f"   å¹³å‡ä¸ç¡®å®šæ€§: {best_causal_metrics['avg_uncertainty']:.3f}")
    
    print(f"\nğŸ¥ˆ æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_baseline}")
    print(f"   ç²¾ç¡®å‡†ç¡®ç‡: {best_baseline_metrics['accuracy']:.4f}")
    print(f"   ç›¸é‚»å‡†ç¡®ç‡: {best_baseline_metrics['adjacent_accuracy']:.4f}")
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {best_baseline_metrics['mae']:.4f}")
    print(f"   C-index: {best_baseline_metrics['c_index']:.4f}")
    
    accuracy_improvement = (best_causal_metrics['accuracy'] - best_baseline_metrics['accuracy']) * 100
    mae_improvement = (best_baseline_metrics['mae'] - best_causal_metrics['mae']) / best_baseline_metrics['mae'] * 100
    
    print(f"\nğŸ“ˆ CausalEngine ä¼˜åŠ¿:")
    print(f"   ç²¾ç¡®å‡†ç¡®ç‡æå‡: {accuracy_improvement:+.1f}%")
    print(f"   MAEé™ä½: {mae_improvement:+.1f}%")
    print(f"   å•è°ƒæ€§ä¿æŒåº¦: {ordinal_analysis['monotonicity_preservation']:.3f}")
    print(f"   å­¦ä¹ é˜ˆå€¼æ•°é‡: {len(ordinal_analysis['learned_thresholds'])}")
    print(f"   æœ‰åºå…³ç³»å»ºæ¨¡: âœ… æ”¯æŒ")
    print(f"   ä¸ç¡®å®šæ€§é‡åŒ–: âœ… æ”¯æŒ")
    print(f"   æ¨ç†æ¨¡å¼: âœ… 3ç§æ¨¡å¼å¯é€‰")
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print(f"   â€¢ ç¦»æ•£æœ‰åºæ¿€æ´»æˆåŠŸå­¦ä¹ äº†è¯„åˆ†é˜ˆå€¼")
    print(f"   â€¢ ç›¸é‚»å‡†ç¡®ç‡æ˜¾è‘—é«˜äºç²¾ç¡®å‡†ç¡®ç‡ (å®¹é”™æ€§å¼º)")
    print(f"   â€¢ C-indexè¡¨æ˜æ¨¡å‹å¾ˆå¥½åœ°ä¿æŒäº†è¯„åˆ†çš„ç›¸å¯¹é¡ºåº")
    print(f"   â€¢ å†³ç­–å¾—åˆ†å‘ˆç°æ˜æ˜¾çš„æœ‰åºåˆ†å¸ƒç‰¹å¾")
    print(f"   â€¢ çº¯å› æœæ¨¡å¼åœ¨æœ‰åºåˆ†ç±»ä¸­æœ€ç¨³å®š")
    
    print("\nğŸŒŸ æœ‰åºåˆ†ç±»ç‰¹è‰²:")
    print(f"   â€¢ è‡ªåŠ¨å­¦ä¹ è¯„åˆ†é˜ˆå€¼: {ordinal_analysis['learned_thresholds']}")
    print(f"   â€¢ ç›¸é‚»ç±»åˆ«è¯¯åˆ†ç±»æƒ©ç½šè¾ƒå°")
    print(f"   â€¢ ä¿æŒç±»åˆ«é—´çš„è‡ªç„¶é¡ºåºå…³ç³»")
    print(f"   â€¢ é€‚ç”¨äºè¯„åˆ†ã€ç­‰çº§ã€å¼ºåº¦ç­‰æœ‰åºæ ‡ç­¾")
    
    print("\nâœ… æœ‰åºåˆ†ç±»æ•™ç¨‹å®Œæˆ! è¿™å±•ç¤ºäº† CausalEngine v2.0.4 çš„é‡è¦æ–°åŠŸèƒ½ã€‚")


if __name__ == "__main__":
    main()
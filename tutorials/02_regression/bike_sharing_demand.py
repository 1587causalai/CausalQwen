"""
Bike Sharing éœ€æ±‚é¢„æµ‹æ•™ç¨‹ (å¸¦æ¶ˆèå®éªŒ)
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CausalEngineè¿›è¡Œå…±äº«å•è½¦éœ€æ±‚é¢„æµ‹ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

è¿™æ˜¯æœ€é‡è¦çš„å›å½’æ¼”ç¤ºä¹‹ä¸€ï¼ŒåŒ…å«å®Œæ•´çš„æ¶ˆèå®éªŒï¼
"""

import sys
import os
import time
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from tutorials.utils.data_loaders import load_dataset
from tutorials.utils.baseline_networks import create_baseline_regressor, BaselineTrainer
from tutorials.utils.ablation_networks import (
    create_ablated_regressor, create_full_causal_regressor
)
from tutorials.utils.evaluation_metrics import (
    calculate_regression_metrics, compare_model_performance,
    plot_regression_diagnostics, generate_evaluation_report
)


def explore_bike_sharing_dataset():
    """
    æ¢ç´¢Bike Sharingæ•°æ®é›†
    """
    print("ğŸš² Bike Sharing æ•°æ®é›†æ¢ç´¢")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    data_dict = load_dataset('bike_sharing', batch_size=64)
    
    print(f"\nğŸ“ˆ æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    print(f"   æ•°æ®é›†åç§°: {data_dict['name']}")
    print(f"   ä»»åŠ¡ç±»å‹: {data_dict['task_type']}")
    print(f"   è¾“å…¥ç‰¹å¾æ•°: {data_dict['input_size']}")
    print(f"   è¾“å‡ºç»´åº¦: {data_dict['output_size']}")
    print(f"   è®­ç»ƒæ ·æœ¬: {data_dict['train_size']}")
    print(f"   éªŒè¯æ ·æœ¬: {data_dict['val_size']}")
    print(f"   æµ‹è¯•æ ·æœ¬: {data_dict['test_size']}")
    
    # æ˜¾ç¤ºç‰¹å¾ä¿¡æ¯
    print(f"\nğŸ” ç‰¹å¾åˆ—è¡¨:")
    for i, feature in enumerate(data_dict['feature_names']):
        print(f"   {i+1:2d}. {feature}")
    
    # åˆ†æç›®æ ‡å˜é‡åˆ†å¸ƒ
    y_train = data_dict['y_train']
    
    print(f"\nğŸ“Š ç›®æ ‡å˜é‡ç»Ÿè®¡:")
    print(f"   æœ€å°å€¼: {y_train.min():.2f}")
    print(f"   æœ€å¤§å€¼: {y_train.max():.2f}")
    print(f"   å‡å€¼: {y_train.mean():.2f}")
    print(f"   æ ‡å‡†å·®: {y_train.std():.2f}")
    print(f"   ä¸­ä½æ•°: {np.median(y_train):.2f}")
    
    # åˆ†æç‰¹å¾ç»Ÿè®¡
    X_train = data_dict['X_train']
    print(f"\nğŸ” ç‰¹å¾ç»Ÿè®¡æ‘˜è¦:")
    print(f"   ç‰¹å¾ç»´åº¦: {X_train.shape}")
    print(f"   ç‰¹å¾å‡å€¼èŒƒå›´: [{X_train.mean(axis=0).min():.3f}, {X_train.mean(axis=0).max():.3f}]")
    print(f"   ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{X_train.std(axis=0).min():.3f}, {X_train.std(axis=0).max():.3f}]")
    
    # å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
    visualize_data_exploration(data_dict)
    
    return data_dict


def visualize_data_exploration(data_dict):
    """
    å¯è§†åŒ–æ•°æ®æ¢ç´¢ç»“æœ
    """
    # Setup plotting style
    plt.style.use('default')
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    y_train = data_dict['y_train']
    X_train = data_dict['X_train']
    
    # 1. Target variable distribution
    axes[0, 0].hist(y_train, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_xlabel('Bike Demand')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Target Variable Distribution')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Target variable Q-Q plot (normality test)
    stats.probplot(y_train, dist="norm", plot=axes[0, 1])
    axes[0, 1].set_title('Target Variable Normality Test')
    
    # 3. Target variable box plot
    axes[0, 2].boxplot(y_train, vert=True)
    axes[0, 2].set_ylabel('Bike Demand')
    axes[0, 2].set_title('Target Variable Box Plot')
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾ï¼ˆé€‰æ‹©å‰10ä¸ªç‰¹å¾ï¼‰
    n_features_to_show = min(10, X_train.shape[1])
    feature_subset = X_train[:, :n_features_to_show]
    feature_names_subset = data_dict['feature_names'][:n_features_to_show]
    
    # æ·»åŠ ç›®æ ‡å˜é‡
    data_for_corr = np.column_stack([feature_subset, y_train])
    feature_names_with_target = feature_names_subset + ['target']
    
    corr_matrix = np.corrcoef(data_for_corr.T)
    
    im = axes[1, 0].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)
    axes[1, 0].set_xticks(range(len(feature_names_with_target)))
    axes[1, 0].set_yticks(range(len(feature_names_with_target)))
    axes[1, 0].set_xticklabels(feature_names_with_target, rotation=45, ha='right')
    axes[1, 0].set_yticklabels(feature_names_with_target)
    axes[1, 0].set_title('Feature Correlation Matrix')
    
    # Add correlation coefficient text
    for i in range(len(feature_names_with_target)):
        for j in range(len(feature_names_with_target)):
            text = axes[1, 0].text(j, i, f'{corr_matrix[i, j]:.2f}',
                                 ha="center", va="center", color="black", fontsize=8)
    
    plt.colorbar(im, ax=axes[1, 0])
    
    # 5. Feature distribution examples (select important features)
    for idx in range(min(2, X_train.shape[1])):
        axes[1, 1].hist(X_train[:, idx], bins=30, alpha=0.6, 
                       label=f'{data_dict["feature_names"][idx]}', density=True)
    
    axes[1, 1].set_xlabel('Feature Value')
    axes[1, 1].set_ylabel('Density')
    axes[1, 1].set_title('Feature Distribution Examples')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. Feature vs target variable scatter plot
    if X_train.shape[1] > 0:
        axes[1, 2].scatter(X_train[:, 0], y_train, alpha=0.5, s=10)
        axes[1, 2].set_xlabel(f'{data_dict["feature_names"][0]}')
        axes[1, 2].set_ylabel('Bike Demand')
        axes[1, 2].set_title('Feature vs Target Variable Relationship')
        axes[1, 2].grid(True, alpha=0.3)
        
        # Add trend line
        z = np.polyfit(X_train[:, 0], y_train, 1)
        p = np.poly1d(z)
        axes[1, 2].plot(X_train[:, 0], p(X_train[:, 0]), "r--", alpha=0.8)
    
    plt.tight_layout()
    plt.savefig('tutorials/02_regression/bike_sharing_exploration.png', dpi=300, bbox_inches='tight')
    plt.close()  # Close instead of show to avoid blocking


def run_ablation_experiment(data_dict):
    """
    è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ
    """
    print("\nğŸ”¬ Bike Sharing æ¶ˆèå®éªŒ")
    print("=" * 50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_size = data_dict['input_size']
    output_size = data_dict['output_size']
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"è¾“å…¥ç»´åº¦: {input_size}")
    print(f"è¾“å‡ºç»´åº¦: {output_size}")
    
    results = {}
    
    # 1. è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†
    print(f"\nğŸ—ï¸  ç¬¬1æ­¥: è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†")
    start_time = time.time()
    
    baseline_model = create_baseline_regressor(
        input_size=input_size,
        output_size=output_size,
        hidden_sizes=[128, 64, 32],
        dropout_rate=0.2
    )
    
    baseline_trainer = BaselineTrainer(
        model=baseline_model,
        device=device,
        learning_rate=0.001,
        weight_decay=0.01
    )
    
    baseline_trainer.train_regression(
        train_loader=data_dict['train_loader'],
        val_loader=data_dict['val_loader'],
        num_epochs=100,
        early_stopping_patience=15
    )
    
    baseline_time = time.time() - start_time
    
    # è¯„ä¼°åŸºå‡†æ¨¡å‹
    baseline_metrics = evaluate_model(
        baseline_model, data_dict['test_loader'], device, "Traditional NN"
    )
    baseline_metrics['training_time'] = baseline_time
    results['baseline'] = baseline_metrics
    
    print(f"âœ… ä¼ ç»Ÿç¥ç»ç½‘ç»œè®­ç»ƒå®Œæˆ ({baseline_time:.2f}s)")
    
    # 2. è®­ç»ƒCausalEngineæ¶ˆèç‰ˆæœ¬
    print(f"\nâš—ï¸  ç¬¬2æ­¥: è®­ç»ƒCausalEngineæ¶ˆèç‰ˆæœ¬ (ä»…ä½ç½®è¾“å‡º)")
    start_time = time.time()
    
    ablated_model = create_ablated_regressor(
        input_size=input_size,
        output_size=output_size,
        causal_size=128
    )
    
    ablated_trainer = BaselineTrainer(
        model=ablated_model,
        device=device,
        learning_rate=0.001,
        weight_decay=0.01
    )
    
    ablated_trainer.train_regression(
        train_loader=data_dict['train_loader'],
        val_loader=data_dict['val_loader'],
        num_epochs=100,
        early_stopping_patience=15
    )
    
    ablated_time = time.time() - start_time
    
    # è¯„ä¼°æ¶ˆèæ¨¡å‹
    ablated_metrics = evaluate_model(
        ablated_model, data_dict['test_loader'], device, "CausalEngine(Ablated)"
    )
    ablated_metrics['training_time'] = ablated_time
    results['ablated'] = ablated_metrics
    
    print(f"âœ… CausalEngineæ¶ˆèç‰ˆæœ¬è®­ç»ƒå®Œæˆ ({ablated_time:.2f}s)")
    
    # 3. è®­ç»ƒå®Œæ•´CausalEngine
    print(f"\nğŸŒŸ ç¬¬3æ­¥: è®­ç»ƒå®Œæ•´CausalEngine (ä½ç½®+å°ºåº¦)")
    start_time = time.time()
    
    full_model = create_full_causal_regressor(
        input_size=input_size,
        output_size=output_size,
        causal_size=128
    )
    
    full_trainer = BaselineTrainer(
        model=full_model,
        device=device,
        learning_rate=0.001,
        weight_decay=0.01
    )
    
    full_trainer.train_regression(
        train_loader=data_dict['train_loader'],
        val_loader=data_dict['val_loader'],
        num_epochs=100,
        early_stopping_patience=15
    )
    
    full_time = time.time() - start_time
    
    # è¯„ä¼°å®Œæ•´æ¨¡å‹ (å¤šç§æ¨ç†æ¨¡å¼)
    for mode_name, (temp, do_sample) in [
        ("CausalEngine(Causal)", (0, False)),
        ("CausalEngine(Standard)", (1.0, False)),
        ("CausalEngine(Sampling)", (0.8, True))
    ]:
        full_metrics = evaluate_model(
            full_model, data_dict['test_loader'], device, mode_name,
            temperature=temp, do_sample=do_sample
        )
        full_metrics['training_time'] = full_time
        # ä½¿ç”¨ç®€åŒ–çš„keyå
        key_mapping = {
            'CausalEngine(Causal)': 'causal',
            'CausalEngine(Standard)': 'standard', 
            'CausalEngine(Sampling)': 'sampling'
        }
        results[key_mapping.get(mode_name, mode_name.lower())] = full_metrics
    
    print(f"âœ… å®Œæ•´CausalEngineè®­ç»ƒå®Œæˆ ({full_time:.2f}s)")
    
    return results


def evaluate_model(model, test_loader, device, model_name, temperature=1.0, do_sample=False):
    """
    è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
    """
    print(f"   è¯„ä¼° {model_name}...")
    
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            # è·å–é¢„æµ‹
            if hasattr(model, 'predict') and hasattr(model, 'causal_encoder'):
                # çœŸæ­£çš„CausalEngineæ¨¡å‹ï¼ˆæœ‰causal_encoderå±æ€§ï¼‰
                preds = model.predict(batch_x, temperature=temperature, do_sample=do_sample)
            else:
                # ä¼ ç»Ÿæ¨¡å‹æˆ–åŸºå‡†æ¨¡å‹ï¼ˆå¿½ç•¥temperatureå’Œdo_sampleå‚æ•°ï¼‰
                if hasattr(model, 'predict'):
                    preds = model.predict(batch_x)
                else:
                    preds = model(batch_x)
                    
            # ä¿®å¤ç»´åº¦ä¸åŒ¹é…ï¼šå¦‚æœè¾“å‡ºæ˜¯[batch_size, 1]ï¼Œflattenåˆ°[batch_size]
            if preds.dim() > 1 and preds.size(-1) == 1:
                preds = preds.squeeze(-1)
            
            all_preds.extend(preds.cpu().numpy().flatten())
            all_targets.extend(batch_y.cpu().numpy().flatten())
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_regression_metrics(
        y_true=np.array(all_targets),
        y_pred=np.array(all_preds)
    )
    
    print(f"     RÂ²: {metrics['r2']:.4f}")
    print(f"     RMSE: {metrics['rmse']:.4f}")
    print(f"     MAE: {metrics['mae']:.4f}")
    
    return metrics


def analyze_results(results, data_dict):
    """
    åˆ†æå’Œå¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š ç»“æœåˆ†æ")
    print("=" * 50)
    
    # 1. æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
    print("\nğŸ“‹ æ€§èƒ½å¯¹æ¯”è¡¨æ ¼:")
    print("   æ¨¡å‹                      | RÂ²        | MAE       | RMSE      | MAPE      | è®­ç»ƒæ—¶é—´")
    print("   ------------------------- | --------- | --------- | --------- | --------- | ---------")
    
    for model_name, metrics in results.items():
        display_name = {
            'baseline': 'Traditional NN',
            'ablated': 'CausalEngine(Ablated)',
            'causal': 'CausalEngine(Causal)',
            'standard': 'CausalEngine(Standard)',
            'sampling': 'CausalEngine(Sampling)'
        }.get(model_name, model_name)
        
        r2 = metrics.get('r2', 0)
        mae = metrics.get('mae', 0)
        rmse = metrics.get('rmse', 0)
        mape = metrics.get('mape', 0)
        time_val = metrics.get('training_time', 0)
        
        print(f"   {display_name:25} | {r2:.4f}    | {mae:.4f}    | {rmse:.4f}    | {mape:.2f}%   | {time_val:.1f}s")
    
    # 2. æ€§èƒ½æå‡åˆ†æ
    print("\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:")
    
    baseline_r2 = results['baseline']['r2']
    
    for model_name, metrics in results.items():
        if model_name != 'baseline':
            model_r2 = metrics['r2']
            improvement = ((model_r2 - baseline_r2) / abs(baseline_r2)) * 100 if baseline_r2 != 0 else 0
            
            display_name = {
                'ablated': 'CausalEngine(æ¶ˆè)',
                'å› æœ': 'CausalEngine(å› æœ)',
                'ä¸ç¡®å®šæ€§': 'CausalEngine(ä¸ç¡®å®šæ€§)',
                'é‡‡æ ·': 'CausalEngine(é‡‡æ ·)'
            }.get(model_name, model_name)
            
            print(f"   {display_name}: {improvement:+.2f}% ç›¸å¯¹äºåŸºå‡† (RÂ²)")
    
    # 3. æ¶ˆèå®éªŒéªŒè¯
    print("\nğŸ”¬ æ¶ˆèå®éªŒéªŒè¯:")
    
    baseline_r2 = results['baseline']['r2']
    ablated_r2 = results['ablated']['r2']
    
    r2_diff = abs(baseline_r2 - ablated_r2)
    print(f"   ä¼ ç»Ÿç¥ç»ç½‘ç»œRÂ²: {baseline_r2:.4f}")
    print(f"   CausalEngine(æ¶ˆè)RÂ²: {ablated_r2:.4f}")
    print(f"   å·®å¼‚: {r2_diff:.4f}")
    
    if r2_diff < 0.05:  # å·®å¼‚å°äº5%
        print("   âœ… æ¶ˆèå‡è®¾éªŒè¯æˆåŠŸï¼šä»…ä½¿ç”¨ä½ç½®è¾“å‡ºæ—¶æ€§èƒ½æ¥è¿‘ä¼ ç»Ÿç½‘ç»œ")
    else:
        print("   âš ï¸  æ¶ˆèå‡è®¾éœ€è¦è¿›ä¸€æ­¥éªŒè¯ï¼šå­˜åœ¨è¾ƒå¤§æ€§èƒ½å·®å¼‚")
    
    # 4. é¢„æµ‹è´¨é‡åˆ†æ
    analyze_prediction_quality(results, data_dict)
    
    # 5. å¯è§†åŒ–ç»“æœ
    visualize_results(results)
    
    # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    comparison = compare_model_performance(results, 'regression')
    report = generate_evaluation_report(
        results, 'regression', 'Bike Sharing',
        'tutorials/02_regression/bike_sharing_report.md'
    )
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: tutorials/02_regression/bike_sharing_report.md")


def analyze_prediction_quality(results, data_dict):
    """
    åˆ†æé¢„æµ‹è´¨é‡
    """
    print("\nğŸ¯ é¢„æµ‹è´¨é‡åˆ†æ:")
    
    # è·å–æœ€ä½³æ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
    best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])
    best_metrics = results[best_model_name]
    
    print(f"   æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {best_metrics['r2']:.4f})")
    
    # æ®‹å·®åˆ†æ
    print(f"   æ®‹å·®åˆ†æ:")
    print(f"     æ®‹å·®å‡å€¼: {best_metrics.get('residual_mean', 0):.4f}")
    print(f"     æ®‹å·®æ ‡å‡†å·®: {best_metrics.get('residual_std', 0):.4f}")
    print(f"     æ®‹å·®ååº¦: {best_metrics.get('residual_skewness', 0):.4f}")
    print(f"     æ®‹å·®å³°åº¦: {best_metrics.get('residual_kurtosis', 0):.4f}")
    
    # é¢„æµ‹åŒºé—´åˆ†æï¼ˆå¦‚æœæœ‰ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼‰
    if 'prediction_coverage_95' in best_metrics:
        print(f"   é¢„æµ‹åŒºé—´åˆ†æ:")
        print(f"     95%é¢„æµ‹åŒºé—´è¦†ç›–ç‡: {best_metrics['prediction_coverage_95']:.2%}")
        print(f"     å¹³å‡é¢„æµ‹åŒºé—´å®½åº¦: {best_metrics['mean_prediction_interval_width']:.4f}")
        print(f"     æ ¡å‡†è¯¯å·®: {best_metrics['calibration_error']:.4f}")


def visualize_results(results):
    """
    å¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # Setup plotting style
    plt.style.use('default')
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. RÂ² comparison
    models = list(results.keys())
    model_names = [
        {'baseline': 'Traditional NN', 'ablated': 'CausalEngine(Ablated)', 
         'å› æœ': 'CausalEngine(Causal)', 'ä¸ç¡®å®šæ€§': 'CausalEngine(Uncertainty)', 
         'é‡‡æ ·': 'CausalEngine(Sampling)'}.get(m, m) for m in models
    ]
    r2_scores = [results[m]['r2'] for m in models]
    
    bars = axes[0, 0].bar(model_names, r2_scores, color=['skyblue', 'lightgreen', 'gold', 'coral', 'plum'])
    axes[0, 0].set_ylabel('RÂ² Score')
    axes[0, 0].set_title('Bike Sharing Regression RÂ² Comparison')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # Add value labels
    for bar, r2 in zip(bars, r2_scores):
        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{r2:.4f}', ha='center', va='bottom')
    
    # 2. Multi-metric comparison
    metrics = ['mae', 'rmse', 'mape']
    metric_names = ['MAE', 'RMSE', 'MAPE(%)']
    
    x = np.arange(len(models))
    width = 0.25
    
    for i, metric in enumerate(metrics):
        values = [results[m].get(metric, 0) for m in models]
        # MAPE may need scaling for display
        if metric == 'mape':
            values = [v/10 for v in values]  # Scale for display
        
        axes[0, 1].bar(x + i*width, values, width, label=metric_names[i], alpha=0.8)
    
    axes[0, 1].set_xlabel('Model')
    axes[0, 1].set_ylabel('Metric Value')
    axes[0, 1].set_title('Multi-metric Performance Comparison')
    axes[0, 1].set_xticks(x + width)
    axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')
    axes[0, 1].legend()
    
    # 3. Training time comparison
    training_times = [results[m].get('training_time', 0) for m in models]
    
    bars = axes[1, 0].bar(model_names, training_times, color='lightsteelblue')
    axes[1, 0].set_ylabel('Training Time (seconds)')
    axes[1, 0].set_title('Training Time Comparison')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # Add value labels
    for bar, time_val in zip(bars, training_times):
        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                       f'{time_val:.1f}s', ha='center', va='bottom')
    
    # 4. Relative improvement percentage (based on RÂ²)
    baseline_r2 = results['baseline']['r2']
    improvements = []
    improved_models = []
    
    for model_name in models[1:]:  # Skip baseline
        model_r2 = results[model_name]['r2']
        improvement = ((model_r2 - baseline_r2) / abs(baseline_r2)) * 100 if baseline_r2 != 0 else 0
        improvements.append(improvement)
        improved_models.append(model_names[models.index(model_name)])
    
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    bars = axes[1, 1].bar(improved_models, improvements, color=colors, alpha=0.7)
    axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)
    axes[1, 1].set_ylabel('Relative Improvement (%)')
    axes[1, 1].set_title('RÂ² Improvement Relative to Baseline Model')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # Add value labels
    for bar, imp in zip(bars, improvements):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2, 
                       bar.get_height() + (0.5 if imp > 0 else -1.0),
                       f'{imp:+.2f}%', ha='center', va='bottom' if imp > 0 else 'top')
    
    plt.tight_layout()
    plt.savefig('tutorials/02_regression/bike_sharing_results.png', dpi=300, bbox_inches='tight')
    plt.close()  # Close instead of show to avoid blocking
    
    print("   å›¾è¡¨å·²ä¿å­˜: tutorials/02_regression/bike_sharing_results.png")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„Bike Sharingéœ€æ±‚é¢„æµ‹æ•™ç¨‹
    """
    print("ğŸš² Bike Sharing éœ€æ±‚é¢„æµ‹ - CausalEngineæ¶ˆèå®éªŒæ•™ç¨‹")
    print("æœ¬æ•™ç¨‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CausalEngineè¿›è¡Œå…±äº«å•è½¦éœ€æ±‚é¢„æµ‹ï¼Œå¹¶é€šè¿‡æ¶ˆèå®éªŒéªŒè¯å…¶ä¼˜åŠ¿")
    print("=" * 85)
    
    # 1. æ•°æ®æ¢ç´¢
    data_dict = explore_bike_sharing_dataset()
    
    # 2. è¿è¡Œæ¶ˆèå®éªŒ
    results = run_ablation_experiment(data_dict)
    
    # 3. åˆ†æç»“æœ
    analyze_results(results, data_dict)
    
    # 4. æ€»ç»“
    print("\nğŸ‰ Bike Sharing æ¶ˆèå®éªŒå®Œæˆï¼")
    print("\nğŸ” å…³é”®å‘ç°:")
    
    baseline_r2 = results['baseline']['r2']
    best_causal = max([
        results.get('å› æœ', {}).get('r2', 0),
        results.get('ä¸ç¡®å®šæ€§', {}).get('r2', 0),
        results.get('é‡‡æ ·', {}).get('r2', 0)
    ])
    
    if best_causal > baseline_r2:
        improvement = ((best_causal - baseline_r2) / abs(baseline_r2)) * 100 if baseline_r2 != 0 else 0
        print(f"   âœ… CausalEngineåœ¨Bike Sharingæ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³• ({improvement:.2f}%æå‡)")
    else:
        print(f"   ğŸ“Š ä¼ ç»Ÿæ–¹æ³•åœ¨æ­¤æ•°æ®é›†ä¸Šè¡¨ç°æ›´å¥½ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    
    print(f"\nğŸ§  å› æœæ¨ç†çš„ä¼˜åŠ¿:")
    print(f"   â€¢ æ¸©åº¦å‚æ•°æ§åˆ¶ï¼šå¯è°ƒèŠ‚ç¡®å®šæ€§vsä¸ç¡®å®šæ€§")
    print(f"   â€¢ å¤šæ¨¡å¼æ¨ç†ï¼šæ”¯æŒå› æœ/ä¸ç¡®å®šæ€§/é‡‡æ ·æ¨ç†")
    print(f"   â€¢ å¯è§£é‡Šæ€§ï¼šæ¯ä¸ªé¢„æµ‹å¯è¿½æº¯åˆ°ä¸ªä½“ç‰¹å¾U + å› æœæ³•åˆ™f")
    print(f"   â€¢ æ³›åŒ–èƒ½åŠ›ï¼šåŸºäºå› æœå…³ç³»è€Œéç»Ÿè®¡ç›¸å…³æ€§")
    
    print(f"\nğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print(f"   1. å°è¯•å…¶ä»–å›å½’æ•°æ®é›†ï¼štutorials/02_regression/")
    print(f"   2. äº†è§£åˆ†ç±»ä»»åŠ¡ï¼štutorials/01_classification/")
    print(f"   3. è¿è¡Œå®Œæ•´è¯„ä¼°ï¼štutorials/03_ablation_studies/comprehensive_comparison.py")
    print(f"   4. æ·±å…¥ç†è®ºå­¦ä¹ ï¼šcausal_engine/MATHEMATICAL_FOUNDATIONS.md")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
CausalEngine æˆ¿ä»·é¢„æµ‹æ•™ç¨‹

è¿™ä¸ªæ•™ç¨‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CausalEngine è¿›è¡Œå›å½’ä»»åŠ¡ï¼ˆæˆ¿ä»·é¢„æµ‹ï¼‰ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨ California Housing æ•°æ®é›†ï¼Œé‡ç‚¹å±•ç¤º CausalEngine åœ¨å›å½’ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚

é‡ç‚¹å±•ç¤ºï¼š
1. CausalEngine çš„å›å½’æ¿€æ´»å‡½æ•°
2. æŸ¯è¥¿åˆ†å¸ƒçš„å›å½’ä¼˜åŠ¿å’Œå¼‚å¸¸å€¼é²æ£’æ€§
3. é¢„æµ‹åŒºé—´å’Œä¸ç¡®å®šæ€§é‡åŒ–
4. ä¸ä¼ ç»Ÿå›å½’æ–¹æ³•çš„å¯¹æ¯”
5. ä¸‰ç§æ¨ç†æ¨¡å¼åœ¨å›å½’ä¸­çš„è¡¨ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_absolute_percentage_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ CausalEngine
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine import CausalEngine, ActivationMode


class HousingDataset(Dataset):
    """California Housing æ•°æ®é›†"""
    
    def __init__(self, features, targets):
        self.features = torch.FloatTensor(features)
        self.targets = torch.FloatTensor(targets)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


class CausalHousingRegressor(nn.Module):
    """åŸºäº CausalEngine çš„æˆ¿ä»·å›å½’å™¨"""
    
    def __init__(self, input_size, hidden_size=128):
        super().__init__()
        
        # ç‰¹å¾åµŒå…¥å±‚
        self.feature_embedding = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # CausalEngine æ ¸å¿ƒ (å›å½’æ¨¡å¼)
        self.causal_engine = CausalEngine(
            hidden_size=hidden_size,
            vocab_size=1,  # å•ä¸€å›å½’è¾“å‡º
            activation_modes="regression",  # å›å½’æ¿€æ´»
            b_noise_init=0.1,
            gamma_init=1.0,
            regression_scale_init=1.0,
            regression_bias_init=0.0
        )
    
    def forward(self, x, temperature=1.0, do_sample=False, return_details=False):
        # ç‰¹å¾åµŒå…¥
        hidden_states = self.feature_embedding(x)
        
        # CausalEngine æ¨ç†
        output = self.causal_engine(
            hidden_states.unsqueeze(1),  # æ·»åŠ åºåˆ—ç»´åº¦
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True
        )
        
        if return_details:
            return output
        else:
            return output['output'].squeeze()  # ç§»é™¤å¤šä½™ç»´åº¦


def load_and_prepare_data():
    """åŠ è½½å¹¶é¢„å¤„ç† California Housing æ•°æ®"""
    
    print("ğŸ  åŠ è½½ California Housing æ•°æ®é›†...")
    
    # åŠ è½½æ•°æ®
    housing = fetch_california_housing()
    X = housing.data
    y = housing.target
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    print(f"   ç›®æ ‡å˜é‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"   ç‰¹å¾åç§°: {housing.feature_names}")
    
    # æ˜¾ç¤ºæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
    print("\nğŸ“Š California Housing æ•°æ®é›†ä¿¡æ¯:")
    print("   è¿™æ˜¯ä¸€ä¸ªçœŸå®çš„åŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ŒåŒ…å«1990å¹´åŠ å·å„ä¸ªè¡—åŒºçš„æˆ¿ä»·ä¿¡æ¯")
    print("   æ•°æ®æ¥æº: 1990å¹´ç¾å›½äººå£æ™®æŸ¥")
    print(f"   æ ·æœ¬æ•°é‡: {X.shape[0]:,} ä¸ªè¡—åŒº")
    print("   ç‰¹å¾è¯´æ˜:")
    print("     - MedInc: è¡—åŒºä¸­ä½æ•°æ”¶å…¥ï¼ˆå•ä½ï¼šä¸‡ç¾å…ƒï¼‰")
    print("     - HouseAge: è¡—åŒºæˆ¿å±‹ä¸­ä½æ•°å¹´é¾„")
    print("     - AveRooms: æ¯æˆ·å¹³å‡æˆ¿é—´æ•°")
    print("     - AveBedrms: æ¯æˆ·å¹³å‡å§å®¤æ•°")
    print("     - Population: è¡—åŒºäººå£")
    print("     - AveOccup: æ¯æˆ·å¹³å‡å±…ä½äººæ•°")
    print("     - Latitude: çº¬åº¦")
    print("     - Longitude: ç»åº¦")
    print(f"   ç›®æ ‡å˜é‡: æˆ¿ä»·ä¸­ä½æ•°ï¼ˆå•ä½ï¼šåä¸‡ç¾å…ƒï¼ŒèŒƒå›´ {y.min():.2f} - {y.max():.2f}ï¼‰")
    
    # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
    print("\n   æ•°æ®æ ·ä¾‹ï¼ˆå‰5è¡Œï¼‰:")
    sample_df = pd.DataFrame(X[:5], columns=housing.feature_names)
    sample_df['Price'] = y[:5]
    print(sample_df.to_string(index=False))
    
    # æ•°æ®é¢„å¤„ç†
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()
    
    # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼æ¥æµ‹è¯•é²æ£’æ€§
    np.random.seed(42)
    outlier_idx = np.random.choice(len(y_scaled), size=int(0.05 * len(y_scaled)), replace=False)
    y_scaled[outlier_idx] += np.random.normal(0, 3, len(outlier_idx))  # æ·»åŠ 5%çš„å¼‚å¸¸å€¼
    
    print(f"   å·²æ ‡å‡†åŒ–ç‰¹å¾å’Œç›®æ ‡å˜é‡")
    print(f"   æ·»åŠ äº† {len(outlier_idx)} ä¸ªå¼‚å¸¸å€¼æ ·æœ¬ (5%)")
    
    return X_scaled, y_scaled, scaler_X, scaler_y, housing.feature_names


def cauchy_nll_loss(predictions, targets, scale=1.0):
    """Cauchy åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°
    
    è¿™æ˜¯ CausalEngine å›å½’çš„ç†è®ºæœ€ä¼˜æŸå¤±å‡½æ•°
    ç›¸æ¯”äº MSEï¼Œå¯¹å¼‚å¸¸å€¼æ›´åŠ é²æ£’
    """
    # Cauchy åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶: -log(f(x)) = log(Ï€) + log(Î³) + log(1 + ((x-Î¼)/Î³)Â²)
    diff = (targets - predictions) / scale
    nll = torch.log(torch.tensor(torch.pi)) + torch.log(torch.tensor(scale)) + torch.log(1 + diff**2)
    return nll.mean()


def train_causal_model(model, train_loader, val_loader, epochs=100):
    """è®­ç»ƒ CausalEngine å›å½’æ¨¡å‹"""
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ CausalEngine å›å½’æ¨¡å‹...")
    
    # ä½¿ç”¨ Cauchy NLL æŸå¤±ï¼ˆç†è®ºæœ€ä¼˜ï¼‰å’Œ MSE æŸå¤±çš„ç»„åˆ
    mse_criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.8)
    
    train_losses = []
    val_losses = []
    val_r2_scores = []
    
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 20
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_samples = 0
        
        for features, targets in train_loader:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            predictions = model(features, temperature=1.0, do_sample=False)
            
            # ç»„åˆæŸå¤±ï¼šMSE + Cauchy NLL
            mse_loss = mse_criterion(predictions, targets)
            cauchy_loss = cauchy_nll_loss(predictions, targets, scale=1.0)
            loss = 0.7 * mse_loss + 0.3 * cauchy_loss  # åŠ æƒç»„åˆ
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            
            train_loss += loss.item() * len(features)
            train_samples += len(features)
        
        avg_train_loss = train_loss / train_samples
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_samples = 0
        val_predictions = []
        val_targets = []
        
        with torch.no_grad():
            for features, targets in val_loader:
                predictions = model(features, temperature=0.0)  # çº¯å› æœæ¨¡å¼éªŒè¯
                
                loss = mse_criterion(predictions, targets)
                val_loss += loss.item() * len(features)
                val_samples += len(features)
                
                val_predictions.extend(predictions.numpy())
                val_targets.extend(targets.numpy())
        
        avg_val_loss = val_loss / val_samples
        val_r2 = r2_score(val_targets, val_predictions)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        val_r2_scores.append(val_r2)
        
        scheduler.step(avg_val_loss)
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), '/Users/gongqian/DailyLog/CausalQwen/tutorials/02_regression/best_model.pth')
        else:
            patience_counter += 1
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, Val RÂ²: {val_r2:.4f}")
        
        if patience_counter >= early_stop_patience:
            print(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½® (éªŒè¯æŸå¤±è¿ç»­ {early_stop_patience} è½®æœªæ”¹å–„)")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('/Users/gongqian/DailyLog/CausalQwen/tutorials/02_regression/best_model.pth'))
    print("âœ… è®­ç»ƒå®Œæˆï¼Œå·²åŠ è½½æœ€ä½³æ¨¡å‹!")
    
    return train_losses, val_losses, val_r2_scores


def calculate_regression_metrics(y_true, y_pred):
    """è®¡ç®—å…¨é¢çš„å›å½’è¯„ä¼°æŒ‡æ ‡"""
    metrics = {
        'r2': r2_score(y_true, y_pred),
        'mse': mean_squared_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mae': mean_absolute_error(y_true, y_pred),
        'mdae': median_absolute_error(y_true, y_pred),  # ä¸­ä½æ•°ç»å¯¹è¯¯å·®
        'mape': mean_absolute_percentage_error(y_true, y_pred) if not (y_true == 0).any() else np.nan,  # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
    }
    return metrics


def train_baseline_models(X_train, y_train, X_val, y_val):
    """è®­ç»ƒä¼ ç»Ÿå›å½’åŸºçº¿æ¨¡å‹ï¼ˆæ‰©å±•ç‰ˆï¼‰"""
    
    print("\nğŸ“Š è®­ç»ƒä¼ ç»Ÿå›å½’åŸºçº¿æ¨¡å‹...")
    
    baselines = {}
    
    # å®šä¹‰æ‰€æœ‰åŸºçº¿æ¨¡å‹
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0),
        'Lasso Regression': Lasso(alpha=0.01, max_iter=1000),
        'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=1000),
        'Huber Regression': HuberRegressor(epsilon=1.35, max_iter=100),  # å¯¹å¼‚å¸¸å€¼é²æ£’
        'SVR (RBF)': SVR(kernel='rbf', C=1.0, epsilon=0.1),
        'KNN Regression': KNeighborsRegressor(n_neighbors=10),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
    }
    
    print(f"\n   è®­ç»ƒ {len(models)} ä¸ªåŸºçº¿æ¨¡å‹...")
    
    for name, model in models.items():
        print(f"   æ­£åœ¨è®­ç»ƒ {name}...")
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        val_pred = model.predict(X_val)
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = calculate_regression_metrics(y_val, val_pred)
        
        baselines[name] = {
            'model': model,
            'predictions': val_pred,
            **metrics
        }
        
        print(f"     RÂ² = {metrics['r2']:.4f}, RMSE = {metrics['rmse']:.4f}, MAE = {metrics['mae']:.4f}, MdAE = {metrics['mdae']:.4f}")
    
    return baselines


def evaluate_inference_modes(model, test_loader, scaler_y):
    """è¯„ä¼° CausalEngine çš„ä¸‰ç§æ¨ç†æ¨¡å¼åœ¨å›å½’ä¸­çš„è¡¨ç°"""
    
    print("\nğŸ” è¯„ä¼°ä¸‰ç§æ¨ç†æ¨¡å¼çš„å›å½’è¡¨ç°...")
    
    model.eval()
    results = {}
    
    modes = [
        ("çº¯å› æœæ¨¡å¼", {"temperature": 0.0, "do_sample": False}),
        ("æ ‡å‡†æ¨¡å¼", {"temperature": 1.0, "do_sample": False}), 
        ("é‡‡æ ·æ¨¡å¼", {"temperature": 0.8, "do_sample": True})
    ]
    
    for mode_name, params in modes:
        predictions = []
        uncertainties = []
        true_values = []
        prediction_intervals = []
        
        with torch.no_grad():
            for features, targets in test_loader:
                # è·å–è¯¦ç»†è¾“å‡º
                output = model(features, return_details=True, **params)
                
                # æå–é¢„æµ‹å€¼ï¼ˆä½¿ç”¨ä½ç½®å‚æ•°ï¼‰
                pred_scaled = output['output'].squeeze()
                predictions.extend(pred_scaled.numpy())
                
                # ä¸ç¡®å®šæ€§ï¼ˆå°ºåº¦å‚æ•°ï¼‰
                scale_S = output['scale_S'].squeeze()
                uncertainties.extend(scale_S.numpy())
                
                # çœŸå®å€¼
                true_values.extend(targets.numpy())
                
                # è®¡ç®—é¢„æµ‹åŒºé—´ï¼ˆåŸºäºæŸ¯è¥¿åˆ†å¸ƒçš„åˆ†ä½æ•°ï¼‰
                # 90% é¢„æµ‹åŒºé—´ï¼šP(-6.314 < Z < 6.314) â‰ˆ 0.9ï¼Œå…¶ä¸­ Z = (X-Î¼)/Î³
                # å› æ­¤é¢„æµ‹åŒºé—´ä¸º [Î¼ - 6.314*Î³, Î¼ + 6.314*Î³]
                interval_width = 6.314 * scale_S  # 90% é¢„æµ‹åŒºé—´
                lower_bound = pred_scaled - interval_width
                upper_bound = pred_scaled + interval_width
                prediction_intervals.extend(list(zip(lower_bound.numpy(), upper_bound.numpy())))
        
        # è½¬æ¢å›åŸå§‹å°ºåº¦
        predictions = np.array(predictions)
        true_values = np.array(true_values)
        uncertainties = np.array(uncertainties)
        
        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        metrics = calculate_regression_metrics(true_values, predictions)
        
        # é¢„æµ‹åŒºé—´è¦†ç›–ç‡
        prediction_intervals = np.array(prediction_intervals)
        coverage = np.mean((true_values >= prediction_intervals[:, 0]) & 
                          (true_values <= prediction_intervals[:, 1]))
        
        results[mode_name] = {
            **metrics,
            'avg_uncertainty': np.mean(uncertainties),
            'coverage': coverage,
            'predictions': predictions,
            'true_values': true_values,
            'uncertainties': uncertainties,
            'prediction_intervals': prediction_intervals
        }
        
        print(f"   {mode_name}:")
        print(f"     RÂ² = {metrics['r2']:.4f}, RMSE = {metrics['rmse']:.4f}, MAE = {metrics['mae']:.4f}, MdAE = {metrics['mdae']:.4f}")
        print(f"     å¹³å‡ä¸ç¡®å®šæ€§ = {np.mean(uncertainties):.3f}, 90%åŒºé—´è¦†ç›–ç‡ = {coverage:.3f}")
    
    return results


def analyze_robustness(model, test_loader, outlier_strength=3.0):
    """åˆ†æ CausalEngine å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§"""
    
    print(f"\nğŸ›¡ï¸ åˆ†æå¼‚å¸¸å€¼é²æ£’æ€§ (å¼‚å¸¸å€¼å¼ºåº¦: {outlier_strength})...")
    
    model.eval()
    
    with torch.no_grad():
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•æ•°æ®
        all_features = []
        all_targets = []
        for features, targets in test_loader:
            all_features.append(features)
            all_targets.append(targets)
        
        all_features = torch.cat(all_features, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        
        # åŸå§‹é¢„æµ‹
        original_predictions = model(all_features, temperature=0.0)
        original_r2 = r2_score(all_targets.numpy(), original_predictions.numpy())
        
        # æ·»åŠ å¼‚å¸¸å€¼
        n_outliers = int(0.1 * len(all_targets))  # 10% å¼‚å¸¸å€¼
        outlier_idx = torch.randperm(len(all_targets))[:n_outliers]
        
        corrupted_targets = all_targets.clone()
        corrupted_targets[outlier_idx] += torch.randn(n_outliers) * outlier_strength
        
        # å¼‚å¸¸å€¼å­˜åœ¨æ—¶çš„é¢„æµ‹ï¼ˆæ¨¡æ‹Ÿåœ¨çº¿åœºæ™¯ï¼‰
        corrupted_predictions = model(all_features, temperature=0.0)
        corrupted_r2 = r2_score(corrupted_targets.numpy(), corrupted_predictions.numpy())
        
        # è®¡ç®—é²æ£’æ€§æŒ‡æ ‡
        robustness_score = corrupted_r2 / original_r2  # æ¯”å€¼è¶Šæ¥è¿‘1è¶Šé²æ£’
        
        print(f"   åŸå§‹ RÂ²: {original_r2:.4f}")
        print(f"   å¼‚å¸¸å€¼æ±¡æŸ“å RÂ²: {corrupted_r2:.4f}")
        print(f"   é²æ£’æ€§å¾—åˆ†: {robustness_score:.4f} (1.0ä¸ºå®Œå…¨é²æ£’)")
        
        return {
            'original_r2': original_r2,
            'corrupted_r2': corrupted_r2,
            'robustness_score': robustness_score,
            'n_outliers': n_outliers
        }


def visualize_regression_results(train_losses, val_losses, val_r2_scores, 
                               inference_results, baseline_results):
    """Visualize regression analysis results"""
    
    print("\nğŸ“ˆ Generating regression analysis visualization...")
    
    # Set matplotlib to use English and avoid font issues
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 12))
    
    # 1. Training curves
    axes[0, 0].plot(train_losses, label='Training Loss', color='blue', alpha=0.7)
    axes[0, 0].plot(val_losses, label='Validation Loss', color='red', alpha=0.7)
    axes[0, 0].set_title('CausalEngine Training Curves')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # 2. RÂ² curves
    axes[0, 1].plot(val_r2_scores, label='Validation RÂ²', color='green')
    axes[0, 1].set_title('CausalEngine RÂ² Score')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('RÂ² Score')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # 3. Inference mode comparison - RÂ²
    # Map Chinese mode names to English
    mode_mapping = {
        'çº¯å› æœæ¨¡å¼': 'Pure Causal',
        'æ ‡å‡†æ¨¡å¼': 'Standard',
        'é‡‡æ ·æ¨¡å¼': 'Sampling'
    }
    
    modes_en = [mode_mapping.get(mode, mode) for mode in inference_results.keys()]
    r2_scores = [inference_results[mode]['r2'] for mode in inference_results.keys()]
    
    bars = axes[0, 2].bar(modes_en, r2_scores, alpha=0.7, color=['blue', 'orange', 'green'])
    axes[0, 2].set_title('RÂ² Scores across Inference Modes')
    axes[0, 2].set_ylabel('RÂ² Score')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    for bar, r2 in zip(bars, r2_scores):
        axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{r2:.3f}', ha='center', va='bottom')
    
    # 4. Inference mode comparison - Uncertainty
    uncertainties = [inference_results[mode]['avg_uncertainty'] for mode in inference_results.keys()]
    
    axes[0, 3].bar(modes_en, uncertainties, alpha=0.7, color=['red', 'purple', 'cyan'])
    axes[0, 3].set_title('Average Uncertainty across Inference Modes')
    axes[0, 3].set_ylabel('Uncertainty')
    axes[0, 3].tick_params(axis='x', rotation=45)
    
    # 5. Comparison with traditional methods - Enhanced version
    # Select top methods for better visualization
    top_methods = ['Linear Regression', 'Ridge Regression', 'Huber Regression', 
                   'Random Forest', 'Gradient Boosting', 'CausalEngine (Pure Causal)']
    
    selected_methods = []
    selected_r2_scores = []
    selected_mae_scores = []
    selected_mdae_scores = []
    
    for method in top_methods[:-1]:  # All except CausalEngine
        if method in baseline_results:
            selected_methods.append(method)
            selected_r2_scores.append(baseline_results[method]['r2'])
            selected_mae_scores.append(baseline_results[method]['mae'])
            selected_mdae_scores.append(baseline_results[method]['mdae'])
    
    # Add CausalEngine
    selected_methods.append('CausalEngine (Pure Causal)')
    selected_r2_scores.append(inference_results['çº¯å› æœæ¨¡å¼']['r2'])
    selected_mae_scores.append(inference_results['çº¯å› æœæ¨¡å¼']['mae'])
    selected_mdae_scores.append(inference_results['çº¯å› æœæ¨¡å¼']['mdae'])
    
    # Create subplot for multiple metrics comparison
    x = np.arange(len(selected_methods))
    width = 0.25
    
    # Normalize scores for better visualization
    max_r2 = max(selected_r2_scores)
    max_mae = max(selected_mae_scores)
    max_mdae = max(selected_mdae_scores)
    
    norm_r2 = [r2/max_r2 for r2 in selected_r2_scores]
    norm_mae = [1 - mae/max_mae for mae in selected_mae_scores]  # Invert for "higher is better"
    norm_mdae = [1 - mdae/max_mdae for mdae in selected_mdae_scores]  # Invert for "higher is better"
    
    colors = ['gray'] * (len(selected_methods) - 1) + ['red']
    
    bars1 = axes[1, 0].bar(x - width, norm_r2, width, label='RÂ² (normalized)', alpha=0.8, color=colors)
    bars2 = axes[1, 0].bar(x, norm_mae, width, label='MAE (inverted & normalized)', alpha=0.8, color=colors)
    bars3 = axes[1, 0].bar(x + width, norm_mdae, width, label='MdAE (inverted & normalized)', alpha=0.8, color=colors)
    
    axes[1, 0].set_title('Multi-Metric Comparison (Normalized)')
    axes[1, 0].set_ylabel('Normalized Score (Higher = Better)')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(selected_methods, rotation=45, ha='right')
    axes[1, 0].legend()
    axes[1, 0].grid(True, axis='y')
    
    # 6. Predictions vs true values scatter plot
    mode_data = inference_results['çº¯å› æœæ¨¡å¼']
    predictions = mode_data['predictions']
    true_values = mode_data['true_values']
    
    axes[1, 1].scatter(true_values, predictions, alpha=0.6, s=20)
    min_val = min(true_values.min(), predictions.min())
    max_val = max(true_values.max(), predictions.max())
    axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
    axes[1, 1].set_xlabel('True Values')
    axes[1, 1].set_ylabel('Predicted Values')
    axes[1, 1].set_title('Predictions vs True Values (Pure Causal Mode)')
    axes[1, 1].grid(True)
    
    # 7. Prediction interval visualization
    sample_size = min(100, len(true_values))
    sample_idx = np.random.choice(len(true_values), sample_size, replace=False)
    
    sample_true = true_values[sample_idx]
    sample_pred = predictions[sample_idx]
    sample_intervals = mode_data['prediction_intervals'][sample_idx]
    
    sorted_idx = np.argsort(sample_true)
    sample_true_sorted = sample_true[sorted_idx]
    sample_pred_sorted = sample_pred[sorted_idx]
    sample_intervals_sorted = sample_intervals[sorted_idx]
    
    x_range = np.arange(len(sample_true_sorted))
    axes[1, 2].fill_between(x_range, 
                           sample_intervals_sorted[:, 0], 
                           sample_intervals_sorted[:, 1],
                           alpha=0.3, color='blue', label='90% Prediction Interval')
    axes[1, 2].plot(x_range, sample_true_sorted, 'go', markersize=4, label='True Values')
    axes[1, 2].plot(x_range, sample_pred_sorted, 'ro', markersize=4, label='Predicted Values')
    axes[1, 2].set_title('Prediction Interval Visualization (Sample)')
    axes[1, 2].set_xlabel('Sample Index (sorted by true values)')
    axes[1, 2].set_ylabel('Values')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    
    # 8. Uncertainty vs error relationship
    uncertainties = mode_data['uncertainties']
    errors = np.abs(predictions - true_values)
    
    axes[1, 3].scatter(uncertainties, errors, alpha=0.6, s=20)
    axes[1, 3].set_xlabel('Uncertainty')
    axes[1, 3].set_ylabel('Absolute Error')
    axes[1, 3].set_title('Uncertainty vs Prediction Error')
    axes[1, 3].grid(True)
    
    # Add trend line
    z = np.polyfit(uncertainties, errors, 1)
    p = np.poly1d(z)
    axes[1, 3].plot(uncertainties, p(uncertainties), "r--", alpha=0.8)
    
    plt.tight_layout()
    plt.savefig('/Users/gongqian/DailyLog/CausalQwen/tutorials/02_regression/house_price_results.png', 
                dpi=300, bbox_inches='tight')
    
    # Save and close without blocking
    plt.close()  # Close the figure to free memory and avoid blocking
    
    print("âœ… Visualization completed, image saved to house_price_results.png")


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„æˆ¿ä»·é¢„æµ‹æ•™ç¨‹"""
    
    print("ğŸ  CausalEngine æˆ¿ä»·é¢„æµ‹æ•™ç¨‹")
    print("=" * 50)
    
    # 1. æ•°æ®å‡†å¤‡
    X, y, scaler_X, scaler_y, feature_names = load_and_prepare_data()
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = HousingDataset(X_train, y_train)
    val_dataset = HousingDataset(X_val, y_val)
    test_dataset = HousingDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    # 3. åˆ›å»ºå¹¶è®­ç»ƒ CausalEngine æ¨¡å‹
    model = CausalHousingRegressor(input_size=X_train.shape[1])
    print(f"\nğŸ—ï¸ CausalEngine å›å½’æ¨¡å‹æ¶æ„:")
    print(f"   è¾“å…¥ç»´åº¦: {X_train.shape[1]} ({', '.join(feature_names)})")
    print(f"   éšè—ç»´åº¦: 128")
    print(f"   è¾“å‡ºç»´åº¦: 1 (æˆ¿ä»·å›å½’)")
    print(f"   æ¿€æ´»æ¨¡å¼: å›å½’")
    print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    train_losses, val_losses, val_r2_scores = train_causal_model(
        model, train_loader, val_loader, epochs=100
    )
    
    # 4. è®­ç»ƒä¼ ç»ŸåŸºçº¿æ¨¡å‹
    baseline_results = train_baseline_models(X_train, y_train, X_val, y_val)
    
    # 5. è¯„ä¼°ä¸‰ç§æ¨ç†æ¨¡å¼
    inference_results = evaluate_inference_modes(model, test_loader, scaler_y)
    
    # 6. é²æ£’æ€§åˆ†æ
    robustness_analysis = analyze_robustness(model, test_loader)
    
    # 7. å¯è§†åŒ–ç»“æœ
    visualize_regression_results(train_losses, val_losses, val_r2_scores,
                               inference_results, baseline_results)
    
    # 8. æ€»ç»“æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“‹ å›å½’ä»»åŠ¡æ€»ç»“æŠ¥å‘Š")
    print("="*50)
    
    best_causal_mode = max(inference_results.keys(), 
                          key=lambda x: inference_results[x]['r2'])
    best_causal_r2 = inference_results[best_causal_mode]['r2']
    
    best_baseline = max(baseline_results.keys(),
                       key=lambda x: baseline_results[x]['r2'])
    best_baseline_r2 = baseline_results[best_baseline]['r2']
    
    print(f"ğŸ† æœ€ä½³ CausalEngine æ¨¡å¼: {best_causal_mode}")
    print(f"   RÂ² å¾—åˆ†: {best_causal_r2:.4f}")
    print(f"   RMSE: {inference_results[best_causal_mode]['rmse']:.4f}")
    print(f"   MAE: {inference_results[best_causal_mode]['mae']:.4f}")
    print(f"   MdAE: {inference_results[best_causal_mode]['mdae']:.4f}")
    print(f"   å¹³å‡ä¸ç¡®å®šæ€§: {inference_results[best_causal_mode]['avg_uncertainty']:.3f}")
    print(f"   90% é¢„æµ‹åŒºé—´è¦†ç›–ç‡: {inference_results[best_causal_mode]['coverage']:.3f}")
    
    print(f"\nğŸ¥ˆ æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_baseline}")
    print(f"   RÂ² å¾—åˆ†: {best_baseline_r2:.4f}")
    print(f"   RMSE: {baseline_results[best_baseline]['rmse']:.4f}")
    print(f"   MAE: {baseline_results[best_baseline]['mae']:.4f}")
    print(f"   MdAE: {baseline_results[best_baseline]['mdae']:.4f}")
    
    improvement = (best_causal_r2 - best_baseline_r2) * 100
    print(f"\nğŸ“ˆ CausalEngine ä¼˜åŠ¿:")
    print(f"   RÂ² å¾—åˆ†æå‡: {improvement:+.1f}%")
    print(f"   å¼‚å¸¸å€¼é²æ£’æ€§å¾—åˆ†: {robustness_analysis['robustness_score']:.3f}")
    print(f"   é¢„æµ‹åŒºé—´é‡åŒ–: âœ… æ”¯æŒ")
    print(f"   æ¨ç†æ¨¡å¼: âœ… 3ç§æ¨¡å¼å¯é€‰")
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print(f"   â€¢ æŸ¯è¥¿åˆ†å¸ƒæŸå¤±å‡½æ•°æä¾›æ›´å¥½çš„å¼‚å¸¸å€¼é²æ£’æ€§")
    print(f"   â€¢ é¢„æµ‹åŒºé—´è¦†ç›–ç‡æ¥è¿‘ç†è®ºå€¼ (90%)")
    print(f"   â€¢ ä¸ç¡®å®šæ€§ä¸é¢„æµ‹è¯¯å·®å‘ˆæ­£ç›¸å…³")
    print(f"   â€¢ çº¯å› æœæ¨¡å¼æä¾›æœ€ç¨³å®šçš„å›å½’é¢„æµ‹")
    print(f"   â€¢ æ ‡å‡†æ¨¡å¼åœ¨ç²¾åº¦å’Œä¸ç¡®å®šæ€§ä¹‹é—´å¹³è¡¡")
    
    print("\nâœ… å›å½’æ•™ç¨‹å®Œæˆ! æ£€æŸ¥ç”Ÿæˆçš„å¯è§†åŒ–å›¾ç‰‡äº†è§£æ›´å¤šç»†èŠ‚ã€‚")


if __name__ == "__main__":
    main()
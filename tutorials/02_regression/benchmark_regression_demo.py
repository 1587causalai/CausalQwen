"""
CausalEngine å›å½’ä»»åŠ¡åŸºå‡†æµ‹è¯•æ¼”ç¤º (2024æ›´æ–°ç‰ˆ)
============================================

åŸºäºå®˜æ–¹åŸºå‡†æµ‹è¯•åè®® (causal_engine/misc/benchmark_strategy.md) çš„å®Œæ•´å›å½’ä»»åŠ¡æ¼”ç¤º
åŒ…å«å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°çš„å¯¹æ¯”å®éªŒå’Œå››ç§æ¨ç†æ¨¡å¼çš„è¯„ä¼°

å®éªŒè®¾è®¡:
- å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ (b_noise âˆˆ [0.0, 0.1, 1.0, 10.0])
- å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ (b_noiseå¯å­¦ä¹ )
- å››ç§æ¨ç†æ¨¡å¼: å› æœã€æ ‡å‡†ã€é‡‡æ ·ã€å…¼å®¹
- æ ‡å‡†åŒ–è¶…å‚æ•°: AdamW, lr=1e-4, early stopping

æ•°æ®é›†: California Housing (æˆ¿ä»·é¢„æµ‹)
"""

import sys
import os
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from typing import Dict, List, Tuple

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from causal_engine import CausalEngine


def load_california_housing_dataset():
    """
    åŠ è½½å’Œé¢„å¤„ç†California Housingæ•°æ®é›†
    """
    print("ğŸ“Š åŠ è½½California Housingæ•°æ®é›†")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®é›†
    data = fetch_california_housing()
    X, y = data.data, data.target
    feature_names = data.feature_names
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç›®æ ‡å˜é‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"ç‰¹å¾åˆ—è¡¨: {list(feature_names)}")
    
    # ç‰¹å¾å·¥ç¨‹: æ·»åŠ ä¸€äº›äº¤äº’ç‰¹å¾
    # æ³¨æ„ï¼šç»åº¦å€¼ä¸ºè´Ÿæ•°ï¼Œéœ€è¦å…ˆå–ç»å¯¹å€¼å†åº”ç”¨logå˜æ¢
    X_engineered = np.column_stack([
        X,
        X[:, 0] * X[:, 1],  # MedInc * HouseAge
        X[:, 2] / X[:, 3],  # AveRooms / AveBedrms (rooms per bedroom)
        X[:, 4] / X[:, 5],  # Population / AveOccup (people per household)
        np.log1p(X[:, 6]),  # log(Latitude + 1) - çº¬åº¦ä¸ºæ­£æ•°ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
        np.log1p(np.abs(X[:, 7]))   # log(|Longitude| + 1) - ç»åº¦ä¸ºè´Ÿæ•°ï¼Œéœ€è¦å–ç»å¯¹å€¼
    ])
    
    engineered_feature_names = list(feature_names) + [
        'MedInc_x_HouseAge', 'Rooms_per_Bedroom', 'People_per_Household', 
        'log_Latitude', 'log_Longitude'
    ]
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X_engineered)
    
    # æ ‡å‡†åŒ–ç›®æ ‡å˜é‡ (ç”¨äºè®­ç»ƒï¼Œä¾¿äºæ”¶æ•›)
    scaler_y = StandardScaler()
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_temp, y_train, y_temp = train_test_split(
        X_scaled, y_scaled, test_size=0.4, random_state=42
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42
    )
    
    print(f"å·¥ç¨‹ç‰¹å¾æ•°: {X_engineered.shape[1]}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"éªŒè¯é›†å¤§å°: {X_val.shape[0]}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    return {
        'X_train': X_train.astype(np.float32),
        'X_val': X_val.astype(np.float32),
        'X_test': X_test.astype(np.float32),
        'y_train': y_train.astype(np.float32),
        'y_val': y_val.astype(np.float32),
        'y_test': y_test.astype(np.float32),
        'feature_names': engineered_feature_names,
        'input_size': X_scaled.shape[1],
        'scaler_y': scaler_y  # ç”¨äºåæ ‡å‡†åŒ–
    }


def create_data_loaders(data_dict, batch_size=64):
    """
    åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨
    """
    train_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_train']),
        torch.FloatTensor(data_dict['y_train'])
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_val']),
        torch.FloatTensor(data_dict['y_val'])
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_test']),
        torch.FloatTensor(data_dict['y_test'])
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, test_loader


class TraditionalMLPRegressor(nn.Module):
    """
    ä¼ ç»ŸMLPå›å½’å™¨ (åŸºå‡†å¯¹æ¯”)
    """
    def __init__(self, input_size, hidden_sizes=[128, 64]):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, 1))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x).squeeze(-1)


class CausalEngineRegressor(nn.Module):
    """
    åŸºäºCausalEngineçš„å›å½’å™¨
    """
    def __init__(self, input_size, causal_size=None, b_noise_learnable=True):
        super().__init__()
        
        if causal_size is None:
            causal_size = input_size
        
        self.causal_engine = CausalEngine(
            hidden_size=input_size,
            vocab_size=1,  # å›å½’ä»»åŠ¡è¾“å‡ºç»´åº¦ä¸º1
            causal_size=causal_size,
            activation_modes="regression"
        )
        
        # æ§åˆ¶å™ªå£°å‚æ•°æ˜¯å¦å¯å­¦ä¹ 
        if hasattr(self.causal_engine.action, 'b_noise'):
            self.causal_engine.action.b_noise.requires_grad = b_noise_learnable
    
    def forward(self, x, temperature=1.0, do_sample=False):
        # æ·»åŠ åºåˆ—ç»´åº¦
        if x.dim() == 2:
            x = x.unsqueeze(1)  # (batch, seq=1, features)
        
        output = self.causal_engine(
            hidden_states=x,
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True,
            apply_activation=True
        )
        
        return output['output'].squeeze()  # ç§»é™¤é¢å¤–ç»´åº¦
    
    def set_fixed_noise(self, noise_value):
        """è®¾ç½®å›ºå®šçš„å™ªå£°å€¼"""
        if hasattr(self.causal_engine.action, 'b_noise'):
            self.causal_engine.action.b_noise.requires_grad = False
            self.causal_engine.action.b_noise.data.fill_(noise_value)


def train_model(model, train_loader, val_loader, device, num_epochs=100, learning_rate=1e-4):
    """
    è®­ç»ƒæ¨¡å‹ (åŸºäºåŸºå‡†åè®®çš„æ ‡å‡†åŒ–é…ç½®)
    """
    model = model.to(device)
    
    # åŸºå‡†åè®®æ ‡å‡†åŒ–é…ç½®
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦: å‰10%çš„stepsçº¿æ€§warm-upï¼Œç„¶åcosine decay
    total_steps = len(train_loader) * num_epochs
    warmup_steps = int(0.1 * total_steps)
    
    def lr_schedule(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1 + np.cos(np.pi * progress))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)
    
    # å¯¹äºCausalEngineå›å½’å™¨ï¼Œä½¿ç”¨Cauchy NLLæŸå¤±ï¼›å¯¹äºä¼ ç»Ÿæ¨¡å‹ï¼Œä½¿ç”¨MSE
    if hasattr(model, 'causal_engine'):
        def cauchy_nll_loss(pred_loc, pred_scale, target):
            # Cauchyåˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
            # NLL = log(Ï€ * Î³) + log(1 + ((x - Î¼) / Î³)Â²)
            # æ³¨æ„: predåŒ…å«ä½ç½®å’Œå°ºåº¦å‚æ•°
            diff = (target - pred_loc) / (pred_scale + 1e-8)
            return torch.mean(torch.log(np.pi * pred_scale + 1e-8) + torch.log(1 + diff**2))
        
        def loss_fn(pred, target):
            # å‡è®¾predæ˜¯ä¸€ä¸ªå€¼ï¼Œæˆ‘ä»¬éœ€è¦ä»æ¨¡å‹è·å–åˆ†å¸ƒå‚æ•°
            return nn.MSELoss()(pred, target)  # ç®€åŒ–å¤„ç†
    else:
        loss_fn = nn.MSELoss()
    
    # æ—©åœé…ç½®
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    
    print(f"è®­ç»ƒé…ç½®: epochs={num_epochs}, lr={learning_rate}, weight_decay=0.01")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = loss_fn(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            epoch_train_loss += loss.item()
        
        avg_train_loss = epoch_train_loss / len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0.0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = loss_fn(outputs, batch_y)
                epoch_val_loss += loss.item()
        
        avg_val_loss = epoch_val_loss / len(val_loader)
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"æ—©åœäºç¬¬ {epoch+1} è½®")
            break
    
    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss
    }


def evaluate_model(model, test_loader, device, temperature=1.0, do_sample=False):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            
            if hasattr(model, 'causal_engine'):
                # CausalEngineæ¨¡å‹
                outputs = model(batch_x, temperature=temperature, do_sample=do_sample)
            else:
                # ä¼ ç»Ÿæ¨¡å‹
                outputs = model(batch_x)
            
            all_predictions.extend(outputs.cpu().numpy())
            all_targets.extend(batch_y.numpy())
    
    predictions = np.array(all_predictions)
    targets = np.array(all_targets)
    
    # è®¡ç®—å›å½’æŒ‡æ ‡
    mae = mean_absolute_error(targets, predictions)
    mse = mean_squared_error(targets, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets, predictions)
    
    # ä¸­ä½æ•°ç»å¯¹è¯¯å·®
    mdae = np.median(np.abs(targets - predictions))
    
    return {
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'mdae': mdae,
        'predictions': predictions,
        'targets': targets
    }


def run_fixed_noise_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ
    """
    print("\nğŸ§ª å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ")
    print("=" * 60)
    
    noise_values = [0.0, 0.1, 1.0, 10.0]  # åŸºå‡†åè®®æ›´æ–°ï¼šæ›´æœ‰åŒºåˆ†åº¦çš„å…³é”®å€¼
    results = {}
    
    for noise in noise_values:
        print(f"\n  æµ‹è¯•å›ºå®šå™ªå£°: b_noise = {noise}")
        
        # åˆ›å»ºæ¨¡å‹
        model = CausalEngineRegressor(
            input_size=data_dict['input_size'],
            b_noise_learnable=False
        )
        model.set_fixed_noise(noise)
        
        # è®­ç»ƒæ¨¡å‹
        train_history = train_model(model, train_loader, val_loader, device)
        
        # è¯„ä¼°æ¨¡å‹ (å››ç§æ¨ç†æ¨¡å¼)
        mode_results = {}
        for mode, (temp, do_sample) in [
            ('causal', (0, False)),
            ('standard', (1.0, False)),
            ('sampling', (0.8, True)),
            ('compatible', (1.0, False))
        ]:
            eval_result = evaluate_model(model, test_loader, device, temp, do_sample)
            mode_results[mode] = eval_result
            print(f"    {mode}æ¨¡å¼: RÂ²={eval_result['r2']:.4f}, MAE={eval_result['mae']:.4f}")
        
        results[noise] = {
            'train_history': train_history,
            'mode_results': mode_results
        }
    
    return results


def run_adaptive_noise_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ
    """
    print("\nğŸ§ª å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹ (å™ªå£°å¯å­¦ä¹ )
    model = CausalEngineRegressor(
        input_size=data_dict['input_size'],
        b_noise_learnable=True
    )
    
    # åˆå§‹åŒ–å™ªå£°ä¸º0.1
    if hasattr(model.causal_engine.action, 'b_noise'):
        model.causal_engine.action.b_noise.data.fill_(0.1)
    
    print("  åˆå§‹å™ªå£°å€¼: 0.1")
    print("  å™ªå£°å­¦ä¹ : å¯ç”¨ (requires_grad=True)")
    
    # è®­ç»ƒæ¨¡å‹
    train_history = train_model(model, train_loader, val_loader, device)
    
    # è·å–å­¦åˆ°çš„å™ªå£°å€¼ (b_noise æ˜¯å‘é‡ï¼Œæ˜¾ç¤ºå…¶å‡å€¼å’Œæ ‡å‡†å·®)
    learned_noise = None
    if hasattr(model.causal_engine.action, 'b_noise'):
        b_noise_tensor = model.causal_engine.action.b_noise
        learned_noise = {
            'mean': b_noise_tensor.mean().item(),
            'std': b_noise_tensor.std().item(),
            'min': b_noise_tensor.min().item(),
            'max': b_noise_tensor.max().item()
        }
        print(f"  å­¦åˆ°çš„å™ªå£°å€¼ç»Ÿè®¡:")
        print(f"    å‡å€¼: {learned_noise['mean']:.4f}")
        print(f"    æ ‡å‡†å·®: {learned_noise['std']:.4f}")
        print(f"    èŒƒå›´: [{learned_noise['min']:.4f}, {learned_noise['max']:.4f}]")
    
    # è¯„ä¼°æ¨¡å‹ (å››ç§æ¨ç†æ¨¡å¼)
    mode_results = {}
    for mode, (temp, do_sample) in [
        ('causal', (0, False)),
        ('standard', (1.0, False)),
        ('sampling', (0.8, True)),
        ('compatible', (1.0, False))
    ]:
        eval_result = evaluate_model(model, test_loader, device, temp, do_sample)
        mode_results[mode] = eval_result
        print(f"    {mode}æ¨¡å¼: RÂ²={eval_result['r2']:.4f}, MAE={eval_result['mae']:.4f}")
    
    return {
        'train_history': train_history,
        'mode_results': mode_results,
        'learned_noise': learned_noise
    }


def run_baseline_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    ä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†å®éªŒ
    """
    print("\nğŸ—ï¸ ä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºä¼ ç»ŸMLP
    model = TraditionalMLPRegressor(
        input_size=data_dict['input_size'],
        hidden_sizes=[128, 64]  # ä¸CausalEngineç›¸å½“çš„å‚æ•°é‡
    )
    
    print("  æ¨¡å‹ç±»å‹: ä¼ ç»ŸMLP")
    print(f"  éšè—å±‚: {[128, 64]}")
    
    # è®­ç»ƒæ¨¡å‹
    train_history = train_model(model, train_loader, val_loader, device)
    
    # è¯„ä¼°æ¨¡å‹
    eval_result = evaluate_model(model, test_loader, device)
    print(f"    RÂ²: {eval_result['r2']:.4f}")
    print(f"    MAE: {eval_result['mae']:.4f}")
    print(f"    RMSE: {eval_result['rmse']:.4f}")
    
    return {
        'train_history': train_history,
        'eval_result': eval_result
    }


def analyze_results(fixed_noise_results, adaptive_noise_result, baseline_result):
    """
    åˆ†æå’Œå¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š å®éªŒç»“æœåˆ†æ")
    print("=" * 60)
    
    # 1. å›ºå®šå™ªå£°å®éªŒåˆ†æ
    print("\n  1. å›ºå®šå™ªå£°å¼ºåº¦å¯¹æ€§èƒ½çš„å½±å“:")
    noise_performance = {}
    for noise, result in fixed_noise_results.items():
        best_r2 = max([mode_result['r2'] 
                      for mode_result in result['mode_results'].values()])
        noise_performance[noise] = best_r2
        print(f"     b_noise={noise}: æœ€ä½³RÂ²={best_r2:.4f}")
    
    # æ‰¾åˆ°æœ€ä¼˜å™ªå£°
    best_noise = max(noise_performance, key=noise_performance.get)
    best_fixed_r2 = noise_performance[best_noise]
    print(f"     æœ€ä¼˜å›ºå®šå™ªå£°: {best_noise} (RÂ²: {best_fixed_r2:.4f})")
    
    # 2. è‡ªé€‚åº”å™ªå£°åˆ†æ
    adaptive_best_r2 = max([mode_result['r2'] 
                           for mode_result in adaptive_noise_result['mode_results'].values()])
    learned_noise = adaptive_noise_result['learned_noise']
    print(f"\n  2. è‡ªé€‚åº”å™ªå£°å­¦ä¹ :")
    print(f"     å­¦åˆ°çš„å™ªå£°å€¼ç»Ÿè®¡:")
    print(f"       å‡å€¼: {learned_noise['mean']:.4f}")
    print(f"       æ ‡å‡†å·®: {learned_noise['std']:.4f}")
    print(f"       èŒƒå›´: [{learned_noise['min']:.4f}, {learned_noise['max']:.4f}]")
    print(f"     æœ€ä½³RÂ²: {adaptive_best_r2:.4f}")
    
    # 3. åŸºå‡†å¯¹æ¯”
    baseline_r2 = baseline_result['eval_result']['r2']
    print(f"\n  3. ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”:")
    print(f"     ä¼ ç»ŸMLP RÂ²: {baseline_r2:.4f}")
    print(f"     æœ€ä½³å›ºå®šå™ªå£°æå‡: {best_fixed_r2 - baseline_r2:.4f}")
    print(f"     è‡ªé€‚åº”å™ªå£°æå‡: {adaptive_best_r2 - baseline_r2:.4f}")
    
    # 4. æ¨ç†æ¨¡å¼å¯¹æ¯”
    print(f"\n  4. æ¨ç†æ¨¡å¼æ€§èƒ½å¯¹æ¯” (æœ€ä½³é…ç½®):")
    best_config = fixed_noise_results[best_noise]['mode_results']
    for mode, result in best_config.items():
        print(f"     {mode}æ¨¡å¼: RÂ²={result['r2']:.4f}, MAE={result['mae']:.4f}")


def visualize_results(fixed_noise_results, adaptive_noise_result, baseline_result, data_dict):
    """
    å¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š ç”Ÿæˆç»“æœå¯è§†åŒ–å›¾è¡¨")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('CausalEngine Regression Benchmark Results', fontsize=16, fontweight='bold')
    
    # 1. å™ªå£°å¼ºåº¦vsæ€§èƒ½æ›²çº¿
    noise_values = list(fixed_noise_results.keys())
    r2_values = [max([mode_result['r2'] 
                     for mode_result in result['mode_results'].values()])
                for result in fixed_noise_results.values()]
    
    ax1.plot(noise_values, r2_values, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('Noise Strength (b_noise)')
    ax1.set_ylabel('Best RÂ²')
    ax1.set_title('Fixed Noise Experiment: Performance vs Noise')
    ax1.grid(True, alpha=0.3)
    ax1.set_xscale('log')
    
    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    best_idx = np.argmax(r2_values)
    ax1.annotate(f'Optimal: {noise_values[best_idx]}', 
                xy=(noise_values[best_idx], r2_values[best_idx]),
                xytext=(noise_values[best_idx]*2, r2_values[best_idx]+0.01),
                arrowprops=dict(arrowstyle='->', color='red'))
    
    # 2. æ¨ç†æ¨¡å¼å¯¹æ¯”
    best_noise = noise_values[best_idx]
    mode_results = fixed_noise_results[best_noise]['mode_results']
    modes = list(mode_results.keys())
    mode_r2s = [mode_results[mode]['r2'] for mode in modes]
    mode_maes = [mode_results[mode]['mae'] for mode in modes]
    
    x = np.arange(len(modes))
    width = 0.35
    
    ax2_twin = ax2.twinx()
    bars1 = ax2.bar(x - width/2, mode_r2s, width, label='RÂ²', alpha=0.8, color='blue')
    bars2 = ax2_twin.bar(x + width/2, mode_maes, width, label='MAE', alpha=0.8, color='red')
    
    ax2.set_xlabel('Inference Mode')
    ax2.set_ylabel('RÂ²', color='blue')
    ax2_twin.set_ylabel('MAE', color='red')
    ax2.set_title('Inference Modes Comparison')
    ax2.set_xticks(x)
    ax2.set_xticklabels(modes)
    
    # åˆå¹¶å›¾ä¾‹
    lines1, labels1 = ax2.get_legend_handles_labels()
    lines2, labels2 = ax2_twin.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
    
    ax2.grid(True, alpha=0.3)
    
    # 3. æ€»ä½“æ–¹æ³•å¯¹æ¯”
    methods = ['Traditional MLP', 'CausalEngine\n(Best Fixed)', 'CausalEngine\n(Adaptive)']
    method_r2s = [
        baseline_result['eval_result']['r2'],
        max([mode_results[mode]['r2'] for mode in mode_results]),
        max([adaptive_noise_result['mode_results'][mode]['r2'] 
             for mode in adaptive_noise_result['mode_results']])
    ]
    
    colors_methods = ['lightcoral', 'lightblue', 'lightgreen']
    bars_methods = ax3.bar(methods, method_r2s, color=colors_methods, alpha=0.8)
    ax3.set_ylabel('RÂ²')
    ax3.set_title('Overall Method Comparison')
    ax3.grid(True, alpha=0.3, axis='y')
    
    for bar, r2 in zip(bars_methods, method_r2s):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{r2:.3f}', ha='center', va='bottom')
    
    # 4. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾ (æœ€ä½³é…ç½®)
    best_mode_result = mode_results['standard']  # ä½¿ç”¨æ ‡å‡†æ¨¡å¼
    predictions = best_mode_result['predictions']
    targets = best_mode_result['targets']
    
    ax4.scatter(targets, predictions, alpha=0.6, s=20)
    
    # æ·»åŠ ç†æƒ³çº¿ (y=x)
    min_val = min(targets.min(), predictions.min())
    max_val = max(targets.max(), predictions.max())
    ax4.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
    
    ax4.set_xlabel('True Values')
    ax4.set_ylabel('Predicted Values')
    ax4.set_title(f'Predictions vs True (RÂ²={best_mode_result["r2"]:.3f})')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('tutorials/02_regression/benchmark_regression_results.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ç»“æœå›¾è¡¨å·²ä¿å­˜: tutorials/02_regression/benchmark_regression_results.png")


def main():
    """
    ä¸»å‡½æ•°: å®Œæ•´çš„åŸºå‡†æµ‹è¯•æµç¨‹
    """
    print("ğŸŒŸ CausalEngine å›å½’ä»»åŠ¡åŸºå‡†æµ‹è¯•æ¼”ç¤º")
    print("åŸºäºå®˜æ–¹åŸºå‡†æµ‹è¯•åè®® (benchmark_strategy.md)")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.manual_seed(42)
    np.random.seed(42)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"éšæœºç§å­: 42 (ç¡®ä¿å¯å¤ç°æ€§)")
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½å’Œå‡†å¤‡")
    data_dict = load_california_housing_dataset()
    train_loader, val_loader, test_loader = create_data_loaders(data_dict, batch_size=64)
    
    # 2. è¿è¡ŒåŸºå‡†å®éªŒ
    print("\nğŸ—ï¸ æ­¥éª¤2: ä¼ ç»ŸåŸºå‡†å®éªŒ")
    baseline_result = run_baseline_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 3. è¿è¡Œå›ºå®šå™ªå£°å®éªŒ
    print("\nğŸ§ª æ­¥éª¤3: å›ºå®šå™ªå£°å®éªŒç»„A")
    fixed_noise_results = run_fixed_noise_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 4. è¿è¡Œè‡ªé€‚åº”å™ªå£°å®éªŒ
    print("\nğŸ§ª æ­¥éª¤4: è‡ªé€‚åº”å™ªå£°å®éªŒç»„B")
    adaptive_noise_result = run_adaptive_noise_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 5. åˆ†æç»“æœ
    print("\nğŸ“Š æ­¥éª¤5: ç»“æœåˆ†æ")
    analyze_results(fixed_noise_results, adaptive_noise_result, baseline_result)
    
    # 6. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“Š æ­¥éª¤6: ç»“æœå¯è§†åŒ–")
    visualize_results(fixed_noise_results, adaptive_noise_result, baseline_result, data_dict)
    
    # 7. æ€»ç»“
    print("\nğŸ‰ å›å½’åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print("ğŸ”¬ å…³é”®å‘ç°:")
    print("  âœ… éªŒè¯äº†å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°åœ¨å›å½’ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§")
    print("  âœ… é‡åŒ–äº†å››ç§æ¨ç†æ¨¡å¼åœ¨è¿ç»­é¢„æµ‹ä¸­çš„æ€§èƒ½å·®å¼‚")
    print("  âœ… è¯æ˜äº†CausalEngineåœ¨å›å½’ä»»åŠ¡ä¸­ç›¸å¯¹ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿")
    print("  âœ… éµå¾ªäº†å®˜æ–¹åŸºå‡†æµ‹è¯•åè®®çš„æ ‡å‡†é…ç½®")
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("  1. æ¶ˆèç ”ç©¶: tutorials/03_ablation_studies/")
    print("  2. é«˜çº§ä¸»é¢˜: tutorials/04_advanced_topics/")
    print("  3. åˆ†ç±»å¯¹æ¯”: tutorials/01_classification/benchmark_classification_demo.py")
    print("  4. ç†è®ºåŸºç¡€: causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md")


if __name__ == "__main__":
    main()
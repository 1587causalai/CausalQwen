#!/usr/bin/env python3
"""
CausalEngine å¤šä»»åŠ¡å­¦ä¹ æ•™ç¨‹ï¼šç”µå•†è¯„è®ºç»¼åˆåˆ†æ

è¿™ä¸ªæ•™ç¨‹å±•ç¤º CausalEngine çš„æ ¸å¿ƒä¼˜åŠ¿ä¹‹ä¸€ï¼šåœ¨å•ä¸ªæ¨¡å‹ä¸­åŒæ—¶å¤„ç†å¤šç§ç±»å‹çš„è¾“å‡ºä»»åŠ¡ã€‚
æˆ‘ä»¬å°†åˆ†æç”µå•†è¯„è®ºï¼ŒåŒæ—¶é¢„æµ‹ï¼š
1. æƒ…æ„Ÿåˆ†ç±» (äºŒåˆ†ç±»)  
2. è¯„åˆ†ç­‰çº§ (æœ‰åºåˆ†ç±»)
3. æœ‰ç”¨æ€§å¾—åˆ† (å›å½’)

é‡ç‚¹å±•ç¤ºï¼š
1. æ··åˆæ¿€æ´»æ¨¡å¼çš„é…ç½®å’Œä½¿ç”¨
2. å…±äº«å› æœè¡¨å¾çš„ä¼˜åŠ¿
3. å¤šä»»åŠ¡é—´çš„ååŒå­¦ä¹ æ•ˆåº”
4. ç»Ÿä¸€ä¸ç¡®å®šæ€§é‡åŒ–
5. ä»»åŠ¡æƒé‡å¹³è¡¡ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, mean_squared_error, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ CausalEngine
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine import CausalEngine, ActivationMode


class EcommerceDataset(Dataset):
    """ç”µå•†è¯„è®ºå¤šä»»åŠ¡æ•°æ®é›†"""
    
    def __init__(self, features, sentiment, rating, helpfulness):
        self.features = torch.FloatTensor(features)
        self.sentiment = torch.LongTensor(sentiment)  # äºŒåˆ†ç±»ï¼š0=è´Ÿé¢, 1=æ­£é¢
        self.rating = torch.LongTensor(rating)        # æœ‰åºåˆ†ç±»ï¼š0-4 (1-5æ˜Ÿ)
        self.helpfulness = torch.FloatTensor(helpfulness)  # å›å½’ï¼šæœ‰ç”¨æ€§å¾—åˆ†
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return (self.features[idx], 
                self.sentiment[idx], 
                self.rating[idx], 
                self.helpfulness[idx])


class CausalEcommerceAnalyzer(nn.Module):
    """åŸºäº CausalEngine çš„ç”µå•†è¯„è®ºå¤šä»»åŠ¡åˆ†æå™¨"""
    
    def __init__(self, input_size, hidden_size=128):
        super().__init__()
        
        # ç‰¹å¾åµŒå…¥å±‚
        self.feature_embedding = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(), 
            nn.Dropout(0.2)
        )
        
        # CausalEngine æ ¸å¿ƒ - æ··åˆæ¿€æ´»æ¨¡å¼
        # è¾“å‡ºç»´åº¦å®‰æ’ï¼š
        # [0]: æƒ…æ„Ÿåˆ†ç±» (classification)
        # [1]: è¯„åˆ†ç­‰çº§ (ordinal) 
        # [2]: æœ‰ç”¨æ€§å¾—åˆ† (regression)
        activation_modes = ["classification", "ordinal", "regression"]
        
        self.causal_engine = CausalEngine(
            hidden_size=hidden_size,
            vocab_size=3,  # ä¸‰ä¸ªè¾“å‡ºç»´åº¦
            activation_modes=activation_modes,
            # åˆ†ç±»å‚æ•°
            classification_threshold_init=0.0,
            # æœ‰åºåˆ†ç±»å‚æ•°  
            ordinal_num_classes=5,  # 5çº§è¯„åˆ†
            ordinal_threshold_init=1.0,
            # å›å½’å‚æ•°
            regression_scale_init=1.0,
            regression_bias_init=0.0,
            # å™ªå£°å‚æ•°
            b_noise_init=0.1,
            gamma_init=1.0
        )
    
    def forward(self, x, temperature=1.0, do_sample=False, return_details=False):
        # ç‰¹å¾åµŒå…¥
        hidden_states = self.feature_embedding(x)
        
        # CausalEngine å¤šä»»åŠ¡æ¨ç†
        output = self.causal_engine(
            hidden_states.unsqueeze(1),  # æ·»åŠ åºåˆ—ç»´åº¦
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True
        )
        
        if return_details:
            return output
        else:
            # è§£ææ··åˆè¾“å‡º
            mixed_output = output['output'].squeeze()
            sentiment_probs = mixed_output[:, 0]  # æƒ…æ„Ÿæ¦‚ç‡ (åˆ†ç±»)
            rating_classes = mixed_output[:, 1]   # è¯„åˆ†ç±»åˆ« (æœ‰åº)
            helpfulness_scores = mixed_output[:, 2]  # æœ‰ç”¨æ€§å¾—åˆ† (å›å½’)
            
            return sentiment_probs, rating_classes, helpfulness_scores


def create_ecommerce_data(n_samples=3000, n_features=60):
    """åˆ›å»ºæ¨¡æ‹Ÿç”µå•†è¯„è®ºæ•°æ®"""
    
    print("ğŸ›’ åˆ›å»ºæ¨¡æ‹Ÿç”µå•†è¯„è®ºæ•°æ®...")
    
    np.random.seed(42)
    
    # ç‰¹å¾è®¾è®¡ï¼šæ¨¡æ‹Ÿè¯„è®ºçš„å„ç§å±æ€§
    # 0-19: äº§å“ç‰¹å¾ (è´¨é‡ã€ä»·æ ¼ã€åŠŸèƒ½ç­‰)
    # 20-39: è¯„è®ºç‰¹å¾ (é•¿åº¦ã€è¯¦ç»†ç¨‹åº¦ã€å›¾ç‰‡æ•°ç­‰)
    # 40-59: ç”¨æˆ·ç‰¹å¾ (è´­ä¹°å†å²ã€å¯ä¿¡åº¦ç­‰)
    
    features = np.random.randn(n_samples, n_features)
    
    # è®¾è®¡ç”Ÿæˆè§„å¾‹ï¼šä¸‰ä¸ªä»»åŠ¡ç›¸äº’å…³è”ä½†ä¸å®Œå…¨é‡å¤
    
    # æ ¸å¿ƒè´¨é‡å¾—åˆ†ï¼ˆå½±å“æ‰€æœ‰ä»»åŠ¡ï¼‰
    product_quality = (
        features[:, 0:10].mean(axis=1) +    # äº§å“åŸºç¡€è´¨é‡
        0.5 * features[:, 10:20].mean(axis=1)  # äº§å“é™„åŠ ç‰¹æ€§
    )
    
    # è¯„è®ºè´¨é‡å¾—åˆ†ï¼ˆä¸»è¦å½±å“æœ‰ç”¨æ€§ï¼‰
    review_quality = (
        features[:, 20:30].mean(axis=1) +   # è¯„è®ºè¯¦ç»†ç¨‹åº¦
        0.3 * features[:, 30:40].mean(axis=1)  # è¯„è®ºè¡¨è¾¾è´¨é‡
    )
    
    # ç”¨æˆ·å¯ä¿¡åº¦ï¼ˆå½±å“æ‰€æœ‰ä»»åŠ¡çš„æƒé‡ï¼‰
    user_credibility = features[:, 40:50].mean(axis=1)
    
    # 1. æƒ…æ„Ÿåˆ†ç±» (ä¸»è¦ç”±äº§å“è´¨é‡å†³å®š)
    sentiment_score = product_quality + 0.3 * user_credibility + np.random.normal(0, 0.5, n_samples)
    sentiment = (sentiment_score > 0).astype(int)
    
    # 2. è¯„åˆ†ç­‰çº§ (äº§å“è´¨é‡ + ä¸ªäººåå¥½)
    personal_bias = features[:, 50:60].mean(axis=1)  # ä¸ªäººè¯„åˆ†åå¥½
    rating_score = product_quality + 0.4 * personal_bias + 0.2 * user_credibility
    rating_score += np.random.normal(0, 0.6, n_samples)
    
    # è½¬æ¢ä¸º0-4çš„æœ‰åºç±»åˆ«
    rating_percentiles = np.percentile(rating_score, [20, 40, 60, 80])
    rating = np.zeros(n_samples, dtype=int)
    rating[rating_score <= rating_percentiles[0]] = 0  # 1æ˜Ÿ
    rating[(rating_score > rating_percentiles[0]) & (rating_score <= rating_percentiles[1])] = 1  # 2æ˜Ÿ
    rating[(rating_score > rating_percentiles[1]) & (rating_score <= rating_percentiles[2])] = 2  # 3æ˜Ÿ
    rating[(rating_score > rating_percentiles[2]) & (rating_score <= rating_percentiles[3])] = 3  # 4æ˜Ÿ
    rating[rating_score > rating_percentiles[3]] = 4  # 5æ˜Ÿ
    
    # 3. æœ‰ç”¨æ€§å¾—åˆ† (è¯„è®ºè´¨é‡ + äº§å“è´¨é‡ + ç”¨æˆ·å¯ä¿¡åº¦)
    helpfulness_raw = (
        0.5 * review_quality +      # è¯„è®ºæœ¬èº«çš„è´¨é‡æœ€é‡è¦
        0.3 * product_quality +     # äº§å“å¥½è¯„è®ºç›¸å¯¹æ›´æœ‰ç”¨
        0.2 * user_credibility      # å¯ä¿¡ç”¨æˆ·çš„è¯„è®ºæ›´æœ‰ç”¨
    )
    helpfulness_raw += np.random.normal(0, 0.4, n_samples)
    
    # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
    helpfulness = (helpfulness_raw - helpfulness_raw.min()) / (helpfulness_raw.max() - helpfulness_raw.min())
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    features = scaler.fit_transform(features)
    
    print(f"âœ… æ•°æ®åˆ›å»ºå®Œæˆ: {features.shape[0]} æ ·æœ¬, {features.shape[1]} ç‰¹å¾")
    print(f"   æƒ…æ„Ÿåˆ†å¸ƒ: è´Ÿé¢={sum(sentiment==0)}, æ­£é¢={sum(sentiment==1)}")
    print(f"   è¯„åˆ†åˆ†å¸ƒ: {np.bincount(rating)} (0=1æ˜Ÿ, 1=2æ˜Ÿ, 2=3æ˜Ÿ, 3=4æ˜Ÿ, 4=5æ˜Ÿ)")
    print(f"   æœ‰ç”¨æ€§èŒƒå›´: [{helpfulness.min():.3f}, {helpfulness.max():.3f}]")
    
    return features, sentiment, rating, helpfulness, scaler


def multitask_loss_function(sentiment_pred, rating_pred, helpfulness_pred, 
                           sentiment_true, rating_true, helpfulness_true,
                           task_weights=None):
    """å¤šä»»åŠ¡æŸå¤±å‡½æ•°
    
    ç»„åˆä¸‰ä¸ªä»»åŠ¡çš„æŸå¤±ï¼Œæ”¯æŒåŠ¨æ€æƒé‡è°ƒæ•´
    """
    if task_weights is None:
        task_weights = [1.0, 1.0, 1.0]  # é»˜è®¤ç­‰æƒé‡
    
    # 1. æƒ…æ„Ÿåˆ†ç±»æŸå¤± (BCE)
    sentiment_loss = nn.BCELoss()(sentiment_pred, sentiment_true.float())
    
    # 2. è¯„åˆ†ç­‰çº§æŸå¤± (äº¤å‰ç†µï¼Œè€ƒè™‘æœ‰åºæ€§)
    rating_loss = nn.CrossEntropyLoss()(
        torch.eye(5)[rating_pred.long()], 
        rating_true.float()
    )
    
    # æœ‰åºåˆ†ç±»çš„é¢å¤–æƒ©ç½šï¼šè·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§
    rating_distance_penalty = torch.abs(rating_pred.float() - rating_true.float()).mean()
    rating_loss = rating_loss + 0.1 * rating_distance_penalty
    
    # 3. æœ‰ç”¨æ€§å›å½’æŸå¤± (MSE)
    helpfulness_loss = nn.MSELoss()(helpfulness_pred, helpfulness_true)
    
    # åŠ æƒç»„åˆ
    total_loss = (task_weights[0] * sentiment_loss + 
                  task_weights[1] * rating_loss + 
                  task_weights[2] * helpfulness_loss)
    
    return total_loss, sentiment_loss, rating_loss, helpfulness_loss


def train_multitask_model(model, train_loader, val_loader, epochs=100):
    """è®­ç»ƒå¤šä»»åŠ¡ CausalEngine æ¨¡å‹"""
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒå¤šä»»åŠ¡ CausalEngine æ¨¡å‹...")
    
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.8)
    
    # åŠ¨æ€ä»»åŠ¡æƒé‡ (å¼€å§‹æ—¶ç›¸ç­‰ï¼Œåç»­æ ¹æ®æŸå¤±è°ƒæ•´)
    task_weights = [1.0, 1.0, 1.0]
    
    train_losses = {'total': [], 'sentiment': [], 'rating': [], 'helpfulness': []}
    val_metrics = {'sentiment_acc': [], 'rating_acc': [], 'helpfulness_mse': []}
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_losses = {'total': 0, 'sentiment': 0, 'rating': 0, 'helpfulness': 0}
        train_samples = 0
        
        for features, sentiment_true, rating_true, helpfulness_true in train_loader:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            sentiment_pred, rating_pred, helpfulness_pred = model(
                features, temperature=1.0, do_sample=False
            )
            
            # è®¡ç®—æŸå¤±
            total_loss, sentiment_loss, rating_loss, helpfulness_loss = multitask_loss_function(
                sentiment_pred, rating_pred, helpfulness_pred,
                sentiment_true, rating_true, helpfulness_true,
                task_weights
            )
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            batch_size = len(features)
            epoch_losses['total'] += total_loss.item() * batch_size
            epoch_losses['sentiment'] += sentiment_loss.item() * batch_size
            epoch_losses['rating'] += rating_loss.item() * batch_size
            epoch_losses['helpfulness'] += helpfulness_loss.item() * batch_size
            train_samples += batch_size
        
        # å¹³å‡è®­ç»ƒæŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= train_samples
            train_losses[key].append(epoch_losses[key])
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_total_loss = 0
        val_samples = 0
        
        # æ”¶é›†é¢„æµ‹ç»“æœ
        sentiment_preds, sentiment_trues = [], []
        rating_preds, rating_trues = [], []
        helpfulness_preds, helpfulness_trues = [], []
        
        with torch.no_grad():
            for features, sentiment_true, rating_true, helpfulness_true in val_loader:
                sentiment_pred, rating_pred, helpfulness_pred = model(
                    features, temperature=0.0  # çº¯å› æœæ¨¡å¼éªŒè¯
                )
                
                # è®¡ç®—éªŒè¯æŸå¤±
                total_loss, _, _, _ = multitask_loss_function(
                    sentiment_pred, rating_pred, helpfulness_pred,
                    sentiment_true, rating_true, helpfulness_true,
                    task_weights
                )
                
                val_total_loss += total_loss.item() * len(features)
                val_samples += len(features)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                sentiment_preds.extend((sentiment_pred > 0.5).long().numpy())
                sentiment_trues.extend(sentiment_true.numpy())
                rating_preds.extend(rating_pred.long().numpy())
                rating_trues.extend(rating_true.numpy())
                helpfulness_preds.extend(helpfulness_pred.numpy())
                helpfulness_trues.extend(helpfulness_true.numpy())
        
        avg_val_loss = val_total_loss / val_samples
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        sentiment_acc = accuracy_score(sentiment_trues, sentiment_preds)
        rating_acc = accuracy_score(rating_trues, rating_preds)
        helpfulness_mse = mean_squared_error(helpfulness_trues, helpfulness_preds)
        
        val_metrics['sentiment_acc'].append(sentiment_acc)
        val_metrics['rating_acc'].append(rating_acc)
        val_metrics['helpfulness_mse'].append(helpfulness_mse)
        
        scheduler.step(avg_val_loss)
        
        # åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ (æ ¹æ®ä»»åŠ¡æŸå¤±å¹³è¡¡)
        if epoch > 10:  # å‰10è½®ä½¿ç”¨å›ºå®šæƒé‡
            current_losses = [epoch_losses['sentiment'], epoch_losses['rating'], epoch_losses['helpfulness']]
            # æŸå¤±å¤§çš„ä»»åŠ¡æƒé‡ç¨å¾®å¢åŠ 
            loss_ratios = np.array(current_losses) / np.mean(current_losses)
            task_weights = [min(2.0, max(0.5, w * r)) for w, r in zip(task_weights, loss_ratios)]
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), '/Users/gongqian/DailyLog/CausalQwen/tutorials/04_multitask/best_multitask_model.pth')
        else:
            patience_counter += 1
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch [{epoch+1}/{epochs}]:")
            print(f"  è®­ç»ƒæŸå¤±: æ€»ä½“={epoch_losses['total']:.4f}, " +
                  f"æƒ…æ„Ÿ={epoch_losses['sentiment']:.4f}, " +
                  f"è¯„åˆ†={epoch_losses['rating']:.4f}, " +
                  f"æœ‰ç”¨æ€§={epoch_losses['helpfulness']:.4f}")
            print(f"  éªŒè¯æŒ‡æ ‡: æƒ…æ„Ÿå‡†ç¡®ç‡={sentiment_acc:.4f}, " +
                  f"è¯„åˆ†å‡†ç¡®ç‡={rating_acc:.4f}, " +
                  f"æœ‰ç”¨æ€§MSE={helpfulness_mse:.4f}")
            print(f"  ä»»åŠ¡æƒé‡: {[f'{w:.2f}' for w in task_weights]}")
        
        if patience_counter >= 20:
            print(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('/Users/gongqian/DailyLog/CausalQwen/tutorials/04_multitask/best_multitask_model.pth'))
    print("âœ… è®­ç»ƒå®Œæˆï¼Œå·²åŠ è½½æœ€ä½³æ¨¡å‹!")
    
    return train_losses, val_metrics


def train_single_task_baselines(X_train, sentiment_train, rating_train, helpfulness_train,
                               X_val, sentiment_val, rating_val, helpfulness_val):
    """è®­ç»ƒå•ä»»åŠ¡åŸºçº¿æ¨¡å‹å¯¹æ¯”"""
    
    print("\nğŸ“Š è®­ç»ƒå•ä»»åŠ¡åŸºçº¿æ¨¡å‹...")
    
    from sklearn.linear_model import LogisticRegression, Ridge
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    
    baselines = {}
    
    # 1. æƒ…æ„Ÿåˆ†ç±»åŸºçº¿
    print("   è®­ç»ƒæƒ…æ„Ÿåˆ†ç±»åŸºçº¿...")
    lr_sentiment = LogisticRegression(random_state=42, max_iter=1000)
    lr_sentiment.fit(X_train, sentiment_train)
    sentiment_pred = lr_sentiment.predict(X_val)
    sentiment_acc = accuracy_score(sentiment_val, sentiment_pred)
    baselines['sentiment'] = {'accuracy': sentiment_acc, 'model': lr_sentiment}
    
    # 2. è¯„åˆ†é¢„æµ‹åŸºçº¿
    print("   è®­ç»ƒè¯„åˆ†é¢„æµ‹åŸºçº¿...")
    rf_rating = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_rating.fit(X_train, rating_train)
    rating_pred = rf_rating.predict(X_val)
    rating_acc = accuracy_score(rating_val, rating_pred)
    baselines['rating'] = {'accuracy': rating_acc, 'model': rf_rating}
    
    # 3. æœ‰ç”¨æ€§å›å½’åŸºçº¿
    print("   è®­ç»ƒæœ‰ç”¨æ€§å›å½’åŸºçº¿...")
    ridge_helpfulness = Ridge(alpha=1.0)
    ridge_helpfulness.fit(X_train, helpfulness_train)
    helpfulness_pred = ridge_helpfulness.predict(X_val)
    helpfulness_mse = mean_squared_error(helpfulness_val, helpfulness_pred)
    baselines['helpfulness'] = {'mse': helpfulness_mse, 'model': ridge_helpfulness}
    
    print(f"   å•ä»»åŠ¡åŸºçº¿ç»“æœ:")
    print(f"     æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡: {sentiment_acc:.4f}")
    print(f"     è¯„åˆ†é¢„æµ‹å‡†ç¡®ç‡: {rating_acc:.4f}")
    print(f"     æœ‰ç”¨æ€§å›å½’MSE: {helpfulness_mse:.4f}")
    
    return baselines


def evaluate_multitask_inference_modes(model, test_loader):
    """è¯„ä¼°å¤šä»»åŠ¡æ¨¡å‹çš„ä¸‰ç§æ¨ç†æ¨¡å¼"""
    
    print("\nğŸ” è¯„ä¼°å¤šä»»åŠ¡æ¨¡å‹çš„ä¸‰ç§æ¨ç†æ¨¡å¼...")
    
    model.eval()
    results = {}
    
    modes = [
        ("çº¯å› æœæ¨¡å¼", {"temperature": 0.0, "do_sample": False}),
        ("æ ‡å‡†æ¨¡å¼", {"temperature": 1.0, "do_sample": False}), 
        ("é‡‡æ ·æ¨¡å¼", {"temperature": 0.8, "do_sample": True})
    ]
    
    for mode_name, params in modes:
        sentiment_preds, sentiment_trues = [], []
        rating_preds, rating_trues = [], []
        helpfulness_preds, helpfulness_trues = [], []
        uncertainties = {'sentiment': [], 'rating': [], 'helpfulness': []}
        
        with torch.no_grad():
            for features, sentiment_true, rating_true, helpfulness_true in test_loader:
                # è·å–è¯¦ç»†è¾“å‡º
                output = model(features, return_details=True, **params)
                
                # è§£ææ··åˆè¾“å‡º
                mixed_output = output['output'].squeeze()
                sentiment_pred = mixed_output[:, 0]  # æƒ…æ„Ÿæ¦‚ç‡
                rating_pred = mixed_output[:, 1]     # è¯„åˆ†ç±»åˆ«
                helpfulness_pred = mixed_output[:, 2] # æœ‰ç”¨æ€§å¾—åˆ†
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                sentiment_preds.extend((sentiment_pred > 0.5).long().numpy())
                sentiment_trues.extend(sentiment_true.numpy())
                rating_preds.extend(rating_pred.long().numpy())
                rating_trues.extend(rating_true.numpy())
                helpfulness_preds.extend(helpfulness_pred.numpy())
                helpfulness_trues.extend(helpfulness_true.numpy())
                
                # ä¸ç¡®å®šæ€§
                scale_S = output['scale_S'].squeeze()
                uncertainties['sentiment'].extend(scale_S[:, 0].numpy())
                uncertainties['rating'].extend(scale_S[:, 1].numpy())
                uncertainties['helpfulness'].extend(scale_S[:, 2].numpy())
        
        # è®¡ç®—å„ä»»åŠ¡æŒ‡æ ‡
        sentiment_acc = accuracy_score(sentiment_trues, sentiment_preds)
        rating_acc = accuracy_score(rating_trues, rating_preds)
        helpfulness_mse = mean_squared_error(helpfulness_trues, helpfulness_preds)
        
        results[mode_name] = {
            'sentiment_accuracy': sentiment_acc,
            'rating_accuracy': rating_acc,
            'helpfulness_mse': helpfulness_mse,
            'uncertainties': uncertainties,
            'predictions': {
                'sentiment': np.array(sentiment_preds),
                'rating': np.array(rating_preds),
                'helpfulness': np.array(helpfulness_preds)
            },
            'true_values': {
                'sentiment': np.array(sentiment_trues),
                'rating': np.array(rating_trues),
                'helpfulness': np.array(helpfulness_trues)
            }
        }
        
        print(f"   {mode_name}:")
        print(f"     æƒ…æ„Ÿå‡†ç¡®ç‡: {sentiment_acc:.4f}")
        print(f"     è¯„åˆ†å‡†ç¡®ç‡: {rating_acc:.4f}")
        print(f"     æœ‰ç”¨æ€§MSE: {helpfulness_mse:.4f}")
        print(f"     å¹³å‡ä¸ç¡®å®šæ€§: æƒ…æ„Ÿ={np.mean(uncertainties['sentiment']):.3f}, " +
              f"è¯„åˆ†={np.mean(uncertainties['rating']):.3f}, " +
              f"æœ‰ç”¨æ€§={np.mean(uncertainties['helpfulness']):.3f}")
    
    return results


def analyze_task_correlations(model, test_loader):
    """åˆ†æä»»åŠ¡é—´çš„ç›¸å…³æ€§å’Œå…±äº«è¡¨å¾æ•ˆæœ"""
    
    print("\nğŸ”— åˆ†æä»»åŠ¡é—´ç›¸å…³æ€§å’Œå…±äº«è¡¨å¾æ•ˆæœ...")
    
    model.eval()
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼
    sentiment_preds, sentiment_trues = [], []
    rating_preds, rating_trues = [], []
    helpfulness_preds, helpfulness_trues = [], []
    shared_representations = []
    
    with torch.no_grad():
        for features, sentiment_true, rating_true, helpfulness_true in test_loader:
            # è·å–å…±äº«çš„å› æœè¡¨å¾
            output = model(features, temperature=0.0, return_details=True)
            
            # ä¸ªä½“è¡¨å¾ (å…±äº«çš„å› æœç‰¹å¾)
            loc_U = output['loc_U'].squeeze()
            shared_representations.append(loc_U.numpy())
            
            # é¢„æµ‹ç»“æœ
            mixed_output = output['output'].squeeze()
            sentiment_preds.extend((mixed_output[:, 0] > 0.5).long().numpy())
            sentiment_trues.extend(sentiment_true.numpy())
            rating_preds.extend(mixed_output[:, 1].long().numpy())
            rating_trues.extend(rating_true.numpy())
            helpfulness_preds.extend(mixed_output[:, 2].numpy())
            helpfulness_trues.extend(helpfulness_true.numpy())
    
    # è½¬æ¢ä¸ºæ•°ç»„
    sentiment_preds = np.array(sentiment_preds)
    sentiment_trues = np.array(sentiment_trues)
    rating_preds = np.array(rating_preds)
    rating_trues = np.array(rating_trues)
    helpfulness_preds = np.array(helpfulness_preds)
    helpfulness_trues = np.array(helpfulness_trues)
    shared_representations = np.vstack(shared_representations)
    
    # 1. ä»»åŠ¡é—´é¢„æµ‹ç›¸å…³æ€§
    from scipy.stats import pearsonr, spearmanr
    
    # æƒ…æ„Ÿä¸è¯„åˆ†ç›¸å…³æ€§
    sentiment_rating_corr, _ = spearmanr(sentiment_trues, rating_trues)
    sentiment_helpfulness_corr, _ = pearsonr(sentiment_trues.astype(float), helpfulness_trues)
    rating_helpfulness_corr, _ = pearsonr(rating_trues.astype(float), helpfulness_trues)
    
    print(f"   çœŸå®æ ‡ç­¾é—´ç›¸å…³æ€§:")
    print(f"     æƒ…æ„Ÿ-è¯„åˆ†: {sentiment_rating_corr:.3f}")
    print(f"     æƒ…æ„Ÿ-æœ‰ç”¨æ€§: {sentiment_helpfulness_corr:.3f}")
    print(f"     è¯„åˆ†-æœ‰ç”¨æ€§: {rating_helpfulness_corr:.3f}")
    
    # é¢„æµ‹ç»“æœç›¸å…³æ€§
    pred_sentiment_rating_corr, _ = spearmanr(sentiment_preds, rating_preds)
    pred_sentiment_helpfulness_corr, _ = pearsonr(sentiment_preds.astype(float), helpfulness_preds)
    pred_rating_helpfulness_corr, _ = pearsonr(rating_preds.astype(float), helpfulness_preds)
    
    print(f"   é¢„æµ‹ç»“æœé—´ç›¸å…³æ€§:")
    print(f"     æƒ…æ„Ÿ-è¯„åˆ†: {pred_sentiment_rating_corr:.3f}")
    print(f"     æƒ…æ„Ÿ-æœ‰ç”¨æ€§: {pred_sentiment_helpfulness_corr:.3f}")
    print(f"     è¯„åˆ†-æœ‰ç”¨æ€§: {pred_rating_helpfulness_corr:.3f}")
    
    # 2. å…±äº«è¡¨å¾çš„æœ‰æ•ˆæ€§åˆ†æ
    # ä½¿ç”¨PCAé™ç»´å¯è§†åŒ–å…±äº«è¡¨å¾
    from sklearn.decomposition import PCA
    from sklearn.metrics import silhouette_score
    
    pca = PCA(n_components=2)
    shared_repr_2d = pca.fit_transform(shared_representations)
    
    # è®¡ç®—ä¸åŒä»»åŠ¡æ ‡ç­¾åœ¨å…±äº«ç©ºé—´ä¸­çš„èšç±»è´¨é‡
    sentiment_silhouette = silhouette_score(shared_repr_2d, sentiment_trues)
    rating_silhouette = silhouette_score(shared_repr_2d, rating_trues)
    
    print(f"   å…±äº«è¡¨å¾èšç±»è´¨é‡ (è½®å»“ç³»æ•°):")
    print(f"     æŒ‰æƒ…æ„Ÿåˆ†ç»„: {sentiment_silhouette:.3f}")
    print(f"     æŒ‰è¯„åˆ†åˆ†ç»„: {rating_silhouette:.3f}")
    
    return {
        'task_correlations': {
            'sentiment_rating': sentiment_rating_corr,
            'sentiment_helpfulness': sentiment_helpfulness_corr,
            'rating_helpfulness': rating_helpfulness_corr
        },
        'prediction_correlations': {
            'sentiment_rating': pred_sentiment_rating_corr,
            'sentiment_helpfulness': pred_sentiment_helpfulness_corr,
            'rating_helpfulness': pred_rating_helpfulness_corr
        },
        'shared_repr_quality': {
            'sentiment_silhouette': sentiment_silhouette,
            'rating_silhouette': rating_silhouette
        },
        'shared_representations_2d': shared_repr_2d,
        'pca_explained_variance': pca.explained_variance_ratio_
    }


def visualize_multitask_results(train_losses, val_metrics, inference_results, 
                               baseline_results, correlation_analysis):
    """Visualize multitask learning results"""
    
    print("\nğŸ“ˆ Generating multitask learning visualization...")
    
    # Set matplotlib to use English and avoid font issues
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    
    # 1. Training loss curves
    epochs = range(1, len(train_losses['total']) + 1)
    
    axes[0, 0].plot(epochs, train_losses['total'], label='Total Loss', color='black', linewidth=2)
    axes[0, 0].plot(epochs, train_losses['sentiment'], label='Sentiment Loss', color='blue', alpha=0.7)
    axes[0, 0].plot(epochs, train_losses['rating'], label='Rating Loss', color='green', alpha=0.7)
    axes[0, 0].plot(epochs, train_losses['helpfulness'], label='Helpfulness Loss', color='red', alpha=0.7)
    axes[0, 0].set_title('Multitask Training Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # 2. Validation metrics curves
    axes[0, 1].plot(epochs[:len(val_metrics['sentiment_acc'])], val_metrics['sentiment_acc'], 
                   label='Sentiment Accuracy', color='blue')
    axes[0, 1].plot(epochs[:len(val_metrics['rating_acc'])], val_metrics['rating_acc'], 
                   label='Rating Accuracy', color='green')
    axes[0, 1].set_ylabel('Accuracy', color='black')
    axes[0, 1].tick_params(axis='y', labelcolor='black')
    axes[0, 1].set_title('Validation Accuracy')
    axes[0, 1].legend(loc='upper left')
    
    # Helpfulness MSE (right axis)
    ax2 = axes[0, 1].twinx()
    ax2.plot(epochs[:len(val_metrics['helpfulness_mse'])], val_metrics['helpfulness_mse'], 
            label='Helpfulness MSE', color='red')
    ax2.set_ylabel('MSE', color='red')
    ax2.tick_params(axis='y', labelcolor='red')
    ax2.legend(loc='upper right')
    
    axes[0, 1].grid(True)
    
    # 3. Inference mode comparison - Sentiment task
    # Map Chinese mode names to English
    mode_mapping = {
        'çº¯å› æœæ¨¡å¼': 'Pure Causal',
        'æ ‡å‡†æ¨¡å¼': 'Standard',
        'é‡‡æ ·æ¨¡å¼': 'Sampling'
    }
    
    modes_en = [mode_mapping.get(mode, mode) for mode in inference_results.keys()]
    sentiment_accs = [inference_results[mode]['sentiment_accuracy'] for mode in inference_results.keys()]
    
    bars = axes[0, 2].bar(modes_en, sentiment_accs, alpha=0.7, color='blue')
    axes[0, 2].set_title('Sentiment Classification: Inference Modes')
    axes[0, 2].set_ylabel('Accuracy')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    for bar, acc in zip(bars, sentiment_accs):
        axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{acc:.3f}', ha='center', va='bottom')
    
    # 4. Inference mode comparison - Rating task
    rating_accs = [inference_results[mode]['rating_accuracy'] for mode in inference_results.keys()]
    
    bars = axes[0, 3].bar(modes_en, rating_accs, alpha=0.7, color='green')
    axes[0, 3].set_title('Rating Prediction: Inference Modes')
    axes[0, 3].set_ylabel('Accuracy')
    axes[0, 3].tick_params(axis='x', rotation=45)
    
    for bar, acc in zip(bars, rating_accs):
        axes[0, 3].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{acc:.3f}', ha='center', va='bottom')
    
    # 5. Multitask vs Single-task comparison
    multitask_results = inference_results['çº¯å› æœæ¨¡å¼']
    
    tasks = ['Sentiment Classification', 'Rating Prediction', 'Helpfulness Regression']
    multitask_scores = [
        multitask_results['sentiment_accuracy'],
        multitask_results['rating_accuracy'],
        1.0 / (1.0 + multitask_results['helpfulness_mse'])  # Convert MSE to relative score
    ]
    single_task_scores = [
        baseline_results['sentiment']['accuracy'],
        baseline_results['rating']['accuracy'],
        1.0 / (1.0 + baseline_results['helpfulness']['mse'])
    ]
    
    x = np.arange(len(tasks))
    width = 0.35
    
    axes[1, 0].bar(x - width/2, single_task_scores, width, label='Single-task', alpha=0.7, color='gray')
    axes[1, 0].bar(x + width/2, multitask_scores, width, label='Multitask', alpha=0.7, color='red')
    axes[1, 0].set_title('Multitask vs Single-task Performance')
    axes[1, 0].set_ylabel('Performance Score')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(tasks, rotation=45)
    axes[1, 0].legend()
    
    # 6. Task correlation heatmap
    task_names = ['Sentiment', 'Rating', 'Helpfulness']
    correlation_matrix = np.array([
        [1.0, correlation_analysis['task_correlations']['sentiment_rating'], 
         correlation_analysis['task_correlations']['sentiment_helpfulness']],
        [correlation_analysis['task_correlations']['sentiment_rating'], 1.0, 
         correlation_analysis['task_correlations']['rating_helpfulness']],
        [correlation_analysis['task_correlations']['sentiment_helpfulness'],
         correlation_analysis['task_correlations']['rating_helpfulness'], 1.0]
    ])
    
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                xticklabels=task_names, yticklabels=task_names, ax=axes[1, 1])
    axes[1, 1].set_title('Task Correlations (True Labels)')
    
    # 7. Shared representation visualization
    shared_repr_2d = correlation_analysis['shared_representations_2d']
    sentiment_trues = inference_results['çº¯å› æœæ¨¡å¼']['true_values']['sentiment']
    
    # Color by sentiment labels
    scatter = axes[1, 2].scatter(shared_repr_2d[:, 0], shared_repr_2d[:, 1], 
                                c=sentiment_trues, cmap='coolwarm', alpha=0.6, s=10)
    axes[1, 2].set_title('Shared Representation Space (colored by sentiment)')
    axes[1, 2].set_xlabel(f'PC1 ({correlation_analysis["pca_explained_variance"][0]:.2%})')
    axes[1, 2].set_ylabel(f'PC2 ({correlation_analysis["pca_explained_variance"][1]:.2%})')
    plt.colorbar(scatter, ax=axes[1, 2])
    
    # 8. Uncertainty analysis
    mode_data = inference_results['æ ‡å‡†æ¨¡å¼']
    uncertainties = mode_data['uncertainties']
    
    task_names_short = ['Sentiment', 'Rating', 'Helpfulness']
    uncertainty_means = [np.mean(uncertainties[task]) for task in ['sentiment', 'rating', 'helpfulness']]
    
    bars = axes[1, 3].bar(task_names_short, uncertainty_means, alpha=0.7, 
                         color=['blue', 'green', 'red'])
    axes[1, 3].set_title('Average Uncertainty by Task')
    axes[1, 3].set_ylabel('Uncertainty')
    
    for bar, unc in zip(bars, uncertainty_means):
        axes[1, 3].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{unc:.3f}', ha='center', va='bottom')
    
    # 9. Prediction correlation analysis
    pred_sentiment = inference_results['çº¯å› æœæ¨¡å¼']['predictions']['sentiment']
    pred_rating = inference_results['çº¯å› æœæ¨¡å¼']['predictions']['rating']
    pred_helpfulness = inference_results['çº¯å› æœæ¨¡å¼']['predictions']['helpfulness']
    
    axes[2, 0].scatter(pred_sentiment, pred_rating, alpha=0.6, s=20)
    axes[2, 0].set_xlabel('Sentiment Prediction (0=negative, 1=positive)')
    axes[2, 0].set_ylabel('Rating Prediction (0-4)')
    axes[2, 0].set_title('Sentiment vs Rating Prediction Relationship')
    axes[2, 0].grid(True)
    
    # 10. Helpfulness prediction scatter plot
    true_helpfulness = inference_results['çº¯å› æœæ¨¡å¼']['true_values']['helpfulness']
    
    axes[2, 1].scatter(true_helpfulness, pred_helpfulness, alpha=0.6, s=20)
    min_val = min(true_helpfulness.min(), pred_helpfulness.min())
    max_val = max(true_helpfulness.max(), pred_helpfulness.max())
    axes[2, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
    axes[2, 1].set_xlabel('True Helpfulness')
    axes[2, 1].set_ylabel('Predicted Helpfulness')
    axes[2, 1].set_title('Helpfulness Prediction vs True Values')
    axes[2, 1].grid(True)
    
    # 11. Confusion matrix - Rating task
    from sklearn.metrics import confusion_matrix
    
    true_rating = inference_results['çº¯å› æœæ¨¡å¼']['true_values']['rating']
    cm = confusion_matrix(true_rating, pred_rating)
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2, 2])
    axes[2, 2].set_title('Rating Prediction Confusion Matrix')
    axes[2, 2].set_xlabel('Predicted Rating')
    axes[2, 2].set_ylabel('True Rating')
    
    # 12. Performance improvement summary
    sentiment_improvement = (multitask_results['sentiment_accuracy'] - baseline_results['sentiment']['accuracy']) * 100
    rating_improvement = (multitask_results['rating_accuracy'] - baseline_results['rating']['accuracy']) * 100
    helpfulness_improvement = (baseline_results['helpfulness']['mse'] - multitask_results['helpfulness_mse']) / baseline_results['helpfulness']['mse'] * 100
    
    improvements = [sentiment_improvement, rating_improvement, helpfulness_improvement]
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    
    bars = axes[2, 3].bar(tasks, improvements, alpha=0.7, color=colors)
    axes[2, 3].set_title('Multitask Learning Performance Improvement (%)')
    axes[2, 3].set_ylabel('Improvement Percentage')
    axes[2, 3].axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    for bar, imp in zip(bars, improvements):
        axes[2, 3].text(bar.get_x() + bar.get_width()/2, 
                       bar.get_height() + (1 if imp > 0 else -3),
                       f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top')
    
    plt.tight_layout()
    plt.savefig('/Users/gongqian/DailyLog/CausalQwen/tutorials/04_multitask/multitask_results.png', 
                dpi=300, bbox_inches='tight')
    
    # Save and close without blocking
    plt.close()  # Close the figure to free memory and avoid blocking
    
    print("âœ… Visualization completed, image saved to multitask_results.png")


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„å¤šä»»åŠ¡å­¦ä¹ æ•™ç¨‹"""
    
    print("ğŸ›’ CausalEngine å¤šä»»åŠ¡å­¦ä¹ æ•™ç¨‹ï¼šç”µå•†è¯„è®ºç»¼åˆåˆ†æ")
    print("=" * 60)
    
    # 1. æ•°æ®å‡†å¤‡
    features, sentiment, rating, helpfulness, scaler = create_ecommerce_data(n_samples=4000, n_features=60)
    
    # åˆ’åˆ†æ•°æ®é›†
    indices = np.arange(len(features))
    train_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)
    
    X_train, X_val, X_test = features[train_idx], features[val_idx], features[test_idx]
    sentiment_train, sentiment_val, sentiment_test = sentiment[train_idx], sentiment[val_idx], sentiment[test_idx]
    rating_train, rating_val, rating_test = rating[train_idx], rating[val_idx], rating[test_idx]
    helpfulness_train, helpfulness_val, helpfulness_test = helpfulness[train_idx], helpfulness[val_idx], helpfulness[test_idx]
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = EcommerceDataset(X_train, sentiment_train, rating_train, helpfulness_train)
    val_dataset = EcommerceDataset(X_val, sentiment_val, rating_val, helpfulness_val)
    test_dataset = EcommerceDataset(X_test, sentiment_test, rating_test, helpfulness_test)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    # 3. åˆ›å»ºå¹¶è®­ç»ƒå¤šä»»åŠ¡ CausalEngine æ¨¡å‹
    model = CausalEcommerceAnalyzer(input_size=X_train.shape[1])
    print(f"\nğŸ—ï¸ CausalEngine å¤šä»»åŠ¡æ¨¡å‹æ¶æ„:")
    print(f"   è¾“å…¥ç»´åº¦: {X_train.shape[1]} (ç”µå•†è¯„è®ºç‰¹å¾)")
    print(f"   éšè—ç»´åº¦: 128")
    print(f"   è¾“å‡ºä»»åŠ¡: 3ä¸ª")
    print(f"     - æƒ…æ„Ÿåˆ†ç±» (classification)")
    print(f"     - è¯„åˆ†ç­‰çº§ (ordinal, 5çº§)")
    print(f"     - æœ‰ç”¨æ€§å¾—åˆ† (regression)")
    print(f"   æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    train_losses, val_metrics = train_multitask_model(model, train_loader, val_loader, epochs=100)
    
    # 4. è®­ç»ƒå•ä»»åŠ¡åŸºçº¿æ¨¡å‹
    baseline_results = train_single_task_baselines(
        X_train, sentiment_train, rating_train, helpfulness_train,
        X_val, sentiment_val, rating_val, helpfulness_val
    )
    
    # 5. è¯„ä¼°ä¸‰ç§æ¨ç†æ¨¡å¼
    inference_results = evaluate_multitask_inference_modes(model, test_loader)
    
    # 6. åˆ†æä»»åŠ¡ç›¸å…³æ€§å’Œå…±äº«è¡¨å¾
    correlation_analysis = analyze_task_correlations(model, test_loader)
    
    # 7. å¯è§†åŒ–ç»“æœ
    visualize_multitask_results(train_losses, val_metrics, inference_results, 
                              baseline_results, correlation_analysis)
    
    # 8. æ€»ç»“æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“‹ å¤šä»»åŠ¡å­¦ä¹ æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    best_mode = "çº¯å› æœæ¨¡å¼"  # é€šå¸¸æ˜¯æœ€ç¨³å®šçš„
    multitask_results = inference_results[best_mode]
    
    print(f"ğŸ† CausalEngine å¤šä»»åŠ¡æ€§èƒ½ ({best_mode}):")
    print(f"   æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡: {multitask_results['sentiment_accuracy']:.4f}")
    print(f"   è¯„åˆ†é¢„æµ‹å‡†ç¡®ç‡: {multitask_results['rating_accuracy']:.4f}")
    print(f"   æœ‰ç”¨æ€§å›å½’MSE: {multitask_results['helpfulness_mse']:.4f}")
    
    print(f"\nğŸ¥ˆ å•ä»»åŠ¡åŸºçº¿æ€§èƒ½:")
    print(f"   æƒ…æ„Ÿåˆ†ç±»å‡†ç¡®ç‡: {baseline_results['sentiment']['accuracy']:.4f}")
    print(f"   è¯„åˆ†é¢„æµ‹å‡†ç¡®ç‡: {baseline_results['rating']['accuracy']:.4f}")
    print(f"   æœ‰ç”¨æ€§å›å½’MSE: {baseline_results['helpfulness']['mse']:.4f}")
    
    # è®¡ç®—æ”¹è¿›
    sentiment_improvement = (multitask_results['sentiment_accuracy'] - baseline_results['sentiment']['accuracy']) * 100
    rating_improvement = (multitask_results['rating_accuracy'] - baseline_results['rating']['accuracy']) * 100
    helpfulness_improvement = (baseline_results['helpfulness']['mse'] - multitask_results['helpfulness_mse']) / baseline_results['helpfulness']['mse'] * 100
    
    print(f"\nğŸ“ˆ å¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿:")
    print(f"   æƒ…æ„Ÿåˆ†ç±»æå‡: {sentiment_improvement:+.1f}%")
    print(f"   è¯„åˆ†é¢„æµ‹æå‡: {rating_improvement:+.1f}%")
    print(f"   æœ‰ç”¨æ€§MSEæ”¹å–„: {helpfulness_improvement:+.1f}%")
    
    print(f"\nğŸ”— ä»»åŠ¡ç›¸å…³æ€§åˆ†æ:")
    corr = correlation_analysis['task_correlations']
    print(f"   æƒ…æ„Ÿ-è¯„åˆ†ç›¸å…³æ€§: {corr['sentiment_rating']:.3f}")
    print(f"   æƒ…æ„Ÿ-æœ‰ç”¨æ€§ç›¸å…³æ€§: {corr['sentiment_helpfulness']:.3f}")
    print(f"   è¯„åˆ†-æœ‰ç”¨æ€§ç›¸å…³æ€§: {corr['rating_helpfulness']:.3f}")
    
    print(f"\nğŸ§  å…±äº«è¡¨å¾è´¨é‡:")
    quality = correlation_analysis['shared_repr_quality']
    print(f"   æŒ‰æƒ…æ„Ÿèšç±»è´¨é‡: {quality['sentiment_silhouette']:.3f}")
    print(f"   æŒ‰è¯„åˆ†èšç±»è´¨é‡: {quality['rating_silhouette']:.3f}")
    print(f"   PCAå‰ä¸¤ç»´è§£é‡Šæ–¹å·®: {correlation_analysis['pca_explained_variance'][:2].sum():.1%}")
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print(f"   â€¢ å•ä¸ª CausalEngine æˆåŠŸå¤„ç†ä¸‰ç§ä¸åŒç±»å‹çš„ä»»åŠ¡")
    print(f"   â€¢ å…±äº«çš„å› æœè¡¨å¾æå‡äº†æ•´ä½“æ€§èƒ½")
    print(f"   â€¢ ä»»åŠ¡é—´å­˜åœ¨æœ‰æ„ä¹‰çš„ç›¸å…³æ€§ï¼Œæ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ")
    print(f"   â€¢ æ··åˆæ¿€æ´»æ¨¡å¼å·¥ä½œæ­£å¸¸ï¼Œå„ä»»åŠ¡ä¿æŒç‹¬ç«‹æ€§")
    print(f"   â€¢ ç»Ÿä¸€çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ç±»å‹")
    
    print(f"\nğŸŒŸ å¤šä»»åŠ¡å­¦ä¹ ä¼˜åŠ¿:")
    print(f"   â€¢ å‚æ•°æ•ˆç‡: å•æ¨¡å‹å¤„ç†å¤šä»»åŠ¡ï¼Œå‚æ•°å…±äº«")
    print(f"   â€¢ æ³›åŒ–èƒ½åŠ›: ä»»åŠ¡é—´äº’è¡¥ä¿¡æ¯æå‡æ³›åŒ–")
    print(f"   â€¢ ä¸€è‡´æ€§: ç»Ÿä¸€çš„æ¨ç†å’Œä¸ç¡®å®šæ€§æ¡†æ¶")
    print(f"   â€¢ å¯æ‰©å±•æ€§: æ˜“äºæ·»åŠ æ–°ä»»åŠ¡ç±»å‹")
    
    print("\nâœ… å¤šä»»åŠ¡å­¦ä¹ æ•™ç¨‹å®Œæˆ! è¿™å±•ç¤ºäº† CausalEngine å¤„ç†å¤æ‚ç°å®é—®é¢˜çš„èƒ½åŠ›ã€‚")


if __name__ == "__main__":
    main()
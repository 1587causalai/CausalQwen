"""
CausalEngine åŸºå‡†æµ‹è¯•åè®®ä»‹ç»
=============================

æœ¬æ•™ç¨‹è¯¦ç»†ä»‹ç»CausalEngineçš„å®˜æ–¹åŸºå‡†æµ‹è¯•åè®®ï¼ŒåŒ…æ‹¬ï¼š
1. å®éªŒè®¾è®¡åŸç†å’Œç§‘å­¦æ§åˆ¶æ–¹æ³•
2. å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°ä¸¤ç»„å®éªŒ
3. å››ç§æ¨ç†æ¨¡å¼çš„å®Œæ•´æ¼”ç¤º
4. å‚æ•°é…ç½®å’Œè¯„ä¼°æŒ‡æ ‡ä½“ç³»

åŸºäºæ–‡æ¡£: causal_engine/misc/benchmark_strategy.md
"""

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from causal_engine import CausalEngine


def introduce_benchmark_protocol():
    """
    ä»‹ç»CausalEngineåŸºå‡†æµ‹è¯•åè®®çš„æ ¸å¿ƒè®¾è®¡ç†å¿µ
    """
    print("ğŸ”¬ CausalEngine åŸºå‡†æµ‹è¯•åè®®")
    print("=" * 60)
    
    print("\nğŸ“‹ å®éªŒç›®æ ‡:")
    print("  1. æ€§èƒ½è¯„ä¼°: åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šç³»ç»Ÿæ€§è¯„ä¼°CausalEngineç›¸å¯¹äºä¼ ç»ŸåŸºçº¿çš„æ€§èƒ½")
    print("  2. å»ºç«‹åŸºçº¿: ä¸ºåç»­æ¨¡å‹æ”¹è¿›ã€æ¶ˆèå®éªŒå»ºç«‹å¯é çš„æ€§èƒ½åŸºå‡†")
    print("  3. å‡è®¾éªŒè¯: ç§‘å­¦éªŒè¯'è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ å…¨å±€å™ªå£°'çš„æ ¸å¿ƒå‡è®¾")
    
    print("\nğŸ¯ æŒ‡å¯¼åŸåˆ™:")
    print("  ç§‘å­¦å¯¹ç…§åŸåˆ™: å°†æœºåˆ¶å‚æ•°(å¦‚b_noise)è§†ä¸ºå›ºå®šçš„å®éªŒæ¡ä»¶æˆ–æ˜ç¡®çš„æ¢ç´¢å˜é‡")
    print("  å› æœå½’å› : å®ç°æ¸…æ™°çš„å› æœå½’å› å’Œå¯è§£é‡Šçš„ç»“æœ")
    print("  å¯å¤ç°æ€§: æ‰€æœ‰å®éªŒé…ç½®æ ‡å‡†åŒ–ï¼Œç¡®ä¿ç»“æœå¯å¤ç°")


def demonstrate_model_architecture():
    """
    æ¼”ç¤ºCausalEngineçš„ä¸‰é˜¶æ®µæ¶æ„å’Œå‚æ•°ä½“ç³»
    """
    print("\n\nğŸ—ï¸ CausalEngine æ¶æ„ä¸å‚æ•°å®šä¹‰")
    print("=" * 60)
    
    print("\nğŸ“ ä¸‰é˜¶æ®µæ¶æ„:")
    print("  é˜¶æ®µ1: å½’å› ç½‘ç»œ (AbductionNetwork)")
    print("    - åŠŸèƒ½: ä»è¯æ®zæ¨æ–­ä¸ªä½“æŸ¯è¥¿åˆ†å¸ƒå‚æ•°")
    print("    - å‚æ•°: W_loc, b_loc (ä½ç½®), W_scale, b_scale (å°ºåº¦)")
    print("    - æ•°å­¦: Î¼_U = W_locÂ·z + b_loc")
    print("            Î³_U = softplus(W_scaleÂ·z + b_scale)")
    
    print("\n  é˜¶æ®µ2: è¡ŒåŠ¨ç½‘ç»œ (ActionNetwork)")
    print("    - åŠŸèƒ½: åº”ç”¨æ™®é€‚å› æœå¾‹ï¼ŒU â†’ S")
    print("    - å‚æ•°: W_cls, b_cls (å¯å­¦ä¹ ), b_noise (å¯é…ç½®)")
    print("    - æ•°å­¦: loc_S = W_clsÂ·loc_U + b_cls")
    print("            scale_S = (scale_U + |b_noise|)Â·|W_cls|")
    
    print("\n  é˜¶æ®µ3: ä»»åŠ¡æ¿€æ´»å¤´ (TaskActivationHeads)")
    print("    - åˆ†ç±»å¤´: P_k = P(S_k > C_ovr), C_ovrä¸ºå…¨å±€å†³ç­–é˜ˆå€¼")
    print("    - å›å½’å¤´: Y_k ~ Cauchy(w_kÂ·loc_S + b_k, |w_k|Â·scale_S)")
    print("    - æœ‰åºåˆ†ç±»: P(y_i) = P(C_{k,i} < S_k â‰¤ C_{k,i+1})")
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹å±•ç¤ºå‚æ•°
    print("\nâš¡ å®é™…æ¶æ„æ¼”ç¤º:")
    engine = CausalEngine(
        hidden_size=128,
        vocab_size=10,
        causal_size=64,
        activation_modes="classification"
    )
    
    print(f"  æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    total_params = sum(p.numel() for p in engine.parameters())
    trainable_params = sum(p.numel() for p in engine.parameters() if p.requires_grad)
    print(f"    æ€»å‚æ•°é‡: {total_params:,}")
    print(f"    å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # å±•ç¤ºå„ä¸ªç½‘ç»œçš„å‚æ•°
    print(f"  ç½‘ç»œç»“æ„:")
    print(f"    å½’å› ç½‘ç»œ - ä½ç½®ç½‘ç»œ: {engine.abduction.loc_net}")
    print(f"    å½’å› ç½‘ç»œ - å°ºåº¦ç½‘ç»œ: {engine.abduction.scale_net}")
    print(f"    è¡ŒåŠ¨ç½‘ç»œç»´åº¦: {engine.causal_size} â†’ {engine.vocab_size}")


def demonstrate_experimental_groups():
    """
    æ¼”ç¤ºä¸¤ç»„å¹³è¡Œå¯¹æ¯”å®éªŒçš„è®¾è®¡
    """
    print("\n\nğŸ§ª å®éªŒç»„è®¾è®¡: å›ºå®šå™ªå£° vs è‡ªé€‚åº”å™ªå£°")
    print("=" * 60)
    
    print("\nğŸ“Š å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ")
    print("  ç›®æ ‡: æµ‹è¯•ä¸åŒå›ºå®šå™ªå£°å¼ºåº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“")
    print("  æ§åˆ¶: b_noise.requires_grad = False")
    print("  æµ‹è¯•å™ªå£°å€¼: [0.0, 0.1, 1.0, 10.0]")
    
    # æ¼”ç¤ºä¸åŒå™ªå£°å¼ºåº¦çš„æ•ˆæœ
    noise_values = [0.0, 0.1, 1.0, 10.0]  # åŸºå‡†åè®®æ›´æ–°ï¼šæ›´æœ‰åŒºåˆ†åº¦çš„å…³é”®å€¼
    print(f"\n  å™ªå£°å¼ºåº¦åˆ†æ:")
    
    for noise in noise_values:
        if noise == 0.01:
            effect = "ä½å™ªå£° - é«˜ç½®ä¿¡åº¦ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ"
        elif noise == 0.05:
            effect = "ä¸­ä½å™ªå£° - è¾ƒé«˜ç½®ä¿¡åº¦"
        elif noise == 0.1:
            effect = "æ ‡å‡†å™ªå£°(é»˜è®¤) - å¹³è¡¡çš„ä¸ç¡®å®šæ€§è¡¨è¾¾"
        elif noise == 0.2:
            effect = "ä¸­é«˜å™ªå£° - ä¿å®ˆé¢„æµ‹"
        else:  # 0.5
            effect = "é«˜å™ªå£° - é«˜åº¦ä¸ç¡®å®šï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ"
        
        print(f"    b_noise = {noise}: {effect}")
    
    print("\nğŸ“Š å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ")
    print("  ç›®æ ‡: éªŒè¯è®©æ¨¡å‹è‡ªä¸»å­¦ä¹ å…¨å±€ç¯å¢ƒå™ªå£°çš„æœ‰æ•ˆæ€§")
    print("  æ§åˆ¶: b_noise.requires_grad = True")
    print("  åˆå§‹åŒ–: b_noise = nn.Parameter(torch.tensor([0.1]))")
    print("  ä¼˜åŒ–: ä¸ä¸»ç½‘ç»œåŒæ—¶è®­ç»ƒï¼Œå¯ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡")


def demonstrate_training_hyperparameters():
    """
    æ¼”ç¤ºæ ‡å‡†åŒ–çš„è®­ç»ƒè¶…å‚æ•°é…ç½®
    """
    print("\n\nâš™ï¸ æ ‡å‡†åŒ–è®­ç»ƒè¶…å‚æ•°")
    print("=" * 60)
    
    config = {
        "optimizer": "AdamW",
        "learning_rate": 1e-4,
        "learning_schedule": "çº¿æ€§warm-up(å‰10% steps) + cosine decay",
        "weight_decay": 0.01,
        "epochs": 100,
        "early_stopping": True,
        "patience": 10,
        "batch_size": 64,
        "monitor_metric": "éªŒè¯é›†æŸå¤±"
    }
    
    print("  è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"    {key}: {value}")
    
    # å›ºå®šå‚æ•°é…ç½®
    print("\n  å›ºå®šè¶…å‚æ•°:")
    print("    C_ovr (åˆ†ç±»é˜ˆå€¼): 0.0")
    print("    w_k, b_k (å›å½’å¤´): 1.0, 0.0")
    print("    éšæœºç§å­: 42 (å¯å¤ç°æ€§)")
    
    # è¯„ä¼°æŒ‡æ ‡
    print("\n  è¯„ä¼°æŒ‡æ ‡:")
    print("    åˆ†ç±»ä»»åŠ¡: Accuracy, F1-Score(Macro), Precision, Recall")
    print("    å›å½’ä»»åŠ¡: MAE, RMSE, MdAE, MSE, RÂ²")
    print("    ç»Ÿè®¡æ£€éªŒ: t-test, Wilcoxon signed-rank test")


def demonstrate_inference_modes():
    """
    æ¼”ç¤ºå››ç§æ¨ç†æ¨¡å¼çš„æ•°å­¦åŸç†å’Œåº”ç”¨åœºæ™¯
    """
    print("\n\nğŸ¯ å››ç§æ¨ç†æ¨¡å¼è¯¦è§£")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    engine = CausalEngine(
        hidden_size=64,
        vocab_size=8,
        causal_size=32,
        activation_modes="classification"
    )
    
    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    batch_size = 4
    evidence = torch.randn(batch_size, 1, 64)
    
    print("\nğŸ² è¾“å…¥è¯æ®:")
    print(f"  å½¢çŠ¶: {evidence.shape}")
    print(f"  ç¤ºä¾‹å€¼èŒƒå›´: [{evidence.min():.3f}, {evidence.max():.3f}]")
    
    with torch.no_grad():
        # æ¨¡å¼1: å› æœæ¨ç† (T=0)
        print("\n  æ¨¡å¼1: å› æœæ¨ç† (Causal Mode)")
        print("    è®¾ç½®: temperature=0, do_sample=any")
        print("    æœºåˆ¶: å®Œå…¨å…³é—­å¤–ç”Ÿå™ªå£°å½±å“")
        print("    æ•°å­¦: U' ~ Cauchy(Î¼_U, Î³_U)")
        print("    åº”ç”¨: ç¡®å®šæ€§æ¨ç†ã€ç¡¬å†³ç­–ã€ç‚¹ä¼°è®¡")
        
        causal_output = engine(evidence, temperature=0, do_sample=False, return_dict=True)
        causal_uncertainty = causal_output['scale_S'].mean().item()
        print(f"    å¹³å‡ä¸ç¡®å®šæ€§: {causal_uncertainty:.4f}")
        
        # æ¨¡å¼2: æ ‡å‡†æ¨ç† (T>0, no sample)
        print("\n  æ¨¡å¼2: æ ‡å‡†æ¨ç† (Standard Mode)")
        print("    è®¾ç½®: temperature>0, do_sample=False")
        print("    æœºåˆ¶: å™ªå£°ç¼©æ”¾åå¢åŠ å°ºåº¦å‚æ•°ï¼Œæ‰©å¤§ä¸ç¡®å®šæ€§")
        print("    æ•°å­¦: U' ~ Cauchy(Î¼_U, Î³_U + TÂ·|b_noise|)")
        print("    åº”ç”¨: ç¨³å®šç”Ÿæˆã€è½¯å†³ç­–ã€ç½®ä¿¡åŒºé—´")
        
        standard_output = engine(evidence, temperature=1.0, do_sample=False, return_dict=True)
        standard_uncertainty = standard_output['scale_S'].mean().item()
        print(f"    å¹³å‡ä¸ç¡®å®šæ€§: {standard_uncertainty:.4f} (â†‘ å¢åŠ äº† {standard_uncertainty - causal_uncertainty:.4f})")
        
        # æ¨¡å¼3: é‡‡æ ·æ¨ç† (T>0, sample)
        print("\n  æ¨¡å¼3: é‡‡æ ·æ¨ç† (Sampling Mode)")
        print("    è®¾ç½®: temperature>0, do_sample=True")
        print("    æœºåˆ¶: å™ªå£°ç¼©æ”¾åæ‰°åŠ¨ä½ç½®å‚æ•°ï¼Œæ”¹å˜ä¸ªä½“èº«ä»½")
        print("    æ•°å­¦: Îµ~Cauchy(0,1), U' ~ Cauchy(Î¼_U + TÂ·|b_noise|Â·Îµ, Î³_U)")
        print("    åº”ç”¨: åˆ›é€ æ€§ç”Ÿæˆã€æ¢ç´¢è¾¹ç•Œã€è’™ç‰¹å¡æ´›")
        
        sampling_output = engine(evidence, temperature=0.8, do_sample=True, return_dict=True)
        sampling_loc_shift = (sampling_output['loc_U'] - causal_output['loc_U']).abs().mean().item()
        print(f"    ä½ç½®å‚æ•°å¹³å‡åç§»: {sampling_loc_shift:.4f}")
        
        # æ¨¡å¼4: å…¼å®¹æ¨¡å¼
        print("\n  æ¨¡å¼4: å…¼å®¹æ¨ç† (Compatible Mode)")
        print("    è®¾ç½®: ä»»æ„temperature, ç‰¹æ®Šå…¼å®¹æ ‡å¿—")
        print("    æœºåˆ¶: å…¼å®¹ä¼ ç»Ÿtransformerè¡Œä¸º")
        print("    åº”ç”¨: ä¸ä¼ ç»Ÿæ¨¡å‹å¯¹æ¯”åŸºå‡†")


def demonstrate_scientific_control():
    """
    æ¼”ç¤ºç§‘å­¦æ§åˆ¶çš„å®éªŒè®¾è®¡
    """
    print("\n\nğŸ”¬ ç§‘å­¦æ§åˆ¶å®éªŒè®¾è®¡")
    print("=" * 60)
    
    print("\nğŸ›ï¸ ä¼˜é›…çš„å®éªŒæ§åˆ¶:")
    print("  æ ¸å¿ƒè®¾è®¡: é€šè¿‡b_noise.requires_gradçš„True/Falseæ§åˆ¶å®éªŒç»„")
    print("  ä¼˜åŠ¿1: åªéœ€ä¸€ä¸ªå¸ƒå°”å¼€å…³åˆ‡æ¢ä¸¤ç§æ¨¡å¼")
    print("  ä¼˜åŠ¿2: é™¤å™ªå£°å­¦ä¹ æ–¹å¼å¤–ï¼Œå…¶ä»–è®¾ç½®å®Œå…¨ç›¸åŒ")
    print("  ä¼˜åŠ¿3: ç¡®ä¿å¯¹æ¯”çš„å…¬å¹³æ€§å’Œç§‘å­¦æ€§")
    
    # ä»£ç ç¤ºä¾‹
    print("\nğŸ’» å®ç°ç¤ºä¾‹:")
    print("  # å®éªŒç»„A: å›ºå®šå™ªå£°")
    print("  engine.action.b_noise.requires_grad = False")
    print("  engine.action.b_noise.data.fill_(0.1)  # å›ºå®šä¸º0.1")
    print("")
    print("  # å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°")
    print("  engine.action.b_noise.requires_grad = True")
    print("  # b_noiseä½œä¸ºå¯å­¦ä¹ å‚æ•°åŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­")
    
    print("\nğŸ“Š é¢„æœŸå®éªŒç»“æœ:")
    print("  å™ªå£°å¼ºåº¦-æ€§èƒ½æ›²çº¿: æ­ç¤ºéçº¿æ€§å…³ç³»å’Œæœ€ä¼˜åŒºé—´")
    print("  å›ºå®švsè‡ªé€‚åº”å¯¹æ¯”: éªŒè¯è‡ªé€‚åº”å­¦ä¹ çš„æœ‰æ•ˆæ€§")
    print("  è·¨æ•°æ®é›†åˆ†æ: ä¸åŒä»»åŠ¡çš„æœ€ä¼˜å™ªå£°ç‰¹æ€§")


def demonstrate_evaluation_framework():
    """
    æ¼”ç¤ºå®Œæ•´çš„è¯„ä¼°ä¸åˆ†ææ¡†æ¶
    """
    print("\n\nğŸ“Š è¯„ä¼°ä¸åˆ†ææ¡†æ¶")
    print("=" * 60)
    
    print("\nğŸ¯ æ ¸å¿ƒå¯¹æ¯”åˆ†æ:")
    print("  1. å™ªå£°å¼ºåº¦å½±å“åˆ†æ:")
    print("     - å¯¹æ¯”ä¸åŒå›ºå®šå™ªå£°å€¼(0.01-0.5)çš„æ€§èƒ½")
    print("     - ç»˜åˆ¶æ€§èƒ½-å™ªå£°æ›²çº¿")
    print("     - åˆ†ææœ€ä¼˜å™ªå£°åŒºé—´")
    
    print("\n  2. å›ºå®švsè‡ªé€‚åº”å™ªå£°:")
    print("     - å¯¹æ¯”æœ€ä¼˜å›ºå®šå™ªå£°vsè‡ªé€‚åº”å­¦ä¹ ")
    print("     - åˆ†ææ¨¡å‹å­¦åˆ°çš„å™ªå£°å€¼åˆç†æ€§")
    print("     - è¯„ä¼°è‡ªé€‚åº”å­¦ä¹ çš„æ€§èƒ½æå‡")
    
    print("\n  3. CausalEngine vsä¼ ç»ŸåŸºçº¿:")
    print("     - CausalEngineæœ€ä½³é…ç½®vsæ ‡å‡†MLP")
    print("     - éªŒè¯å› æœæ¶æ„çš„æœ‰æ•ˆæ€§")
    print("     - é‡åŒ–å› æœæ¨ç†çš„ä¼˜åŠ¿")
    
    print("\nğŸ“ˆ ç»Ÿè®¡åˆ†ææ–¹æ³•:")
    print("  - å¤šæ¬¡è¿è¡Œ(â‰¥5æ¬¡)ç¡®ä¿ç»“æœç¨³å®šæ€§")
    print("  - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ(p < 0.05)")
    print("  - æ•ˆåº”é‡åˆ†æ(Cohen's d)")
    print("  - ç½®ä¿¡åŒºé—´è®¡ç®—")


def visualize_benchmark_protocol():
    """
    å¯è§†åŒ–åŸºå‡†æµ‹è¯•åè®®çš„å®éªŒæµç¨‹
    """
    print("\n\nğŸ“Š ç”ŸæˆåŸºå‡†åè®®æµç¨‹å›¾")
    print("=" * 60)
    
    # åˆ›å»ºå®éªŒæµç¨‹å¯è§†åŒ–
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('CausalEngine Benchmark Protocol Overview', fontsize=16, fontweight='bold')
    
    # 1. å™ªå£°å¼ºåº¦å¯¹æ¯”å®éªŒ
    noise_values = [0.0, 0.1, 1.0, 10.0]  # åŸºå‡†åè®®æ›´æ–°ï¼šæ›´æœ‰åŒºåˆ†åº¦çš„å…³é”®å€¼
    # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®ï¼ˆå®é™…åº”è¯¥ä»çœŸå®å®éªŒè·å¾—ï¼‰
    performance = [0.75, 0.85, 0.83, 0.78]
    
    ax1.plot(noise_values, performance, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('Noise Strength (b_noise)')
    ax1.set_ylabel('Performance')
    ax1.set_title('Fixed Noise Experiment (Group A)')
    ax1.grid(True, alpha=0.3)
    ax1.set_xscale('log')
    
    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    best_idx = np.argmax(performance)
    ax1.annotate(f'Optimal: {noise_values[best_idx]}', 
                xy=(noise_values[best_idx], performance[best_idx]),
                xytext=(noise_values[best_idx]*2, performance[best_idx]+0.02),
                arrowprops=dict(arrowstyle='->', color='red'))
    
    # 2. æ¨ç†æ¨¡å¼å¯¹æ¯”
    modes = ['Causal\n(T=0)', 'Standard\n(T=1.0)', 'Sampling\n(T=0.8)', 'Compatible']
    mode_performance = [0.83, 0.85, 0.84, 0.79]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    bars = ax2.bar(modes, mode_performance, color=colors, alpha=0.7)
    ax2.set_ylabel('Performance')
    ax2.set_title('Inference Modes Comparison')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, perf in zip(bars, mode_performance):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{perf:.3f}', ha='center', va='bottom')
    
    # 3. æ¶æ„å¯¹æ¯”
    architectures = ['Traditional\nMLP', 'CausalEngine\n(Ablated)', 'CausalEngine\n(Full)']
    arch_performance = [0.79, 0.81, 0.85]
    colors_arch = ['lightcoral', 'lightblue', 'lightgreen']
    
    bars_arch = ax3.bar(architectures, arch_performance, color=colors_arch, alpha=0.8)
    ax3.set_ylabel('Performance')
    ax3.set_title('Architecture Comparison')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, perf in zip(bars_arch, arch_performance):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{perf:.3f}', ha='center', va='bottom')
    
    # 4. å®éªŒç»„A vs Bå¯¹æ¯”
    experiments = ['Fixed Noise\n(Best)', 'Adaptive Noise\n(Learned)']
    exp_performance = [0.85, 0.87]
    learned_noise = 0.12  # æ¨¡æ‹Ÿå­¦åˆ°çš„å™ªå£°å€¼
    
    bars_exp = ax4.bar(experiments, exp_performance, color=['skyblue', 'orange'], alpha=0.8)
    ax4.set_ylabel('Performance')
    ax4.set_title('Fixed vs Adaptive Noise Learning')
    ax4.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ ‡ç­¾
    ax4.text(0, exp_performance[0] + 0.01, f'{exp_performance[0]:.3f}', ha='center')
    ax4.text(1, exp_performance[1] + 0.01, f'{exp_performance[1]:.3f}\n(Learned: {learned_noise})', ha='center')
    
    plt.tight_layout()
    plt.savefig('tutorials/00_getting_started/benchmark_protocol_overview.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… åŸºå‡†åè®®æµç¨‹å›¾å·²ä¿å­˜: tutorials/00_getting_started/benchmark_protocol_overview.png")


def main():
    """
    ä¸»å‡½æ•°ï¼šå®Œæ•´çš„åŸºå‡†åè®®ä»‹ç»
    """
    print("ğŸŒŸ CausalEngine åŸºå‡†æµ‹è¯•åè®®å®Œæ•´ä»‹ç»")
    print("åŸºäºå®˜æ–¹æ–‡æ¡£: causal_engine/misc/benchmark_strategy.md")
    print("=" * 80)
    
    # 1. åè®®æ¦‚è¿°
    introduce_benchmark_protocol()
    
    # 2. æ¨¡å‹æ¶æ„ä»‹ç»
    demonstrate_model_architecture()
    
    # 3. å®éªŒç»„è®¾è®¡
    demonstrate_experimental_groups()
    
    # 4. è®­ç»ƒè¶…å‚æ•°
    demonstrate_training_hyperparameters()
    
    # 5. æ¨ç†æ¨¡å¼æ¼”ç¤º
    demonstrate_inference_modes()
    
    # 6. ç§‘å­¦æ§åˆ¶è®¾è®¡
    demonstrate_scientific_control()
    
    # 7. è¯„ä¼°æ¡†æ¶
    demonstrate_evaluation_framework()
    
    # 8. å¯è§†åŒ–
    visualize_benchmark_protocol()
    
    # 9. æ€»ç»“
    print("\n\nğŸ‰ åŸºå‡†æµ‹è¯•åè®®ä»‹ç»å®Œæˆï¼")
    print("=" * 80)
    print("ğŸ”¬ æ ¸å¿ƒè®¾è®¡ç†å¿µ:")
    print("  âœ… ç§‘å­¦å¯¹ç…§: é€šè¿‡requires_gradæ§åˆ¶å˜é‡")
    print("  âœ… ç³»ç»Ÿè¯„ä¼°: å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°å…¨é¢å¯¹æ¯”")
    print("  âœ… æ ‡å‡†é…ç½®: ç»Ÿä¸€çš„è¶…å‚æ•°å’Œè¯„ä¼°æŒ‡æ ‡")
    print("  âœ… å¯å¤ç°æ€§: å›ºå®šéšæœºç§å­å’Œæ ‡å‡†æµç¨‹")
    
    print("\nğŸš€ å®éªŒä»·å€¼:")
    print("  ğŸ¯ ç†è®ºéªŒè¯: éªŒè¯'è‡ªä¸»å­¦ä¹ å…¨å±€å™ªå£°'å‡è®¾")
    print("  ğŸ“ æœºåˆ¶ç†è§£: æ­ç¤ºå™ªå£°å¼ºåº¦ä¸æ€§èƒ½çš„å…³ç³»")
    print("  ğŸ”„ åŸºçº¿å»ºç«‹: ä¸ºåç»­ç ”ç©¶æä¾›æ ‡å‡†åŸºå‡†")
    print("  ğŸŒ¡ï¸ æ¨¡å¼ä¼˜åŒ–: ä¼˜åŒ–å››ç§æ¨ç†æ¨¡å¼çš„åº”ç”¨")
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("  1. ç†è®ºåŸºç¡€: tutorials/00_getting_started/theoretical_foundations.py")
    print("  2. å®é™…åº”ç”¨: tutorials/01_classification/ & tutorials/02_regression/")
    print("  3. æ¶ˆèå®éªŒ: tutorials/03_ablation_studies/comprehensive_comparison.py")
    print("  4. æ•°å­¦æ–‡æ¡£: causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md")


if __name__ == "__main__":
    main()
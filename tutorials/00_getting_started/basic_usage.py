"""
CausalEngine åŸºç¡€ä½¿ç”¨æ•™ç¨‹ (2024æ›´æ–°ç‰ˆ)
=========================================

åŸºäºæœ€æ–°çš„åŸºå‡†æµ‹è¯•åè®®å’Œæ•°å­¦ç†è®ºæ›´æ–°
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CausalEngineè¿›è¡Œå› æœæ¨ç†é©±åŠ¨çš„åˆ†ç±»å’Œå›å½’ä»»åŠ¡

æ ¸å¿ƒæ›´æ–°:
- åŸºäºåŸºå‡†æµ‹è¯•åè®®çš„æ ‡å‡†åŒ–å‚æ•°é…ç½®
- å››ç§æ¨ç†æ¨¡å¼çš„å®Œæ•´æ¼”ç¤º
- å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°çš„å¯¹æ¯”å®éªŒè®¾è®¡
- ä¸‰ç§ä»»åŠ¡æ¿€æ´»æœºåˆ¶çš„åº”ç”¨ç¤ºä¾‹

è¿™æ˜¯æ‚¨å¼€å§‹ä½¿ç”¨CausalEngineçš„ç¬¬ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹ï¼
"""

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥CausalEngine
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

try:
    from src.causal_qwen_mvp.causal_engine import CausalEngine
    CAUSAL_ENGINE_AVAILABLE = True
    print("âœ… CausalEngineæ¨¡å—åŠ è½½æˆåŠŸï¼")
except ImportError:
    try:
        from causal_engine import CausalEngine
        CAUSAL_ENGINE_AVAILABLE = True
        print("âœ… CausalEngineæ¨¡å—åŠ è½½æˆåŠŸï¼")
    except ImportError:
        CAUSAL_ENGINE_AVAILABLE = False
        print("âš ï¸  CausalEngineæ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¼”ç¤ºç‰ˆæœ¬")
        
        # å®šä¹‰ä¸€ä¸ªæ¨¡æ‹Ÿçš„CausalEngineç±»
        class CausalEngine:
            def __init__(self, **kwargs):
                self.hidden_size = kwargs.get('hidden_size', 128)
                self.causal_size = kwargs.get('causal_size', 128)
                
            def __call__(self, hidden_states, temperature=1.0, do_sample=False):
                batch_size, seq_length, _ = hidden_states.shape
                vocab_size = 10
                
                class Output:
                    def __init__(self):
                        self.logits = torch.randn(batch_size, seq_length, vocab_size)
                
                return Output()

from tutorials.utils.ablation_networks import (
    create_ablation_experiment, AblationTrainer,
    create_ablated_classifier, create_ablated_regressor,
    create_full_causal_classifier, create_full_causal_regressor
)
from tutorials.utils.baseline_networks import (
    TraditionalMLPClassifier, TraditionalMLPRegressor, BaselineTrainer
)

# æ·»åŠ è¾…åŠ©å‡½æ•°
def create_baseline_classifier(input_size, num_classes, **kwargs):
    return TraditionalMLPClassifier(input_size=input_size, num_classes=num_classes, hidden_sizes=[128, 64], dropout_rate=0.1)

def create_baseline_regressor(input_size, output_size, **kwargs):
    return TraditionalMLPRegressor(input_size=input_size, output_size=output_size, hidden_sizes=[128, 64], dropout_rate=0.1)
from tutorials.utils.evaluation_metrics import (
    calculate_classification_metrics, calculate_regression_metrics
)


def demo_basic_causal_engine():
    """
    æ¼”ç¤ºCausalEngineçš„åŸºç¡€APIä½¿ç”¨
    """
    print("\nğŸŒŸ CausalEngine åŸºç¡€APIæ¼”ç¤º")
    print("=" * 40)
    
    if not CAUSAL_ENGINE_AVAILABLE:
        print("ç”±äºCausalEngineæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æ¼”ç¤º")
        return
    
    # 1. åˆ›å»ºCausalEngineå®ä¾‹
    print("\n1. åˆ›å»ºCausalEngineå®ä¾‹")
    
    engine = CausalEngine(
        hidden_size=128,        # éšè—å±‚å¤§å°
        vocab_size=10,          # è¾“å‡ºè¯æ±‡è¡¨å¤§å°ï¼ˆåˆ†ç±»ç±»åˆ«æ•°ï¼‰
        causal_size=128,        # å› æœè¡¨ç¤ºå¤§å°
        activation_modes="classification"  # æ¿€æ´»æ¨¡å¼
    )
    
    print(f"   éšè—å±‚å¤§å°: {engine.hidden_size}")
    print(f"   å› æœè¡¨ç¤ºå¤§å°: {engine.causal_size}")
    print(f"   æ¿€æ´»æ¨¡å¼: classification")
    
    # 2. å‡†å¤‡è¾“å…¥æ•°æ®
    print("\n2. å‡†å¤‡è¾“å…¥æ•°æ®")
    
    batch_size = 16
    seq_length = 5
    hidden_size = 128
    
    # æ¨¡æ‹Ÿtransformerçš„éšè—çŠ¶æ€è¾“å‡º
    hidden_states = torch.randn(batch_size, seq_length, hidden_size)
    print(f"   è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"   è¾“å…¥ç±»å‹: æ¨¡æ‹Ÿçš„transformeréšè—çŠ¶æ€")
    
    # 3. è¿›è¡Œå› æœæ¨ç†
    print("\n3. å› æœæ¨ç†è¿‡ç¨‹")
    
    # çº¯å› æœæ¨ç†ï¼ˆæ¸©åº¦=0ï¼‰
    print("   æ¨¡å¼1: çº¯å› æœæ¨ç† (æ¸©åº¦=0)")
    output_causal = engine(hidden_states, temperature=0, do_sample=False)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_causal.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: ç¡®å®šæ€§å› æœæ¨ç†ç»“æœ")
    
    # å¸¦ä¸ç¡®å®šæ€§çš„æ¨ç†ï¼ˆæ¸©åº¦>0ï¼‰
    print("   æ¨¡å¼2: å¸¦ä¸ç¡®å®šæ€§æ¨ç† (æ¸©åº¦=1.0)")
    output_uncertain = engine(hidden_states, temperature=1.0, do_sample=False)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_uncertain.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: ä¸ç¡®å®šæ€§é‡åŒ–ç»“æœ")
    
    # é‡‡æ ·æ¨¡å¼ï¼ˆèº«ä»½æ¢ç´¢ï¼‰
    print("   æ¨¡å¼3: é‡‡æ ·æ¨¡å¼ (æ¸©åº¦=0.8, é‡‡æ ·=True)")
    output_sampling = engine(hidden_states, temperature=0.8, do_sample=True)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_sampling.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: èº«ä»½æ¢ç´¢é‡‡æ ·ç»“æœ")
    
    # 4. åˆ†æè¾“å‡ºå·®å¼‚
    print("\n4. ä¸åŒæ¨ç†æ¨¡å¼çš„è¾“å‡ºå¯¹æ¯”")
    
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºè¿›è¡Œå¯¹æ¯”
    sample_idx = 0
    seq_idx = 0
    
    causal_probs = torch.softmax(output_causal.logits[sample_idx, seq_idx], dim=-1)
    uncertain_probs = torch.softmax(output_uncertain.logits[sample_idx, seq_idx], dim=-1)
    sampling_probs = torch.softmax(output_sampling.logits[sample_idx, seq_idx], dim=-1)
    
    print(f"   çº¯å› æœæ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {causal_probs.max().item():.4f}")
    print(f"   ä¸ç¡®å®šæ€§æ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {uncertain_probs.max().item():.4f}")
    print(f"   é‡‡æ ·æ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {sampling_probs.max().item():.4f}")
    
    # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒçš„ç†µï¼ˆä¸ç¡®å®šæ€§åº¦é‡ï¼‰
    def entropy(probs):
        return -(probs * torch.log(probs + 1e-8)).sum()
    
    print(f"   çº¯å› æœæ¨¡å¼ - ç†µ: {entropy(causal_probs).item():.4f}")
    print(f"   ä¸ç¡®å®šæ€§æ¨¡å¼ - ç†µ: {entropy(uncertain_probs).item():.4f}")
    print(f"   é‡‡æ ·æ¨¡å¼ - ç†µ: {entropy(sampling_probs).item():.4f}")
    
    print("\nâœ… CausalEngineåŸºç¡€APIæ¼”ç¤ºå®Œæˆï¼")


def demo_classification_task():
    """
    æ¼”ç¤ºCausalEngineåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨
    """
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡æ¼”ç¤º")
    print("=" * 40)
    
    # 1. ç”Ÿæˆåˆ†ç±»æ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿåˆ†ç±»æ•°æ®")
    
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=3,
        random_state=42
    )
    
    print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   ç±»åˆ«æ•°: {len(np.unique(y))}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    print(f"   è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"   æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\n2. æ¨¡å‹å¯¹æ¯”å®éªŒ")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_size = X_train.shape[1]
    num_classes = len(np.unique(y))
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train), 
        torch.LongTensor(y_train)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test), 
        torch.LongTensor(y_test)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    results = {}
    
    # è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ
    print("   è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ...")
    baseline_model = create_baseline_classifier(input_size, num_classes)
    baseline_trainer = BaselineTrainer(baseline_model, device, learning_rate=1e-4, weight_decay=0.01)
    baseline_trainer.train_classification(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹
    baseline_model.eval()
    baseline_preds = []
    baseline_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            outputs = baseline_model(batch_x)
            preds = torch.argmax(outputs, dim=-1)
            baseline_preds.extend(preds.cpu().numpy())
            baseline_targets.extend(batch_y.numpy())
    
    baseline_metrics = calculate_classification_metrics(
        np.array(baseline_targets), np.array(baseline_preds)
    )
    results['Traditional NN'] = baseline_metrics
    
    # è®­ç»ƒCausalEngine
    print("   è®­ç»ƒCausalEngine...")
    causal_model = create_full_causal_classifier(input_size, num_classes)
    causal_trainer = BaselineTrainer(causal_model, device, learning_rate=1e-4, weight_decay=0.01)
    causal_trainer.train_classification(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°CausalEngineï¼ˆå¤šç§æ¨ç†æ¨¡å¼ï¼‰
    causal_model.eval()
    
    for mode, (temp, do_sample) in [
        ("Causal", (0, False)),
        ("Standard", (1.0, False)),
        ("Sampling", (0.8, True))
    ]:
        causal_preds = []
        causal_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                
                if hasattr(causal_model, 'predict'):
                    preds = causal_model.predict(batch_x, temperature=temp, do_sample=do_sample)
                else:
                    outputs = causal_model(batch_x, temperature=temp, do_sample=do_sample)
                    preds = torch.argmax(outputs, dim=-1)
                
                causal_preds.extend(preds.cpu().numpy())
                causal_targets.extend(batch_y.numpy())
        
        causal_metrics = calculate_classification_metrics(
            np.array(causal_targets), np.array(causal_preds)
        )
        results[f'CausalEngine({mode})'] = causal_metrics
    
    # 4. æ˜¾ç¤ºç»“æœ
    print("\n3. åˆ†ç±»ç»“æœå¯¹æ¯”")
    print("   æ¨¡å‹                    | å‡†ç¡®ç‡    | F1åˆ†æ•°    | ç²¾ç¡®ç‡    | å¬å›ç‡")
    print("   ---------------------- | --------- | --------- | --------- | ---------")
    
    for model_name, metrics in results.items():
        acc = metrics['accuracy']
        f1 = metrics['f1_score']
        prec = metrics['precision']
        rec = metrics['recall']
        print(f"   {model_name:22} | {acc:.4f}    | {f1:.4f}    | {prec:.4f}    | {rec:.4f}")
    
    print("\nâœ… åˆ†ç±»ä»»åŠ¡æ¼”ç¤ºå®Œæˆï¼")
    return results


def demo_regression_task():
    """
    æ¼”ç¤ºCausalEngineåœ¨å›å½’ä»»åŠ¡ä¸­çš„åº”ç”¨
    """
    print("\nğŸ“ˆ å›å½’ä»»åŠ¡æ¼”ç¤º")
    print("=" * 40)
    
    # 1. ç”Ÿæˆå›å½’æ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿå›å½’æ•°æ®")
    
    X, y = make_regression(
        n_samples=1000,
        n_features=15,
        n_informative=10,
        noise=0.1,
        random_state=42
    )
    
    print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   ç›®æ ‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"   è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"   æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\n2. æ¨¡å‹å¯¹æ¯”å®éªŒ")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_size = X_train.shape[1]
    output_size = 1
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train), 
        torch.FloatTensor(y_train)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test), 
        torch.FloatTensor(y_test)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    results = {}
    
    # è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ
    print("   è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ...")
    baseline_model = create_baseline_regressor(input_size, output_size)
    baseline_trainer = BaselineTrainer(baseline_model, device, learning_rate=1e-4, weight_decay=0.01)
    baseline_trainer.train_regression(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹
    baseline_model.eval()
    baseline_preds = []
    baseline_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            outputs = baseline_model(batch_x)
            baseline_preds.extend(outputs.cpu().numpy().flatten())
            baseline_targets.extend(batch_y.numpy())
    
    baseline_metrics = calculate_regression_metrics(
        np.array(baseline_targets), np.array(baseline_preds)
    )
    results['Traditional NN'] = baseline_metrics
    
    # è®­ç»ƒCausalEngine
    print("   è®­ç»ƒCausalEngine...")
    causal_model = create_full_causal_regressor(input_size, output_size)
    causal_trainer = BaselineTrainer(causal_model, device, learning_rate=1e-4, weight_decay=0.01)
    causal_trainer.train_regression(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°CausalEngineï¼ˆå¤šç§æ¨ç†æ¨¡å¼ï¼‰
    causal_model.eval()
    
    for mode, (temp, do_sample) in [
        ("Causal", (0, False)),
        ("Standard", (1.0, False)),
        ("Sampling", (0.8, True))
    ]:
        causal_preds = []
        causal_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                
                if hasattr(causal_model, 'predict'):
                    preds = causal_model.predict(batch_x, temperature=temp, do_sample=do_sample)
                else:
                    preds = causal_model(batch_x, temperature=temp, do_sample=do_sample)
                
                causal_preds.extend(preds.cpu().numpy().flatten())
                causal_targets.extend(batch_y.numpy())
        
        causal_metrics = calculate_regression_metrics(
            np.array(causal_targets), np.array(causal_preds)
        )
        results[f'CausalEngine({mode})'] = causal_metrics
    
    # 4. æ˜¾ç¤ºç»“æœ
    print("\n3. å›å½’ç»“æœå¯¹æ¯”")
    print("   æ¨¡å‹                    | RÂ²        | MAE       | MdAE      | RMSE")
    print("   ---------------------- | --------- | --------- | --------- | ---------")
    
    for model_name, metrics in results.items():
        r2 = metrics['r2']
        mae = metrics['mae']
        mdae = metrics['mdae']
        rmse = metrics['rmse']
        print(f"   {model_name:22} | {r2:.4f}    | {mae:.4f}    | {mdae:.4f}    | {rmse:.4f}")
    
    print("\nâœ… å›å½’ä»»åŠ¡æ¼”ç¤ºå®Œæˆï¼")
    return results


def demo_causality_vs_correlation():
    """
    æ¼”ç¤ºå› æœæ¨ç†vsç›¸å…³æ€§æ¨ç†çš„åŒºåˆ« (åŸºäºæœ€æ–°ç†è®º)
    """
    print("\nğŸ§  å› æœæ¨ç† vs ç›¸å…³æ€§æ¨ç† (2024ç†è®ºæ›´æ–°)")
    print("=" * 60)
    
    print("\nğŸ“ ç†è®ºæ¡†æ¶å¯¹æ¯”:")
    print("  ä¼ ç»Ÿç¥ç»ç½‘ç»œ: P(Y|X) - ç»Ÿè®¡ç›¸å…³æ€§å»ºæ¨¡")
    print("  CausalEngine: Y = f(U, Îµ) - ä¸ªä½“å› æœæœºåˆ¶å»ºæ¨¡")
    
    print("\nğŸ”¬ æ ¸å¿ƒåŒºåˆ«:")
    print("  1. å»ºæ¨¡å¯¹è±¡:")
    print("     ä¼ ç»Ÿ: æ•°æ®çš„ç»Ÿè®¡åˆ†å¸ƒ â†’ æ¨¡å¼è¯†åˆ«")
    print("     å› æœ: ä¸ªä½“çš„å†…åœ¨ç‰¹å¾ â†’ å› æœæ¨ç†")
    
    print("\n  2. ä¸ç¡®å®šæ€§æ¥æº:")
    print("     ä¼ ç»Ÿ: æ•°æ®å™ªå£°å’Œæ¨¡å‹ä¸ç¡®å®šæ€§")
    print("     å› æœ: ä¸ªä½“å·®å¼‚(U)å’Œå¤–ç”Ÿå™ªå£°(Îµ)çš„è§£è€¦")
    
    print("\n  3. æ¨ç†æœºåˆ¶:")
    print("     ä¼ ç»Ÿ: ç›´æ¥æ˜ å°„ X â†’ Y")
    print("     å› æœ: ä¸‰é˜¶æ®µ E â†’ U â†’ S â†’ Y")
    
    print("\nğŸš€ å› æœæ¨ç†çš„é©å‘½æ€§ä¼˜åŠ¿:")
    print("  âœ¨ æ³›åŒ–èƒ½åŠ›: åŸºäºå› æœæœºåˆ¶çš„è·¨åŸŸè¿ç§»")
    print("  âœ¨ å¯è§£é‡Šæ€§: å½’å› -è¡ŒåŠ¨-æ¿€æ´»çš„æ¸…æ™°é“¾æ¡")
    print("  âœ¨ ä¸ç¡®å®šæ€§é‡åŒ–: ä¸ªä½“ä¸ç¡®å®šæ€§ä¸ç¯å¢ƒä¸ç¡®å®šæ€§åˆ†ç¦»")
    print("  âœ¨ åäº‹å®æ¨ç†: æ”¯æŒ'å¦‚æœ...ä¼šæ€æ ·'çš„æ¨ç†")
    print("  âœ¨ èº«ä»½æ¢ç´¢: ç†è§£åŒä¸€ä¸ªä½“åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„è¡¨ç°")
    
    print("\nğŸŒ¡ï¸ å››ç§æ¨ç†æ¨¡å¼çš„æ·±åº¦è§£æ:")
    print("  1. å› æœæ¨¡å¼ (T=0):")
    print("     å“²å­¦: åœ¨æ— å¤–ç”Ÿå¹²æ‰°ä¸‹ä¸ªä½“çš„å¿…ç„¶é€‰æ‹©")
    print("     åº”ç”¨: ç¡®å®šæ€§å†³ç­–ã€ç¡¬åˆ†ç±»ã€ç‚¹ä¼°è®¡")
    
    print("\n  2. æ ‡å‡†æ¨¡å¼ (T>0, do_sample=False):")
    print("     å“²å­¦: æ‰¿è®¤ç¯å¢ƒä¸ç¡®å®šæ€§å¯¹å†³ç­–çš„å½±å“")
    print("     åº”ç”¨: ä¸ç¡®å®šæ€§é‡åŒ–ã€è½¯å†³ç­–ã€ç½®ä¿¡åŒºé—´")
    
    print("\n  3. é‡‡æ ·æ¨¡å¼ (T>0, do_sample=True):")
    print("     å“²å­¦: æ¢ç´¢ä¸ªä½“åœ¨éšæœºæ‰°åŠ¨ä¸‹çš„å¤šæ ·è¡¨ç°")
    print("     åº”ç”¨: åˆ›é€ æ€§ç”Ÿæˆã€å¤šæ ·æ€§æ¢ç´¢ã€è’™ç‰¹å¡æ´›")
    
    print("\n  4. å…¼å®¹æ¨¡å¼:")
    print("     å“²å­¦: ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹é½ï¼Œä¾¿äºæ€§èƒ½æ¯”è¾ƒ")
    print("     åº”ç”¨: åŸºå‡†æµ‹è¯•ã€æ¸è¿›å¼è¿ç§»")
    
    print("\nğŸ¯ åŸºå‡†æµ‹è¯•åè®®ä¸­çš„åº”ç”¨:")
    print("  å›ºå®šå™ªå£°å®éªŒ: ç†è§£å™ªå£°å¼ºåº¦å¯¹æ€§èƒ½çš„å½±å“")
    print("  è‡ªé€‚åº”å™ªå£°å®éªŒ: éªŒè¯æ¨¡å‹è‡ªä¸»å­¦ä¹ å™ªå£°çš„èƒ½åŠ›")
    print("  å››æ¨¡å¼å¯¹æ¯”: è¯„ä¼°ä¸åŒæ¨ç†æ¨¡å¼çš„é€‚ç”¨åœºæ™¯")
    print("  æ¶æ„æ¶ˆè: é‡åŒ–å› æœæ¶æ„ç›¸å¯¹ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿")


def visualize_results(classification_results, regression_results):
    """
    å¯è§†åŒ–æ¼”ç¤ºç»“æœ
    """
    print("\nğŸ“Š ç”Ÿæˆç»“æœå¯è§†åŒ–å›¾è¡¨")
    
    # Setup plotting style
    plt.style.use('default')
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Classification results visualization
    if classification_results:
        models = list(classification_results.keys())
        accuracies = [classification_results[model]['accuracy'] for model in models]
        f1_scores = [classification_results[model]['f1_score'] for model in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        ax1.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)
        ax1.bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.8)
        
        ax1.set_xlabel('Model')
        ax1.set_ylabel('Score')
        ax1.set_title('Classification Performance Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # Regression results visualization  
    if regression_results:
        models = list(regression_results.keys())
        r2_scores = [regression_results[model]['r2'] for model in models]
        mae_scores = [regression_results[model]['mae'] for model in models]
        
        x = np.arange(len(models))
        
        ax2_twin = ax2.twinx()
        
        bars1 = ax2.bar(x - 0.2, r2_scores, 0.4, label='RÂ²', alpha=0.8, color='blue')
        bars2 = ax2_twin.bar(x + 0.2, mae_scores, 0.4, label='MAE', alpha=0.8, color='red')
        
        ax2.set_xlabel('Model')
        ax2.set_ylabel('RÂ²', color='blue')
        ax2_twin.set_ylabel('MAE', color='red')
        ax2.set_title('Regression Performance Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(models, rotation=45, ha='right')
        
        # Combine legends
        lines1, labels1 = ax2.get_legend_handles_labels()
        lines2, labels2 = ax2_twin.get_legend_handles_labels()
        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save chart
    output_dir = "tutorials/00_getting_started"
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(f"{output_dir}/basic_usage_results.png", dpi=300, bbox_inches='tight')
    plt.close()  # Close instead of show to avoid blocking
    
    print(f"   å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}/basic_usage_results.png")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º
    """
    print("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ CausalEngine åŸºç¡€æ•™ç¨‹ï¼")
    print("è¿™ä¸ªæ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CausalEngine è¿›è¡Œå› æœæ¨ç†")
    print("=" * 60)
    
    # 1. åŸºç¡€APIæ¼”ç¤º (æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºéœ€è¦çœŸå®çš„CausalEngine)
    # demo_basic_causal_engine()
    print("\nâš ï¸  è·³è¿‡åŸºç¡€APIæ¼”ç¤ºï¼ˆéœ€è¦å®Œæ•´çš„CausalEngineå®ç°ï¼‰")
    
    # 2. å› æœvsç›¸å…³æ€§ç†è®ºè¯´æ˜
    demo_causality_vs_correlation()
    
    # 3. åˆ†ç±»ä»»åŠ¡æ¼”ç¤º
    classification_results = demo_classification_task()
    
    # 4. å›å½’ä»»åŠ¡æ¼”ç¤º
    regression_results = demo_regression_task()
    
    # 5. ç»“æœå¯è§†åŒ–
    visualize_results(classification_results, regression_results)
    
    # 6. æ€»ç»“
    print("\nğŸ‰ CausalEngineåŸºç¡€æ•™ç¨‹å®Œæˆï¼(2024æ›´æ–°ç‰ˆ)")
    print("=" * 60)
    
    print("\nğŸ“Š æœ¬æ•™ç¨‹æ¶µç›–å†…å®¹:")
    print("  âœ… å› æœæ¨ç†vsç›¸å…³æ€§æ¨ç†çš„ç†è®ºå¯¹æ¯”")
    print("  âœ… å››ç§æ¨ç†æ¨¡å¼çš„å®é™…åº”ç”¨")
    print("  âœ… åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¼”ç¤º")
    print("  âœ… åŸºäºåŸºå‡†åè®®çš„æ ‡å‡†åŒ–é…ç½®")
    
    print("\nğŸ“– æ¨èå­¦ä¹ è·¯å¾„ (åŸºäºæœ€æ–°æ•™ç¨‹ä½“ç³»):")
    print("  1. ç†è®ºæ·±å…¥: tutorials/00_getting_started/theoretical_foundations.py")
    print("  2. åŸºå‡†åè®®: tutorials/00_getting_started/benchmark_protocol_intro.py")
    print("  3. åˆ†ç±»åº”ç”¨: tutorials/01_classification/ (åŸºäºæ–°å®éªŒè®¾è®¡)")
    print("  4. å›å½’åº”ç”¨: tutorials/02_regression/ (åŸºäºæ–°å®éªŒè®¾è®¡)")
    print("  5. æ¶ˆèå®éªŒ: tutorials/03_ablation_studies/ (å›ºå®švsè‡ªé€‚åº”å™ªå£°)")
    print("  6. é«˜çº§ä¸»é¢˜: tutorials/04_advanced_topics/ (å››ç§æ¨ç†æ¨¡å¼)")
    
    print("\nğŸ”— æ ¸å¿ƒèµ„æº (æœ€æ–°ç‰ˆæœ¬):")
    print("  ğŸ“ æ•°å­¦åŸºç¡€: causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md")
    print("  ğŸ§ª å®éªŒåè®®: causal_engine/misc/benchmark_strategy.md")
    print("  ğŸ“Š é¡¹ç›®æ€»è§ˆ: causal_engine/ONE_PAGER.md")
    print("  ğŸ—ï¸ æ¶æ„æ–‡æ¡£: causal_engine/README.md")
    
    print("\nğŸ¯ å…³é”®æ”¶è·:")
    print("  ğŸ§  CausalEngineåŸºäºä¸ªä½“é€‰æ‹©å˜é‡å®ç°çœŸæ­£çš„å› æœæ¨ç†")
    print("  ğŸ² æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§ç¨³å®šæ€§æä¾›è§£æä¸ç¡®å®šæ€§ä¼ æ’­")
    print("  ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°å®ç°ä»ç¡®å®šæ€§åˆ°éšæœºæ€§çš„ç»Ÿä¸€è°ƒåˆ¶")
    print("  ğŸ”„ ä¸‰é˜¶æ®µæ¶æ„æä¾›æ¸…æ™°å¯è§£é‡Šçš„æ¨ç†é“¾æ¡")
    print("  ğŸ“ˆ åŸºå‡†æµ‹è¯•åè®®ç¡®ä¿ç§‘å­¦ä¸¥è°¨çš„æ€§èƒ½è¯„ä¼°")


if __name__ == "__main__":
    main()
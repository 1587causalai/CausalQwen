"""
CausalEngine åŸºç¡€ä½¿ç”¨æ•™ç¨‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨CausalEngineè¿›è¡Œç®€å•çš„åˆ†ç±»å’Œå›å½’ä»»åŠ¡

è¿™æ˜¯æ‚¨å¼€å§‹ä½¿ç”¨CausalEngineçš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼
"""

import sys
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥CausalEngine
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

try:
    from src.causal_qwen_mvp.causal_engine import CausalEngine
    CAUSAL_ENGINE_AVAILABLE = True
    print("âœ… CausalEngineæ¨¡å—åŠ è½½æˆåŠŸï¼")
except ImportError:
    try:
        from causal_engine import CausalEngine
        CAUSAL_ENGINE_AVAILABLE = True
        print("âœ… CausalEngineæ¨¡å—åŠ è½½æˆåŠŸï¼")
    except ImportError:
        CAUSAL_ENGINE_AVAILABLE = False
        print("âš ï¸  CausalEngineæ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¼”ç¤ºç‰ˆæœ¬")
        
        # å®šä¹‰ä¸€ä¸ªæ¨¡æ‹Ÿçš„CausalEngineç±»
        class CausalEngine:
            def __init__(self, **kwargs):
                self.hidden_size = kwargs.get('hidden_size', 128)
                self.causal_size = kwargs.get('causal_size', 128)
                
            def __call__(self, hidden_states, temperature=1.0, do_sample=False):
                batch_size, seq_length, _ = hidden_states.shape
                vocab_size = 10
                
                class Output:
                    def __init__(self):
                        self.logits = torch.randn(batch_size, seq_length, vocab_size)
                
                return Output()

from tutorials.utils.ablation_networks import (
    create_ablation_experiment, AblationTrainer,
    create_ablated_classifier, create_ablated_regressor,
    create_full_causal_classifier, create_full_causal_regressor
)
from tutorials.utils.baseline_networks import (
    TraditionalMLPClassifier, TraditionalMLPRegressor, BaselineTrainer
)

# æ·»åŠ è¾…åŠ©å‡½æ•°
def create_baseline_classifier(input_size, num_classes, **kwargs):
    return TraditionalMLPClassifier(input_size=input_size, num_classes=num_classes, hidden_sizes=[128, 64], dropout_rate=0.1)

def create_baseline_regressor(input_size, output_size, **kwargs):
    return TraditionalMLPRegressor(input_size=input_size, output_size=output_size, hidden_sizes=[128, 64], dropout_rate=0.1)
from tutorials.utils.evaluation_metrics import (
    calculate_classification_metrics, calculate_regression_metrics
)


def demo_basic_causal_engine():
    """
    æ¼”ç¤ºCausalEngineçš„åŸºç¡€APIä½¿ç”¨
    """
    print("\nğŸŒŸ CausalEngine åŸºç¡€APIæ¼”ç¤º")
    print("=" * 40)
    
    if not CAUSAL_ENGINE_AVAILABLE:
        print("ç”±äºCausalEngineæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æ¼”ç¤º")
        return
    
    # 1. åˆ›å»ºCausalEngineå®ä¾‹
    print("\n1. åˆ›å»ºCausalEngineå®ä¾‹")
    
    engine = CausalEngine(
        hidden_size=128,        # éšè—å±‚å¤§å°
        vocab_size=10,          # è¾“å‡ºè¯æ±‡è¡¨å¤§å°ï¼ˆåˆ†ç±»ç±»åˆ«æ•°ï¼‰
        causal_size=128,        # å› æœè¡¨ç¤ºå¤§å°
        activation_modes="classification"  # æ¿€æ´»æ¨¡å¼
    )
    
    print(f"   éšè—å±‚å¤§å°: {engine.hidden_size}")
    print(f"   å› æœè¡¨ç¤ºå¤§å°: {engine.causal_size}")
    print(f"   æ¿€æ´»æ¨¡å¼: classification")
    
    # 2. å‡†å¤‡è¾“å…¥æ•°æ®
    print("\n2. å‡†å¤‡è¾“å…¥æ•°æ®")
    
    batch_size = 16
    seq_length = 5
    hidden_size = 128
    
    # æ¨¡æ‹Ÿtransformerçš„éšè—çŠ¶æ€è¾“å‡º
    hidden_states = torch.randn(batch_size, seq_length, hidden_size)
    print(f"   è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"   è¾“å…¥ç±»å‹: æ¨¡æ‹Ÿçš„transformeréšè—çŠ¶æ€")
    
    # 3. è¿›è¡Œå› æœæ¨ç†
    print("\n3. å› æœæ¨ç†è¿‡ç¨‹")
    
    # çº¯å› æœæ¨ç†ï¼ˆæ¸©åº¦=0ï¼‰
    print("   æ¨¡å¼1: çº¯å› æœæ¨ç† (æ¸©åº¦=0)")
    output_causal = engine(hidden_states, temperature=0, do_sample=False)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_causal.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: ç¡®å®šæ€§å› æœæ¨ç†ç»“æœ")
    
    # å¸¦ä¸ç¡®å®šæ€§çš„æ¨ç†ï¼ˆæ¸©åº¦>0ï¼‰
    print("   æ¨¡å¼2: å¸¦ä¸ç¡®å®šæ€§æ¨ç† (æ¸©åº¦=1.0)")
    output_uncertain = engine(hidden_states, temperature=1.0, do_sample=False)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_uncertain.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: ä¸ç¡®å®šæ€§é‡åŒ–ç»“æœ")
    
    # é‡‡æ ·æ¨¡å¼ï¼ˆèº«ä»½æ¢ç´¢ï¼‰
    print("   æ¨¡å¼3: é‡‡æ ·æ¨¡å¼ (æ¸©åº¦=0.8, é‡‡æ ·=True)")
    output_sampling = engine(hidden_states, temperature=0.8, do_sample=True)
    print(f"     è¾“å‡ºå½¢çŠ¶: {output_sampling.logits.shape}")
    print(f"     è¾“å‡ºç±»å‹: èº«ä»½æ¢ç´¢é‡‡æ ·ç»“æœ")
    
    # 4. åˆ†æè¾“å‡ºå·®å¼‚
    print("\n4. ä¸åŒæ¨ç†æ¨¡å¼çš„è¾“å‡ºå¯¹æ¯”")
    
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å‡ºè¿›è¡Œå¯¹æ¯”
    sample_idx = 0
    seq_idx = 0
    
    causal_probs = torch.softmax(output_causal.logits[sample_idx, seq_idx], dim=-1)
    uncertain_probs = torch.softmax(output_uncertain.logits[sample_idx, seq_idx], dim=-1)
    sampling_probs = torch.softmax(output_sampling.logits[sample_idx, seq_idx], dim=-1)
    
    print(f"   çº¯å› æœæ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {causal_probs.max().item():.4f}")
    print(f"   ä¸ç¡®å®šæ€§æ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {uncertain_probs.max().item():.4f}")
    print(f"   é‡‡æ ·æ¨¡å¼ - æœ€å¤§æ¦‚ç‡: {sampling_probs.max().item():.4f}")
    
    # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒçš„ç†µï¼ˆä¸ç¡®å®šæ€§åº¦é‡ï¼‰
    def entropy(probs):
        return -(probs * torch.log(probs + 1e-8)).sum()
    
    print(f"   çº¯å› æœæ¨¡å¼ - ç†µ: {entropy(causal_probs).item():.4f}")
    print(f"   ä¸ç¡®å®šæ€§æ¨¡å¼ - ç†µ: {entropy(uncertain_probs).item():.4f}")
    print(f"   é‡‡æ ·æ¨¡å¼ - ç†µ: {entropy(sampling_probs).item():.4f}")
    
    print("\nâœ… CausalEngineåŸºç¡€APIæ¼”ç¤ºå®Œæˆï¼")


def demo_classification_task():
    """
    æ¼”ç¤ºCausalEngineåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨
    """
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡æ¼”ç¤º")
    print("=" * 40)
    
    # 1. ç”Ÿæˆåˆ†ç±»æ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿåˆ†ç±»æ•°æ®")
    
    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=3,
        random_state=42
    )
    
    print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   ç±»åˆ«æ•°: {len(np.unique(y))}")
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    print(f"   è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"   æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\n2. æ¨¡å‹å¯¹æ¯”å®éªŒ")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_size = X_train.shape[1]
    num_classes = len(np.unique(y))
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train), 
        torch.LongTensor(y_train)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test), 
        torch.LongTensor(y_test)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    results = {}
    
    # è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ
    print("   è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ...")
    baseline_model = create_baseline_classifier(input_size, num_classes)
    baseline_trainer = BaselineTrainer(baseline_model, device, learning_rate=0.001)
    baseline_trainer.train_classification(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹
    baseline_model.eval()
    baseline_preds = []
    baseline_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            outputs = baseline_model(batch_x)
            preds = torch.argmax(outputs, dim=-1)
            baseline_preds.extend(preds.cpu().numpy())
            baseline_targets.extend(batch_y.numpy())
    
    baseline_metrics = calculate_classification_metrics(
        np.array(baseline_targets), np.array(baseline_preds)
    )
    results['Traditional NN'] = baseline_metrics
    
    # è®­ç»ƒCausalEngine
    print("   è®­ç»ƒCausalEngine...")
    causal_model = create_full_causal_classifier(input_size, num_classes)
    causal_trainer = BaselineTrainer(causal_model, device, learning_rate=0.001)
    causal_trainer.train_classification(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°CausalEngineï¼ˆå¤šç§æ¨ç†æ¨¡å¼ï¼‰
    causal_model.eval()
    
    for mode, (temp, do_sample) in [
        ("Causal", (0, False)),
        ("Standard", (1.0, False)),
        ("Sampling", (0.8, True))
    ]:
        causal_preds = []
        causal_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                
                if hasattr(causal_model, 'predict'):
                    preds = causal_model.predict(batch_x, temperature=temp, do_sample=do_sample)
                else:
                    outputs = causal_model(batch_x, temperature=temp, do_sample=do_sample)
                    preds = torch.argmax(outputs, dim=-1)
                
                causal_preds.extend(preds.cpu().numpy())
                causal_targets.extend(batch_y.numpy())
        
        causal_metrics = calculate_classification_metrics(
            np.array(causal_targets), np.array(causal_preds)
        )
        results[f'CausalEngine({mode})'] = causal_metrics
    
    # 4. æ˜¾ç¤ºç»“æœ
    print("\n3. åˆ†ç±»ç»“æœå¯¹æ¯”")
    print("   æ¨¡å‹                    | å‡†ç¡®ç‡    | F1åˆ†æ•°    | ç²¾ç¡®ç‡    | å¬å›ç‡")
    print("   ---------------------- | --------- | --------- | --------- | ---------")
    
    for model_name, metrics in results.items():
        acc = metrics['accuracy']
        f1 = metrics['f1_score']
        prec = metrics['precision']
        rec = metrics['recall']
        print(f"   {model_name:22} | {acc:.4f}    | {f1:.4f}    | {prec:.4f}    | {rec:.4f}")
    
    print("\nâœ… åˆ†ç±»ä»»åŠ¡æ¼”ç¤ºå®Œæˆï¼")
    return results


def demo_regression_task():
    """
    æ¼”ç¤ºCausalEngineåœ¨å›å½’ä»»åŠ¡ä¸­çš„åº”ç”¨
    """
    print("\nğŸ“ˆ å›å½’ä»»åŠ¡æ¼”ç¤º")
    print("=" * 40)
    
    # 1. ç”Ÿæˆå›å½’æ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿå›å½’æ•°æ®")
    
    X, y = make_regression(
        n_samples=1000,
        n_features=15,
        n_informative=10,
        noise=0.1,
        random_state=42
    )
    
    print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"   ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"   ç›®æ ‡èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train = scaler_X.fit_transform(X_train)
    X_test = scaler_X.transform(X_test)
    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"   è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"   æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
    print("\n2. æ¨¡å‹å¯¹æ¯”å®éªŒ")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_size = X_train.shape[1]
    output_size = 1
    
    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train), 
        torch.FloatTensor(y_train)
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test), 
        torch.FloatTensor(y_test)
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
    
    results = {}
    
    # è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ
    print("   è®­ç»ƒä¼ ç»Ÿç¥ç»ç½‘ç»œ...")
    baseline_model = create_baseline_regressor(input_size, output_size)
    baseline_trainer = BaselineTrainer(baseline_model, device, learning_rate=0.001)
    baseline_trainer.train_regression(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°ä¼ ç»Ÿæ¨¡å‹
    baseline_model.eval()
    baseline_preds = []
    baseline_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            outputs = baseline_model(batch_x)
            baseline_preds.extend(outputs.cpu().numpy().flatten())
            baseline_targets.extend(batch_y.numpy())
    
    baseline_metrics = calculate_regression_metrics(
        np.array(baseline_targets), np.array(baseline_preds)
    )
    results['Traditional NN'] = baseline_metrics
    
    # è®­ç»ƒCausalEngine
    print("   è®­ç»ƒCausalEngine...")
    causal_model = create_full_causal_regressor(input_size, output_size)
    causal_trainer = BaselineTrainer(causal_model, device, learning_rate=0.001)
    causal_trainer.train_regression(train_loader, test_loader, num_epochs=50)
    
    # è¯„ä¼°CausalEngineï¼ˆå¤šç§æ¨ç†æ¨¡å¼ï¼‰
    causal_model.eval()
    
    for mode, (temp, do_sample) in [
        ("Causal", (0, False)),
        ("Standard", (1.0, False)),
        ("Sampling", (0.8, True))
    ]:
        causal_preds = []
        causal_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(device)
                
                if hasattr(causal_model, 'predict'):
                    preds = causal_model.predict(batch_x, temperature=temp, do_sample=do_sample)
                else:
                    preds = causal_model(batch_x, temperature=temp, do_sample=do_sample)
                
                causal_preds.extend(preds.cpu().numpy().flatten())
                causal_targets.extend(batch_y.numpy())
        
        causal_metrics = calculate_regression_metrics(
            np.array(causal_targets), np.array(causal_preds)
        )
        results[f'CausalEngine({mode})'] = causal_metrics
    
    # 4. æ˜¾ç¤ºç»“æœ
    print("\n3. å›å½’ç»“æœå¯¹æ¯”")
    print("   æ¨¡å‹                    | RÂ²        | MAE       | MdAE      | RMSE")
    print("   ---------------------- | --------- | --------- | --------- | ---------")
    
    for model_name, metrics in results.items():
        r2 = metrics['r2']
        mae = metrics['mae']
        mdae = metrics['mdae']
        rmse = metrics['rmse']
        print(f"   {model_name:22} | {r2:.4f}    | {mae:.4f}    | {mdae:.4f}    | {rmse:.4f}")
    
    print("\nâœ… å›å½’ä»»åŠ¡æ¼”ç¤ºå®Œæˆï¼")
    return results


def demo_causality_vs_correlation():
    """
    æ¼”ç¤ºå› æœæ¨ç†vsç›¸å…³æ€§æ¨ç†çš„åŒºåˆ«
    """
    print("\nğŸ§  å› æœæ¨ç† vs ç›¸å…³æ€§æ¨ç†")
    print("=" * 40)
    
    print("\nç†è®ºè¯´æ˜:")
    print("â€¢ ä¼ ç»Ÿç¥ç»ç½‘ç»œ: åŸºäºç»Ÿè®¡ç›¸å…³æ€§è¿›è¡Œé¢„æµ‹")
    print("â€¢ CausalEngine: åŸºäºå› æœå…³ç³»è¿›è¡Œæ¨ç†")
    print("\nå…³é”®åŒºåˆ«:")
    print("1. ä¼ ç»Ÿæ–¹æ³•: P(Y|X) - ç»™å®šè¾“å…¥Xï¼Œé¢„æµ‹è¾“å‡ºYçš„æ¦‚ç‡")
    print("2. å› æœæ–¹æ³•: Y = f(U, Îµ) - ä¸ªä½“ç‰¹å¾U + å› æœæ³•åˆ™f + å¤–ç”Ÿå™ªå£°Îµ")
    print("\nå› æœæ¨ç†çš„ä¼˜åŠ¿:")
    print("â€¢ æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›")
    print("â€¢ å¯è§£é‡Šçš„å†³ç­–è¿‡ç¨‹")
    print("â€¢ é²æ£’çš„ä¸ç¡®å®šæ€§é‡åŒ–")
    print("â€¢ æ”¯æŒåäº‹å®æ¨ç†")
    
    print("\næ¸©åº¦å‚æ•°çš„ä½œç”¨:")
    print("â€¢ æ¸©åº¦ = 0: çº¯ç¡®å®šæ€§å› æœæ¨ç†")
    print("â€¢ æ¸©åº¦ > 0: å¼•å…¥è®¤è¯†ä¸ç¡®å®šæ€§")
    print("â€¢ do_sample = True: æ¢ç´¢ä¸ªä½“èº«ä»½ç©ºé—´")
    
    print("\nå››ç§æ¨ç†æ¨¡å¼:")
    print("1. Causal (T=0, any): çº¯å› æœæ¨ç†")
    print("2. Standard (T>0, do_sample=False): å¸¦ä¸ç¡®å®šæ€§çš„å†³ç­–")
    print("3. Sampling (T>0, do_sample=True): èº«ä»½æ¢ç´¢")
    print("4. Compatible: å…¼å®¹ä¼ ç»Ÿæ¨¡å¼")


def visualize_results(classification_results, regression_results):
    """
    å¯è§†åŒ–æ¼”ç¤ºç»“æœ
    """
    print("\nğŸ“Š ç”Ÿæˆç»“æœå¯è§†åŒ–å›¾è¡¨")
    
    # Setup plotting style
    plt.style.use('default')
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Classification results visualization
    if classification_results:
        models = list(classification_results.keys())
        accuracies = [classification_results[model]['accuracy'] for model in models]
        f1_scores = [classification_results[model]['f1_score'] for model in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        ax1.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)
        ax1.bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.8)
        
        ax1.set_xlabel('Model')
        ax1.set_ylabel('Score')
        ax1.set_title('Classification Performance Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
    
    # Regression results visualization  
    if regression_results:
        models = list(regression_results.keys())
        r2_scores = [regression_results[model]['r2'] for model in models]
        mae_scores = [regression_results[model]['mae'] for model in models]
        
        x = np.arange(len(models))
        
        ax2_twin = ax2.twinx()
        
        bars1 = ax2.bar(x - 0.2, r2_scores, 0.4, label='RÂ²', alpha=0.8, color='blue')
        bars2 = ax2_twin.bar(x + 0.2, mae_scores, 0.4, label='MAE', alpha=0.8, color='red')
        
        ax2.set_xlabel('Model')
        ax2.set_ylabel('RÂ²', color='blue')
        ax2_twin.set_ylabel('MAE', color='red')
        ax2.set_title('Regression Performance Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(models, rotation=45, ha='right')
        
        # Combine legends
        lines1, labels1 = ax2.get_legend_handles_labels()
        lines2, labels2 = ax2_twin.get_legend_handles_labels()
        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save chart
    output_dir = "tutorials/00_getting_started"
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(f"{output_dir}/basic_usage_results.png", dpi=300, bbox_inches='tight')
    plt.close()  # Close instead of show to avoid blocking
    
    print(f"   å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}/basic_usage_results.png")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º
    """
    print("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ CausalEngine åŸºç¡€æ•™ç¨‹ï¼")
    print("è¿™ä¸ªæ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CausalEngine è¿›è¡Œå› æœæ¨ç†")
    print("=" * 60)
    
    # 1. åŸºç¡€APIæ¼”ç¤º (æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºéœ€è¦çœŸå®çš„CausalEngine)
    # demo_basic_causal_engine()
    print("\nâš ï¸  è·³è¿‡åŸºç¡€APIæ¼”ç¤ºï¼ˆéœ€è¦å®Œæ•´çš„CausalEngineå®ç°ï¼‰")
    
    # 2. å› æœvsç›¸å…³æ€§ç†è®ºè¯´æ˜
    demo_causality_vs_correlation()
    
    # 3. åˆ†ç±»ä»»åŠ¡æ¼”ç¤º
    classification_results = demo_classification_task()
    
    # 4. å›å½’ä»»åŠ¡æ¼”ç¤º
    regression_results = demo_regression_task()
    
    # 5. ç»“æœå¯è§†åŒ–
    visualize_results(classification_results, regression_results)
    
    # 6. æ€»ç»“
    print("\nğŸ‰ åŸºç¡€æ•™ç¨‹å®Œæˆï¼")
    print("\nğŸ“– ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:")
    print("1. æŸ¥çœ‹ tutorials/01_classification/ äº†è§£æ›´å¤šåˆ†ç±»ä»»åŠ¡")
    print("2. æŸ¥çœ‹ tutorials/02_regression/ äº†è§£æ›´å¤šå›å½’ä»»åŠ¡")
    print("3. æŸ¥çœ‹ tutorials/03_ablation_studies/ äº†è§£æ¶ˆèå®éªŒ")
    print("4. é˜…è¯» causal_engine/MATHEMATICAL_FOUNDATIONS.md äº†è§£æ•°å­¦åŸç†")
    
    print("\nğŸ”— ç›¸å…³èµ„æº:")
    print("â€¢ é¡¹ç›®æ–‡æ¡£: causal_engine/README.md")
    print("â€¢ æ•°å­¦ç†è®º: causal_engine/MATHEMATICAL_FOUNDATIONS.md")
    print("â€¢ æ¶æ„è¯´æ˜: causal_engine/ONE_PAGER.md")


if __name__ == "__main__":
    main()
"""
CausalEngine åˆ†ç±»ä»»åŠ¡åŸºå‡†æµ‹è¯•æ¼”ç¤º (2024æ›´æ–°ç‰ˆ)
==============================================

åŸºäºå®˜æ–¹åŸºå‡†æµ‹è¯•åè®® (causal_engine/misc/benchmark_strategy.md) çš„å®Œæ•´åˆ†ç±»ä»»åŠ¡æ¼”ç¤º
åŒ…å«å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°çš„å¯¹æ¯”å®éªŒå’Œå››ç§æ¨ç†æ¨¡å¼çš„è¯„ä¼°

å®éªŒè®¾è®¡:
- å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ (b_noise âˆˆ [0.0, 0.1, 1.0, 10.0])
- å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ (b_noiseå¯å­¦ä¹ )
- å››ç§æ¨ç†æ¨¡å¼: å› æœã€æ ‡å‡†ã€é‡‡æ ·ã€å…¼å®¹
- æ ‡å‡†åŒ–è¶…å‚æ•°: AdamW, lr=1e-4, early stopping

æ•°æ®é›†: Adult Income (æ”¶å…¥é¢„æµ‹)
"""

import sys
import os
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from typing import Dict, List, Tuple

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from causal_engine import CausalEngine


def load_adult_dataset():
    """
    åŠ è½½å’Œé¢„å¤„ç†Adult Incomeæ•°æ®é›†
    """
    print("ğŸ“Š åŠ è½½Adult Incomeæ•°æ®é›†")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®é›†
    data = fetch_openml('adult', version=2, as_frame=True, parser='auto')
    df = data.frame
    
    # é¢„å¤„ç†ï¼šæ›¿æ¢ '?' ä¸º NaN å¹¶åˆ é™¤å«æœ‰ NaN çš„è¡Œ
    # åœ¨ fetch_openml(parser='auto') ä¸­ï¼Œåˆ†ç±»åˆ—ä¸­çš„ '?' å·²ç»è¢«å¤„ç†ä¸º pd.NA
    df.replace('?', pd.NA, inplace=True)
    df.dropna(inplace=True)
    
    X = df[data.feature_names]
    y = df['class']

    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ:\\n{y.value_counts()}")
    
    # ç¼–ç ç›®æ ‡å˜é‡
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # åˆ’åˆ†æ•°æ®é›† (åœ¨é¢„å¤„ç†ä¹‹å‰ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y_encoded, test_size=0.4, random_state=42, stratify=y_encoded
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    # è¯†åˆ«åˆ—ç±»å‹
    categorical_cols = X_train.select_dtypes(include=['category', 'object']).columns
    numerical_cols = X_train.select_dtypes(include=np.number).columns
    
    # å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
    X_val[numerical_cols] = scaler.transform(X_val[numerical_cols])
    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])
    
    # å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç‹¬çƒ­ç¼–ç 
    X_train = pd.get_dummies(X_train, columns=categorical_cols, dummy_na=False)
    X_val = pd.get_dummies(X_val, columns=categorical_cols, dummy_na=False)
    X_test = pd.get_dummies(X_test, columns=categorical_cols, dummy_na=False)
    
    # å¯¹é½åˆ—ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®é›†çš„åˆ—ä¸€è‡´
    train_cols = X_train.columns
    X_val = X_val.reindex(columns=train_cols, fill_value=0)
    X_test = X_test.reindex(columns=train_cols, fill_value=0)

    print(f"é¢„å¤„ç†åç‰¹å¾æ•°: {X_train.shape[1]}")
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"éªŒè¯é›†å¤§å°: {X_val.shape[0]}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    return {
        'X_train': X_train.values.astype(np.float32),
        'X_val': X_val.values.astype(np.float32),
        'X_test': X_test.values.astype(np.float32),
        'y_train': y_train.astype(np.int64),
        'y_val': y_val.astype(np.int64),
        'y_test': y_test.astype(np.int64),
        'feature_names': list(X_train.columns),
        'input_size': X_train.shape[1],
        'num_classes': len(np.unique(y_encoded)),
        'class_names': list(le.classes_)
    }


def create_data_loaders(data_dict, batch_size=64):
    """
    åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨
    """
    train_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_train']),
        torch.LongTensor(data_dict['y_train'])
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_val']),
        torch.LongTensor(data_dict['y_val'])
    )
    test_dataset = TensorDataset(
        torch.FloatTensor(data_dict['X_test']),
        torch.LongTensor(data_dict['y_test'])
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader, test_loader


class TraditionalMLPClassifier(nn.Module):
    """
    ä¼ ç»ŸMLPåˆ†ç±»å™¨ (åŸºå‡†å¯¹æ¯”)
    """
    def __init__(self, input_size, num_classes, hidden_sizes=[128, 64]):
        super().__init__()
        
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, num_classes))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


class CausalEngineClassifier(nn.Module):
    """
    åŸºäºCausalEngineçš„åˆ†ç±»å™¨
    """
    def __init__(self, input_size, num_classes, causal_size=None, b_noise_learnable=True):
        super().__init__()
        
        if causal_size is None:
            causal_size = input_size
        
        self.causal_engine = CausalEngine(
            hidden_size=input_size,
            vocab_size=num_classes,
            causal_size=causal_size,
            activation_modes="classification"
        )
        
        # æ§åˆ¶å™ªå£°å‚æ•°æ˜¯å¦å¯å­¦ä¹ 
        if hasattr(self.causal_engine.action, 'b_noise'):
            self.causal_engine.action.b_noise.requires_grad = b_noise_learnable
    
    def forward(self, x, temperature=1.0, do_sample=False):
        # æ·»åŠ åºåˆ—ç»´åº¦
        if x.dim() == 2:
            x = x.unsqueeze(1)  # (batch, seq=1, features)
        
        output = self.causal_engine(
            hidden_states=x,
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True,
            apply_activation=True
        )
        
        return output['output'].squeeze(1)  # ç§»é™¤åºåˆ—ç»´åº¦
    
    def set_fixed_noise(self, noise_value):
        """è®¾ç½®å›ºå®šçš„å™ªå£°å€¼"""
        if hasattr(self.causal_engine.action, 'b_noise'):
            self.causal_engine.action.b_noise.requires_grad = False
            self.causal_engine.action.b_noise.data.fill_(noise_value)


def train_model(model, train_loader, val_loader, device, num_epochs=100, learning_rate=1e-4):
    """
    è®­ç»ƒæ¨¡å‹ (åŸºäºåŸºå‡†åè®®çš„æ ‡å‡†åŒ–é…ç½®)
    """
    model = model.to(device)
    
    # åŸºå‡†åè®®æ ‡å‡†åŒ–é…ç½®
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦: å‰10%çš„stepsçº¿æ€§warm-upï¼Œç„¶åcosine decay
    total_steps = len(train_loader) * num_epochs
    warmup_steps = int(0.1 * total_steps)
    
    def lr_schedule(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1 + np.cos(np.pi * progress))
    
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)
    criterion = nn.CrossEntropyLoss()
    
    # æ—©åœé…ç½®
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    train_losses = []
    val_losses = []
    val_accuracies = []
    
    print(f"è®­ç»ƒé…ç½®: epochs={num_epochs}, lr={learning_rate}, weight_decay=0.01")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            epoch_train_loss += loss.item()
        
        avg_train_loss = epoch_train_loss / len(train_loader)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        epoch_val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                
                epoch_val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
        
        avg_val_loss = epoch_val_loss / len(val_loader)
        val_accuracy = correct / total
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_accuracy)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:3d}: train_loss={avg_train_loss:.4f}, "
                  f"val_loss={avg_val_loss:.4f}, val_acc={val_accuracy:.4f}")
        
        # æ—©åœæ£€æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"æ—©åœäºç¬¬ {epoch+1} è½®")
            break
    
    return {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'best_val_loss': best_val_loss
    }


def evaluate_model(model, test_loader, device, temperature=1.0, do_sample=False):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            
            if hasattr(model, 'causal_engine'):
                # CausalEngineæ¨¡å‹
                outputs = model(batch_x, temperature=temperature, do_sample=do_sample)
            else:
                # ä¼ ç»Ÿæ¨¡å‹
                outputs = model(batch_x)
            
            _, predicted = torch.max(outputs, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(batch_y.numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_targets, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted'
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': all_predictions,
        'targets': all_targets
    }


def run_fixed_noise_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ
    """
    print("\nğŸ§ª å®éªŒç»„A: å›ºå®šå™ªå£°å¼ºåº¦å®éªŒ")
    print("=" * 60)
    
    noise_values = [0.0, 0.1, 1.0, 10.0]  # åŸºå‡†åè®®æ›´æ–°ï¼šæ›´æœ‰åŒºåˆ†åº¦çš„å…³é”®å€¼
    results = {}
    
    for noise in noise_values:
        print(f"\n  æµ‹è¯•å›ºå®šå™ªå£°: b_noise = {noise}")
        
        # åˆ›å»ºæ¨¡å‹
        model = CausalEngineClassifier(
            input_size=data_dict['input_size'],
            num_classes=data_dict['num_classes'],
            b_noise_learnable=False
        )
        model.set_fixed_noise(noise)
        
        # è®­ç»ƒæ¨¡å‹
        train_history = train_model(model, train_loader, val_loader, device)
        
        # è¯„ä¼°æ¨¡å‹ (å››ç§æ¨ç†æ¨¡å¼)
        mode_results = {}
        for mode, (temp, do_sample) in [
            ('causal', (0, False)),
            ('standard', (1.0, False)),
            ('sampling', (0.8, True)),
            ('compatible', (1.0, False))
        ]:
            eval_result = evaluate_model(model, test_loader, device, temp, do_sample)
            mode_results[mode] = eval_result
            print(f"    {mode}æ¨¡å¼: accuracy={eval_result['accuracy']:.4f}")
        
        results[noise] = {
            'train_history': train_history,
            'mode_results': mode_results
        }
    
    return results


def run_adaptive_noise_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ
    """
    print("\nğŸ§ª å®éªŒç»„B: è‡ªé€‚åº”å™ªå£°å­¦ä¹ å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹ (å™ªå£°å¯å­¦ä¹ )
    model = CausalEngineClassifier(
        input_size=data_dict['input_size'],
        num_classes=data_dict['num_classes'],
        b_noise_learnable=True
    )
    
    # åˆå§‹åŒ–å™ªå£°ä¸º0.1
    if hasattr(model.causal_engine.action, 'b_noise'):
        model.causal_engine.action.b_noise.data.fill_(0.1)
    
    print("  åˆå§‹å™ªå£°å€¼: 0.1")
    print("  å™ªå£°å­¦ä¹ : å¯ç”¨ (requires_grad=True)")
    
    # è®­ç»ƒæ¨¡å‹
    train_history = train_model(model, train_loader, val_loader, device)
    
    # è·å–å­¦åˆ°çš„å™ªå£°å€¼ (b_noise æ˜¯å‘é‡ï¼Œæ˜¾ç¤ºå…¶å‡å€¼å’Œæ ‡å‡†å·®)
    learned_noise = None
    if hasattr(model.causal_engine.action, 'b_noise'):
        b_noise_tensor = model.causal_engine.action.b_noise
        learned_noise = {
            'mean': b_noise_tensor.mean().item(),
            'std': b_noise_tensor.std().item(),
            'min': b_noise_tensor.min().item(),
            'max': b_noise_tensor.max().item()
        }
        print(f"  å­¦åˆ°çš„å™ªå£°å€¼ç»Ÿè®¡:")
        print(f"    å‡å€¼: {learned_noise['mean']:.4f}")
        print(f"    æ ‡å‡†å·®: {learned_noise['std']:.4f}")
        print(f"    èŒƒå›´: [{learned_noise['min']:.4f}, {learned_noise['max']:.4f}]")
    
    # è¯„ä¼°æ¨¡å‹ (å››ç§æ¨ç†æ¨¡å¼)
    mode_results = {}
    for mode, (temp, do_sample) in [
        ('causal', (0, False)),
        ('standard', (1.0, False)),
        ('sampling', (0.8, True)),
        ('compatible', (1.0, False))
    ]:
        eval_result = evaluate_model(model, test_loader, device, temp, do_sample)
        mode_results[mode] = eval_result
        print(f"    {mode}æ¨¡å¼: accuracy={eval_result['accuracy']:.4f}")
    
    return {
        'train_history': train_history,
        'mode_results': mode_results,
        'learned_noise': learned_noise
    }


def run_baseline_experiment(data_dict, train_loader, val_loader, test_loader, device):
    """
    ä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†å®éªŒ
    """
    print("\nğŸ—ï¸ ä¼ ç»Ÿç¥ç»ç½‘ç»œåŸºå‡†å®éªŒ")
    print("=" * 60)
    
    # åˆ›å»ºä¼ ç»ŸMLP
    model = TraditionalMLPClassifier(
        input_size=data_dict['input_size'],
        num_classes=data_dict['num_classes'],
        hidden_sizes=[128, 64]  # ä¸CausalEngineç›¸å½“çš„å‚æ•°é‡
    )
    
    print("  æ¨¡å‹ç±»å‹: ä¼ ç»ŸMLP")
    print(f"  éšè—å±‚: {[128, 64]}")
    
    # è®­ç»ƒæ¨¡å‹
    train_history = train_model(model, train_loader, val_loader, device)
    
    # è¯„ä¼°æ¨¡å‹
    eval_result = evaluate_model(model, test_loader, device)
    print(f"    æµ‹è¯•å‡†ç¡®ç‡: {eval_result['accuracy']:.4f}")
    
    return {
        'train_history': train_history,
        'eval_result': eval_result
    }


def analyze_results(fixed_noise_results, adaptive_noise_result, baseline_result):
    """
    åˆ†æå’Œå¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š å®éªŒç»“æœåˆ†æ")
    print("=" * 60)
    
    # 1. å›ºå®šå™ªå£°å®éªŒåˆ†æ
    print("\n  1. å›ºå®šå™ªå£°å¼ºåº¦å¯¹æ€§èƒ½çš„å½±å“:")
    noise_performance = {}
    for noise, result in fixed_noise_results.items():
        best_acc = max([mode_result['accuracy'] 
                       for mode_result in result['mode_results'].values()])
        noise_performance[noise] = best_acc
        print(f"     b_noise={noise}: æœ€ä½³å‡†ç¡®ç‡={best_acc:.4f}")
    
    # æ‰¾åˆ°æœ€ä¼˜å™ªå£°
    best_noise = max(noise_performance, key=noise_performance.get)
    best_fixed_acc = noise_performance[best_noise]
    print(f"     æœ€ä¼˜å›ºå®šå™ªå£°: {best_noise} (å‡†ç¡®ç‡: {best_fixed_acc:.4f})")
    
    # 2. è‡ªé€‚åº”å™ªå£°åˆ†æ
    adaptive_best_acc = max([mode_result['accuracy'] 
                            for mode_result in adaptive_noise_result['mode_results'].values()])
    learned_noise = adaptive_noise_result['learned_noise']
    print(f"\n  2. è‡ªé€‚åº”å™ªå£°å­¦ä¹ :")
    print(f"     å­¦åˆ°çš„å™ªå£°å€¼ç»Ÿè®¡:")
    print(f"       å‡å€¼: {learned_noise['mean']:.4f}")
    print(f"       æ ‡å‡†å·®: {learned_noise['std']:.4f}")
    print(f"       èŒƒå›´: [{learned_noise['min']:.4f}, {learned_noise['max']:.4f}]")
    print(f"     æœ€ä½³å‡†ç¡®ç‡: {adaptive_best_acc:.4f}")
    
    # 3. åŸºå‡†å¯¹æ¯”
    baseline_acc = baseline_result['eval_result']['accuracy']
    print(f"\n  3. ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”:")
    print(f"     ä¼ ç»ŸMLPå‡†ç¡®ç‡: {baseline_acc:.4f}")
    print(f"     æœ€ä½³å›ºå®šå™ªå£°æå‡: {best_fixed_acc - baseline_acc:.4f}")
    print(f"     è‡ªé€‚åº”å™ªå£°æå‡: {adaptive_best_acc - baseline_acc:.4f}")
    
    # 4. æ¨ç†æ¨¡å¼å¯¹æ¯”
    print(f"\n  4. æ¨ç†æ¨¡å¼æ€§èƒ½å¯¹æ¯” (æœ€ä½³é…ç½®):")
    best_config = fixed_noise_results[best_noise]['mode_results']
    for mode, result in best_config.items():
        print(f"     {mode}æ¨¡å¼: {result['accuracy']:.4f}")


def visualize_results(fixed_noise_results, adaptive_noise_result, baseline_result, data_dict):
    """
    å¯è§†åŒ–å®éªŒç»“æœ
    """
    print("\nğŸ“Š ç”Ÿæˆç»“æœå¯è§†åŒ–å›¾è¡¨")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('CausalEngine Classification Benchmark Results', fontsize=16, fontweight='bold')
    
    # 1. å™ªå£°å¼ºåº¦vsæ€§èƒ½æ›²çº¿
    noise_values = list(fixed_noise_results.keys())
    performance_values = [max([mode_result['accuracy'] 
                              for mode_result in result['mode_results'].values()])
                         for result in fixed_noise_results.values()]
    
    ax1.plot(noise_values, performance_values, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('Noise Strength (b_noise)')
    ax1.set_ylabel('Best Accuracy')
    ax1.set_title('Fixed Noise Experiment: Performance vs Noise')
    ax1.grid(True, alpha=0.3)
    ax1.set_xscale('log')
    
    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    best_idx = np.argmax(performance_values)
    ax1.annotate(f'Optimal: {noise_values[best_idx]}', 
                xy=(noise_values[best_idx], performance_values[best_idx]),
                xytext=(noise_values[best_idx]*2, performance_values[best_idx]+0.01),
                arrowprops=dict(arrowstyle='->', color='red'))
    
    # 2. æ¨ç†æ¨¡å¼å¯¹æ¯”
    best_noise = noise_values[best_idx]
    mode_results = fixed_noise_results[best_noise]['mode_results']
    modes = list(mode_results.keys())
    mode_accuracies = [mode_results[mode]['accuracy'] for mode in modes]
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    bars = ax2.bar(modes, mode_accuracies, color=colors, alpha=0.7)
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Inference Modes Comparison')
    ax2.grid(True, alpha=0.3, axis='y')
    
    for bar, acc in zip(bars, mode_accuracies):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # 3. æ€»ä½“æ–¹æ³•å¯¹æ¯”
    methods = ['Traditional MLP', 'CausalEngine\n(Best Fixed)', 'CausalEngine\n(Adaptive)']
    method_accuracies = [
        baseline_result['eval_result']['accuracy'],
        max([mode_results[mode]['accuracy'] for mode in mode_results]),
        max([adaptive_noise_result['mode_results'][mode]['accuracy'] 
             for mode in adaptive_noise_result['mode_results']])
    ]
    
    colors_methods = ['lightcoral', 'lightblue', 'lightgreen']
    bars_methods = ax3.bar(methods, method_accuracies, color=colors_methods, alpha=0.8)
    ax3.set_ylabel('Accuracy')
    ax3.set_title('Overall Method Comparison')
    ax3.grid(True, alpha=0.3, axis='y')
    
    for bar, acc in zip(bars_methods, method_accuracies):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # 4. æ··æ·†çŸ©é˜µ (æœ€ä½³é…ç½®)
    best_mode_result = mode_results['standard']  # ä½¿ç”¨æ ‡å‡†æ¨¡å¼
    cm = confusion_matrix(best_mode_result['targets'], best_mode_result['predictions'])
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax4,
                xticklabels=data_dict['class_names'],
                yticklabels=data_dict['class_names'])
    ax4.set_title('Confusion Matrix (Best Configuration)')
    ax4.set_xlabel('Predicted')
    ax4.set_ylabel('Actual')
    
    plt.tight_layout()
    plt.savefig('tutorials/01_classification/benchmark_classification_results.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ç»“æœå›¾è¡¨å·²ä¿å­˜: tutorials/01_classification/benchmark_classification_results.png")


def main():
    """
    ä¸»å‡½æ•°: å®Œæ•´çš„åŸºå‡†æµ‹è¯•æµç¨‹
    """
    print("ğŸŒŸ CausalEngine åˆ†ç±»ä»»åŠ¡åŸºå‡†æµ‹è¯•æ¼”ç¤º")
    print("åŸºäºå®˜æ–¹åŸºå‡†æµ‹è¯•åè®® (benchmark_strategy.md)")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.manual_seed(42)
    np.random.seed(42)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"éšæœºç§å­: 42 (ç¡®ä¿å¯å¤ç°æ€§)")
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
    print("\nğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½å’Œå‡†å¤‡")
    data_dict = load_adult_dataset()
    train_loader, val_loader, test_loader = create_data_loaders(data_dict, batch_size=64)
    
    # 2. è¿è¡ŒåŸºå‡†å®éªŒ
    print("\nğŸ—ï¸ æ­¥éª¤2: ä¼ ç»ŸåŸºå‡†å®éªŒ")
    baseline_result = run_baseline_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 3. è¿è¡Œå›ºå®šå™ªå£°å®éªŒ
    print("\nğŸ§ª æ­¥éª¤3: å›ºå®šå™ªå£°å®éªŒç»„A")
    fixed_noise_results = run_fixed_noise_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 4. è¿è¡Œè‡ªé€‚åº”å™ªå£°å®éªŒ
    print("\nğŸ§ª æ­¥éª¤4: è‡ªé€‚åº”å™ªå£°å®éªŒç»„B")
    adaptive_noise_result = run_adaptive_noise_experiment(data_dict, train_loader, val_loader, test_loader, device)
    
    # 5. åˆ†æç»“æœ
    print("\nğŸ“Š æ­¥éª¤5: ç»“æœåˆ†æ")
    analyze_results(fixed_noise_results, adaptive_noise_result, baseline_result)
    
    # 6. å¯è§†åŒ–ç»“æœ
    print("\nğŸ“Š æ­¥éª¤6: ç»“æœå¯è§†åŒ–")
    visualize_results(fixed_noise_results, adaptive_noise_result, baseline_result, data_dict)
    
    # 7. æ€»ç»“
    print("\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print("ğŸ”¬ å…³é”®å‘ç°:")
    print("  âœ… éªŒè¯äº†å›ºå®šå™ªå£°vsè‡ªé€‚åº”å™ªå£°çš„æœ‰æ•ˆæ€§")
    print("  âœ… é‡åŒ–äº†å››ç§æ¨ç†æ¨¡å¼çš„æ€§èƒ½å·®å¼‚")
    print("  âœ… è¯æ˜äº†CausalEngineç›¸å¯¹ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿")
    print("  âœ… éµå¾ªäº†å®˜æ–¹åŸºå‡†æµ‹è¯•åè®®çš„æ ‡å‡†é…ç½®")
    
    print("\nğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("  1. å›å½’ä»»åŠ¡: tutorials/02_regression/benchmark_regression_demo.py")
    print("  2. æ¶ˆèç ”ç©¶: tutorials/03_ablation_studies/")
    print("  3. é«˜çº§ä¸»é¢˜: tutorials/04_advanced_topics/")
    print("  4. ç†è®ºåŸºç¡€: causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
CausalEngine æ‰‹å†™æ•°å­—è¯†åˆ«æ•™ç¨‹

è¿™ä¸ªæ•™ç¨‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CausalEngine è¿›è¡Œæ‰‹å†™æ•°å­—è¯†åˆ«ï¼ˆå¤šåˆ†ç±»ä»»åŠ¡ï¼‰ã€‚
æˆ‘ä»¬å°†ä½¿ç”¨ sklearn çš„ Digits æ•°æ®é›†ï¼Œå¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•å’Œ CausalEngine çš„ä¼˜åŠ¿ã€‚

é‡ç‚¹å±•ç¤ºï¼š
1. CausalEngine çš„å¤šåˆ†ç±»æ¿€æ´»å‡½æ•°
2. ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›  
3. OvR (One-vs-Rest) æ¦‚ç‡è®¡ç®—
4. ä¸‰ç§æ¨ç†æ¨¡å¼çš„å®é™…æ•ˆæœ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ CausalEngine
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine import CausalEngine, ActivationMode


class DigitsDataset(Dataset):
    """æ‰‹å†™æ•°å­—æ•°æ®é›†"""
    
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


class CausalDigitClassifier(nn.Module):
    """åŸºäº CausalEngine çš„æ•°å­—åˆ†ç±»å™¨"""
    
    def __init__(self, input_size=64, hidden_size=128, num_classes=10):
        super().__init__()
        
        # ç‰¹å¾ç¼–ç å±‚
        self.feature_encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # CausalEngine æ ¸å¿ƒ
        self.causal_engine = CausalEngine(
            hidden_size=hidden_size,
            vocab_size=num_classes,  # 10ä¸ªæ•°å­—ç±»åˆ«
            activation_modes="classification",
            b_noise_init=0.1,
            gamma_init=1.0
        )
    
    def forward(self, x, temperature=1.0, do_sample=False, return_details=False):
        # ç‰¹å¾ç¼–ç 
        hidden_states = self.feature_encoder(x)
        
        # CausalEngine æ¨ç†
        output = self.causal_engine(
            hidden_states.unsqueeze(1),  # æ·»åŠ åºåˆ—ç»´åº¦
            temperature=temperature,
            do_sample=do_sample,
            return_dict=True
        )
        
        if return_details:
            return output
        else:
            return output['output'].squeeze(1)  # ç§»é™¤åºåˆ—ç»´åº¦


def prepare_digits_data():
    """å‡†å¤‡æ‰‹å†™æ•°å­—æ•°æ®"""
    
    print("ğŸ”¢ åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†...")
    
    # åŠ è½½æ•°æ®
    digits = load_digits()
    X, y = digits.data, digits.target
    
    # æ•°æ®ä¿¡æ¯
    print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
    print(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]} (8x8 å›¾åƒå±•å¹³)")
    print(f"   ç±»åˆ«æ•°: {len(np.unique(y))} (æ•°å­— 0-9)")
    print(f"   ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ’åˆ†æ•°æ®é›† (60% è®­ç»ƒ, 20% éªŒè¯, 20% æµ‹è¯•)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    
    print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    return X_train, X_val, X_test, y_train, y_val, y_test, scaler


def visualize_sample_digits(X_test, y_test, n_samples=10):
    """å¯è§†åŒ–ä¸€äº›æ ·æœ¬æ•°å­—"""
    
    print("\nğŸ‘ï¸ å¯è§†åŒ–æ ·æœ¬æ•°å­—...")
    
    # Set matplotlib to use English
    plt.rcParams['font.family'] = 'DejaVu Sans'
    
    fig, axes = plt.subplots(2, 5, figsize=(12, 6))
    axes = axes.ravel()
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    indices = np.random.choice(len(X_test), n_samples, replace=False)
    
    for i, idx in enumerate(indices):
        # é‡å¡‘ä¸º8x8å›¾åƒ
        image = X_test[idx].reshape(8, 8)
        axes[i].imshow(image, cmap='gray')
        axes[i].set_title(f'Label: {y_test[idx]}')
        axes[i].axis('off')
    
    plt.suptitle('Sample Digits from Test Set')
    plt.tight_layout()
    plt.savefig('digit_samples.png', dpi=150, bbox_inches='tight')
    print("   æ ·æœ¬å›¾åƒå·²ä¿å­˜åˆ° digit_samples.png")
    plt.close()


def train_causal_model(model, train_loader, val_loader, epochs=30):
    """è®­ç»ƒ CausalEngine æ¨¡å‹"""
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ CausalEngine æ¨¡å‹...")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)
    
    train_losses = []
    val_accuracies = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        
        for features, labels in train_loader:
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ ‡å‡†æ¨¡å¼ï¼‰
            outputs = model(features, temperature=1.0, do_sample=False)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                outputs = model(features, temperature=0.0)  # çº¯å› æœæ¨¡å¼
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = train_loss / len(train_loader)
        
        train_losses.append(avg_loss)
        val_accuracies.append(accuracy)
        
        scheduler.step()
        
        if (epoch + 1) % 5 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Val Acc: {accuracy:.2f}%")
    
    print("âœ… è®­ç»ƒå®Œæˆ!")
    return train_losses, val_accuracies


def train_baseline_models(X_train, y_train, X_val, y_val):
    """è®­ç»ƒä¼ ç»ŸåŸºçº¿æ¨¡å‹"""
    
    print("\nğŸ“Š è®­ç»ƒä¼ ç»ŸåŸºçº¿æ¨¡å‹...")
    
    baselines = {}
    
    # 1. é€»è¾‘å›å½’
    lr = LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr')
    lr.fit(X_train, y_train)
    lr_pred = lr.predict(X_val)
    lr_acc = accuracy_score(y_val, lr_pred)
    baselines['Logistic Regression'] = {
        'model': lr, 
        'accuracy': lr_acc,
        'predictions': lr_pred
    }
    print(f"   Logistic Regression éªŒè¯å‡†ç¡®ç‡: {lr_acc:.3f}")
    
    # 2. SVM
    svm = SVC(kernel='rbf', random_state=42, probability=True)
    svm.fit(X_train, y_train)
    svm_pred = svm.predict(X_val)
    svm_acc = accuracy_score(y_val, svm_pred)
    baselines['SVM (RBF)'] = {
        'model': svm,
        'accuracy': svm_acc,
        'predictions': svm_pred
    }
    print(f"   SVM (RBF) éªŒè¯å‡†ç¡®ç‡: {svm_acc:.3f}")
    
    # 3. éšæœºæ£®æ—
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_val)
    rf_acc = accuracy_score(y_val, rf_pred)
    baselines['Random Forest'] = {
        'model': rf,
        'accuracy': rf_acc,
        'predictions': rf_pred
    }
    print(f"   Random Forest éªŒè¯å‡†ç¡®ç‡: {rf_acc:.3f}")
    
    return baselines


def evaluate_inference_modes(model, test_loader):
    """è¯„ä¼° CausalEngine çš„ä¸‰ç§æ¨ç†æ¨¡å¼"""
    
    print("\nğŸ” è¯„ä¼°ä¸‰ç§æ¨ç†æ¨¡å¼...")
    
    model.eval()
    results = {}
    
    modes = [
        ("çº¯å› æœæ¨¡å¼", {"temperature": 0.0, "do_sample": False}),
        ("æ ‡å‡†æ¨¡å¼", {"temperature": 1.0, "do_sample": False}), 
        ("é‡‡æ ·æ¨¡å¼", {"temperature": 0.8, "do_sample": True})
    ]
    
    for mode_name, params in modes:
        all_predictions = []
        all_probabilities = []
        all_uncertainties = []
        all_labels = []
        
        with torch.no_grad():
            for features, labels in test_loader:
                # è·å–è¯¦ç»†è¾“å‡º
                output = model(features, return_details=True, **params)
                
                # æå–æ¦‚ç‡
                probs = output['output'].squeeze(1)
                all_probabilities.append(probs.numpy())
                
                # é¢„æµ‹ç±»åˆ«
                _, pred = torch.max(probs, 1)
                all_predictions.extend(pred.numpy())
                
                # ä¸ç¡®å®šæ€§ï¼ˆæ‰€æœ‰ç±»åˆ«çš„å¹³å‡å°ºåº¦å‚æ•°ï¼‰
                scale_S = output['scale_S'].squeeze(1)
                avg_uncertainty = scale_S.mean(dim=1)
                all_uncertainties.extend(avg_uncertainty.numpy())
                
                all_labels.extend(labels.numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        all_predictions = np.array(all_predictions)
        all_labels = np.array(all_labels)
        all_probabilities = np.vstack(all_probabilities)
        
        accuracy = accuracy_score(all_labels, all_predictions)
        avg_uncertainty = np.mean(all_uncertainties)
        
        # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        class_accuracies = []
        for i in range(10):
            mask = all_labels == i
            if mask.sum() > 0:
                class_acc = (all_predictions[mask] == i).mean()
                class_accuracies.append(class_acc)
            else:
                class_accuracies.append(0.0)
        
        results[mode_name] = {
            'accuracy': accuracy,
            'avg_uncertainty': avg_uncertainty,
            'predictions': all_predictions,
            'probabilities': all_probabilities,
            'uncertainties': all_uncertainties,
            'true_labels': all_labels,
            'class_accuracies': class_accuracies
        }
        
        print(f"   {mode_name}: å‡†ç¡®ç‡={accuracy:.3f}, å¹³å‡ä¸ç¡®å®šæ€§={avg_uncertainty:.3f}")
    
    return results


def visualize_results(train_losses, val_accuracies, inference_results, baseline_results):
    """å¯è§†åŒ–åˆ†æç»“æœ"""
    
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")
    
    # Set matplotlib to use English
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig = plt.figure(figsize=(20, 15))
    
    # 1. è®­ç»ƒæ›²çº¿
    ax1 = plt.subplot(3, 3, 1)
    ax1.plot(train_losses, label='Training Loss', color='blue')
    ax1.set_title('CausalEngine Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    ax2 = plt.subplot(3, 3, 2)
    ax2.plot(val_accuracies, label='Validation Accuracy', color='green')
    ax2.set_title('CausalEngine Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    # 2. æ¨ç†æ¨¡å¼å¯¹æ¯”
    ax3 = plt.subplot(3, 3, 3)
    mode_mapping = {
        'çº¯å› æœæ¨¡å¼': 'Pure Causal',
        'æ ‡å‡†æ¨¡å¼': 'Standard',
        'é‡‡æ ·æ¨¡å¼': 'Sampling'
    }
    
    modes_en = [mode_mapping.get(mode, mode) for mode in inference_results.keys()]
    accuracies = [inference_results[mode]['accuracy'] for mode in inference_results.keys()]
    uncertainties = [inference_results[mode]['avg_uncertainty'] for mode in inference_results.keys()]
    
    x = np.arange(len(modes_en))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)
    ax3_twin = ax3.twinx()
    bars2 = ax3_twin.bar(x + width/2, uncertainties, width, label='Uncertainty', 
                         color='orange', alpha=0.8)
    
    ax3.set_xlabel('Inference Mode')
    ax3.set_ylabel('Accuracy', color='blue')
    ax3_twin.set_ylabel('Uncertainty', color='orange')
    ax3.set_title('Accuracy and Uncertainty across Inference Modes')
    ax3.set_xticks(x)
    ax3.set_xticklabels(modes_en)
    ax3.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars1, accuracies):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # 3. ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    ax4 = plt.subplot(3, 3, 4)
    all_methods = list(baseline_results.keys()) + ['CausalEngine']
    all_accuracies = [baseline_results[method]['accuracy'] for method in baseline_results.keys()]
    all_accuracies.append(inference_results['çº¯å› æœæ¨¡å¼']['accuracy'])
    
    colors = ['gray'] * len(baseline_results) + ['red']
    bars = ax4.bar(all_methods, all_accuracies, alpha=0.7, color=colors)
    ax4.set_title('CausalEngine vs Traditional Methods')
    ax4.set_ylabel('Accuracy')
    ax4.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars, all_accuracies):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.3f}', ha='center', va='bottom')
    
    # 4. æ··æ·†çŸ©é˜µ (çº¯å› æœæ¨¡å¼)
    ax5 = plt.subplot(3, 3, 5)
    mode_data = inference_results['çº¯å› æœæ¨¡å¼']
    cm = confusion_matrix(mode_data['true_labels'], mode_data['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5)
    ax5.set_title('Confusion Matrix (Pure Causal Mode)')
    ax5.set_xlabel('Predicted')
    ax5.set_ylabel('True')
    
    # 5. æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    ax6 = plt.subplot(3, 3, 6)
    class_accs = mode_data['class_accuracies']
    bars = ax6.bar(range(10), class_accs, alpha=0.7, color='green')
    ax6.set_title('Per-Class Accuracy (Pure Causal Mode)')
    ax6.set_xlabel('Digit Class')
    ax6.set_ylabel('Accuracy')
    ax6.set_xticks(range(10))
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, class_accs)):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.2f}', ha='center', va='bottom')
    
    # 6. ä¸ç¡®å®šæ€§ä¸é¢„æµ‹æ­£ç¡®æ€§çš„å…³ç³»
    ax7 = plt.subplot(3, 3, 7)
    predictions = mode_data['predictions']
    true_labels = mode_data['true_labels']
    uncertainties_array = np.array(mode_data['uncertainties'])
    
    correct_mask = predictions == true_labels
    incorrect_mask = ~correct_mask
    
    ax7.hist(uncertainties_array[correct_mask], bins=30, alpha=0.6, 
             label='Correct', color='green', density=True)
    ax7.hist(uncertainties_array[incorrect_mask], bins=30, alpha=0.6,
             label='Incorrect', color='red', density=True)
    ax7.set_xlabel('Uncertainty')
    ax7.set_ylabel('Density')
    ax7.set_title('Uncertainty Distribution by Prediction Correctness')
    ax7.legend()
    
    # 7. æ¦‚ç‡åˆ†å¸ƒçƒ­å›¾
    ax8 = plt.subplot(3, 3, 8)
    # å¯¹æ¯ä¸ªçœŸå®ç±»åˆ«ï¼Œæ˜¾ç¤ºå¹³å‡é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
    prob_matrix = np.zeros((10, 10))
    for true_class in range(10):
        mask = true_labels == true_class
        if mask.sum() > 0:
            prob_matrix[true_class] = mode_data['probabilities'][mask].mean(axis=0)
    
    sns.heatmap(prob_matrix, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax8)
    ax8.set_title('Average Prediction Probabilities')
    ax8.set_xlabel('Predicted Class')
    ax8.set_ylabel('True Class')
    
    # 8. é”™è¯¯æ¡ˆä¾‹åˆ†æ
    ax9 = plt.subplot(3, 3, 9)
    # æ‰¾å‡ºé¢„æµ‹é”™è¯¯çš„æ ·æœ¬
    error_indices = np.where(incorrect_mask)[0]
    if len(error_indices) > 0:
        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_pairs = []
        for idx in error_indices:
            true = true_labels[idx]
            pred = predictions[idx]
            error_pairs.append(f'{true}â†’{pred}')
        
        from collections import Counter
        error_counts = Counter(error_pairs)
        top_errors = error_counts.most_common(10)
        
        if top_errors:
            error_types, counts = zip(*top_errors)
            y_pos = np.arange(len(error_types))
            ax9.barh(y_pos, counts, alpha=0.7, color='red')
            ax9.set_yticks(y_pos)
            ax9.set_yticklabels(error_types)
            ax9.set_xlabel('Count')
            ax9.set_title('Top 10 Error Types (Trueâ†’Predicted)')
            ax9.invert_yaxis()
    else:
        ax9.text(0.5, 0.5, 'No Errors!', ha='center', va='center', 
                transform=ax9.transAxes, fontsize=20)
    
    plt.tight_layout()
    plt.savefig('causal_digit_analysis.png', dpi=150, bbox_inches='tight')
    print("   åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ° causal_digit_analysis.png")
    plt.close()


def analyze_uncertainty_quality(model, test_loader):
    """æ·±å…¥åˆ†æä¸ç¡®å®šæ€§çš„è´¨é‡"""
    
    print("\nğŸ”¬ åˆ†æä¸ç¡®å®šæ€§è´¨é‡...")
    
    model.eval()
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_probs = []
    all_uncertainties = []
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for features, labels in test_loader:
            output = model(features, temperature=1.0, return_details=True)
            
            probs = output['output'].squeeze(1)
            scale_S = output['scale_S'].squeeze(1)
            
            # é¢„æµ‹æ¦‚ç‡å’Œç±»åˆ«
            max_probs, predictions = torch.max(probs, 1)
            
            # é¢„æµ‹ç±»åˆ«çš„ä¸ç¡®å®šæ€§
            pred_uncertainties = scale_S[range(len(predictions)), predictions]
            
            all_probs.extend(max_probs.numpy())
            all_uncertainties.extend(pred_uncertainties.numpy())
            all_predictions.extend(predictions.numpy())
            all_labels.extend(labels.numpy())
    
    all_probs = np.array(all_probs)
    all_uncertainties = np.array(all_uncertainties)
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # è®¡ç®—æ ¡å‡†è¯¯å·®
    correct_mask = all_predictions == all_labels
    
    # æŒ‰ä¸ç¡®å®šæ€§åˆ†ç»„
    n_bins = 10
    uncertainty_bins = np.percentile(all_uncertainties, np.linspace(0, 100, n_bins + 1))
    
    calibration_data = []
    for i in range(n_bins):
        if i == n_bins - 1:
            mask = (all_uncertainties >= uncertainty_bins[i])
        else:
            mask = (all_uncertainties >= uncertainty_bins[i]) & (all_uncertainties < uncertainty_bins[i+1])
        
        if mask.sum() > 0:
            bin_accuracy = correct_mask[mask].mean()
            bin_confidence = all_probs[mask].mean()
            bin_uncertainty = all_uncertainties[mask].mean()
            calibration_data.append({
                'bin': i,
                'accuracy': bin_accuracy,
                'confidence': bin_confidence,
                'uncertainty': bin_uncertainty,
                'count': mask.sum()
            })
    
    # å¯è§†åŒ–æ ¡å‡†ç»“æœ
    plt.figure(figsize=(12, 5))
    plt.rcParams['font.family'] = 'DejaVu Sans'
    
    # å­å›¾1: å‡†ç¡®ç‡ vs ä¸ç¡®å®šæ€§
    plt.subplot(1, 2, 1)
    bins_centers = [d['uncertainty'] for d in calibration_data]
    accuracies = [d['accuracy'] for d in calibration_data]
    plt.plot(bins_centers, accuracies, 'o-', markersize=8)
    plt.xlabel('Average Uncertainty')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Uncertainty Calibration')
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: æ ·æœ¬åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.hist(all_uncertainties[correct_mask], bins=30, alpha=0.6, 
             label='Correct', color='green', density=True)
    plt.hist(all_uncertainties[~correct_mask], bins=30, alpha=0.6,
             label='Incorrect', color='red', density=True)
    plt.xlabel('Uncertainty')
    plt.ylabel('Density')
    plt.title('Uncertainty Distribution')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('uncertainty_calibration.png', dpi=150, bbox_inches='tight')
    print("   ä¸ç¡®å®šæ€§æ ¡å‡†åˆ†æå·²ä¿å­˜åˆ° uncertainty_calibration.png")
    plt.close()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"   æ­£ç¡®é¢„æµ‹çš„å¹³å‡ä¸ç¡®å®šæ€§: {all_uncertainties[correct_mask].mean():.3f}")
    print(f"   é”™è¯¯é¢„æµ‹çš„å¹³å‡ä¸ç¡®å®šæ€§: {all_uncertainties[~correct_mask].mean():.3f}")
    print(f"   ä¸ç¡®å®šæ€§ä¸å‡†ç¡®ç‡çš„ç›¸å…³ç³»æ•°: {np.corrcoef(all_uncertainties, correct_mask.astype(float))[0,1]:.3f}")


def main():
    """ä¸»å‡½æ•°"""
    
    print("="*70)
    print("ğŸ¯ CausalEngine æ‰‹å†™æ•°å­—è¯†åˆ«æ•™ç¨‹")
    print("   å±•ç¤ºå¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„å› æœæ¨ç†èƒ½åŠ›")
    print("="*70)
    
    # 1. å‡†å¤‡æ•°æ®
    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_digits_data()
    
    # 2. å¯è§†åŒ–æ ·æœ¬
    visualize_sample_digits(X_test, y_test)
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = DigitsDataset(X_train, y_train)
    val_dataset = DigitsDataset(X_val, y_val)
    test_dataset = DigitsDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 4. åˆ›å»ºå¹¶è®­ç»ƒ CausalEngine æ¨¡å‹
    model = CausalDigitClassifier(input_size=64, hidden_size=128, num_classes=10)
    train_losses, val_accuracies = train_causal_model(model, train_loader, val_loader, epochs=30)
    
    # 5. è®­ç»ƒåŸºçº¿æ¨¡å‹
    baseline_results = train_baseline_models(X_train, y_train, X_val, y_val)
    
    # 6. è¯„ä¼°ä¸åŒæ¨ç†æ¨¡å¼
    inference_results = evaluate_inference_modes(model, test_loader)
    
    # 7. å¯è§†åŒ–ç»“æœ
    visualize_results(train_losses, val_accuracies, inference_results, baseline_results)
    
    # 8. æ·±å…¥åˆ†æä¸ç¡®å®šæ€§
    analyze_uncertainty_quality(model, test_loader)
    
    # 9. åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
    print("\nğŸ“Š æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°:")
    
    # CausalEngine (çº¯å› æœæ¨¡å¼)
    model.eval()
    with torch.no_grad():
        test_preds = []
        for features, _ in test_loader:
            outputs = model(features, temperature=0.0)
            _, predicted = torch.max(outputs, 1)
            test_preds.extend(predicted.numpy())
    
    print("\nCausalEngine (çº¯å› æœæ¨¡å¼):")
    print(classification_report(y_test, test_preds, digits=3))
    
    # åŸºçº¿æ¨¡å‹
    for name, baseline in baseline_results.items():
        test_pred = baseline['model'].predict(X_test)
        test_acc = accuracy_score(y_test, test_pred)
        print(f"\n{name} æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.3f}")
    
    print("\nâœ… æ•™ç¨‹å®Œæˆï¼")
    print("ğŸ“Š è¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶äº†è§£è¯¦ç»†åˆ†æç»“æœï¼š")
    print("   - digit_samples.png: æ•°æ®é›†æ ·æœ¬å±•ç¤º")
    print("   - causal_digit_analysis.png: å®Œæ•´çš„æ¨¡å‹åˆ†æ")
    print("   - uncertainty_calibration.png: ä¸ç¡®å®šæ€§æ ¡å‡†åˆ†æ")


if __name__ == "__main__":
    main() 
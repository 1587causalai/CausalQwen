#!/usr/bin/env python3
"""
CausalEngine æ¶ˆèå®éªŒ

å®Œç¾çš„å¯¹ç…§å®éªŒï¼š
- Aç»„ï¼šå®Œæ•´ CausalEngine (loc + scale + OvR + BCE)  
- Bç»„ï¼šç®€åŒ–ç‰ˆæœ¬ (ä»…loc + Softmax + CrossEntropy)
- ç½‘ç»œç»“æ„å®Œå…¨ç›¸åŒï¼Œå”¯ä¸€å·®å¼‚æ˜¯æ˜¯å¦ä½¿ç”¨å› æœæ¨ç†çš„ä¸ç¡®å®šæ€§å»ºæ¨¡

ğŸ”§ ä¿®å¤äº†å…³é”®è®­ç»ƒé—®é¢˜ï¼š
1. âœ… ä¿å­˜å’Œæ¢å¤æœ€ä½³æ¨¡å‹æƒé‡  
2. âœ… æ¯ä¸ªepochéƒ½éªŒè¯
3. âœ… å›ºå®šæ¸©åº¦é¿å…è®­ç»ƒä¸ç¨³å®š
4. âœ… è¯¦ç»†è®­ç»ƒæ—¥å¿—
5. âœ… å¢åŠ patienceæé«˜è®­ç»ƒå……åˆ†æ€§
6. âœ… æ·»åŠ verboseæ¨¡å¼æ˜¾ç¤ºç½‘ç»œç»“æ„å’Œé…ç½®
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import copy
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ CausalEngine
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine import CausalEngine


class SimpleDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


class SharedEncoder(nn.Module):
    """å…±äº«çš„ç‰¹å¾ç¼–ç å™¨ - ç¡®ä¿å®Œå…¨ç›¸åŒçš„ç½‘ç»œç»“æ„"""
    
    def __init__(self, input_size, output_size=32, verbose=False):
        super().__init__()
        
        # æ ¹æ®è¾“å…¥å¤§å°è°ƒæ•´ç½‘ç»œæ·±åº¦
        if input_size <= 4:
            hidden_sizes = [32, 16]
        elif input_size <= 20:
            hidden_sizes = [64, 32]
        else:
            hidden_sizes = [128, 64, 32]
        
        # ç‰¹å¾ç¼–ç å™¨
        layers = []
        prev_size = input_size
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.LayerNorm(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_size = hidden_size
        
        # æœ€ç»ˆè¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))
        
        self.encoder = nn.Sequential(*layers)
        self.output_size = output_size
        
        if verbose:
            print(f"      ğŸ“ SharedEncoder Architecture:")
            print(f"         è¾“å…¥ç»´åº¦: {input_size}")
            print(f"         éšè—å±‚ç»´åº¦: {hidden_sizes}")
            print(f"         è¾“å‡ºç»´åº¦: {output_size}")
            print(f"         æ€»å‚æ•°æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x):
        return self.encoder(x)


class FullCausalClassifier(nn.Module):
    """å®Œæ•´çš„ CausalEngine åˆ†ç±»å™¨ (Aç»„) - loc + scale + OvR + BCE"""
    
    def __init__(self, input_size, num_classes, verbose=False):
        super().__init__()
        
        self.encoder = SharedEncoder(input_size, output_size=32, verbose=verbose)
        
        # å®Œæ•´çš„ CausalEngine
        self.causal_engine = CausalEngine(
            hidden_size=self.encoder.output_size,
            vocab_size=num_classes,
            activation_modes="classification",
            b_noise_init=0.1,
            gamma_init=1.0
        )
        
        self.num_classes = num_classes
        
        if verbose:
            print(f"      ğŸ”§ FullCausalClassifier Configuration:")
            print(f"         è¾“å…¥ç»´åº¦: {input_size}")
            print(f"         ç±»åˆ«æ•°: {num_classes}")
            print(f"         ç¼–ç å™¨è¾“å‡ºç»´åº¦: {self.encoder.output_size}")
            print(f"         CausalEngine é…ç½®:")
            print(f"           - hidden_size: {self.encoder.output_size}")
            print(f"           - vocab_size: {num_classes}")
            print(f"           - activation_modes: classification")
            print(f"           - b_noise_init: 0.1")
            print(f"           - gamma_init: 1.0")
            total_params = sum(p.numel() for p in self.parameters())
            encoder_params = sum(p.numel() for p in self.encoder.parameters())
            causal_params = sum(p.numel() for p in self.causal_engine.parameters())
            print(f"         å‚æ•°ç»Ÿè®¡:")
            print(f"           - ç¼–ç å™¨å‚æ•°: {encoder_params:,}")
            print(f"           - CausalEngineå‚æ•°: {causal_params:,}")
            print(f"           - æ€»å‚æ•°: {total_params:,}")
    
    def forward(self, x, temperature=1.0, return_components=False):
        features = self.encoder(x)
        output = self.causal_engine(
            features.unsqueeze(1),
            temperature=temperature,
            do_sample=False,
            return_dict=True
        )
        
        if return_components:
            return {
                'probs': output['output'].squeeze(1),
                'loc_S': output['loc_S'].squeeze(1),
                'scale_S': output['scale_S'].squeeze(1)
            }
        else:
            return output['output'].squeeze(1)


class SimplifiedCausalClassifier(nn.Module):
    """ç®€åŒ–çš„ CausalEngine åˆ†ç±»å™¨ (Bç»„) - ä»…loc + Softmax + CrossEntropy"""
    
    def __init__(self, input_size, num_classes, verbose=False):
        super().__init__()
        
        # ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ç¼–ç å™¨ï¼
        self.encoder = SharedEncoder(input_size, output_size=32, verbose=verbose)
        
        # åˆ›å»º CausalEngine ä½†åªä½¿ç”¨ loc éƒ¨åˆ†
        self.causal_engine = CausalEngine(
            hidden_size=self.encoder.output_size,
            vocab_size=num_classes,
            activation_modes="classification",
            b_noise_init=0.1,
            gamma_init=1.0
        )
        
        self.num_classes = num_classes
        
        if verbose:
            print(f"      ğŸ“Š SimplifiedCausalClassifier Configuration:")
            print(f"         è¾“å…¥ç»´åº¦: {input_size}")
            print(f"         ç±»åˆ«æ•°: {num_classes}")
            print(f"         ç¼–ç å™¨è¾“å‡ºç»´åº¦: {self.encoder.output_size}")
            print(f"         å…³é”®å·®å¼‚: ä»…ä½¿ç”¨ loc_Sï¼Œå¿½ç•¥ scale_S")
            print(f"         è¾“å‡ºæ–¹å¼: Softmax å½’ä¸€åŒ–")
            print(f"         æŸå¤±å‡½æ•°: æ ‡å‡†äº¤å‰ç†µ")
            total_params = sum(p.numel() for p in self.parameters())
            print(f"         æ€»å‚æ•°: {total_params:,} (ä¸å®Œæ•´ç‰ˆç›¸åŒ)")
    
    def forward(self, x, temperature=1.0, return_components=False):
        features = self.encoder(x)
        output = self.causal_engine(
            features.unsqueeze(1),
            temperature=temperature,
            do_sample=False,
            return_dict=True
        )
        
        # å…³é”®ï¼šåªä½¿ç”¨ loc_Sï¼Œå¿½ç•¥ scale_Sï¼
        loc_S = output['loc_S'].squeeze(1)  # [batch_size, num_classes]
        
        # åº”ç”¨ softmax å¾—åˆ°ä¼ ç»Ÿçš„æ¦‚ç‡åˆ†å¸ƒ
        logits = loc_S  # å°† loc å½“ä½œ logits
        probs = F.softmax(logits, dim=1)
        
        if return_components:
            return {
                'probs': probs,
                'logits': logits,
                'loc_S': loc_S,
                'scale_S': output['scale_S'].squeeze(1)
            }
        else:
            return probs


def full_causal_loss(probs, labels, num_classes):
    """å®Œæ•´ CausalEngine çš„ OvR æŸå¤±å‡½æ•°"""
    targets = F.one_hot(labels, num_classes=num_classes).float()
    return F.binary_cross_entropy(probs, targets, reduction='mean')


def simplified_causal_loss(probs, labels):
    """ç®€åŒ–ç‰ˆæœ¬çš„æ ‡å‡†äº¤å‰ç†µæŸå¤±"""
    return F.cross_entropy(torch.log(probs + 1e-8), labels)


def train_model_fixed(model, train_loader, val_loader, loss_fn, model_name, epochs=100, verbose=False):
    """ğŸ”§ ä¿®å¤çš„è®­ç»ƒå‡½æ•°ï¼šä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡ + æ¯è½®éªŒè¯"""
    
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=12
    )
    
    best_val_acc = 0
    best_model_state = None  # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
    patience = 25  # ğŸ”§ å¢åŠ patienceç¡®ä¿è®­ç»ƒå……åˆ†
    patience_counter = 0
    
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å›ºå®šæ¸©åº¦æé«˜è®­ç»ƒç¨³å®šæ€§
    fixed_temperature = 1.0
    
    if verbose:
        print(f"      ğŸš€ è®­ç»ƒé…ç½®è¯¦æƒ…:")
        print(f"         ä¼˜åŒ–å™¨: AdamW (lr=0.001, weight_decay=1e-4)")
        print(f"         å­¦ä¹ ç‡è°ƒåº¦: ReduceLROnPlateau (patience=12)")
        print(f"         æ—©åœ: patience={patience}")
        print(f"         æœ€å¤§è½®æ•°: {epochs}")
        print(f"         å›ºå®šæ¸©åº¦: {fixed_temperature}")
        print(f"         æ¢¯åº¦è£å‰ª: max_norm=1.0")
    
    print(f"      ğŸ”§ Training {model_name} (FIXED - best model saving)...")
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        
        for features, labels in train_loader:
            optimizer.zero_grad()
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å›ºå®šæ¸©åº¦ï¼Œé¿å…è®­ç»ƒä¸ç¨³å®š
            if 'Full' in model_name:
                probs = model(features, temperature=fixed_temperature)
                loss = loss_fn(probs, labels, model.num_classes)
            else:
                probs = model(features, temperature=fixed_temperature)
                loss = loss_fn(probs, labels)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss += loss.item()
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¯ä¸ªepochéƒ½éªŒè¯ï¼Œç²¾ç¡®æ—©åœ
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                probs = model(features, temperature=fixed_temperature)
                predictions = torch.argmax(probs, dim=1)
                total += labels.size(0)
                correct += (predictions == labels).sum().item()
        
        val_acc = correct / total
        avg_loss = total_loss / len(train_loader)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå½“éªŒè¯å‡†ç¡®ç‡æå‡æ—¶ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = copy.deepcopy(model.state_dict())  # ğŸ”§ ä¿å­˜æƒé‡ï¼
            patience_counter = 0
            improvement_marker = "â­ NEW BEST"
        else:
            patience_counter += 1
            improvement_marker = ""
        
        # å­¦ä¹ ç‡è°ƒåº¦
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_acc)
        new_lr = optimizer.param_groups[0]['lr']
        
        # è¯¦ç»†æ—¥å¿—
        if verbose or epoch % 10 == 0 or epoch == epochs - 1 or improvement_marker:
            lr_info = f", LR: {new_lr:.6f}" if old_lr != new_lr else ""
            print(f"      Epoch {epoch:3d}: Loss={avg_loss:.4f}, Val_Acc={val_acc:.4f}{lr_info} {improvement_marker}")
        
        # æ—©åœ
        if patience_counter >= patience:
            print(f"      Early stop at epoch {epoch}, best val acc: {best_val_acc:.4f}")
            break
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ¢å¤æœ€ä½³æ¨¡å‹æƒé‡ï¼
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"      âœ… Restored BEST model weights (val_acc={best_val_acc:.4f})")
    else:
        print(f"      âš ï¸ No improvement found, using final weights")
    
    return best_val_acc


def run_ablation_experiment(verbose=False):
    """è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ"""
    
    print("="*70)
    print("ğŸ”¬ CausalEngine æ¶ˆèå®éªŒ - æœ€ç»ˆç‰ˆ")
    print("   ğŸ”§ ä¿®å¤äº†æ‰€æœ‰è®­ç»ƒé—®é¢˜ï¼š")
    print("   âœ… ä¿å­˜å’Œæ¢å¤æœ€ä½³æ¨¡å‹æƒé‡")
    print("   âœ… æ¯ä¸ªepochéªŒè¯ï¼Œç²¾ç¡®æ—©åœ")  
    print("   âœ… å›ºå®šæ¸©åº¦æé«˜è®­ç»ƒç¨³å®šæ€§")
    print("   âœ… å¢åŠ patienceç¡®ä¿è®­ç»ƒå……åˆ†")
    if verbose:
        print("   âœ… è¯¦ç»†è¾“å‡ºç½‘ç»œç»“æ„å’Œé…ç½®")
    print("="*70)
    
    # åŠ è½½æ•°æ®é›†
    datasets_info = [
        {
            'name': 'Iris',
            'loader': lambda: datasets.load_iris(),
            'classes': 3
        },
        {
            'name': 'Wine', 
            'loader': lambda: datasets.load_wine(),
            'classes': 3
        },
        {
            'name': 'Breast Cancer',
            'loader': lambda: datasets.load_breast_cancer(),
            'classes': 2
        },
        {
            'name': 'Digits',
            'loader': lambda: datasets.load_digits(),
            'classes': 10
        }
    ]
    
    results = []
    
    for dataset_info in datasets_info:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ•°æ®é›†: {dataset_info['name']}")
        print(f"{'='*60}")
        
        # åŠ è½½æ•°æ®
        data = dataset_info['loader']()
        X, y = data.data, data.target
        
        # å¯¹äº Digitsï¼Œé‡‡æ ·ä»¥åŠ å¿«é€Ÿåº¦
        if dataset_info['name'] == 'Digits':
            indices = np.random.choice(len(X), 800, replace=False)
            X, y = X[indices], y[indices]
        
        print(f"   æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}, ç±»åˆ«æ•°: {dataset_info['classes']}")
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # æ•°æ®åˆ’åˆ†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X_scaled, y, test_size=0.25, random_state=42, stratify=y
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = min(32, len(X_train) // 4) if len(X_train) > 32 else 8
        
        train_dataset = SimpleDataset(X_train, y_train)
        val_dataset = SimpleDataset(X_val, y_val)
        test_dataset = SimpleDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        if verbose:
            print(f"   æ•°æ®åˆ’åˆ†è¯¦æƒ…:")
            print(f"     è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            print(f"     éªŒè¯é›†: {len(X_val)} æ ·æœ¬") 
            print(f"     æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            print(f"     æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # åˆ›å»ºæ¨¡å‹
        print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
        full_model = FullCausalClassifier(X_train.shape[1], dataset_info['classes'], verbose=verbose)
        print("   " + "="*50)
        simplified_model = SimplifiedCausalClassifier(X_train.shape[1], dataset_info['classes'], verbose=verbose)
        
        print("   âœ… ç½‘ç»œç»“æ„å®Œå…¨ç›¸åŒ")
        print("   ğŸ”§ ä½¿ç”¨ä¿®å¤çš„è®­ç»ƒå‡½æ•°")
        
        # ğŸ”§ ä½¿ç”¨ä¿®å¤çš„è®­ç»ƒå‡½æ•°
        print("\nğŸš€ å¼€å§‹ä¿®å¤ç‰ˆè®­ç»ƒ...")
        
        full_val_acc = train_model_fixed(
            full_model, train_loader, val_loader, full_causal_loss, 
            "Full CausalEngine", epochs=100, verbose=verbose
        )
        
        simplified_val_acc = train_model_fixed(
            simplified_model, train_loader, val_loader, simplified_causal_loss,
            "Simplified Version", epochs=100, verbose=verbose
        )
        
        # æµ‹è¯•è¯„ä¼°
        print(f"\nğŸ“Š æµ‹è¯•é›†è¯„ä¼° - {dataset_info['name']}:")
        
        # å®Œæ•´æ¨¡å‹æµ‹è¯•
        full_model.eval()
        full_predictions = []
        full_confidences = []
        with torch.no_grad():
            for features, _ in test_loader:
                probs = full_model(features, temperature=1.0)
                preds = torch.argmax(probs, dim=1)
                confs = torch.max(probs, dim=1)[0]
                full_predictions.extend(preds.numpy())
                full_confidences.extend(confs.numpy())
        
        full_test_acc = accuracy_score(y_test, full_predictions)
        
        # ç®€åŒ–æ¨¡å‹æµ‹è¯•
        simplified_model.eval()
        simp_predictions = []
        simp_confidences = []
        with torch.no_grad():
            for features, _ in test_loader:
                probs = simplified_model(features, temperature=1.0)
                preds = torch.argmax(probs, dim=1)
                confs = torch.max(probs, dim=1)[0]
                simp_predictions.extend(preds.numpy())
                simp_confidences.extend(confs.numpy())
        
        simp_test_acc = accuracy_score(y_test, simp_predictions)
        
        # è®¡ç®—ä¸€è‡´æ€§
        agreement = np.mean(np.array(full_predictions) == np.array(simp_predictions))
        
        print(f"   å®Œæ•´ç‰ˆ - éªŒè¯: {full_val_acc:.4f}, æµ‹è¯•: {full_test_acc:.4f}, ç½®ä¿¡åº¦: {np.mean(full_confidences):.4f}")
        print(f"   ç®€åŒ–ç‰ˆ - éªŒè¯: {simplified_val_acc:.4f}, æµ‹è¯•: {simp_test_acc:.4f}, ç½®ä¿¡åº¦: {np.mean(simp_confidences):.4f}")
        print(f"   æµ‹è¯•é›†å·®å¼‚: {full_test_acc - simp_test_acc:+.4f}")
        print(f"   é¢„æµ‹ä¸€è‡´æ€§: {agreement:.4f}")
        
        results.append({
            'dataset': dataset_info['name'],
            'full_val_acc': full_val_acc,
            'simplified_val_acc': simplified_val_acc,
            'full_test_acc': full_test_acc,
            'simplified_test_acc': simp_test_acc,
            'test_difference': full_test_acc - simp_test_acc,
            'full_confidence': np.mean(full_confidences),
            'simplified_confidence': np.mean(simp_confidences),
            'agreement': agreement
        })
    
    return results


def visualize_ablation_results(results):
    """å¯è§†åŒ–æ¶ˆèå®éªŒç»“æœ"""
    
    print("\nğŸ“ˆ ç”Ÿæˆæ¶ˆèå®éªŒå¯è§†åŒ–...")
    
    plt.rcParams['font.family'] = 'DejaVu Sans'
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    datasets = [r['dataset'] for r in results]
    full_accs = [r['full_test_acc'] for r in results]
    simplified_accs = [r['simplified_test_acc'] for r in results]
    differences = [r['test_difference'] for r in results]
    
    # 1. å‡†ç¡®ç‡å¯¹æ¯”
    ax = axes[0, 0]
    x = np.arange(len(datasets))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, full_accs, width, label='Full CausalEngine', 
                   color='red', alpha=0.8)
    bars2 = ax.bar(x + width/2, simplified_accs, width, label='Simplified (loc+softmax)',
                   color='blue', alpha=0.8)
    
    ax.set_xlabel('Dataset')
    ax.set_ylabel('Test Accuracy')
    ax.set_title('Final Ablation: Full vs Simplified CausalEngine')
    ax.set_xticks(x)
    ax.set_xticklabels(datasets, rotation=45)
    ax.legend()
    ax.grid(True, axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    for bar in bars2:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    # 2. æ€§èƒ½å·®å¼‚
    ax = axes[0, 1]
    colors = ['green' if diff > 0 else 'red' for diff in differences]
    bars = ax.bar(datasets, differences, color=colors, alpha=0.7)
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    ax.set_ylabel('Test Accuracy Difference (Full - Simplified)')
    ax.set_title('Performance Gain from Full CausalEngine')
    ax.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, diff in zip(bars, differences):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, 
                height + (0.005 if height > 0 else -0.005),
                f'{diff:+.3f}', ha='center', 
                va='bottom' if height > 0 else 'top', fontsize=10, weight='bold')
    
    # 3. ç½®ä¿¡åº¦å¯¹æ¯”
    ax = axes[1, 0]
    full_confs = [r['full_confidence'] for r in results]
    simp_confs = [r['simplified_confidence'] for r in results]
    
    x = np.arange(len(datasets))
    bars1 = ax.bar(x - width/2, full_confs, width, label='Full CausalEngine', 
                   color='red', alpha=0.8)
    bars2 = ax.bar(x + width/2, simp_confs, width, label='Simplified',
                   color='blue', alpha=0.8)
    
    ax.set_xlabel('Dataset')
    ax.set_ylabel('Average Confidence')
    ax.set_title('Confidence Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(datasets, rotation=45)
    ax.legend()
    ax.grid(True, axis='y', alpha=0.3)
    
    # 4. å®éªŒæ€»ç»“
    ax = axes[1, 1]
    
    avg_improvement = np.mean(differences)
    positive_improvements = sum(1 for d in differences if d > 0)
    avg_full_acc = np.mean(full_accs)
    avg_simp_acc = np.mean(simplified_accs)
    
    summary_text = f"""Final Ablation Study Summary

Average Test Performance:
â€¢ Full Version: {avg_full_acc:.3f}
â€¢ Simplified: {avg_simp_acc:.3f}
â€¢ Avg Improvement: {avg_improvement:+.3f}

Win Statistics:
â€¢ Full Version Wins: {positive_improvements}/{len(results)}
â€¢ Simplified Wins: {len(results)-positive_improvements}/{len(results)}

Key Findings:
{"âœ… Causal uncertainty modeling effective" if avg_improvement > 0.01 else "âš ï¸ Simplified version competitive"}
{"âœ… OvR strategy > Softmax" if positive_improvements > len(results)/2 else "âš ï¸ Softmax competitive"}

Core Insight:
CausalEngine improvement comes
{"mainly from causal reasoning" if avg_improvement > 0.02 else "partly from architecture"}"""
    
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('causal_ablation_final_results.png', dpi=150, bbox_inches='tight')
    print("   æœ€ç»ˆæ¶ˆèå®éªŒç»“æœå·²ä¿å­˜åˆ° causal_ablation_final_results.png")
    plt.close()


def print_final_conclusions(results):
    """æ‰“å°æœ€ç»ˆå®éªŒç»“è®º"""
    
    print("\n" + "="*70)
    print("ğŸ¯ æœ€ç»ˆæ¶ˆèå®éªŒç»“è®º")
    print("="*70)
    
    test_differences = [r['test_difference'] for r in results]
    avg_improvement = np.mean(test_differences)
    positive_improvements = sum(1 for d in test_differences if d > 0)
    
    print(f"\nğŸ“Š æœ€ç»ˆæ•´ä½“ç»“æœ:")
    print(f"   å¹³å‡æµ‹è¯•é›†æ€§èƒ½æå‡: {avg_improvement:+.4f}")
    print(f"   å®Œæ•´ç‰ˆè·èƒœæ•°æ®é›†: {positive_improvements}/{len(results)}")
    
    print(f"\nğŸ“ˆ å„æ•°æ®é›†è¯¦ç»†å¯¹æ¯”:")
    for result in results:
        status = "ğŸŸ¢" if result['test_difference'] > 0 else "ğŸ”´"
        print(f"   {status} {result['dataset']:15s}: {result['test_difference']:+.4f}")
        print(f"       éªŒè¯é›† - å®Œæ•´: {result['full_val_acc']:.4f}, ç®€åŒ–: {result['simplified_val_acc']:.4f}")
        print(f"       æµ‹è¯•é›† - å®Œæ•´: {result['full_test_acc']:.4f}, ç®€åŒ–: {result['simplified_test_acc']:.4f}")
        print(f"       é¢„æµ‹ä¸€è‡´æ€§: {result['agreement']:.4f}")
        print()
    
    print(f"\nğŸ” æœ€ç»ˆç§‘å­¦ç»“è®º:")
    if avg_improvement > 0.02:
        print("   âœ… å› æœæ¨ç†çš„ä¸ç¡®å®šæ€§å»ºæ¨¡æ˜¾è‘—æœ‰æ•ˆ")
        print("   âœ… OvR + scale_S ç­–ç•¥æ˜æ˜¾ä¼˜äºç®€åŒ–ç‰ˆæœ¬")
        print("   âœ… CausalEngine çš„æ€§èƒ½æå‡ä¸»è¦æ¥è‡ªå› æœæ¨ç†æœºåˆ¶")
    elif avg_improvement > 0.005:
        print("   âš ï¸ å› æœæ¨ç†çš„ä¸ç¡®å®šæ€§å»ºæ¨¡è½»å¾®æœ‰æ•ˆ")
        print("   âš ï¸ æ€§èƒ½æå‡éƒ¨åˆ†æ¥è‡ªå› æœæ¨ç†æœºåˆ¶")
        print("   âš ï¸ ç½‘ç»œæ¶æ„ä¹Ÿæœ‰è´¡çŒ®")
    else:
        print("   âŒ ç®€åŒ–ç‰ˆæœ¬è¡¨ç°ç›¸å½“æˆ–æ›´å¥½")
        print("   âŒ å› æœæ¨ç†çš„ä¸ç¡®å®šæ€§å»ºæ¨¡åœ¨è¿™äº›ä»»åŠ¡ä¸Šæ•ˆæœæœ‰é™")
        print("   âŒ å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°æˆ–é€‚ç”¨åœºæ™¯")
    
    print(f"\nğŸ”§ å®éªŒéªŒè¯:")
    print("   âœ… æœ€ä½³æ¨¡å‹æƒé‡æ­£ç¡®ä¿å­˜å’Œæ¢å¤")
    print("   âœ… æ¯è½®éªŒè¯ç¡®ä¿ç²¾ç¡®æ—©åœ")
    print("   âœ… å›ºå®šæ¸©åº¦æé«˜è®­ç»ƒç¨³å®šæ€§")
    print("   âœ… å¢åŠ patienceç¡®ä¿è®­ç»ƒå……åˆ†")
    print("   âœ… ä¸¥æ ¼æ§åˆ¶ç½‘ç»œæ¶æ„å˜é‡")
    print("   âœ… æ¸…æ™°é‡åŒ–å› æœæ¨ç†æœºåˆ¶çš„è´¡çŒ®")


def main(verbose=False):
    """ä¸»å‡½æ•°"""
    
    # è¿è¡Œæ¶ˆèå®éªŒ
    results = run_ablation_experiment(verbose=verbose)
    
    # å¯è§†åŒ–ç»“æœ
    visualize_ablation_results(results)
    
    # æ‰“å°ç»“è®º
    print_final_conclusions(results)
    
    print("\nâœ… æœ€ç»ˆæ¶ˆèå®éªŒå®Œæˆï¼")
    print("ğŸ“Š è¯·æŸ¥çœ‹ causal_ablation_final_results.png äº†è§£è¯¦ç»†å¯¹æ¯”")


if __name__ == "__main__":
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç›´æ¥ä¿®æ”¹è¿™é‡Œæ¥å¯ç”¨verboseæ¨¡å¼
    import sys
    verbose = len(sys.argv) > 1 and sys.argv[1].lower() == 'verbose'
    main(verbose=verbose) 
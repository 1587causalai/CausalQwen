#!/usr/bin/env python3
"""
Deterministicæ¨¡å¼ä¼˜åŠ¿åˆ†æè„šæœ¬

é€šè¿‡æ§åˆ¶å˜é‡å®éªŒï¼Œé€ä¸€éªŒè¯å¯èƒ½å¯¼è‡´deterministicæ¨¡å¼ä¼˜è¶Šæ€§çš„å› ç´ ï¼š
1. æ•°å€¼ç²¾åº¦ (float32 vs float64)
2. ä¼˜åŒ–å™¨é€‰æ‹© (Adam vs SGD+momentum)
3. åˆå§‹åŒ–ç­–ç•¥
4. æ¢¯åº¦è£å‰ª
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalClassifier

class ControlledPyTorchModel(nn.Module):
    """å—æ§PyTorchæ¨¡å‹ï¼Œç”¨äºé€ä¸€æµ‹è¯•å·®å¼‚å› ç´ """
    
    def __init__(self, input_size, output_size, hidden_sizes):
        super().__init__()
        layers = []
        prev_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
            ])
            prev_size = hidden_size
        
        layers.append(nn.Linear(prev_size, output_size))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)
    
    def init_xavier_uniform(self):
        """ä½¿ç”¨Xavierå‡åŒ€åˆå§‹åŒ–ï¼ˆä¸CausalEngineä¸€è‡´ï¼‰"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def init_identity_first_layer(self):
        """ç¬¬ä¸€å±‚ä½¿ç”¨æ’ç­‰åˆå§‹åŒ–ï¼ˆæ¨¡æ‹ŸCausalEngineçš„AbductionNetworkï¼‰"""
        first_layer = None
        for m in self.modules():
            if isinstance(m, nn.Linear):
                first_layer = m
                break
        
        if first_layer and first_layer.in_features == first_layer.out_features:
            with torch.no_grad():
                first_layer.weight.copy_(torch.eye(first_layer.in_features, dtype=first_layer.weight.dtype))
                first_layer.bias.zero_()


def train_controlled_pytorch(model, X_train, y_train, X_val, y_val, 
                            use_float64=False, optimizer_type='adam', use_grad_clip=False,
                            epochs=1000, lr=0.001):
    """å—æ§è®­ç»ƒPyTorchæ¨¡å‹"""
    
    # æ•°æ®ç±»å‹æ§åˆ¶
    if use_float64:
        model = model.double()
        X_train_tensor = torch.tensor(X_train, dtype=torch.float64)
        y_train_tensor = torch.tensor(y_train, dtype=torch.long)
        X_val_tensor = torch.tensor(X_val, dtype=torch.float64)
        y_val_tensor = torch.tensor(y_val, dtype=torch.long)
    else:
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.LongTensor(y_train)
        X_val_tensor = torch.FloatTensor(X_val)
        y_val_tensor = torch.LongTensor(y_val)
    
    criterion = nn.CrossEntropyLoss()
    
    # ä¼˜åŒ–å™¨æ§åˆ¶
    if optimizer_type == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    elif optimizer_type == 'sgd_momentum':
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    else:
        raise ValueError(f"Unknown optimizer_type: {optimizer_type}")
    
    # æ—©åœè®¾ç½®
    best_loss = float('inf')
    patience = 50
    no_improve = 0
    
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªæ§åˆ¶
        if use_grad_clip:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor).item()
        
        if val_loss < best_loss - 1e-4:
            best_loss = val_loss
            no_improve = 0
        else:
            no_improve += 1
        
        if no_improve >= patience:
            break
    
    model.n_iter_ = epoch + 1
    return model


def run_controlled_experiment():
    """è¿è¡Œæ§åˆ¶å˜é‡å®éªŒ"""
    print("ğŸ”¬ Deterministicæ¨¡å¼ä¼˜åŠ¿åˆ†æå®éªŒ")
    print("=" * 80)
    
    # ç”Ÿæˆæ•°æ®
    n_samples, n_features, n_classes = 1000, 10, 3
    X, y = make_classification(n_samples=n_samples, n_features=n_features, 
                              n_classes=n_classes, n_informative=7, 
                              class_sep=0.8, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
    
    hidden_sizes = (64, 32)
    lr = 0.001
    max_iter = 1000
    
    results = {}
    
    print(f"æ•°æ®: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±»åˆ«")
    print(f"ç½‘ç»œ: {hidden_sizes}, lr={lr}, max_iter={max_iter}")
    print()
    
    # 1. åŸºçº¿: æ ‡å‡†PyTorch (float32 + Adam)
    print("1ï¸âƒ£ åŸºçº¿PyTorch (float32 + Adam + é»˜è®¤åˆå§‹åŒ–)")
    model1 = ControlledPyTorchModel(n_features, n_classes, hidden_sizes)
    model1 = train_controlled_pytorch(model1, X_train, y_train, X_val, y_val,
                                    use_float64=False, optimizer_type='adam', use_grad_clip=False)
    model1.eval()
    with torch.no_grad():
        pred1 = torch.argmax(model1(torch.FloatTensor(X_test)), dim=1).numpy()
    acc1 = accuracy_score(y_test, pred1)
    results['åŸºçº¿PyTorch'] = acc1
    print(f"   å‡†ç¡®ç‡: {acc1:.4f}, è®­ç»ƒè½®æ•°: {model1.n_iter_}")
    
    # 2. float64ç²¾åº¦æµ‹è¯•
    print("2ï¸âƒ£ é«˜ç²¾åº¦PyTorch (float64 + Adam + é»˜è®¤åˆå§‹åŒ–)")
    model2 = ControlledPyTorchModel(n_features, n_classes, hidden_sizes)
    model2 = train_controlled_pytorch(model2, X_train, y_train, X_val, y_val,
                                    use_float64=True, optimizer_type='adam', use_grad_clip=False)
    model2.eval()
    with torch.no_grad():
        pred2 = torch.argmax(model2(torch.tensor(X_test, dtype=torch.float64)), dim=1).numpy()
    acc2 = accuracy_score(y_test, pred2)
    results['é«˜ç²¾åº¦PyTorch'] = acc2
    print(f"   å‡†ç¡®ç‡: {acc2:.4f}, è®­ç»ƒè½®æ•°: {model2.n_iter_}")
    
    # 3. ä¼˜åŒ–å™¨æµ‹è¯•
    print("3ï¸âƒ£ ä¼˜åŒ–å™¨PyTorch (float64 + SGD+momentum + é»˜è®¤åˆå§‹åŒ–)")
    model3 = ControlledPyTorchModel(n_features, n_classes, hidden_sizes)
    model3 = train_controlled_pytorch(model3, X_train, y_train, X_val, y_val,
                                    use_float64=True, optimizer_type='sgd_momentum', use_grad_clip=False)
    model3.eval()
    with torch.no_grad():
        pred3 = torch.argmax(model3(torch.tensor(X_test, dtype=torch.float64)), dim=1).numpy()
    acc3 = accuracy_score(y_test, pred3)
    results['ä¼˜åŒ–å™¨PyTorch'] = acc3
    print(f"   å‡†ç¡®ç‡: {acc3:.4f}, è®­ç»ƒè½®æ•°: {model3.n_iter_}")
    
    # 4. åˆå§‹åŒ–æµ‹è¯•
    print("4ï¸âƒ£ åˆå§‹åŒ–PyTorch (float64 + SGD+momentum + Xavieråˆå§‹åŒ–)")
    model4 = ControlledPyTorchModel(n_features, n_classes, hidden_sizes)
    model4.init_xavier_uniform()  # Xavieråˆå§‹åŒ–
    model4 = train_controlled_pytorch(model4, X_train, y_train, X_val, y_val,
                                    use_float64=True, optimizer_type='sgd_momentum', use_grad_clip=False)
    model4.eval()
    with torch.no_grad():
        pred4 = torch.argmax(model4(torch.tensor(X_test, dtype=torch.float64)), dim=1).numpy()
    acc4 = accuracy_score(y_test, pred4)
    results['åˆå§‹åŒ–PyTorch'] = acc4
    print(f"   å‡†ç¡®ç‡: {acc4:.4f}, è®­ç»ƒè½®æ•°: {model4.n_iter_}")
    
    # 5. æ¢¯åº¦è£å‰ªæµ‹è¯•
    print("5ï¸âƒ£ å®Œæ•´PyTorch (float64 + SGD+momentum + Xavieråˆå§‹åŒ– + æ¢¯åº¦è£å‰ª)")
    model5 = ControlledPyTorchModel(n_features, n_classes, hidden_sizes)
    model5.init_xavier_uniform()
    model5 = train_controlled_pytorch(model5, X_train, y_train, X_val, y_val,
                                    use_float64=True, optimizer_type='sgd_momentum', use_grad_clip=True)
    model5.eval()
    with torch.no_grad():
        pred5 = torch.argmax(model5(torch.tensor(X_test, dtype=torch.float64)), dim=1).numpy()
    acc5 = accuracy_score(y_test, pred5)
    results['å®Œæ•´PyTorch'] = acc5
    print(f"   å‡†ç¡®ç‡: {acc5:.4f}, è®­ç»ƒè½®æ•°: {model5.n_iter_}")
    
    # 6. CausalEngine deterministicæ¨¡å¼
    print("6ï¸âƒ£ CausalEngine (deterministicæ¨¡å¼)")
    causal_det = MLPCausalClassifier(
        hidden_layer_sizes=hidden_sizes,
        mode='deterministic',
        max_iter=max_iter,
        learning_rate=lr,
        early_stopping=True,
        random_state=42,
        verbose=False
    )
    causal_det.fit(X_train, y_train)
    pred6 = causal_det.predict(X_test)
    if isinstance(pred6, dict):
        pred6 = pred6['predictions']
    acc6 = accuracy_score(y_test, pred6)
    results['CausalEngine'] = acc6
    print(f"   å‡†ç¡®ç‡: {acc6:.4f}, è®­ç»ƒè½®æ•°: {causal_det.n_iter_}")
    
    # 7. sklearnåŸºçº¿
    print("7ï¸âƒ£ sklearn MLPClassifier")
    sklearn_clf = MLPClassifier(
        hidden_layer_sizes=hidden_sizes,
        max_iter=max_iter,
        learning_rate_init=lr,
        early_stopping=True,
        validation_fraction=0.15,
        random_state=42
    )
    sklearn_clf.fit(X_train, y_train)
    pred7 = sklearn_clf.predict(X_test)
    acc7 = accuracy_score(y_test, pred7)
    results['sklearn'] = acc7
    print(f"   å‡†ç¡®ç‡: {acc7:.4f}, è®­ç»ƒè½®æ•°: {sklearn_clf.n_iter_}")
    
    # ç»“æœåˆ†æ
    print("\nğŸ“Š å®éªŒç»“æœåˆ†æ:")
    print("=" * 80)
    print(f"{'æ–¹æ³•':<20} {'å‡†ç¡®ç‡':<10} {'æå‡':<10}")
    print("-" * 40)
    
    baseline_acc = results['åŸºçº¿PyTorch']
    for method, acc in results.items():
        improvement = acc - baseline_acc
        print(f"{method:<20} {acc:<10.4f} {improvement:+.4f}")
    
    print("\nğŸ” å·®å¼‚å› ç´ å½±å“åˆ†æ:")
    factors = [
        ('æ•°å€¼ç²¾åº¦', results['é«˜ç²¾åº¦PyTorch'] - results['åŸºçº¿PyTorch']),
        ('ä¼˜åŒ–å™¨', results['ä¼˜åŒ–å™¨PyTorch'] - results['é«˜ç²¾åº¦PyTorch']),
        ('åˆå§‹åŒ–', results['åˆå§‹åŒ–PyTorch'] - results['ä¼˜åŒ–å™¨PyTorch']),
        ('æ¢¯åº¦è£å‰ª', results['å®Œæ•´PyTorch'] - results['åˆå§‹åŒ–PyTorch']),
    ]
    
    for factor, improvement in factors:
        print(f"   {factor}: {improvement:+.4f}")
    
    total_pytorch_improvement = results['å®Œæ•´PyTorch'] - results['åŸºçº¿PyTorch']
    causal_advantage = results['CausalEngine'] - results['å®Œæ•´PyTorch']
    
    print(f"\næ€»PyTorchæ”¹è¿›: {total_pytorch_improvement:+.4f}")
    print(f"å‰©ä½™CausalEngineä¼˜åŠ¿: {causal_advantage:+.4f}")
    
    if causal_advantage > 0.01:
        print("ğŸ¯ CausalEngineä»æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¯èƒ½æ¥è‡ªï¼š")
        print("   - AbductionNetworkçš„æ’ç­‰åˆå§‹åŒ–ç­–ç•¥")
        print("   - æ›´ç²¾ç»†çš„æ—©åœç­–ç•¥")
        print("   - æ¶æ„ä¸Šçš„å¾®å¦™å·®å¼‚")
    else:
        print("âœ… ä¸»è¦å·®å¼‚å·²è¢«è§£é‡Šï¼Œdeterministicæ¨¡å¼çš„ä¼˜åŠ¿ä¸»è¦æ¥è‡ªä¸Šè¿°å› ç´ ")


if __name__ == "__main__":
    run_controlled_experiment()
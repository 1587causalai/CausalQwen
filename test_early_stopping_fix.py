#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æ—©åœç­–ç•¥

éªŒè¯æ‰€æœ‰ç¥ç»ç½‘ç»œå’Œæ”¯æŒæ—©åœçš„æ–¹æ³•éƒ½ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†è¿›è¡Œæ—©åœï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚
"""

import numpy as np
from sklearn.datasets import make_regression
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from causal_sklearn.benchmarks import BaselineBenchmark

def test_early_stopping_consistency():
    """æµ‹è¯•æ—©åœç­–ç•¥çš„ä¸€è‡´æ€§"""
    print("ğŸ” æµ‹è¯•æ—©åœç­–ç•¥ä¸€è‡´æ€§")
    print("=" * 60)
    
    # åˆ›å»ºå°å‹æ•°æ®é›†
    X, y = make_regression(n_samples=500, n_features=8, noise=0.1, random_state=42)
    
    # æµ‹è¯•æ–¹æ³•åˆ—è¡¨ï¼šé‡ç‚¹æµ‹è¯•ç¥ç»ç½‘ç»œå’Œæ”¯æŒæ—©åœçš„æ–¹æ³•
    test_methods = [
        'sklearn_mlp',      # åº”è¯¥ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ
        'pytorch_mlp',      # åº”è¯¥ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ  
        'xgboost',         # åº”è¯¥ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ
        'lightgbm',        # åº”è¯¥ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ
        'random_forest'    # ä¸æ”¯æŒæ—©åœï¼Œæ­£å¸¸è®­ç»ƒ
    ]
    
    try:
        benchmark = BaselineBenchmark()
        
        print(f"ğŸ§ª æµ‹è¯•æ–¹æ³•: {test_methods}")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {X.shape[0]} æ ·æœ¬ Ã— {X.shape[1]} ç‰¹å¾")
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = benchmark.compare_models(
            X=X, 
            y=y,
            task_type='regression',
            baseline_methods=test_methods,
            causal_modes=['deterministic'],  # åªæµ‹è¯•ä¸€ä¸ªCausalEngineæ¨¡å¼
            anomaly_ratio=0.0,  # æ— å™ªå£°ï¼Œå…³æ³¨è®­ç»ƒç­–ç•¥
            verbose=True
        )
        
        print(f"\nâœ… æ—©åœç­–ç•¥æµ‹è¯•å®Œæˆï¼Œå¾—åˆ° {len(results)} ä¸ªç»“æœ")
        
        # åˆ†æç»“æœ
        print("\nğŸ“Š æ€§èƒ½ç»“æœæ‘˜è¦:")
        print("-" * 50)
        
        for method, metrics in results.items():
            test_r2 = metrics['test']['RÂ²']
            test_mae = metrics['test']['MAE']
            val_r2 = metrics['val']['RÂ²']
            val_mae = metrics['val']['MAE']
            
            # æ£€æŸ¥è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆéªŒè¯é›†vsæµ‹è¯•é›†æ€§èƒ½å·®å¼‚ï¼‰
            r2_gap = abs(val_r2 - test_r2)
            mae_gap = abs(val_mae - test_mae)
            
            print(f"  {method:<15}")
            print(f"    æµ‹è¯•é›†:  RÂ² = {test_r2:.4f}, MAE = {test_mae:.3f}")
            print(f"    éªŒè¯é›†:  RÂ² = {val_r2:.4f}, MAE = {val_mae:.3f}")
            print(f"    å·®å¼‚:    Î”RÂ² = {r2_gap:.4f}, Î”MAE = {mae_gap:.3f}")
            
            # å¥åº·æ€§æ£€æŸ¥
            if r2_gap > 0.1 or mae_gap > 0.2:
                print(f"    âš ï¸ å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
            else:
                print(f"    âœ… æ³›åŒ–è‰¯å¥½")
            print()
        
        # éªŒè¯æ—©åœæ˜¯å¦ç”Ÿæ•ˆ
        print("ğŸ” æ—©åœéªŒè¯:")
        print("-" * 30)
        
        sklearn_results = results.get('sklearn MLP', results.get('sklearn', None))
        pytorch_results = results.get('PyTorch MLP', results.get('pytorch', None))
        
        if sklearn_results and pytorch_results:
            sklearn_r2 = sklearn_results['test']['RÂ²']
            pytorch_r2 = pytorch_results['test']['RÂ²']
            
            print(f"sklearn MLP RÂ²: {sklearn_r2:.4f}")
            print(f"PyTorch MLP RÂ²: {pytorch_r2:.4f}")
            
            # å¦‚æœä¸¤è€…æ€§èƒ½ç›¸è¿‘ï¼Œè¯´æ˜æ—©åœç­–ç•¥ç»Ÿä¸€äº†
            if abs(sklearn_r2 - pytorch_r2) < 0.2:
                print("âœ… sklearnå’ŒPyTorch MLPæ€§èƒ½ç›¸è¿‘ï¼Œæ—©åœç­–ç•¥å¯èƒ½ç»Ÿä¸€äº†")
            else:
                print("âš ï¸ sklearnå’ŒPyTorch MLPæ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ—©åœç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_validation_set_usage():
    """æµ‹è¯•éªŒè¯é›†ä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ” æµ‹è¯•éªŒè¯é›†ä½¿ç”¨æƒ…å†µ")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®é›†
    X, y = make_regression(n_samples=300, n_features=5, noise=0.05, random_state=42)
    
    try:
        benchmark = BaselineBenchmark()
        
        # åªæµ‹è¯•ç¥ç»ç½‘ç»œæ–¹æ³•
        neural_methods = ['sklearn_mlp', 'pytorch_mlp']
        
        results = benchmark.compare_models(
            X=X, 
            y=y,
            task_type='regression',
            baseline_methods=neural_methods,
            causal_modes=['standard'],
            test_size=0.2,
            val_size=0.2,  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„éªŒè¯é›†
            anomaly_ratio=0.0,
            verbose=True
        )
        
        print(f"\nâœ… éªŒè¯é›†ä½¿ç”¨æµ‹è¯•å®Œæˆ")
        
        # æ£€æŸ¥ç»“æœçš„åˆç†æ€§
        all_reasonable = True
        
        for method, metrics in results.items():
            test_r2 = metrics['test']['RÂ²']
            val_r2 = metrics['val']['RÂ²']
            
            # æ£€æŸ¥RÂ²æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            if test_r2 < 0.5 or val_r2 < 0.5:
                print(f"âš ï¸ {method} æ€§èƒ½è¾ƒä½: test RÂ² = {test_r2:.3f}, val RÂ² = {val_r2:.3f}")
                all_reasonable = False
            else:
                print(f"âœ… {method} æ€§èƒ½åˆç†: test RÂ² = {test_r2:.3f}, val RÂ² = {val_r2:.3f}")
        
        if all_reasonable:
            print("âœ… æ‰€æœ‰æ–¹æ³•æ€§èƒ½éƒ½åœ¨åˆç†èŒƒå›´å†…ï¼ŒéªŒè¯é›†ç­–ç•¥å¯èƒ½æ­£ç¡®")
        else:
            print("âš ï¸ éƒ¨åˆ†æ–¹æ³•æ€§èƒ½å¼‚å¸¸ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´éªŒè¯é›†ç­–ç•¥")
        
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯é›†ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æ—©åœç›¸å…³æµ‹è¯•"""
    print("ğŸ§ª æ—©åœç­–ç•¥ä¿®å¤éªŒè¯")
    print("=" * 70)
    
    tests = [
        ("æ—©åœç­–ç•¥ä¸€è‡´æ€§", test_early_stopping_consistency),
        ("éªŒè¯é›†ä½¿ç”¨æƒ…å†µ", test_validation_set_usage),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results[test_name] = success
        except Exception as e:
            print(f"âŒ æµ‹è¯• '{test_name}' å‡ºç°å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ€»ç»“æŠ¥å‘Š
    print("\nğŸ“Š æ—©åœä¿®å¤éªŒè¯æ€»ç»“")
    print("=" * 70)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"   {test_name:<20} {status}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ—©åœç­–ç•¥ä¿®å¤éªŒè¯é€šè¿‡ï¼æ‰€æœ‰æ–¹æ³•ç°åœ¨ä½¿ç”¨ä¸€è‡´çš„éªŒè¯é›†ç­–ç•¥ã€‚")
        print("\nğŸ’¡ ä¿®å¤æ€»ç»“:")
        print("   âœ… sklearn MLP: ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœï¼Œä¸å†å†…éƒ¨åˆ’åˆ†")
        print("   âœ… PyTorch MLP: ç»§ç»­ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ")
        print("   âœ… XGBoost/LightGBM: æ·»åŠ äº†å¤–éƒ¨éªŒè¯é›†æ—©åœæ”¯æŒ")
        print("   âœ… CausalEngine: ç»§ç»­ä½¿ç”¨å¤–éƒ¨éªŒè¯é›†æ—©åœ")
        print("   âœ… ç»Ÿä¸€æ—©åœå‚æ•°: patience=50, tol=1e-4")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œæ—©åœç­–ç•¥å¯èƒ½è¿˜éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
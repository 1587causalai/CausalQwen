# CausalEngine Sklearnæ¥å£ä½¿ç”¨æ•™ç¨‹

> **ç›®æ ‡è¯»è€…**: æ•°æ®ç§‘å­¦å®¶ã€æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆã€ç ”ç©¶äººå‘˜  
> **å‰ç½®çŸ¥è¯†**: ç†Ÿæ‚‰sklearnåŸºç¡€ç”¨æ³•  
> **é¢„è®¡ç”¨æ—¶**: 30åˆ†é’Ÿ

## ğŸ“– æ•™ç¨‹æ¦‚è¿°

æœ¬æ•™ç¨‹å°†å¸¦æ‚¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨CausalEngineçš„sklearné£æ ¼æ¥å£è¿›è¡Œæœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚CausalEngineæä¾›äº†ä¸¤ä¸ªæ ¸å¿ƒä¼°è®¡å™¨ï¼š`MLPCausalRegressor`å’Œ`MLPCausalClassifier`ï¼Œå®ƒä»¬å¯ä»¥ç›´æ¥æ›¿ä»£sklearnçš„`MLPRegressor`å’Œ`MLPClassifier`ï¼ŒåŒæ—¶æä¾›æ›´å¼ºçš„å™ªå£°é²æ£’æ€§å’Œå› æœæ¨ç†èƒ½åŠ›ã€‚

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

1. **sklearnå®Œå…¨å…¼å®¹**: æ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç ï¼Œç›´æ¥æ›¿æ¢å³å¯
2. **å™ªå£°é²æ£’æ€§**: å¯¹æ ‡ç­¾å™ªå£°å…·æœ‰å¤©ç„¶å…ç–«åŠ›
3. **ç»Ÿä¸€é¢„æµ‹æ¥å£**: ä»ç®€å•ç‚¹ä¼°è®¡åˆ°å®Œæ•´æ¦‚ç‡åˆ†å¸ƒ
4. **æ•°å­¦åˆ›æ–°**: åŸºäºCauchyåˆ†å¸ƒå’ŒOvRç­–ç•¥çš„å› æœæ¨ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…å’Œå¯¼å…¥

```python
# å¯¼å…¥CausalEngine sklearnæ¥å£
from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier

# å¯¼å…¥sklearnå·¥å…·ï¼ˆå®Œå…¨å…¼å®¹ï¼‰
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, accuracy_score
from sklearn.datasets import make_regression, make_classification
```

### åŸºç¡€å›å½’ç¤ºä¾‹

```python
import numpy as np

# ç”Ÿæˆå›å½’æ•°æ®
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ä½¿ç”¨CausalEngineå›å½’å™¨ï¼ˆç”¨æ³•ä¸sklearnç›¸åŒï¼‰
reg = MLPCausalRegressor(
    hidden_layer_sizes=(64, 32),    # MLPç»“æ„
    max_iter=500,                   # è®­ç»ƒè½®æ•°
    random_state=42                 # éšæœºç§å­
)

# è®­ç»ƒæ¨¡å‹
reg.fit(X_train, y_train)

# é¢„æµ‹ï¼ˆsklearnå…¼å®¹æ¨¡å¼ï¼‰
predictions = reg.predict(X_test)
r2 = r2_score(y_test, predictions)

print(f"RÂ² Score: {r2:.4f}")
```

### åŸºç¡€åˆ†ç±»ç¤ºä¾‹

```python
# ç”Ÿæˆåˆ†ç±»æ•°æ®
X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, 
                          n_redundant=0, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ä½¿ç”¨CausalEngineåˆ†ç±»å™¨
clf = MLPCausalClassifier(
    hidden_layer_sizes=(64, 32),
    max_iter=500,
    random_state=42
)

# è®­ç»ƒæ¨¡å‹
clf.fit(X_train, y_train)

# é¢„æµ‹ç±»åˆ«
predictions = clf.predict(X_test)
accuracy = accuracy_score(y_test, predictions)

# é¢„æµ‹æ¦‚ç‡
probabilities = clf.predict_proba(X_test)

print(f"Accuracy: {accuracy:.4f}")
print(f"Probability shape: {probabilities.shape}")
```

## ğŸ›ï¸ é«˜çº§åŠŸèƒ½ï¼šå¤šæ¨¡å¼é¢„æµ‹

CausalEngineçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºç»Ÿä¸€çš„`predict()`æ¥å£æ”¯æŒå¤šç§é¢„æµ‹æ¨¡å¼ï¼š

### å›å½’å™¨çš„å››ç§æ¨¡å¼

```python
# è®­ç»ƒæ¨¡å‹
reg = MLPCausalRegressor()
reg.fit(X_train, y_train)

# 1. Compatibleæ¨¡å¼ï¼šsklearnå…¼å®¹ï¼Œè¿”å›ç‚¹ä¼°è®¡
compatible_pred = reg.predict(X_test, mode='compatible')
print(f"Compatible: {compatible_pred[:3]}")

# 2. Standardæ¨¡å¼ï¼šè¿”å›å®Œæ•´åˆ†å¸ƒä¿¡æ¯
standard_pred = reg.predict(X_test, mode='standard')
print(f"Standard keys: {list(standard_pred.keys())}")
print(f"Predictions: {standard_pred['predictions'][:3]}")

# 3. Causalæ¨¡å¼ï¼šçº¯å› æœæ¨ç†ï¼ˆæ— å¤–ç”Ÿå™ªå£°ï¼‰
causal_pred = reg.predict(X_test, mode='causal')

# 4. Samplingæ¨¡å¼ï¼šæ¢ç´¢æ€§é¢„æµ‹ï¼ˆå¢å¤§ä¸ç¡®å®šæ€§ï¼‰
sampling_pred = reg.predict(X_test, mode='sampling')
```

### åˆ†ç±»å™¨çš„åŒæ¦‚ç‡ç­–ç•¥

```python
# è®­ç»ƒæ¨¡å‹
clf = MLPCausalClassifier()
clf.fit(X_train, y_train)

# Softmaxå…¼å®¹æ¦‚ç‡ï¼ˆä¸¥æ ¼å½’ä¸€åŒ–ï¼‰
softmax_proba = clf.predict_proba(X_test, mode='compatible')
print(f"Softmaxæ¦‚ç‡å’Œ: {softmax_proba[0].sum():.6f}")  # = 1.000000

# OvRåŸç”Ÿæ¦‚ç‡ï¼ˆç‹¬ç«‹æ¿€æ´»ï¼‰
ovr_proba = clf.predict_proba(X_test, mode='standard')
print(f"OvRæ¦‚ç‡å’Œ: {ovr_proba[0].sum():.6f}")  # â‰ˆ 1.0 ä½†ä¸ä¸¥æ ¼

# é«˜çº§é¢„æµ‹æ¨¡å¼
advanced_pred = clf.predict(X_test, mode='standard')
print(f"Predicted classes: {advanced_pred['predictions'][:5]}")
print(f"Probability shape: {advanced_pred['probabilities'].shape}")
```

## ğŸ›¡ï¸ å™ªå£°é²æ£’æ€§æ¼”ç¤º

CausalEngineçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯å¯¹æ ‡ç­¾å™ªå£°çš„å¤©ç„¶é²æ£’æ€§ï¼š

### åˆ†ç±»ä»»åŠ¡å™ªå£°é²æ£’æ€§

```python
def add_label_noise(y, noise_level):
    """æ·»åŠ éšæœºæ ‡ç­¾ç¿»è½¬å™ªå£°"""
    y_noisy = y.copy()
    n_noise = int(len(y) * noise_level)
    noise_indices = np.random.choice(len(y), n_noise, replace=False)
    
    for idx in noise_indices:
        available_labels = [l for l in np.unique(y) if l != y[idx]]
        y_noisy[idx] = np.random.choice(available_labels)
    
    return y_noisy

# ç”Ÿæˆå¹²å‡€æ•°æ®
X, y_clean = make_classification(n_samples=1000, n_features=10, n_classes=3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y_clean, test_size=0.2, random_state=42)

# æ·»åŠ 20%æ ‡ç­¾å™ªå£°
y_train_noisy = add_label_noise(y_train, noise_level=0.2)

# å¯¹æ¯”ä¼ ç»Ÿæ–¹æ³• vs CausalEngine
from sklearn.neural_network import MLPClassifier

# ä¼ ç»Ÿæ–¹æ³•åœ¨å™ªå£°æ•°æ®ä¸Šè®­ç»ƒ
traditional_clf = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)
traditional_clf.fit(X_train, y_train_noisy)
traditional_acc = accuracy_score(y_test, traditional_clf.predict(X_test))

# CausalEngineåœ¨å™ªå£°æ•°æ®ä¸Šè®­ç»ƒ
causal_clf = MLPCausalClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)
causal_clf.fit(X_train, y_train_noisy)
causal_acc = accuracy_score(y_test, causal_clf.predict(X_test))

print(f"ä¼ ç»Ÿæ–¹æ³•å‡†ç¡®ç‡: {traditional_acc:.4f}")
print(f"CausalEngineå‡†ç¡®ç‡: {causal_acc:.4f}")
print(f"å™ªå£°é²æ£’æ€§ä¼˜åŠ¿: +{(causal_acc - traditional_acc)*100:.1f}%")
```

### å›å½’ä»»åŠ¡å™ªå£°é²æ£’æ€§

```python
def add_magnitude_noise(y, magnitude_factor=10):
    """æ·»åŠ æ•°é‡çº§é”™è¯¯å™ªå£°"""
    y_noisy = y.copy()
    n_errors = int(0.1 * len(y))  # 10%çš„æ•°æ®æœ‰é”™è¯¯
    error_indices = np.random.choice(len(y), n_errors, replace=False)
    y_noisy[error_indices] *= magnitude_factor  # æ•°é‡çº§é”™è¯¯
    return y_noisy

# ç”Ÿæˆå›å½’æ•°æ®å¹¶æ·»åŠ å™ªå£°
X, y_clean = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y_clean, test_size=0.2, random_state=42)

y_train_noisy = add_magnitude_noise(y_train, magnitude_factor=10)

# å¯¹æ¯”å®éªŒ
from sklearn.neural_network import MLPRegressor

traditional_reg = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)
traditional_reg.fit(X_train, y_train_noisy)
traditional_r2 = r2_score(y_test, traditional_reg.predict(X_test))

causal_reg = MLPCausalRegressor(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)
causal_reg.fit(X_train, y_train_noisy)
causal_r2 = r2_score(y_test, causal_reg.predict(X_test))

print(f"ä¼ ç»Ÿæ–¹æ³•RÂ²: {traditional_r2:.4f}")
print(f"CausalEngine RÂ²: {causal_r2:.4f}")
print(f"æ€§èƒ½ä¿æŒ: {causal_r2/traditional_r2:.2f}x")
```

## ğŸ”§ å‚æ•°é…ç½®æŒ‡å—

### å›å½’å™¨å‚æ•°

```python
reg = MLPCausalRegressor(
    # MLPç»“æ„ï¼ˆä¸sklearnå…¼å®¹ï¼‰
    hidden_layer_sizes=(64, 32),       # éšè—å±‚ç»“æ„
    
    # è®­ç»ƒå‚æ•°
    max_iter=1000,                     # æœ€å¤§è¿­ä»£æ¬¡æ•°
    learning_rate=0.001,               # å­¦ä¹ ç‡
    
    # CausalEngineç‰¹æœ‰å‚æ•°
    default_mode='compatible',         # é»˜è®¤é¢„æµ‹æ¨¡å¼
    causal_size=None,                  # å› æœè¡¨å¾ç»´åº¦ï¼ˆé»˜è®¤è‡ªåŠ¨ï¼‰
    
    # æ—©åœå‚æ•°
    early_stopping=True,               # å¯ç”¨æ—©åœ
    validation_fraction=0.1,           # éªŒè¯é›†æ¯”ä¾‹
    n_iter_no_change=20,              # æ—©åœè€å¿ƒå€¼
    tol=1e-4,                         # æ—©åœå®¹å¿åº¦
    
    # å…¶ä»–
    random_state=42,                   # éšæœºç§å­
    verbose=False                      # è®­ç»ƒæ—¥å¿—
)
```

### åˆ†ç±»å™¨å‚æ•°

```python
clf = MLPCausalClassifier(
    # åŸºç¡€å‚æ•°ï¼ˆä¸å›å½’å™¨ç›¸åŒï¼‰
    hidden_layer_sizes=(64, 32),
    max_iter=1000,
    learning_rate=0.001,
    default_mode='compatible',
    causal_size=None,
    
    # åˆ†ç±»ç‰¹æœ‰çš„æ—©åœå‚æ•°
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,
    tol=1e-4,
    
    random_state=42,
    verbose=False
)
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### è®­ç»ƒè¿‡ç¨‹ç›‘æ§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
reg = MLPCausalRegressor(verbose=True, max_iter=1000)
reg.fit(X_train, y_train)

# æŸ¥çœ‹æŸå¤±æ›²çº¿
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(reg.loss_curve_)
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Cauchy NLL Loss')
plt.grid(True)
plt.show()

print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {reg.loss_curve_[-1]:.4f}")
print(f"è®­ç»ƒè½®æ•°: {len(reg.loss_curve_)}")
```

### æ¨¡å‹å±æ€§æ£€æŸ¥

```python
# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
print(f"æ˜¯å¦å·²è®­ç»ƒ: {reg.is_fitted_}")
print(f"è¾“å…¥ç‰¹å¾æ•°: {reg.n_features_in_}")
print(f"è¾“å‡ºç»´åº¦: {reg.n_outputs_}")

# ç‰¹å¾é‡è¦æ€§ï¼ˆç®€å•å®ç°ï¼‰
feature_importance = reg.feature_importances_
print(f"ç‰¹å¾é‡è¦æ€§: {feature_importance}")

# æ¨¡å‹ç»„ä»¶
print(f"éšè—å±‚ç»“æ„: {reg.hidden_layer_sizes}")
print(f"å› æœè¡¨å¾ç»´åº¦: {reg.causal_size}")
```

## ğŸ”— ä¸sklearnç”Ÿæ€é›†æˆ

### äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

# å›å½’äº¤å‰éªŒè¯
reg_scores = cross_val_score(
    MLPCausalRegressor(max_iter=200), 
    X, y, 
    cv=5, 
    scoring='r2'
)
print(f"å›å½’CV RÂ²: {reg_scores.mean():.4f} Â± {reg_scores.std():.4f}")

# åˆ†ç±»äº¤å‰éªŒè¯
clf_scores = cross_val_score(
    MLPCausalClassifier(max_iter=200), 
    X, y, 
    cv=StratifiedKFold(5), 
    scoring='accuracy'
)
print(f"åˆ†ç±»CVå‡†ç¡®ç‡: {clf_scores.mean():.4f} Â± {clf_scores.std():.4f}")
```

### ç½‘æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV

# å‚æ•°ç½‘æ ¼
param_grid = {
    'hidden_layer_sizes': [(32,), (64,), (64, 32)],
    'learning_rate': [0.001, 0.01],
    'default_mode': ['compatible', 'standard']
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    MLPCausalRegressor(max_iter=200),
    param_grid,
    cv=3,
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")
```

### Pipelineé›†æˆ

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# æ„å»ºpipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', MLPCausalRegressor(max_iter=200))
])

# ä½¿ç”¨pipeline
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
score = pipeline.score(X_test, y_test)

print(f"Pipeline RÂ²: {score:.4f}")
```

## âš ï¸ æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ

### æ•°æ®é¢„å¤„ç†

```python
# æ¨èçš„æ•°æ®é¢„å¤„ç†pipeline
from sklearn.preprocessing import StandardScaler

# ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆæ¨èï¼‰
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# CausalEngineå¯¹å™ªå£°é²æ£’ï¼Œä½†æ ‡å‡†åŒ–ä»æœ‰å¸®åŠ©
reg = MLPCausalRegressor()
reg.fit(X_train_scaled, y_train)
```

### è¶…å‚æ•°å»ºè®®

```python
# æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´ç½‘ç»œç»“æ„
def get_recommended_structure(n_features, n_samples):
    """æ ¹æ®æ•°æ®è§„æ¨¡æ¨èç½‘ç»œç»“æ„"""
    if n_features <= 10:
        return (32,)
    elif n_features <= 50:
        return (64, 32)
    elif n_features <= 100:
        return (128, 64)
    else:
        return (256, 128, 64)

# æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´å­¦ä¹ ç‡
def get_recommended_lr(task_type, data_size):
    """æ ¹æ®ä»»åŠ¡ç±»å‹æ¨èå­¦ä¹ ç‡"""
    base_lr = 0.001
    if task_type == 'classification':
        return base_lr * 0.5  # åˆ†ç±»ä»»åŠ¡ç¨ä½
    elif data_size < 1000:
        return base_lr * 2    # å°æ•°æ®é›†ç¨é«˜
    return base_lr
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **GPUåŠ é€Ÿ**: CausalEngineè‡ªåŠ¨ä½¿ç”¨å¯ç”¨çš„GPU
2. **æ—©åœç­–ç•¥**: é»˜è®¤å¯ç”¨ï¼Œå¯é¿å…è¿‡æ‹Ÿåˆ
3. **æ‰¹é‡å¤§å°**: è‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®
4. **å†…å­˜ä¼˜åŒ–**: å¤§æ•°æ®é›†æ—¶è€ƒè™‘åˆ†æ‰¹è®­ç»ƒ

### å¸¸è§é—®é¢˜è§£å†³

**Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**
```python
# å‡å°‘ç½‘ç»œå¤§å°æˆ–è®­ç»ƒè½®æ•°
reg = MLPCausalRegressor(
    hidden_layer_sizes=(32,),  # æ›´å°çš„ç½‘ç»œ
    max_iter=200,              # æ›´å°‘çš„è½®æ•°
    early_stopping=True        # ç¡®ä¿æ—©åœå¯ç”¨
)
```

**Q: å¦‚ä½•å¤„ç†ä¸å¹³è¡¡æ•°æ®ï¼Ÿ**
```python
# åˆ†ç±»ä»»åŠ¡ï¼šstratifiedé‡‡æ ·å·²å†…ç½®
# ç‰¹æ®Šæƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨class_weightï¼ˆæœªæ¥ç‰ˆæœ¬æ”¯æŒï¼‰
from sklearn.utils.class_weight import compute_class_weight

# å½“å‰ç‰ˆæœ¬å»ºè®®é¢„å¤„ç†æ•°æ®å¹³è¡¡
```

**Q: å¦‚ä½•è§£é‡Šæ¨¡å‹é¢„æµ‹ï¼Ÿ**
```python
# ä½¿ç”¨é«˜çº§é¢„æµ‹æ¨¡å¼è·å–æ›´å¤šä¿¡æ¯
advanced_pred = reg.predict(X_test, mode='standard')
distributions = advanced_pred['distributions']

# æ£€æŸ¥ä¸ç¡®å®šæ€§
if 'scale_S' in distributions:
    uncertainty = distributions['scale_S']
    print(f"é¢„æµ‹ä¸ç¡®å®šæ€§: {uncertainty.mean():.4f}")
```

## ğŸ“ æ€»ç»“

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å­¦ä¼šäº†ï¼š

1. âœ… **åŸºç¡€ä½¿ç”¨**: å¦‚ä½•ç”¨CausalEngineæ›¿ä»£sklearnä¼°è®¡å™¨
2. âœ… **é«˜çº§åŠŸèƒ½**: å¤šæ¨¡å¼é¢„æµ‹å’ŒåŒæ¦‚ç‡ç­–ç•¥
3. âœ… **å™ªå£°é²æ£’æ€§**: éªŒè¯å’Œåˆ©ç”¨æŠ—å™ªå£°èƒ½åŠ›
4. âœ… **ç”Ÿæ€é›†æˆ**: ä¸sklearnå·¥å…·é“¾æ— ç¼é…åˆ
5. âœ… **æœ€ä½³å®è·µ**: å‚æ•°è°ƒä¼˜å’Œæ€§èƒ½ä¼˜åŒ–

CausalEngine sklearnæ¥å£ä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªå¼ºå¤§è€Œæ˜“ç”¨çš„æœºå™¨å­¦ä¹ å·¥å…·ï¼Œæ—¢ä¿æŒäº†sklearnçš„ç®€æ´æ€§ï¼Œåˆå¼•å…¥äº†å‰æ²¿çš„å› æœæ¨ç†èƒ½åŠ›ã€‚å¼€å§‹åœ¨æ‚¨çš„é¡¹ç›®ä¸­ä½¿ç”¨å§ï¼

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [CausalEngineæ•°å­¦åŸºç¡€æ–‡æ¡£](../causal_engine/MATHEMATICAL_FOUNDATIONS_CN.md)
- [sklearné£æ ¼APIè®¾è®¡æ–‡æ¡£](./sklearn_style_api_regressor_v1.md)
- [å®éªŒæŠ¥å‘Š](./sklearn_interface_experiment_report.md)

---

*æœ€åæ›´æ–°: 2024å¹´6æœˆ*
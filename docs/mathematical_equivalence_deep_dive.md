# CausalEngine æ•°å­¦ç­‰ä»·æ€§éªŒè¯

> **æ ¸å¿ƒå‘½é¢˜**: å½“ AbductionNetwork çš„ loc_net è¢«å†»ç»“ä¸ºæ’ç­‰æ˜ å°„ä¸”ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°æ—¶ï¼ŒCausalEngine ä¸ä¼ ç»Ÿ MLP æ•°å­¦ç­‰ä»·  
> **éªŒè¯ç»“æœ**: é€šè¿‡ç†è®ºæ¨å¯¼å’Œå®éªŒéªŒè¯è¯æ˜äº†ç­‰ä»·æ€§å‡è®¾

## 1. ç†è®ºåŸºç¡€

### 1.1 ç­‰ä»·æ€§å®šä¹‰

è®¾ä¼ ç»Ÿ MLP ä¸ºå‡½æ•° $f_{MLP}: \mathbb{R}^d \rightarrow \mathbb{R}^k$ï¼š
$$f_{MLP}(x) = W_n \sigma(W_{n-1} \sigma(...\sigma(W_1 x + b_1)...) + b_{n-1}) + b_n$$

è®¾ CausalEngine åœ¨å†»ç»“æ¡ä»¶ä¸‹ä¸ºå‡½æ•° $f_{CE}: \mathbb{R}^d \rightarrow \mathbb{R}^k$ï¼š
$$f_{CE}(x) = \text{ActivationHead}(\text{ActionNetwork}(I(\text{MLPHidden}(x))))$$

å…¶ä¸­ $I$ ä¸ºæ’ç­‰æ˜ å°„ï¼ˆå†»ç»“çš„ AbductionNetwork ä½ç½®ç½‘ç»œï¼‰

**ç­‰ä»·æ€§å‘½é¢˜**ï¼š
$$f_{MLP}(x) \approx f_{CE}(x) \quad \text{å½“æ»¡è¶³å†»ç»“æ¡ä»¶æ—¶}$$

### 1.2 ç­‰ä»·æ€§æ¡ä»¶

```mermaid
graph TB
    subgraph Conditions["ç­‰ä»·æ€§æˆç«‹çš„å¿…è¦æ¡ä»¶"]
        direction TB
        C1["ç›¸åŒçš„ MLP ç‰¹å¾æå–ç½‘ç»œ<br/>MLPHidden(x) å®Œå…¨ä¸€è‡´"]
        C2["AbductionNetwork çš„ loc_net<br/>å†»ç»“ä¸ºæ’ç­‰æ˜ å°„ I(H) = H"]
        C3["ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°<br/>MSE (å›å½’) / CrossEntropy (åˆ†ç±»)"]
        C4["ç›¸åŒçš„è®­ç»ƒé…ç½®<br/>æƒé‡åˆå§‹åŒ–ã€ä¼˜åŒ–å™¨ã€è¶…å‚æ•°"]
    end
    
    C1 --> C2 --> C3 --> C4
    
    classDef conditionStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    class C1,C2,C3,C4 conditionStyle
```

## 2. æ•°å­¦æ¨å¯¼

### 2.1 ä¸¤ç§æ¶æ„çš„æ•°å­¦æµç¨‹å¯¹æ¯”

```mermaid
graph TB
    Input[["è¾“å…¥ X âˆˆ â„^{NÃ—F}"]]
    
    subgraph Shared["å…±åŒçš„ç‰¹å¾æå–å±‚ï¼ˆå®Œå…¨ç›¸åŒï¼‰"]
        direction TB
        MLP["MLP ç‰¹å¾æå–<br/>H = ReLU(Wâ‚‚Â·ReLU(Wâ‚Â·X + bâ‚) + bâ‚‚)<br/>H âˆˆ â„^{NÃ—C}"]
    end
    
    subgraph sklearn["sklearn MLPRegressor/Classifier"]
        direction TB
        Direct["ç›´æ¥è¾“å‡ºå±‚<br/>y = W_out^T Â· H + b_out"]
    end
    
    subgraph CausalFrozen["CausalEngine (å†»ç»“æ¨¡å¼)"]
        direction TB
        Abduction["å½’å› æ¨æ–­ (å†»ç»“)<br/>Î¼_U = I(H) = H<br/>Î³_U = softplus(W_sÂ·H + b_s)"]
        Action["è¡ŒåŠ¨å†³ç­–<br/>Î¼_S = W_A^T Â· Î¼_U + b_A<br/>= W_A^T Â· H + b_A"]
        Activation["ä»»åŠ¡æ¿€æ´»<br/>y = a Â· Î¼_S + b<br/>= aÂ·(W_A^TÂ·H + b_A) + b"]
    end
    
    Input --> Shared
    Shared --> sklearn
    Shared --> CausalFrozen
    
    Abduction --> Action --> Activation
    
    subgraph Proof["æ•°å­¦ç­‰ä»·æ€§è¯æ˜"]
        direction TB
        Equivalence["å±•å¼€ CausalEngine:<br/>y = aÂ·(W_A^TÂ·H + b_A) + b<br/>= (aÂ·W_A^T)Â·H + (aÂ·b_A + b)<br/><br/>ä»¤ W_final = aÂ·W_A^T, b_final = aÂ·b_A + b<br/>åˆ™: y = W_final^TÂ·H + b_final<br/><br/>ä¸ sklearn å½¢å¼å®Œå…¨ä¸€è‡´ï¼"]
    end
    
    sklearn --> Proof
    CausalFrozen --> Proof
    
    %% æ ·å¼å®šä¹‰
    classDef inputStyle fill:#e1f5fe,stroke:#01579b,stroke-width:3px
    classDef sharedStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:3px
    classDef sklearnStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef causalStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef proofStyle fill:#ffebee,stroke:#c62828,stroke-width:3px
    
    class Input inputStyle
    class Shared,MLP sharedStyle
    class sklearn,Direct sklearnStyle
    class CausalFrozen,Abduction,Action,Activation causalStyle
    class Proof,Equivalence proofStyle
```

**å…³é”®æ´å¯Ÿ**ï¼šä¸Šå›¾æ¸…æ™°å±•ç¤ºäº†ä¸¤ä¸ªæ¶æ„å¦‚ä½•ä»å®Œå…¨ç›¸åŒçš„ç‰¹å¾ H å‡ºå‘ï¼Œé€šè¿‡ä¸åŒçš„æ•°å­¦å˜æ¢è·¯å¾„ï¼Œæœ€ç»ˆè¾¾åˆ°ç›¸åŒçš„çº¿æ€§å½¢å¼ã€‚

### 2.2 é€æ­¥æ•°å­¦æ¨å¯¼

ç»™å®šè¾“å…¥ $X \in \mathbb{R}^{N \times F}$ï¼Œæˆ‘ä»¬é€æ­¥æ¨å¯¼ CausalEngine å†»ç»“æ¨¡å¼ï¼š

#### Step 1: å…±åŒçš„ MLP ç‰¹å¾æå–
$$H = \text{MLP}(X) = \text{ReLU}(W_2 \cdot \text{ReLU}(W_1 \cdot X + b_1) + b_2) \in \mathbb{R}^{N \times C}$$

#### Step 2: AbductionNetworkï¼ˆå†»ç»“æ¨¡å¼ï¼‰
- **ä½ç½®ç½‘ç»œ**ï¼ˆå†»ç»“ä¸ºæ’ç­‰æ˜ å°„ï¼‰ï¼š$\mu_U = I(H) = H$
- **å°ºåº¦ç½‘ç»œ**ï¼ˆæ­£å¸¸è®­ç»ƒï¼‰ï¼š$\gamma_U = \text{softplus}(W_{scale} \cdot H + b_{scale})$

#### Step 3: ActionNetwork  
$$\mu_S = W_A^T \cdot \mu_U + b_A = W_A^T \cdot H + b_A$$

#### Step 4: ActivationHead
$$y = a \cdot \mu_S + b = a \cdot (W_A^T \cdot H + b_A) + b$$

#### Step 5: æœ€ç»ˆç­‰ä»·å½¢å¼
$$y = (a \cdot W_A^T) \cdot H + (a \cdot b_A + b)$$

ä»¤ $W_{final} = a \cdot W_A^T$ å’Œ $b_{final} = a \cdot b_A + b$ï¼Œåˆ™ï¼š
$$y = W_{final}^T \cdot \text{MLP}(X) + b_{final}$$

è¿™ä¸ sklearn çš„çº¿æ€§è¾“å‡ºå±‚å½¢å¼å®Œå…¨ä¸€è‡´ï¼š$y = W_{out}^T \cdot H + b_{out}$

## 3. å®éªŒéªŒè¯

### 3.1 å®éªŒè®¾è®¡åŸåˆ™

ä¸ºç¡®ä¿ä¸¥æ ¼çš„æ•°å­¦ç­‰ä»·æ€§éªŒè¯ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ä¸‰å±‚æ§åˆ¶ç­–ç•¥ï¼š

```mermaid
graph TB
    subgraph Design["ä¸‰å±‚ç­‰ä»·æ€§æ§åˆ¶ç­–ç•¥"]
        direction TB
        
        subgraph Layer1["ç¬¬ä¸€å±‚ï¼šåŸºç¡€æ§åˆ¶å˜é‡"]
            direction LR
            C1["ç›¸åŒç½‘ç»œç»“æ„<br/>hidden_layers=(64,32)"]
            C2["ç›¸åŒéšæœºç§å­<br/>random_state=42"]
            C3["ç›¸åŒè®­ç»ƒå‚æ•°<br/>max_iter=500, Î±=0.0"]
            C4["ç›¸åŒæ•°æ®é›†<br/>è®­ç»ƒé›†+æµ‹è¯•é›†"]
        end
        
        subgraph Layer2["ç¬¬äºŒå±‚ï¼šæ¶æ„ç­‰ä»·é…ç½®"]
            direction LR
            O1["å†»ç»“ AbductionNetwork<br/>loc_net â†’ I(x)=x"]
            O2["é…ç½® ActivationHead<br/>â†’ æ’ç­‰æ˜ å°„"]
            O3["åˆ‡æ¢æŸå¤±å‡½æ•°<br/>â†’ MSE/CrossEntropy"]
        end
        
        subgraph Layer3["ç¬¬ä¸‰å±‚ï¼šéªŒè¯æŒ‡æ ‡"]
            direction LR
            M1["æ•°å€¼ç­‰ä»·æ€§<br/>RÂ²/Accuracy å·®å¼‚"]
            M2["é¢„æµ‹å€¼å·®å¼‚<br/>|predâ‚ - predâ‚‚|"]
            M3["å‚æ•°éªŒè¯<br/>ç­‰ä»·æ¡ä»¶æ£€æŸ¥"]
        end
    end
    
    Layer1 --> Layer2 --> Layer3
    
    subgraph KeyInsight["ğŸ”‘ å…³é”®æ´å¯Ÿ"]
        direction TB
        Insight["ä»»åŠ¡å¤´é…ç½®æ˜¯ç­‰ä»·æ€§çš„å…³é”®<br/>å¿…é¡»æ¶ˆé™¤æ‰€æœ‰éçº¿æ€§å˜æ¢<br/>å®ç°çº¯çº¿æ€§æ˜ å°„ y = loc_S"]
    end
    
    Layer2 --> KeyInsight
    
    classDef layer1Style fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef layer2Style fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef layer3Style fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef insightStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    
    class Layer1,C1,C2,C3,C4 layer1Style
    class Layer2,O1,O2,O3 layer2Style
    class Layer3,M1,M2,M3 layer3Style
    class KeyInsight,Insight insightStyle
```

### 3.2 å®éªŒç»“æœ

#### å›å½’ä»»åŠ¡éªŒè¯
**æ•°æ®é›†**: 500æ ·æœ¬ï¼Œ10ç‰¹å¾çš„åˆæˆå›å½’æ•°æ®

**ç»“æœå¯¹æ¯”**:
- **sklearn MLPRegressor**: RÂ² = 0.996927
- **CausalEngine (å†»ç»“+MSE)**: RÂ² = 0.997792  
- **å·®å¼‚**: 0.000865 (ä»… 0.087%)

#### åˆ†ç±»ä»»åŠ¡éªŒè¯  
**æ•°æ®é›†**: 500æ ·æœ¬ï¼Œ10ç‰¹å¾ï¼Œ3ç±»åˆ«çš„åˆæˆåˆ†ç±»æ•°æ®

**ç»“æœå¯¹æ¯”**:
- **sklearn MLPClassifier**: å‡†ç¡®ç‡ = 0.850
- **CausalEngine (å†»ç»“+CrossE)**: å‡†ç¡®ç‡ = 0.840
- **å·®å¼‚**: 0.010 (ä»… 1.0%)

### 3.3 ç»“æœåˆ†æ

```mermaid
graph TB
    subgraph Results["å®éªŒç»“æœåˆ†æ"]
        direction TB
        
        subgraph Success["âœ… éªŒè¯æˆåŠŸ"]
            direction TB
            S1["å›å½’ç­‰ä»·æ€§ç¡®è®¤<br/>RÂ² å·®å¼‚ < 0.1%"]
            S2["åˆ†ç±»ç­‰ä»·æ€§ç¡®è®¤<br/>å‡†ç¡®ç‡å·®å¼‚ < 2%"]
            S3["ç†è®ºæ¨å¯¼éªŒè¯<br/>æ•°å­¦ç­‰ä»·æ€§æˆç«‹"]
        end
        
        subgraph Differences["ğŸ“Š å¾®å°å·®å¼‚åˆ†æ"]
            direction TB
            D1["è®¡ç®—è·¯å¾„é•¿åº¦<br/>CausalEngine è·¯å¾„æ›´å¤æ‚"]
            D2["æµ®ç‚¹ç²¾åº¦ç´¯ç§¯<br/>å¤šæ¬¡çŸ©é˜µè¿ç®—è¯¯å·®"]
            D3["å®ç°ç»†èŠ‚å·®å¼‚<br/>PyTorch vs sklearn"]
        end
        
        subgraph Significance["ğŸ’¡ ç†è®ºæ„ä¹‰"]
            direction TB
            T1["åŸºçº¿å»ºç«‹<br/>ä¸ºå› æœæ¨ç†æä¾›å‚è€ƒ"]
            T2["ç†è®ºéªŒè¯<br/>CausalEngine æ•°å­¦åŸºç¡€æ­£ç¡®"]
            T3["æ–¹æ³•è®ºè´¡çŒ®<br/>AIç®—æ³•éªŒè¯æ–°æ¡†æ¶"]
        end
    end
    
    Success --> Significance
    Differences --> Success
    
    classDef successStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef diffStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef theoryStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    
    class Success,S1,S2,S3 successStyle
    class Differences,D1,D2,D3 diffStyle
    class Significance,T1,T2,T3 theoryStyle
```

## 4. å…³é”®å®ç°

### 4.1 å®Œæ•´çš„ç­‰ä»·æ€§é…ç½®æµç¨‹

ä¸ºå®ç°çœŸæ­£çš„æ•°å­¦ç­‰ä»·æ€§ï¼Œéœ€è¦åŒæ—¶é…ç½®ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š

```mermaid
graph TB
    subgraph ConfigFlow["ç­‰ä»·æ€§é…ç½®æµç¨‹"]
        direction TB
        
        subgraph Step1["æ­¥éª¤1: å†»ç»“ AbductionNetwork"]
            direction TB
            S1_1["è®¾ç½® loc_net ä¸ºæ’ç­‰æ˜ å°„<br/>W = I, b = 0"]
            S1_2["å†»ç»“ loc_net å‚æ•°<br/>requires_grad = False"]
            S1_3["ä¿æŒ scale_net å¯è®­ç»ƒ<br/>å­¦ä¹ ä¸ç¡®å®šæ€§å‚æ•°"]
        end
        
        subgraph Step2["æ­¥éª¤2: é…ç½® ActivationHead"]
            direction TB
            S2_1["å›å½’: y = loc_S<br/>scale=1.0, bias=0.0"]
            S2_2["åˆ†ç±»: logits = loc_S<br/>ç›´æ¥è¾“å‡ºä½ç½®å‚æ•°"]
            S2_3["å†»ç»“æ¿€æ´»å‚æ•°<br/>æ¶ˆé™¤éçº¿æ€§å˜æ¢"]
        end
        
        subgraph Step3["æ­¥éª¤3: åˆ‡æ¢æŸå¤±å‡½æ•°"]
            direction TB
            S3_1["å›å½’: MSE Loss<br/>L = ||y - target||Â²"]
            S3_2["åˆ†ç±»: CrossEntropy Loss<br/>L = CE(logits, target)"]
            S3_3["ä¿æŒæ¢¯åº¦è®¡ç®—<br/>æ”¯æŒåå‘ä¼ æ’­"]
        end
    end
    
    Step1 --> Step2 --> Step3
    
    subgraph Verification["éªŒè¯æ£€æŸ¥ç‚¹"]
        direction LR
        V1["æ•°å­¦å½¢å¼éªŒè¯<br/>y = W_final^TÂ·H + b_final"]
        V2["å‚æ•°å†»ç»“éªŒè¯<br/>æ’ç­‰æ˜ å°„æ¡ä»¶æ£€æŸ¥"]
        V3["æ•°å€¼ç­‰ä»·éªŒè¯<br/>é¢„æµ‹å·®å¼‚ < é˜ˆå€¼"]
    end
    
    Step3 --> Verification
    
    classDef stepStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef verifyStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    
    class Step1,Step2,Step3,S1_1,S1_2,S1_3,S2_1,S2_2,S2_3,S3_1,S3_2,S3_3 stepStyle
    class Verification,V1,V2,V3 verifyStyle
```

### 4.2 æ ¸å¿ƒä»£ç å®ç°

#### æ­¥éª¤1: å†»ç»“ AbductionNetwork

```python
def freeze_abduction_to_identity(model):
    """å°† AbductionNetwork çš„ loc_net å†»ç»“ä¸ºæ’ç­‰æ˜ å°„"""
    abduction = model.causal_engine.abduction
    
    # è®¾ç½®ä¸ºæ’ç­‰æ˜ å°„
    with torch.no_grad():
        causal_size = abduction.causal_size
        abduction.loc_net.weight.copy_(torch.eye(causal_size))  # å•ä½çŸ©é˜µ
        abduction.loc_net.bias.zero_()                          # é›¶åç½®
    
    # å†»ç»“å‚æ•°ï¼ˆç¦ç”¨æ¢¯åº¦æ›´æ–°ï¼‰
    abduction.loc_net.weight.requires_grad = False
    abduction.loc_net.bias.requires_grad = False
    
    # é‡è¦ï¼šscale_net ä¿æŒå¯è®­ç»ƒçŠ¶æ€
    # abduction.scale_net å‚æ•°çš„ requires_grad ä¿æŒ True
    
    return True
```

#### æ­¥éª¤2: é…ç½® ActivationHead ä¸ºæ’ç­‰æ˜ å°„

```python
def configure_activation_head_identity(model, task_type):
    """é…ç½®ä»»åŠ¡å¤´ä¸ºæ’ç­‰æ˜ å°„ï¼Œæ¶ˆé™¤éçº¿æ€§å˜æ¢"""
    activation_head = model.causal_engine.activation_head
    
    if task_type == 'regression':
        # å›å½’ä»»åŠ¡: y = 1.0 * loc_S + 0.0 (æ’ç­‰æ˜ å°„)
        with torch.no_grad():
            activation_head.regression_scales.fill_(1.0)
            activation_head.regression_biases.fill_(0.0)
        
        # å†»ç»“å‚æ•° - è®¾ä¸ºä¸å¯å­¦ä¹ 
        activation_head.regression_scales.requires_grad = False
        activation_head.regression_biases.requires_grad = False
        
        print("âœ… å›å½’ä»»åŠ¡å¤´é…ç½®ä¸ºæ’ç­‰æ˜ å°„: y = loc_S (å‚æ•°å†»ç»“)")
        
    elif task_type == 'classification':
        # åˆ†ç±»ä»»åŠ¡: é˜ˆå€¼è®¾ä¸º0ä¸”ä¸å¯å­¦ä¹ 
        with torch.no_grad():
            activation_head.classification_thresholds.fill_(0.0)
        
        # å†»ç»“é˜ˆå€¼å‚æ•° - è®¾ä¸ºä¸å¯å­¦ä¹   
        activation_head.classification_thresholds.requires_grad = False
        
        print("âœ… åˆ†ç±»ä»»åŠ¡å¤´é…ç½®: é˜ˆå€¼=0ä¸”ä¸å¯å­¦ä¹ ")
        
        # æ³¨æ„ï¼šè¿™é‡Œä¿æŒæŸ¯è¥¿CDFæ¿€æ´»ï¼Œå› ä¸ºé˜ˆå€¼=0æ—¶è¡Œä¸ºè‰¯å¥½
        # P(S > 0) = 0.5 + (1/Ï€)arctan(loc_S/scale_S)
    
    return True

#### æ­¥éª¤3: åˆ‡æ¢æŸå¤±å‡½æ•°

```python
def enable_traditional_loss(model, task_type):
    """åˆ‡æ¢åˆ°ä¼ ç»ŸæŸå¤±å‡½æ•°ï¼Œä¿æŒä¸sklearnä¸€è‡´"""
    
    if task_type == 'regression':
        def mse_loss(predictions, targets):
            """æ ‡å‡†MSEæŸå¤±å‡½æ•°"""
            pred_values = predictions['output'].squeeze()
            targets = targets.squeeze()
            return F.mse_loss(pred_values, targets)
        
        model._compute_loss = mse_loss
        model._loss_mode = 'mse'
        print("âœ… å·²åˆ‡æ¢åˆ°MSEæŸå¤±å‡½æ•°")
        
    elif task_type == 'classification':
        def crossentropy_loss(predictions, targets):
            """æ ‡å‡†CrossEntropyæŸå¤±å‡½æ•°"""
            logits = predictions['output']  # [batch, seq_len, n_classes]
            if logits.dim() == 3:
                logits = logits.squeeze(1)  # [batch, n_classes]
            targets = targets.long().squeeze()
            return F.cross_entropy(logits, targets)
        
        model._compute_loss = crossentropy_loss
        model._loss_mode = 'cross_entropy'
        print("âœ… å·²åˆ‡æ¢åˆ°CrossEntropyæŸå¤±å‡½æ•°")
    
    return True

#### å®Œæ•´é…ç½®å‡½æ•°

```python
def setup_mathematical_equivalence(model, task_type):
    """ä¸€é”®é…ç½®æ•°å­¦ç­‰ä»·æ€§éªŒè¯æ‰€éœ€çš„æ‰€æœ‰è®¾ç½®"""
    
    print(f"ğŸ”§ å¼€å§‹é…ç½®{task_type}ä»»åŠ¡çš„æ•°å­¦ç­‰ä»·æ€§éªŒè¯...")
    
    # æ­¥éª¤1: å†»ç»“AbductionNetwork
    success1 = freeze_abduction_to_identity(model)
    
    # æ­¥éª¤2: é…ç½®ActivationHead
    success2 = configure_activation_head_identity(model, task_type)
    
    # æ­¥éª¤3: åˆ‡æ¢æŸå¤±å‡½æ•°
    success3 = enable_traditional_loss(model, task_type)
    
    if success1 and success2 and success3:
        print("ğŸ‰ æ•°å­¦ç­‰ä»·æ€§é…ç½®å®Œæˆï¼")
        
        # éªŒè¯é…ç½®
        verify_equivalence_setup(model, task_type)
        return True
    else:
        print("âŒ é…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç»“æ„")
        return False

def verify_equivalence_setup(model, task_type):
    """éªŒè¯ç­‰ä»·æ€§é…ç½®æ˜¯å¦æ­£ç¡®"""
    print("\nğŸ” éªŒè¯ç­‰ä»·æ€§é…ç½®...")
    
    # éªŒè¯1: AbductionNetworkæ’ç­‰æ˜ å°„
    abduction = model.causal_engine.abduction
    loc_weight = abduction.loc_net.weight
    loc_bias = abduction.loc_net.bias
    
    is_identity_weight = torch.allclose(loc_weight, torch.eye(loc_weight.size(0)), atol=1e-6)
    is_zero_bias = torch.allclose(loc_bias, torch.zeros_like(loc_bias), atol=1e-6)
    
    print(f"  â€¢ AbductionNetworkæ’ç­‰æ˜ å°„: {'âœ…' if is_identity_weight and is_zero_bias else 'âŒ'}")
    
    # éªŒè¯2: ActivationHeadé…ç½®
    if task_type == 'regression':
        activation_head = model.causal_engine.activation_head
        scale_is_one = torch.allclose(activation_head.regression_scales, torch.ones_like(activation_head.regression_scales))
        bias_is_zero = torch.allclose(activation_head.regression_biases, torch.zeros_like(activation_head.regression_biases))
        scale_frozen = not activation_head.regression_scales.requires_grad
        bias_frozen = not activation_head.regression_biases.requires_grad
        
        reg_ok = scale_is_one and bias_is_zero and scale_frozen and bias_frozen
        print(f"  â€¢ å›å½’ä»»åŠ¡å¤´æ’ç­‰æ˜ å°„: {'âœ…' if reg_ok else 'âŒ'}")
        if not reg_ok:
            print(f"    - å‚æ•°å€¼æ­£ç¡®: {scale_is_one and bias_is_zero}")
            print(f"    - å‚æ•°å·²å†»ç»“: {scale_frozen and bias_frozen}")
    
    elif task_type == 'classification':
        activation_head = model.causal_engine.activation_head
        threshold_is_zero = torch.allclose(activation_head.classification_thresholds, torch.zeros_like(activation_head.classification_thresholds))
        threshold_frozen = not activation_head.classification_thresholds.requires_grad
        
        cls_ok = threshold_is_zero and threshold_frozen
        print(f"  â€¢ åˆ†ç±»ä»»åŠ¡å¤´é…ç½®: {'âœ…' if cls_ok else 'âŒ'}")
        if not cls_ok:
            print(f"    - é˜ˆå€¼ä¸º0: {threshold_is_zero}")
            print(f"    - é˜ˆå€¼å·²å†»ç»“: {threshold_frozen}")
    
    # éªŒè¯3: æŸå¤±å‡½æ•°
    has_loss_mode = hasattr(model, '_loss_mode')
    correct_loss = False
    if has_loss_mode:
        if task_type == 'regression' and model._loss_mode == 'mse':
            correct_loss = True
        elif task_type == 'classification' and model._loss_mode == 'cross_entropy':
            correct_loss = True
    
    print(f"  â€¢ æŸå¤±å‡½æ•°é…ç½®: {'âœ…' if correct_loss else 'âŒ'}")
    
    if is_identity_weight and is_zero_bias and correct_loss:
        print("\nğŸ¯ æ‰€æœ‰ç­‰ä»·æ€§æ¡ä»¶éªŒè¯é€šè¿‡ï¼æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œç­‰ä»·æ€§éªŒè¯ã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†é…ç½®å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°éªŒè¯ç»“æœã€‚")
```

## 5. ç»“è®ºä¸æ„ä¹‰

### 5.1 éªŒè¯ç»“è®º

**âœ… æ•°å­¦ç­‰ä»·æ€§éªŒè¯æˆåŠŸ**ï¼š
1. **ç†è®ºæ¨å¯¼**: ä¸¥æ ¼è¯æ˜äº†å†»ç»“æ¡ä»¶ä¸‹çš„æ•°å­¦ç­‰ä»·æ€§
2. **å®éªŒéªŒè¯**: å›å½’å’Œåˆ†ç±»ä»»åŠ¡éƒ½æ˜¾ç¤ºå‡ºæå°çš„æ€§èƒ½å·®å¼‚ï¼ˆ< 2%ï¼‰
3. **åŸºçº¿ç¡®ç«‹**: ä¸º CausalEngine çš„å› æœæ¨ç†èƒ½åŠ›è¯„ä¼°æä¾›äº†å¯ä¿¡åŸºçº¿

### 5.2 ç†è®ºè´¡çŒ®

```mermaid
graph TB
    subgraph Contributions["ç†è®ºè´¡çŒ®ä¸åº”ç”¨ä»·å€¼"]
        direction TB
        
        subgraph Theory["ç†è®ºä»·å€¼"]
            direction TB
            T1["æ•°å­¦åŸºç¡€éªŒè¯<br/>CausalEngine ç†è®ºæ­£ç¡®æ€§"]
            T2["ç­‰ä»·æ€§æ¡†æ¶<br/>AIç®—æ³•éªŒè¯æ–°æ–¹æ³•"]
            T3["æ¶ˆèå®éªŒåŸºç¡€<br/>ç»„ä»¶è´¡çŒ®åˆ†æå‡†å¤‡"]
        end
        
        subgraph Practice["å®è·µä»·å€¼"]
            direction TB
            P1["å¯ä¿¡åŸºçº¿<br/>å› æœæ¨ç†èƒ½åŠ›è¯„ä¼°"]
            P2["è°ƒè¯•æŒ‡å¯¼<br/>æ€§èƒ½ä¼˜åŒ–å‚è€ƒæ ‡å‡†"]
            P3["ç”¨æˆ·ä¿¡å¿ƒ<br/>ç®—æ³•å¯é æ€§è¯æ˜"]
        end
        
        subgraph Future["æœªæ¥æ–¹å‘"]
            direction TB
            F1["æ¶æ„ä¼˜åŒ–<br/>å‡å°‘è®¡ç®—å¤æ‚åº¦"]
            F2["æ‰©å±•éªŒè¯<br/>æ›´å¤šä»»åŠ¡å’Œæ•°æ®é›†"]
            F3["å› æœèƒ½åŠ›<br/>è§£å†»åçš„å¢ç›Šåˆ†æ"]
        end
    end
    
    Theory --> Practice --> Future
    
    classDef theoryStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef practiceStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef futureStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    
    class Theory,T1,T2,T3 theoryStyle
    class Practice,P1,P2,P3 practiceStyle
    class Future,F1,F2,F3 futureStyle
```

é€šè¿‡è¿™ä¸ªä¸¥æ ¼çš„æ•°å­¦ç­‰ä»·æ€§éªŒè¯ï¼Œæˆ‘ä»¬ä¸ä»…è¯æ˜äº† CausalEngine ç†è®ºåŸºç¡€çš„æ­£ç¡®æ€§ï¼Œæ›´ä¸ºå…¶åœ¨å› æœæ¨ç†é¢†åŸŸçš„åº”ç”¨å»ºç«‹äº†åšå®çš„ä¿¡å¿ƒåŸºç¡€ã€‚å¾®å°çš„æ•°å€¼å·®å¼‚åæ˜ äº†ç®—æ³•å®ç°çš„å¤æ‚æ€§ï¼Œä½†ä¸å½±å“æ ¸å¿ƒçš„æ•°å­¦ç­‰ä»·æ€§ç»“è®ºã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v5.0 (å›¾æ–‡å¹¶èŒ‚ç‰ˆ)  
**æœ€åæ›´æ–°**: 2024å¹´6æœˆ24æ—¥  
**éªŒè¯çŠ¶æ€**: âœ… ç†è®ºä¸å®éªŒåŒé‡éªŒè¯é€šè¿‡  
**ç›¸å…³æ–‡ä»¶**: `mathematical_equivalence_test.py`
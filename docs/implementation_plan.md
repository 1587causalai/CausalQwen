# CausalQwen MVP: å®ç°æŒ‡å—

> **ğŸ“‹ æ–‡æ¡£ç”¨é€”**: æŠ€æœ¯å®ç°æŒ‡å—ï¼Œç¼–ç å‚è€ƒç”¨  
> **ğŸ¯ ç›®æ ‡è¯»è€…**: AIå¼€å‘åŠ©æ‰‹ï¼Œç”¨äºæŒ‡å¯¼å…·ä½“çš„ä»£ç å®ç°  
> **ğŸ“– å†…å®¹å®šä½**: è¯¦ç»†çš„æŠ€æœ¯æ–¹æ¡ˆã€ä»£ç ç»“æ„ã€æ¥å£å®šä¹‰ã€å®ç°æ­¥éª¤

> **ç›®æ ‡**: é€šè¿‡ç»§æ‰¿ `Qwen2ForCausalLM` å®ç° CausalQwen MVP çš„å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆã€‚

## 1. å®ç°ç›®æ ‡ä¸åŸåˆ™

### 1.1 æ ¸å¿ƒç›®æ ‡
- **ç†è®ºä¿çœŸ**: ä¸¥æ ¼éµå¾ªå› æœæ•°å­¦ç†è®ºï¼Œç¡®ä¿ä¸ªä½“é€‰æ‹©å˜é‡Uå’ŒCauchyåˆ†å¸ƒçº¿æ€§ç¨³å®šæ€§
- **è®¡ç®—é«˜æ•ˆ**: æœ€å¤§åŒ–å¤ç”¨Qwen2åŸºç¡€è®¾æ–½ï¼Œæœ€å°åŒ–é¢å¤–è®¡ç®—å¼€é”€
- **å·¥ç¨‹ä¼˜é›…**: ä¿æŒä»£ç æ¸…æ™°ã€æ¨¡å—åŒ–ï¼Œä¾¿äºè°ƒè¯•å’Œæ‰©å±•

### 1.2 å®ç°åŸåˆ™
- ç»§æ‰¿ `Qwen2ForCausalLM`ï¼Œå¤ç”¨Transformeréª¨å¹²
- é‡å†™å…³é”®æ–¹æ³•ï¼š`forward()` å’Œæ¨ç†æ¥å£
- ä¿æŒHuggingFaceç”Ÿæ€å…¼å®¹æ€§
- æ”¯æŒä¸‰ç§æ¨ç†æ¨¡å¼ï¼šæ ‡å‡†ã€å› æœé‡‡æ ·ã€å…¼å®¹ä¼ ç»Ÿ

## 2. Qwen2ForCausalLM ç»§æ‰¿è¦ç‚¹

### 2.1 æ ¸å¿ƒç»„ä»¶ç»“æ„
```python
# ç»§æ‰¿é“¾è·¯
torch.nn.Module â†’ PreTrainedModel â†’ Qwen2PreTrainedModel â†’ Qwen2ForCausalLM â†’ CausalQwenMVPForCausalLM

# å…³é”®ç»„ä»¶
class Qwen2ForCausalLM:
    self.model = Qwen2Model(config)  # Transformeréª¨å¹²ï¼šembed_tokens + layers + norm + rotary_emb
    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # è¾“å‡ºæŠ•å½±å±‚
```

### 2.2 å…³é”®æ–¹æ³•ç­¾å
```python
def forward(self, input_ids, attention_mask, labels=None, **kwargs) -> CausalLMOutputWithPast:
    # 1. Transformerå‰å‘: input_ids â†’ hidden_states
    # 2. è¾“å‡ºæŠ•å½±: hidden_states â†’ logits  
    # 3. æŸå¤±è®¡ç®—: logits + labels â†’ loss
    # 4. è¿”å›ç»“æ„åŒ–è¾“å‡º
```

### 2.3 ç»§æ‰¿ç­–ç•¥
- **å¤ç”¨**: `self.model` (Transformeréª¨å¹²)ã€é…ç½®ç®¡ç†ã€æƒé‡åˆå§‹åŒ–
- **é‡å†™**: `forward()` æ–¹æ³•ï¼Œæ·»åŠ å› æœæ¨ç†é€»è¾‘
- **æ‰©å±•**: æ·»åŠ  `AbductionNetwork`ã€`ActionNetwork`ã€è‡ªå®šä¹‰æ¨ç†æ–¹æ³•

## 3. æ¨¡å—å®ç°æ–¹æ¡ˆ

### 3.1 æ ¸å¿ƒæ•°å­¦å·¥å…· - CauchyMath
```python
class CauchyMath:
    @staticmethod
    def cauchy_linear_stable_loc(loc_input, weight, bias=None):
        """Cauchyåˆ†å¸ƒä½ç½®å‚æ•°çš„çº¿æ€§å˜æ¢"""
        return F.linear(loc_input, weight, bias)
    
    @staticmethod  
    def cauchy_linear_stable_scale(scale_input, weight):
        """Cauchyåˆ†å¸ƒå°ºåº¦å‚æ•°çš„çº¿æ€§å˜æ¢"""
        return F.linear(scale_input, weight.abs())
```

### 3.2 å½’å› ç½‘ç»œ - AbductionNetwork
```python
class AbductionNetwork(nn.Module):
    def __init__(self, config):
        super().__init__()
        # æ’ç­‰æ˜ å°„åˆå§‹åŒ–ï¼Œç¡®ä¿åˆå§‹è¡Œä¸ºä¸åŸå§‹Qwenä¸€è‡´
        self.loc_net = nn.Linear(config.hidden_size, config.causal_size, bias=False)
        self.scale_net = nn.Linear(config.hidden_size, config.causal_size, bias=False)
        self._init_identity_mapping()
    
    def forward(self, hidden_states):
        loc_U = self.loc_net(hidden_states)  # ä¸ªä½“è¡¨å¾ä½ç½®å‚æ•°
        scale_U = torch.abs(self.scale_net(hidden_states)) + 1e-6  # ä¸ªä½“è¡¨å¾å°ºåº¦å‚æ•°
        return loc_U, scale_U
```

### 3.3 è¡ŒåŠ¨ç½‘ç»œ - ActionNetwork  
```python
class ActionNetwork(nn.Module):
    def __init__(self, config):
        super().__init__()
        # å¤åˆ¶åŸå§‹lm_headæƒé‡ï¼Œç¡®ä¿åˆå§‹å…¼å®¹æ€§
        self.lm_head = nn.Linear(config.causal_size, config.vocab_size, bias=False)
        self.b_noise = nn.Parameter(torch.zeros(config.vocab_size))  # å¯å­¦ä¹ å™ªå£°
        self._init_from_original_lm_head()
    
    def forward(self, loc_U, scale_U):
        # Cauchyåˆ†å¸ƒçº¿æ€§ç¨³å®šæ€§å˜æ¢
        loc_S = CauchyMath.cauchy_linear_stable_loc(loc_U, self.lm_head.weight, self.b_noise)
        scale_S = CauchyMath.cauchy_linear_stable_scale(scale_U, self.lm_head.weight)
        return loc_S, scale_S
```

### 3.4 OvRåˆ†ç±»å™¨
```python
class OvRClassifier(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.thresholds = nn.Parameter(torch.zeros(config.vocab_size))
    
    def forward(self, loc_S, scale_S):
        # ç‹¬ç«‹äºŒå…ƒåˆ¤æ–­ï¼šP(y_k=1) = P(Cauchy(loc_S_k, scale_S_k) > threshold_k)
        cauchy_dist = torch.distributions.cauchy.Cauchy(loc_S, scale_S)
        return 0.5 + (1/torch.pi) * torch.atan((loc_S - self.thresholds.unsqueeze(0).unsqueeze(0)) / scale_S)
```

### 3.5 ä¸»æ¨¡å‹é›†æˆ - CausalQwenMVPForCausalLM
```python
class CausalQwenMVPForCausalLM(Qwen2ForCausalLM):
    def __init__(self, config):
        super().__init__(config)
        # æ·»åŠ å› æœæ¨¡å—
        self.abduction_network = AbductionNetwork(config)
        self.action_network = ActionNetwork(config)  
        self.ovr_classifier = OvRClassifier(config)
        self._init_causal_weights()
    
    def forward(self, input_ids=None, labels=None, **kwargs):
        # 1. è·å–Transformerç‰¹å¾
        transformer_outputs = self.model(input_ids=input_ids, **kwargs)
        hidden_states = transformer_outputs[0]
        
        # 2. å› æœæ¨ç†é“¾è·¯
        loc_U, scale_U = self.abduction_network(hidden_states)  # ä¸ªä½“æ¨æ–­
        loc_S, scale_S = self.action_network(loc_U, scale_U)    # å†³ç­–æ¨æ–­
        
        # 3. æŸå¤±è®¡ç®—
        loss = None
        if labels is not None:
            probs = self.ovr_classifier(loc_S, scale_S)
            loss = self._compute_ovr_loss(probs, labels)
        
        return CausalMVPOutput(
            loss=loss, loc_S=loc_S, scale_S=scale_S, 
            loc_U=loc_U, scale_U=scale_U, **transformer_outputs
        )
```

## 4. æ¨ç†å®ç°

### 4.1 ä¸‰ç§æ¨ç†æ¨¡å¼
```python
def inference(self, input_ids, mode='standard', **kwargs):
    if mode == 'standard':
        return self._standard_inference(input_ids, **kwargs)
    elif mode == 'causal':  
        return self._causal_sampling(input_ids, **kwargs)
    elif mode == 'compatible':
        return self._compatible_sampling(input_ids, **kwargs)

def _standard_inference(self, input_ids, **kwargs):
    """ç¡®å®šæ€§æ¨ç†ï¼šä½¿ç”¨æœŸæœ›å€¼è®¡ç®—"""
    outputs = self.forward(input_ids, **kwargs)
    probs = self.ovr_classifier(outputs.loc_S, outputs.scale_S)
    return torch.argmax(probs, dim=-1)

def _causal_sampling(self, input_ids, **kwargs):
    """ä¸ªä½“é‡‡æ ·ï¼šä»ä¸ªä½“åˆ†å¸ƒé‡‡æ ·åå†³ç­–"""  
    outputs = self.forward(input_ids, **kwargs)
    cauchy_U = Cauchy(outputs.loc_U, outputs.scale_U)
    u_sample = cauchy_U.sample()  # é‡‡æ ·ä¸ªä½“è¡¨å¾
    # é€šè¿‡ActionNetworkç¡®å®šæ€§æ˜ å°„
    loc_S_sample = F.linear(u_sample, self.action_network.lm_head.weight, self.action_network.b_noise)
    return torch.argmax(loc_S_sample, dim=-1)

def _compatible_sampling(self, input_ids, **kwargs):
    """ä¼ ç»Ÿå…¼å®¹ï¼šå°†ä½ç½®å‚æ•°ä½œä¸ºlogitsé‡‡æ ·"""
    outputs = self.forward(input_ids, **kwargs)
    return F.softmax(outputs.loc_S, dim=-1)  # å¯é…åˆtop-k/top-p
```

### 4.2 åºåˆ—ç”Ÿæˆ
```python
def generate_step_by_step(self, input_ids, max_length=50, mode='standard', **kwargs):
    """è‡ªå›å½’ç”Ÿæˆå¾ªç¯"""
    for _ in range(max_length):
        next_token = self.inference(input_ids, mode=mode, **kwargs)
        input_ids = torch.cat([input_ids, next_token[:, -1:]], dim=-1)
        if next_token[0, -1].item() == self.config.eos_token_id:
            break
    return input_ids
```

## 5. è®­ç»ƒå®ç°

### 5.1 æŸå¤±å‡½æ•°
```python
def _compute_ovr_loss(self, probs, labels):
    """OvRå¤šæ ‡ç­¾åˆ†ç±»æŸå¤±"""
    # æ„é€ ç‹¬ç«‹äºŒå…ƒæ ‡ç­¾ï¼štarget_k = 1 if labels == k else 0
    targets = F.one_hot(labels, num_classes=self.config.vocab_size).float()
    # äºŒå…ƒäº¤å‰ç†µæŸå¤±
    return F.binary_cross_entropy(probs, targets, reduction='mean')
```

### 5.2 è®­ç»ƒå¾ªç¯
```python
def training_step(self, batch):
    input_ids, labels = batch['input_ids'], batch['labels']
    outputs = self.forward(input_ids=input_ids, labels=labels)
    return outputs.loss
```

## 6. å®ç°è·¯çº¿å›¾

### é˜¶æ®µ1: æ ¸å¿ƒæ¨¡å— (ç¬¬1-2å‘¨)
1. å®ç° `CauchyMath` å·¥å…·ç±»
2. å®ç° `AbductionNetwork` å’Œ `ActionNetwork`
3. å®ç° `OvRClassifier`  
4. é›†æˆåˆ° `CausalQwenMVPForCausalLM`

### é˜¶æ®µ2: æ¨ç†ç³»ç»Ÿ (ç¬¬3å‘¨)
1. å®ç°ä¸‰ç§æ¨ç†æ¨¡å¼
2. å®ç°è‡ªå›å½’ç”Ÿæˆ
3. ç¼–å†™æ¨ç†æµ‹è¯•ç”¨ä¾‹

### é˜¶æ®µ3: è®­ç»ƒç³»ç»Ÿ (ç¬¬4å‘¨)  
1. å®ç°æŸå¤±å‡½æ•°å’Œè®­ç»ƒå¾ªç¯
2. æƒé‡åˆå§‹åŒ–ç­–ç•¥
3. ç¼–å†™è®­ç»ƒæµ‹è¯•ç”¨ä¾‹

### é˜¶æ®µ4: éªŒè¯ä¼˜åŒ– (ç¬¬5-6å‘¨)
1. ç«¯åˆ°ç«¯æµ‹è¯•
2. æ€§èƒ½ä¼˜åŒ–  
3. æ–‡æ¡£å®Œå–„

## 7. æˆåŠŸéªŒè¯æ ‡å‡†
- [ ] æ¨¡å‹å¯ä»¥æ­£ç¡®åŠ è½½å¹¶ç»§æ‰¿Qwen2çš„æ‰€æœ‰åŠŸèƒ½
- [ ] ä¸‰ç§æ¨ç†æ¨¡å¼éƒ½èƒ½æ­£å¸¸å·¥ä½œä¸”ç»“æœåˆç†
- [ ] è®­ç»ƒå¯ä»¥æ­£å¸¸è¿›è¡Œä¸”æŸå¤±æ”¶æ•›
- [ ] è¡Œä¸ºä¸ç†è®ºé¢„æœŸä¸€è‡´ï¼ˆåˆå§‹æ¥è¿‘åŸå§‹Qwenï¼Œè®­ç»ƒåä½“ç°å› æœç‰¹æ€§ï¼‰

## 8. Qwen2æŠ€æœ¯ç»†èŠ‚è¡¥å……

### 8.1 æ³¨æ„åŠ›æœºåˆ¶å…¼å®¹æ€§
Qwen2æ”¯æŒä¸‰ç§æ³¨æ„åŠ›å®ç°ï¼š
```python
QWEN2_ATTENTION_CLASSES = {
    "eager": Qwen2Attention,           # æ ‡å‡†å®ç°
    "flash_attention_2": Qwen2FlashAttention2,  # é«˜æ•ˆå®ç°
    "sdpa": Qwen2SdpaAttention,       # PyTorchåŸç”ŸSDPA
}
```

**ç»§æ‰¿è€ƒè™‘**: æˆ‘ä»¬éœ€è¦ç¡®ä¿å› æœæ¨¡å—ä¸æ‰€æœ‰æ³¨æ„åŠ›å®ç°å…¼å®¹ã€‚

### 8.2 ç”Ÿæˆæ–¹æ³•é›†æˆ

#### HuggingFace `generate()` æ–¹æ³•
```python
def generate(
    self,
    inputs: Optional[torch.Tensor] = None,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    **kwargs,
) -> Union[GenerateOutput, torch.LongTensor]:
```

**é‡è¦æ€§**: è¿™æ˜¯ç”¨æˆ·æœ€å¸¸ç”¨çš„æ¥å£ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å› æœæ¨ç†æ¨¡å¼èƒ½å¤Ÿæ­£ç¡®é›†æˆã€‚

#### è¾“å…¥å‡†å¤‡æ–¹æ³•é‡å†™
```python
def prepare_inputs_for_generation(
    self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs
):
    # ä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥ï¼Œå¤„ç†KVç¼“å­˜ç­‰
    # å¯èƒ½éœ€è¦é‡å†™ä»¥æ”¯æŒå› æœé‡‡æ ·ç­‰ç‰¹æ®Šæ¨ç†æ¨¡å¼
```

### 8.3 é…ç½®æ‰©å±•ç­–ç•¥

#### æ‰©å±•çš„å› æœé…ç½®ç±»
```python
class CausalQwen2Config(Qwen2Config):
    """æ‰©å±•Qwen2Configä»¥æ”¯æŒå› æœæ¨¡å‹å‚æ•°"""
    
    # å› æœæ¨¡å‹ç‰¹æœ‰å‚æ•°
    causal_size: int = None  # å¦‚æœNoneï¼Œåˆ™ç­‰äºhidden_size
    
    # AbductionNetworkå‚æ•°
    abduction_init_strategy: str = "identity"  # identity, xavier, normal
    
    # ActionNetworkå‚æ•°  
    b_noise_init: float = 0.1
    
    # OvRåˆ†ç±»å‚æ•°
    ovr_threshold_init: float = 0.0
    
    # æ¨ç†æ¨¡å¼æ§åˆ¶
    inference_mode: str = "standard"  # standard, causal, compatible
    
    # æŸå¤±å‡½æ•°æƒé‡
    classification_loss_weight: float = 1.0
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # è®¾ç½®é»˜è®¤å€¼
        if self.causal_size is None:
            self.causal_size = self.hidden_size
```

### 8.4 æ•ˆç‡ä¼˜åŒ–è€ƒè™‘

#### æƒé‡å…±äº«ç­–ç•¥
```python
# åœ¨__init__ä¸­å®ç°æƒé‡å…±äº«
self.lm_head = self.action_network.lm_head  # å…±äº«æƒé‡
self.tie_weights()  # è°ƒç”¨HuggingFaceçš„æƒé‡ç»‘å®šæœºåˆ¶
```

#### æ¢¯åº¦æ£€æŸ¥ç‚¹æ”¯æŒ
```python
if self.gradient_checkpointing and self.training:
    layer_outputs = self._gradient_checkpointing_func(
        decoder_layer.__call__, hidden_states, ...
    )
```

#### Flash Attentionå…¼å®¹æ€§
```python
# ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å—ä¸Flash Attentionå…¼å®¹
if hasattr(self.config, '_attn_implementation'):
    if self.config._attn_implementation == "flash_attention_2":
        # ç‰¹æ®Šå¤„ç†é€»è¾‘
```

### 8.5 é‡å†™æœ€ä½³å®è·µ

#### å®Œæ•´çš„forward()æ–¹æ³•å®ç°
```python
def forward(self, input_ids=None, labels=None, **kwargs):
    # Step 1: è°ƒç”¨çˆ¶ç±»è·å–åŸºç¡€ç‰¹å¾
    transformer_outputs = self.model(
        input_ids=input_ids,
        **{k: v for k, v in kwargs.items() 
           if k in ['attention_mask', 'position_ids', 'past_key_values', 'use_cache']}
    )
    hidden_states = transformer_outputs[0]
    
    # Step 2: åº”ç”¨å› æœæ¨¡å—
    loc_U, scale_U = self.abduction_network(hidden_states)
    loc_S, scale_S = self.action_network(loc_U, scale_U)
    
    # Step 3: è®¡ç®—æŸå¤±
    loss = None
    if labels is not None:
        loss = self._compute_causal_loss(loc_S, scale_S, labels, **kwargs)
    
    # Step 4: è¿”å›å…¼å®¹çš„è¾“å‡ºæ ¼å¼
    return CausalMVPOutput(
        loss=loss,
        loc_S=loc_S,
        scale_S=scale_S,
        loc_U=loc_U,
        scale_U=scale_U,
        past_key_values=transformer_outputs.past_key_values,
        hidden_states=transformer_outputs.hidden_states,
        attentions=transformer_outputs.attentions,
    )
```

#### HuggingFaceç”Ÿæ€å…¼å®¹æ€§ä¿è¯
```python
# ç¡®ä¿æ”¯æŒmodel.save_pretrained()å’Œfrom_pretrained()
@classmethod
def _get_config_class(cls):
    return CausalQwen2Config

# ç¡®ä¿æ”¯æŒpipeline
def _get_logits(self, output):
    # ä»æˆ‘ä»¬çš„è¾“å‡ºä¸­æå–å…¼å®¹çš„logits
    return output.loc_S  # æˆ–è€…æ ¹æ®æ¨ç†æ¨¡å¼é€‰æ‹©åˆé€‚çš„è¾“å‡º
```

## 9. é£é™©ç¼“è§£ä¸è°ƒè¯•ç­–ç•¥

### 9.1 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
1. **æƒé‡åˆå§‹åŒ–ä¸å½“**: ç¡®ä¿æ’ç­‰æ˜ å°„åˆå§‹åŒ–
2. **ç»´åº¦ä¸åŒ¹é…**: ä»”ç»†æ£€æŸ¥causal_sizeè®¾ç½®
3. **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**: ä½¿ç”¨æ¢¯åº¦è£å‰ªå’Œåˆé€‚çš„å­¦ä¹ ç‡
4. **æ¨ç†æ¨¡å¼åˆ‡æ¢é—®é¢˜**: æ˜ç¡®æ¨¡å¼é—´çš„å·®å¼‚å’Œé€‚ç”¨åœºæ™¯

### 9.2 è°ƒè¯•å·¥å…·
```python
def debug_forward_pass(self, input_ids):
    """è°ƒè¯•å‰å‘ä¼ æ’­çš„æ¯ä¸ªæ­¥éª¤"""
    with torch.no_grad():
        # 1. Transformerè¾“å‡º
        transformer_outputs = self.model(input_ids)
        print(f"Hidden states shape: {transformer_outputs[0].shape}")
        
        # 2. å½’å› ç½‘ç»œè¾“å‡º
        loc_U, scale_U = self.abduction_network(transformer_outputs[0])
        print(f"U distribution - loc: {loc_U.mean():.4f}, scale: {scale_U.mean():.4f}")
        
        # 3. è¡ŒåŠ¨ç½‘ç»œè¾“å‡º
        loc_S, scale_S = self.action_network(loc_U, scale_U)
        print(f"S distribution - loc: {loc_S.mean():.4f}, scale: {scale_S.mean():.4f}")
        
        return loc_S, scale_S
```

### 9.3 ç›‘æ§æŒ‡æ ‡
- å„å±‚æ¿€æ´»çš„ç»Ÿè®¡ä¿¡æ¯ (å‡å€¼ã€æ–¹å·®ã€èŒƒå›´)
- æ¢¯åº¦èŒƒæ•°
- æŸå¤±æ”¶æ•›æ›²çº¿
- ä¸åŒæ¨ç†æ¨¡å¼çš„è¾“å‡ºä¸€è‡´æ€§æ£€æŸ¥

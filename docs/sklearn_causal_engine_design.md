# Sklearn-Style CausalEngine è®¾è®¡æ–‡æ¡£

> *"åœ¨åäº‹å®ä¸–ç•Œé‡Œï¼Œä¸€åˆ‡çš†æœ‰å¯èƒ½ã€‚"*  
> *"In the counterfactual world, everything is possible."*

## æ ¸å¿ƒæ•°å­¦æ¡†æ¶

MLPCausalRegressor & MLPCausalClassifier

### å‰å‘ä¼ æ’­

CausalEngineåŸºäºå› æœç»“æ„æ–¹ç¨‹ $Y = f(U, E)$ æ„å»ºé¢„æµ‹æ¨¡å‹ï¼Œå…¶ä¸­ $U$ ä¸ºä¸ªä½“å› æœè¡¨å¾ï¼Œ$E \sim \text{Cauchy}(0, I)$ ä¸ºå¤–ç”Ÿå™ªå£°ã€‚æ ¸å¿ƒæ•°å­¦æ¡†æ¶åˆ©ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ï¼šè‹¥ $X \sim \text{Cauchy}(\mu, \gamma)$ï¼Œåˆ™ $aX + b \sim \text{Cauchy}(a\mu + b, |a|\gamma)$ã€‚

äº”ç§æ¨¡å¼éµå¾ªç»Ÿä¸€å…¬å¼ $U' = U + b_{noise} \cdot E$ï¼Œä½†åœ¨ActionNetworkä¸­äº§ç”Ÿä¸åŒçš„ $U'$ åˆ†å¸ƒï¼š

$$\begin{aligned}
\text{Deterministic:} \quad U' &= \mu_U \\
\text{Exogenous:} \quad U' &\sim \text{Cauchy}(\mu_U, |b_{noise}|) \\
\text{Endogenous:} \quad U' &\sim \text{Cauchy}(\mu_U, \gamma_U) \\
\text{Standard:} \quad U' &\sim \text{Cauchy}(\mu_U, \gamma_U + |b_{noise}|) \\
\text{Sampling:} \quad U' &\sim \text{Cauchy}(\mu_U + b_{noise} \cdot e, \gamma_U)
\end{aligned}$$

```mermaid
flowchart LR
    Input["è¾“å…¥ X"]
    MLP["ç‰¹å¾æå–<br/>H = MLP(X)"]
    Abduction["ä¸ªä½“æ¨æ–­<br/>Î¼_U = W_loc*H + b_loc<br/>Î³_U = softplus(W_scale*H + b_scale) + 1e-8"]
    Action["å™ªå£°è°ƒåˆ¶ & çº¿æ€§å› æœå¾‹<br/>ActionNetwork<br/>(æ¨¡å¼å·®å¼‚æ ¸å¿ƒ)"]
    Output["è¾“å‡ºç”Ÿæˆ<br/>å›å½’: Y = Î¼_S<br/>åˆ†ç±»: P(Y=k) via æŸ¯è¥¿CDF"]

    Input --> MLP --> Abduction --> Action --> Output

    classDef inputStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef coreStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef actionStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    classDef outputStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    class Input inputStyle
    class MLP,Abduction coreStyle
    class Action actionStyle
    class Output outputStyle
```

å‰å‘ä¼ æ’­æµç¨‹ä¸º $X \xrightarrow{MLP} H \xrightarrow{Abduction} (\mu_U, \gamma_U) \xrightarrow{Action} (\mu_S, \gamma_S) \xrightarrow{Output} Y$ï¼Œå…¶ä¸­å…³é”®æ­¥éª¤ï¼š

ä¸ªä½“æ¨æ–­ï¼š$\mu_U = W_{loc} \cdot H + b_{loc}$ï¼Œ$\gamma_U = \text{softplus}(W_{scale} \cdot H + b_{scale}) + \epsilon_{stable}$

å…¶ä¸­ $\epsilon_{stable} = 1e\text{-}8$ ç¡®ä¿æ•°å€¼ç¨³å®šæ€§

çº¿æ€§å› æœå¾‹ï¼š$\mu_S = W_A \cdot \mu_{U'} + b_A$ï¼Œ$\gamma_S = |W_A| \cdot \gamma_{U'}$

```mermaid
flowchart LR
    Input["è¾“å…¥ (Î¼_U, Î³_U)"]
    
    Deterministic["ğŸ¯ Deterministic<br/>Î¼_U' = Î¼_U, Î³_U' = 0"]
    Endogenous["ğŸ§  Endogenous<br/>Î¼_U' = Î¼_U, Î³_U' = Î³_U"]
    Exogenous["ğŸŒ Exogenous<br/>Î¼_U' = Î¼_U, Î³_U' = |b_noise|"]
    Standard["âš¡ Standard<br/>Î¼_U' = Î¼_U, Î³_U' = Î³_U + |b_noise|"]
    Sampling["ğŸ² Sampling<br/>Î¼_U' = Î¼_U + b_noise*e, Î³_U' = Î³_U"]

    CausalLaw["çº¿æ€§å› æœå¾‹<br/>Î¼_S = W_A*Î¼_U' + b_A<br/>Î³_S = |W_A|*Î³_U'"]
    Output["è¾“å‡º (Î¼_S, Î³_S)"]

    Input --> Deterministic --> CausalLaw
    Input --> Endogenous --> CausalLaw
    Input --> Exogenous --> CausalLaw
    Input --> Standard --> CausalLaw
    Input --> Sampling --> CausalLaw
    CausalLaw --> Output

    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef commonStyle fill:#f5f5f5,stroke:#666,stroke-width:2px

    class Deterministic deterministicStyle
    class Endogenous endogenousStyle
    class Exogenous exogenousStyle
    class Standard standardStyle
    class Sampling samplingStyle
    class Input,CausalLaw,Output commonStyle
```

è¾“å‡ºç”Ÿæˆï¼šå›å½’ä»»åŠ¡ $Y = \mu_S$ï¼Œåˆ†ç±»ä»»åŠ¡ $$P(Y=k) = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S_k}}{\gamma_{S_k}}\right)$$

### å‚æ•°åˆå§‹åŒ–ç­–ç•¥

CausalEngineçš„æ•°å­¦æ­£ç¡®æ€§å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºåˆç†çš„å‚æ•°åˆå§‹åŒ–ï¼Œç‰¹åˆ«æ˜¯Î³_Uçš„åˆå§‹åŒ–å¯¹æ¨¡å‹æ”¶æ•›å’Œæ€§èƒ½è‡³å…³é‡è¦ã€‚

#### AbductionNetworkåˆå§‹åŒ–

**ä½ç½®ç½‘ç»œ (loc_net) åˆå§‹åŒ–**ï¼š
```python
if input_size == causal_size and mlp_layers == 1:
    # æ’ç­‰æ˜ å°„åˆå§‹åŒ–: W_loc = I, b_loc = 0
    self.loc_net.weight.copy_(torch.eye(causal_size))
    self.loc_net.bias.zero_()
else:
    # Xavierå‡åŒ€åˆå§‹åŒ–
    nn.init.xavier_uniform_(self.loc_net.weight)
    nn.init.zeros_(self.loc_net.bias)
```

**å°ºåº¦ç½‘ç»œ (scale_net) åˆå§‹åŒ–**ï¼š
```python
# æœ€åä¸€å±‚ç‰¹æ®Šåˆå§‹åŒ–
last_layer = scale_net_last_linear_layer
nn.init.uniform_(last_layer.weight, -0.01, 0.01)  # å°éšæœºæƒé‡
# å…³é”®ï¼šbiasè®¾ä¸ºå¸¸æ•°gamma_init
nn.init.constant_(last_layer.bias, gamma_init)  # é»˜è®¤gamma_init=10.0

# ä¸­é—´å±‚æ ‡å‡†Xavieråˆå§‹åŒ–
for middle_layer in scale_net_middle_layers:
    nn.init.xavier_uniform_(middle_layer.weight)
    nn.init.zeros_(middle_layer.bias)
```

#### Î³_Uåˆå§‹åŒ–çš„æ•°å­¦è®¾è®¡

**æ ¸å¿ƒæ•°å­¦å…¬å¼**ï¼š
$$\gamma_U = \text{softplus}(\text{scale\_net}(H))$$

**å…·ä½“åˆå§‹åŒ–æµç¨‹**ï¼š
1. **biasåˆå§‹åŒ–**: `nn.init.constant_(bias, gamma_init)`
   - é»˜è®¤`gamma_init=10.0`ï¼Œæ‰€æœ‰ç»´åº¦è®¾ä¸ºç›¸åŒå¸¸æ•°
   - ä¾‹å¦‚causal_size=4æ—¶: `[10.0, 10.0, 10.0, 10.0]`

2. **æƒé‡åˆå§‹åŒ–**: `nn.init.uniform_(weight, -0.01, 0.01)`
   - å°éšæœºæƒé‡ï¼Œä½¿è¾“å‡ºä¸»è¦ç”±biaså†³å®š

3. **softpluså˜æ¢**: Î³_U â‰ˆ softplus(gamma_init) â‰ˆ gamma_init (å½“gamma_initè¾ƒå¤§æ—¶)
   - softplus(10.0) â‰ˆ 10.0000
   - **ç»“æœ**: Î³_U â‰ˆ 10.0 (æ‰€æœ‰ç»´åº¦)

**åˆå§‹åŒ–èŒƒå›´ç‰¹æ€§**ï¼š
```python
# ç»Ÿä¸€å¸¸æ•°åˆå§‹åŒ– (å®é™…å€¼)
causal_size = 1:   Î³_U â‰ˆ [10.0]
causal_size = 2:   Î³_U â‰ˆ [10.0, 10.0] 
causal_size = 4:   Î³_U â‰ˆ [10.0, 10.0, 10.0, 10.0]
causal_size = 32:  Î³_U â‰ˆ [10.0, 10.0, ..., 10.0]  # æ‰€æœ‰ç»´åº¦ç›¸åŒ
```

#### ActionNetworkåˆå§‹åŒ–

**çº¿æ€§å› æœå¾‹åˆå§‹åŒ–**ï¼š
```python
# æ ‡å‡†Xavieråˆå§‹åŒ–
nn.init.xavier_uniform_(self.linear_law.weight)
nn.init.zeros_(self.linear_law.bias)

# å¤–ç”Ÿå™ªå£°å‚æ•°
nn.init.constant_(self.b_noise, b_noise_init)  # é»˜è®¤0.1
```

#### åˆå§‹åŒ–ç­–ç•¥çš„æ•°å­¦æ„ä¹‰

**Î³_Uåˆå§‹åŒ–åŸç†**ï¼š
1. **æ­£å€¼ä¿è¯**: softplusç¡®ä¿Î³_U > 0ï¼Œæ»¡è¶³Cauchyåˆ†å¸ƒè¦æ±‚
2. **é€‚ä¸­èŒƒå›´**: 10.0å¤§å°é€‚ä¸­ï¼Œé¿å…äº†è¿‡å°(æ•°å€¼ä¸ç¨³å®š)å’Œè¿‡å¤§(è¿‡åº¦åˆ†æ•£)
3. **ç»Ÿä¸€åˆå§‹åŒ–**: æ‰€æœ‰ç»´åº¦ä½¿ç”¨ç›¸åŒåˆå§‹å€¼ï¼Œç®€åŒ–æ”¶æ•›è¡Œä¸º
4. **æ”¶æ•›å‹å¥½**: Î³_U=10.0æ˜¯ç»éªŒä¸Šçš„è‰¯å¥½èµ·å§‹ç‚¹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªé€‚åº”è°ƒæ•´

**ä¸ä¼ ç»Ÿåˆå§‹åŒ–çš„å¯¹æ¯”**ï¼š
```python
# âŒ ä¼ ç»Ÿéšæœºåˆå§‹åŒ–å¯èƒ½å¯¼è‡´çš„é—®é¢˜
Î³_U_random = abs(torch.randn(causal_size))  # å¯èƒ½æ¥è¿‘0æˆ–è¿‡å¤§
# é—®é¢˜: æ¥è¿‘0æ—¶æ•°å€¼ä¸ç¨³å®šï¼Œè¿‡å¤§æ—¶æ¢¯åº¦æ¶ˆå¤±

# âœ… CausalEngineç²¾å¿ƒè®¾è®¡çš„åˆå§‹åŒ–
Î³_U_designed = F.softplus(torch.full((causal_size,), 10.0))
# ä¼˜åŠ¿: ç¨³å®šçš„åˆå§‹å€¼ï¼Œè‰¯å¥½çš„æ¢¯åº¦æ€§è´¨ï¼Œç®€åŒ–è¶…å‚æ•°è°ƒèŠ‚
```

**æ’ç­‰æ˜ å°„ä¼˜åŒ–**ï¼š
å½“`input_size == causal_size`ä¸”`mlp_layers == 1`æ—¶ï¼Œloc_neté‡‡ç”¨æ’ç­‰æ˜ å°„åˆå§‹åŒ–ï¼š
$$\mu_U = \text{loc\_net}(H) = I \cdot H + 0 = H$$

è¿™ä¸ªä¼˜åŒ–åœ¨deterministicæ¨¡å¼ä¸‹ç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºå®ƒç¡®ä¿äº†ä¸ä¼ ç»ŸMLPçš„å®Œç¾æ•°å­¦ç­‰ä»·æ€§ã€‚

#### åˆå§‹åŒ–é€»è¾‘ä»£ç ä½ç½®

| ç»„ä»¶ | æ–‡ä»¶ä½ç½® | å‡½æ•°/æ–¹æ³• | å…·ä½“é€»è¾‘ |
|------|----------|-----------|----------|
| **AbductionNetworkåˆå§‹åŒ–** | `causal_engine/networks.py` | `AbductionNetwork._init_weights()` | lines 165-204 |
| **scale_net biaså¸¸æ•°åˆå§‹åŒ–** | `causal_engine/networks.py` | `_init_weights()` line 199 | `nn.init.constant_(bias, gamma_init)` |
| **loc_netæ’ç­‰æ˜ å°„åˆå§‹åŒ–** | `causal_engine/networks.py` | `_init_weights()` line 171 | `torch.eye(causal_size)` |
| **ActionNetworkåˆå§‹åŒ–** | `causal_engine/networks.py` | `ActionNetwork._init_weights()` | lines 292-296 |
| **b_noiseåˆå§‹åŒ–** | `causal_engine/networks.py` | `_init_weights()` line 296 | `nn.init.constant_(b_noise, 0.1)` |
| **MLPéšè—å±‚åˆå§‹åŒ–** | `causal_engine/sklearn/base.py` | `_init_weights_glorot()` | Xavierå‡åŒ€åˆå§‹åŒ– |

#### åˆå§‹åŒ–å‚æ•°æ±‡æ€»

```python
# é»˜è®¤åˆå§‹åŒ–å‚æ•°è¡¨
CAUSAL_ENGINE_INIT_PARAMS = {
    # AbductionNetwork
    'gamma_init': 10.0,  # Î³_Uåˆå§‹åŒ–å¸¸æ•°
    'scale_bias_init': 'constant',  # å¸¸æ•°åˆå§‹åŒ–ç­–ç•¥
    'scale_weight_range': (-0.01, 0.01),  # å‡åŒ€åˆ†å¸ƒèŒƒå›´
    'loc_identity_enabled': True,  # H=Cæ—¶è‡ªåŠ¨æ’ç­‰æ˜ å°„
    
    # ActionNetwork  
    'b_noise_init': 0.1,  # å¤–ç”Ÿå™ªå£°åˆå§‹å€¼
    'linear_init': 'xavier_uniform',  # çº¿æ€§å±‚åˆå§‹åŒ–
    
    # é¢„æœŸè¾“å‡ºèŒƒå›´
    'gamma_U_range': '~10.0',  # Î³_Uçš„æœŸæœ›åˆå§‹å€¼
    'mu_U_range': 'depends_on_input',  # Î¼_Uå–å†³äºè¾“å…¥H
}
```

**å…³é”®æ•°å­¦ä¸å˜é‡**ï¼š
- âœ… Î³_Uå§‹ç»ˆ > 0 (Cauchyåˆ†å¸ƒæ•°å­¦è¦æ±‚)
- âœ… Î³_Uåˆå§‹å€¼ç¨³å®šç»Ÿä¸€ (~10.0)
- âœ… deterministicæ¨¡å¼ä¸‹Î¼_U = H (ç­‰ä»·æ€§ä¿è¯)
- âœ… æ‰€æœ‰å‚æ•°åˆå§‹åŒ–æ•°å€¼ç¨³å®š

æŸå¤±å‡½æ•°è®¡ç®—ï¼š

**Deterministicæ¨¡å¼** ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°ï¼š

å›å½’ï¼š$$L_{MSE} = \frac{1}{N}\sum_i \sum_j (y_{j,i} - \mu_{S_j,i})^2$$

åˆ†ç±»ï¼š$$L_{CE} = -\frac{1}{N}\sum_i \sum_k y_{k,i} \log \text{softmax}(\mu_{S_k,i})_k$$

**å› æœæ¨¡å¼** ç»Ÿä¸€ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒæŸå¤±ï¼š

ä¸€ç»´å›å½’æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š$$L_{Cauchy} = -\sum_i \log \frac{1}{\pi\gamma_{S,i}[1 + ((y_i-\mu_{S,i})/\gamma_{S,i})^2]}$$

é«˜ç»´å›å½’ï¼ˆç‹¬ç«‹å‡è®¾ï¼‰ï¼š$$L_{Cauchy} = -\sum_i \sum_j \log \frac{1}{\pi\gamma_{S_j,i}[1 + ((y_{j,i}-\mu_{S_j,i})/\gamma_{S_j,i})^2]}$$

åˆ†ç±»OvRäºŒå…ƒäº¤å‰ç†µï¼š$$P_{k,i} = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S_k,i}}{\gamma_{S_k,i}}\right)$$

$$L_{OvR} = -\frac{1}{N}\sum_i \sum_k [y_{k,i} \log P_{k,i} + (1-y_{k,i}) \log (1-P_{k,i})]$$

### æŸå¤±å‡½æ•°è®¡ç®—çš„ç»Ÿä¸€å®ç°

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼šæ‰€æœ‰æŸå¤±å‡½æ•°éƒ½æ¥æ”¶ActionNetworkçš„è¾“å‡º `(loc_S, scale_S)`ï¼Œä¿æŒè¾“å…¥ç­¾åçš„ç»Ÿä¸€æ€§ï¼š

```python
class CausalLossFunction:
    def compute_loss(self, loc_S, scale_S, y_true, mode='standard'):
        """ç»Ÿä¸€çš„æŸå¤±å‡½æ•°æ¥å£
        
        Args:
            loc_S: ActionNetworkè¾“å‡ºçš„ä½ç½®å‚æ•° [batch_size, output_dim]
            scale_S: ActionNetworkè¾“å‡ºçš„å°ºåº¦å‚æ•° [batch_size, output_dim] 
            y_true: çœŸå®æ ‡ç­¾ [batch_size, output_dim]
            mode: æ¨¡å¼é€‰æ‹©ï¼Œå†³å®šæŸå¤±å‡½æ•°ç±»å‹
            
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        if mode == 'deterministic':
            return self._compute_traditional_loss(loc_S, scale_S, y_true)
        else:
            return self._compute_causal_loss(loc_S, scale_S, y_true)
    
    def _compute_traditional_loss(self, loc_S, scale_S, y_true):
        """Deterministicæ¨¡å¼ï¼šä½¿ç”¨ä¼ ç»ŸæŸå¤±ï¼Œå¿½ç•¥scale_S"""
        # å›å½’ï¼šMSEæŸå¤±ï¼Œåªä½¿ç”¨loc_S
        # åˆ†ç±»ï¼šCrossEntropyæŸå¤±ï¼Œåªä½¿ç”¨loc_S
        pass
        
    def _compute_causal_loss(self, loc_S, scale_S, y_true):
        """å› æœæ¨¡å¼ï¼šä½¿ç”¨Cauchyåˆ†å¸ƒæŸå¤±ï¼ŒåŒæ—¶ä½¿ç”¨loc_Så’Œscale_S"""
        # å›å½’ï¼šCauchy NLLï¼Œä½¿ç”¨å®Œæ•´åˆ†å¸ƒå‚æ•°
        # åˆ†ç±»ï¼šOvR BCEï¼ŒåŸºäºCauchy CDFè®¡ç®—æ¦‚ç‡
        pass
```

**å›å½’æŸå¤±å®ç°**ï¼š
```python
def compute_regression_loss(self, loc_S, scale_S, y_true, mode):
    if mode == 'deterministic':
        # ä¼ ç»ŸMSEï¼šåªä½¿ç”¨ä½ç½®å‚æ•°ï¼Œå¿½ç•¥å°ºåº¦å‚æ•°
        return F.mse_loss(loc_S, y_true)
    else:
        # Cauchy NLLï¼šä½¿ç”¨å®Œæ•´åˆ†å¸ƒå‚æ•°
        # æ•°å€¼ç¨³å®šçš„Cauchyå¯¹æ•°æ¦‚ç‡å¯†åº¦å‡½æ•°
        # log p(y|Î¼,Î³) = -log(Ï€) - log(Î³) - log(1 + ((y-Î¼)/Î³)Â²)
        z = (y_true - loc_S) / (scale_S + 1e-8)  # æ ‡å‡†åŒ–
        log_prob = -torch.log(torch.pi) - torch.log(scale_S + 1e-8) - torch.log(1 + z*z)
        return -torch.sum(log_prob)
```

**åˆ†ç±»æŸå¤±å®ç°**ï¼š
```python
def compute_classification_loss(self, loc_S, scale_S, y_true, mode):
    if mode == 'deterministic':
        # ä¼ ç»ŸCrossEntropyï¼šåªä½¿ç”¨ä½ç½®å‚æ•°
        return F.cross_entropy(loc_S, y_true)
    else:
        # OvR BCEï¼šé€šè¿‡Cauchy CDFè®¡ç®—æ¦‚ç‡
        # æ•°å€¼ç¨³å®šæ€§ï¼šé˜²æ­¢é™¤é›¶å’Œæ¢¯åº¦çˆ†ç‚¸
        probs = 0.5 + (1/torch.pi) * torch.atan(loc_S / (scale_S + 1e-8))
        # æ¦‚ç‡å‰ªåˆ‡ï¼šé˜²æ­¢log(0)å’Œlog(1)çš„æ•°å€¼é—®é¢˜
        probs = torch.clamp(probs, min=1e-7, max=1-1e-7)
        return F.binary_cross_entropy(probs, y_true)
```

**ç»Ÿä¸€æ¥å£çš„ä¼˜åŠ¿**ï¼š
- âœ… **æ¥å£ä¸€è‡´æ€§**ï¼šæ‰€æœ‰æŸå¤±å‡½æ•°éƒ½æ¥æ”¶ç›¸åŒçš„è¾“å…¥ç­¾å
- âœ… **æ¨¡å¼é€æ˜æ€§**ï¼šæŸå¤±å‡½æ•°å†…éƒ¨æ ¹æ®modeè‡ªåŠ¨é€‰æ‹©è®¡ç®—æ–¹å¼
- âœ… **å‚æ•°å¤ç”¨æ€§**ï¼šDeterministicæ¨¡å¼å¯ä»¥å¿½ç•¥scale_Sï¼Œä½†ä¿æŒæ¥å£ç»Ÿä¸€
- âœ… **æ‰©å±•æ€§**ï¼šæ–°å¢æŸå¤±å‡½æ•°åªéœ€éµå¾ªç›¸åŒçš„æ¥å£çº¦å®š

äº”æ¨¡å¼ç³»ç»Ÿæœ¬è´¨æ˜¯ActionNetworkçš„äº”ç§ä¸åŒè®¡ç®—æ–¹å¼ï¼Œè¦†ç›–å‚æ•°ç©ºé—´ $(\gamma_U, b_{noise})$ çš„ä¸»è¦æœ‰æ„ä¹‰ç»„åˆï¼Œå®ç°ä»ç¡®å®šæ€§å»ºæ¨¡åˆ°éšæœºæ€§æ¢ç´¢çš„å®Œæ•´å› æœæ¨ç†å…‰è°±ã€‚

## ç»Ÿä¸€APIè®¾è®¡

### MLPCausalRegressorä¸MLPCausalClassifieræ ¸å¿ƒæ¥å£

```python
from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier

# å›å½’ä»»åŠ¡ - sklearné£æ ¼æ¥å£
reg = MLPCausalRegressor(
    hidden_layer_sizes=(64, 32),  # ç½‘ç»œç»“æ„
    mode='standard',              # äº”ç§æ¨¡å¼é€‰æ‹©
    max_iter=1000,               # è®­ç»ƒè½®æ•°
    random_state=42              # éšæœºç§å­
)
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)        # æ•°å€¼è¾“å‡º
distributions = reg.predict(X_test, mode='standard')  # åˆ†å¸ƒä¿¡æ¯ï¼ˆä¸€ç»´æ—¶å®Œæ•´ï¼Œé«˜ç»´æ—¶è¾¹é™…ï¼‰

# åˆ†ç±»ä»»åŠ¡ - ç›¸åŒçš„è®¾è®¡æ¨¡å¼
clf = MLPCausalClassifier(
    hidden_layer_sizes=(64, 32),
    mode='standard',
    max_iter=1000,
    random_state=42
)
clf.fit(X_train, y_train)
labels = clf.predict(X_test)             # ç±»åˆ«æ ‡ç­¾
probabilities = clf.predict_proba(X_test)  # æ¿€æ´»æ¦‚ç‡
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼šä»…æ›¿æ¢è¾“å‡ºå±‚

**æ¶æ„å¯¹æ¯”**ï¼š
```python
# ä¼ ç»ŸMLPRegressor/MLPClassifieræ¶æ„
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ çº¿æ€§è¾“å‡ºå±‚ â†’ ç¡®å®šæ€§é¢„æµ‹å€¼
  X    â†’   MLPs   â†’  y = WÂ·h + b  â†’    Å·

# MLPCausalRegressor/MLPCausalClassifieræ¶æ„
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ CausalEngine â†’ åˆ†å¸ƒè¾“å‡º â†’ æ¦‚ç‡é¢„æµ‹
  X    â†’   MLPs   â†’ (å½’å› +è¡ŒåŠ¨+æ¿€æ´») â†’ S~Cauchy â†’ P(Y)
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- ğŸš€ **è®­ç»ƒæ•ˆç‡**ï¼šå¤§éƒ¨åˆ†ç½‘ç»œç»“æ„å®Œå…¨ç›¸åŒ
- ğŸš€ **å‚æ•°è§„æ¨¡**ï¼šä»…CausalEngineéƒ¨åˆ†å¢åŠ å°‘é‡å‚æ•°  
- ğŸš€ **æ”¶ç›Šå·¨å¤§**ï¼šä»ç¡®å®šæ€§é¢„æµ‹å‡çº§åˆ°åˆ†å¸ƒå»ºæ¨¡å’Œå› æœæ¨ç†

### ç»Ÿä¸€predict()æ¥å£è®¾è®¡

ä¸¤ä¸ªç±»éƒ½æä¾›ç›¸åŒçš„æ¨¡å¼æ§åˆ¶æ¥å£ï¼š

```python
def predict(self, X, mode=None):
    """ç»Ÿä¸€é¢„æµ‹æ¥å£
    
    Parameters:
    -----------
    mode : str, optional
        é¢„æµ‹æ¨¡å¼ (å¯ä¸è®­ç»ƒæ¨¡å¼ä¸åŒ):
        - 'deterministic': ç¡®å®šæ€§å› æœ (ç­‰ä»·sklearn)
        - 'exogenous': å¤–ç”Ÿå™ªå£°å› æœ
        - 'endogenous': å†…ç”Ÿå› æœæ¨ç† 
        - 'standard': æ ‡å‡†å› æœæ¨ç† (é»˜è®¤)
        - 'sampling': æ¢ç´¢æ€§å› æœæ¨ç†
        
    Returns:
    --------
    predictions : array-like or dict
        - MLPCausalRegressor: æ•°å€¼æ•°ç»„ (ä¸€ç»´è¾“å‡ºæ—¶åŒ…å«å®Œæ•´åˆ†å¸ƒä¿¡æ¯)
        - MLPCausalClassifier: ç±»åˆ«æ ‡ç­¾æ•°ç»„
        è‹¥mode != 'deterministic', è¿˜åŒ…å«åˆ†å¸ƒä¿¡æ¯
    """
    return predictions
```

### åˆ†ç±»ä»»åŠ¡çš„OvRç­–ç•¥

**æ•°å­¦åŸç†**ï¼šå„ç±»åˆ«ç‹¬ç«‹æ¿€æ´»åˆ¤æ–­
$$P_{k,i} = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S_k,i}}{\gamma_{S_k,i}}\right)$$

**ä¼˜åŠ¿å¯¹æ¯”**ï¼š
- **ä¼ ç»ŸSoftmax**ï¼š$P_k = \frac{\exp(z_k)}{\sum_j \exp(z_j)}$ (å¼ºåˆ¶å½’ä¸€åŒ–çº¦æŸ)
- **CausalEngine OvR**ï¼š$P_k$ ç‹¬ç«‹è®¡ç®— (ç±»åˆ«é—´æ— ç«äº‰çº¦æŸ)

### æ¦‚ç‡é¢„æµ‹çš„å¯å‘å¼æ–¹æ³•

**æ ¸å¿ƒæ€æƒ³**ï¼šæä¾›ç±»ä¼¼æ¦‚ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œä½†æ‰¿è®¤å…¶å¯å‘å¼æœ¬è´¨

#### åˆ†ç±»ä»»åŠ¡çš„predict_dist

**æ•°å­¦å®šä¹‰**ï¼š
$$P_{k,i} = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S_k,i}}{\gamma_{S_k,i}}\right)$$

**è¾“å‡ºå½¢çŠ¶**ï¼š`[n_samples, n_classes]` - æ¿€æ´»æ¦‚ç‡åˆ†å¸ƒ

```python
clf = MLPCausalClassifier()
labels = clf.predict(X_test)           # ç±»åˆ«é¢„æµ‹ [n_samples]
probs = clf.predict_dist(X_test)       # æ¿€æ´»æ¦‚ç‡åˆ†å¸ƒ [n_samples, n_classes]
```

#### å›å½’ä»»åŠ¡çš„predict_dist

**æ•°å­¦å®šä¹‰**ï¼š
$$\text{predict\_dist}(X)_{i,j} = [\mu_{S_j,i}, \gamma_{S_j,i}]$$

**è¾“å‡ºå½¢çŠ¶**ï¼š`[n_samples, output_dim, 2]` - å®Œæ•´åˆ†å¸ƒå‚æ•°

```python
reg = MLPCausalRegressor()
predictions = reg.predict(X_test)      # é¢„æµ‹å€¼ [n_samples, output_dim]
dist_params = reg.predict_dist(X_test)  # åˆ†å¸ƒå‚æ•° [n_samples, output_dim, 2]

# è®¿é—®åˆ†å¸ƒå‚æ•°
loc = dist_params[:, :, 0]    # ä½ç½®å‚æ•° Î¼_S
scale = dist_params[:, :, 1]  # å°ºåº¦å‚æ•° Î³_S
```

#### é›†ä¸­åº¦è®¡ç®—ç¤ºä¾‹

ç”¨æˆ·å¯ä»¥åŸºäº `predict_dist()` çš„è¾“å‡ºè‡ªè¡Œè®¡ç®—é›†ä¸­åº¦ï¼š

```python
# åˆ†ç±»ï¼šé¢„æµ‹ç±»åˆ«çš„æ¿€æ´»æ¦‚ç‡
clf_probs = clf.predict_dist(X_test)        # [n_samples, n_classes]
clf_predictions = clf.predict(X_test)       # [n_samples]
clf_concentration = clf_probs[range(len(clf_predictions)), clf_predictions]

# å›å½’ï¼šç›¸å¯¹äºæ ‡å‡†Cauchyçš„é›†ä¸­åº¦
reg_dist_params = reg.predict_dist(X_test)  # [n_samples, output_dim, 2]
reg_scale = reg_dist_params[:, :, 1]        # [n_samples, output_dim]
reg_concentration = 1.0 / reg_scale         # [n_samples, output_dim]
```

## äº”æ¨¡å¼å‚æ•°æ§åˆ¶

### modeå‚æ•°çš„ç»Ÿä¸€æ§åˆ¶è®¾è®¡

**æ ¸å¿ƒåŸåˆ™**ï¼šmodeå‚æ•°è´¯ç©¿æ•´ä¸ªå»ºæ¨¡æµç¨‹ï¼Œæ§åˆ¶è®­ç»ƒã€æ¨ç†ã€æŸå¤±è®¡ç®—ï¼š

```python
class MLPCausalRegressor:
    def __init__(self, mode='standard', **kwargs):
        """äº”æ¨¡å¼ç»Ÿä¸€æ¥å£
        
        Parameters:
        -----------
        mode : str, default='standard'
            å»ºæ¨¡æ¨¡å¼é€‰æ‹©ï¼š
            - 'deterministic': Î³_U=0, b_noise=0 (ç­‰ä»·sklearn)
            - 'exogenous': Î³_U=0, b_noiseâ‰ 0 (å¤–ç”Ÿå™ªå£°)
            - 'endogenous': Î³_Uâ‰ 0, b_noise=0 (å†…ç”Ÿå› æœ)
            - 'standard': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’å°ºåº¦)
            - 'sampling': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’ä½ç½®)
        """
        self.mode = mode
        self._configure_mode_parameters()
    
    def _configure_mode_parameters(self):
        """æ ¹æ®æ¨¡å¼é…ç½®å†…éƒ¨å‚æ•°"""
        if self.mode == 'deterministic':
            self.gamma_U_enabled = False
            self.b_noise_enabled = False
            self.loss_type = 'traditional'  # MSE/CrossEntropy
        elif self.mode == 'exogenous':
            self.gamma_U_enabled = False
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode == 'endogenous':
            self.gamma_U_enabled = True
            self.b_noise_enabled = False
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode in ['standard', 'sampling']:
            self.gamma_U_enabled = True
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
```

### äº”æ¨¡å¼ActionNetworkå®ç°

**æ ¸å¿ƒè®¤çŸ¥**ï¼šäº”æ¨¡å¼çš„å·®å¼‚å°±æ˜¯ActionNetworkå¦‚ä½•è®¡ç®— $U'$ åˆ†å¸ƒï¼š

```python
class ActionNetwork(nn.Module):
    def forward(self, loc_U, scale_U, mode='standard'):
        """äº”æ¨¡å¼å·®å¼‚çš„æ ¸å¿ƒå®ç°"""
        
        if mode == 'deterministic':
            # U' = Î¼_U (ç¡®å®šæ€§)
            loc_U_final = loc_U
            scale_U_final = torch.zeros_like(scale_U)
        
        elif mode == 'exogenous':
            # U' ~ Cauchy(Î¼_U, |b_noise|)
            loc_U_final = loc_U
            scale_U_final = torch.full_like(scale_U, abs(self.b_noise))
        
        elif mode == 'endogenous':
            # U' ~ Cauchy(Î¼_U, Î³_U)
            loc_U_final = loc_U
            scale_U_final = scale_U
        
        elif mode == 'standard':
            # U' ~ Cauchy(Î¼_U, Î³_U + |b_noise|) - è§£æèåˆ
            loc_U_final = loc_U
            scale_U_final = scale_U + abs(self.b_noise)
        
        elif mode == 'sampling':
            # U' ~ Cauchy(Î¼_U + b_noise*Îµ, Î³_U) - ä½ç½®æ‰°åŠ¨
            # æ ‡å‡†Cauchyåˆ†å¸ƒé‡‡æ ·ï¼šÎµ ~ Cauchy(0,1)
            # ä½¿ç”¨åå˜æ¢é‡‡æ ·ï¼šÎµ = tan(Ï€(u - 0.5)), u ~ Uniform(0,1)
            u_uniform = torch.rand_like(loc_U)  # [batch_size, latent_dim]
            epsilon = torch.tan(torch.pi * (u_uniform - 0.5))  # [batch_size, latent_dim]
            loc_U_final = loc_U + self.b_noise * epsilon
            scale_U_final = scale_U
        
        # çº¿æ€§å› æœå¾‹ (æ‰€æœ‰æ¨¡å¼ç»Ÿä¸€)
        # ä½ç½®å‚æ•°å˜æ¢ï¼šÎ¼_S = W_A Â· Î¼_U' + b_A
        loc_S = self.lm_head(loc_U_final)  # [batch_size, output_dim]
        
        # å°ºåº¦å‚æ•°å˜æ¢ï¼šÎ³_S = |W_A| Â· Î³_U' (çŸ©é˜µä¹˜æ³•)
        # ç»´åº¦: [batch_size, latent_dim] @ [latent_dim, output_dim] â†’ [batch_size, output_dim]
        scale_S = scale_U_final @ torch.abs(self.lm_head.weight).T
        
        return loc_S, scale_S
```

### å‚æ•°ç©ºé—´å®Œå¤‡æ€§

äº”æ¨¡å¼è¦†ç›– $(\gamma_U, b_{noise})$ å‚æ•°ç©ºé—´çš„æ‰€æœ‰æœ‰æ„ä¹‰ç»„åˆï¼š

| æ¨¡å¼ | å‚æ•°é…ç½® | æ•°å­¦è¡¨è¿° | åº”ç”¨åœºæ™¯ |
|------|----------|----------|----------|
| **Deterministic** | $\gamma_U=0, b_{noise}=0$ | $U' = \mu_U$ | åŸºçº¿éªŒè¯ã€è°ƒè¯•å¼€å‘ |
| **Exogenous** | $\gamma_U=0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ | å¤–éƒ¨å†²å‡»å»ºæ¨¡ |
| **Endogenous** | $\gamma_U \neq 0, b_{noise}=0$ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ | é«˜å¯è§£é‡Šæ€§éœ€æ±‚ |
| **Standard** | $\gamma_U \neq 0, b_{noise} \neq 0$ (å°ºåº¦) | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ | ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² |
| **Sampling** | $\gamma_U \neq 0, b_{noise} \neq 0$ (ä½ç½®) | $U' \sim \text{Cauchy}(\mu_U + b_{noise}e, \gamma_U)$ | æ¢ç´¢æ€§ç ”ç©¶ |

### è®­ç»ƒä¸æ¨ç†çš„æ¨¡å¼çµæ´»æ€§

```python
# è®­ç»ƒæ—¶ä½¿ç”¨ä¸€ç§æ¨¡å¼
reg = MLPCausalRegressor(mode='standard')
reg.fit(X_train, y_train)

# æ¨ç†æ—¶å¯ä»¥åˆ‡æ¢æ¨¡å¼
deterministic_pred = reg.predict(X_test, mode='deterministic')  # sklearnå…¼å®¹
standard_pred = reg.predict(X_test, mode='standard')            # æ ‡å‡†å› æœ
causal_pred = reg.predict(X_test, mode='endogenous')           # çº¯å› æœ
sampling_pred = reg.predict(X_test, mode='sampling')           # æ¢ç´¢æ€§
```

## sklearnå…¼å®¹æ€§è®¾è®¡

### å®Œç¾çš„sklearnç”Ÿæ€é›†æˆ

```python
# ä¸sklearnç”Ÿæ€æ— ç¼é›†æˆ
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# äº¤å‰éªŒè¯
scores = cross_val_score(MLPCausalRegressor(), X, y, cv=5)

# ç½‘æ ¼æœç´¢
param_grid = {
    'hidden_layer_sizes': [(32,), (64,), (64, 32)],
    'mode': ['deterministic', 'standard', 'endogenous']
}
grid_search = GridSearchCV(MLPCausalRegressor(), param_grid, cv=3)

# ç®¡é“é›†æˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('causal', MLPCausalRegressor())
])
```

### æ™ºèƒ½é»˜è®¤é…ç½®ç­–ç•¥

**è‡ªåŠ¨ç½‘ç»œç»“æ„æ¨è**ï¼š
```python
def _auto_hidden_layer_sizes(n_features, n_samples):
    """æ ¹æ®ç‰¹å¾æ•°å’Œæ ·æœ¬æ•°æ™ºèƒ½æ¨èç½‘ç»œç»“æ„"""
    if n_features <= 10:
        return (32,)
    elif n_features <= 50:
        return (64, 32)
    elif n_features <= 100:
        return (128, 64)
    else:
        return (256, 128, 64)

# æ™ºèƒ½é»˜è®¤é…ç½®
AUTO_CONFIG = {
    'early_stopping': True,
    'patience': 20,
    'min_delta': 1e-4,
    'learning_rate_schedule': 'adaptive'
}
```

### æ•°å­¦ç­‰ä»·æ€§éªŒè¯

**Deterministicæ¨¡å¼çš„sklearnç­‰ä»·æ€§**ï¼š
```python
def test_sklearn_equivalence():
    """éªŒè¯Deterministicæ¨¡å¼ä¸sklearnçš„æ•°å­¦ç­‰ä»·æ€§"""
    # sklearnåŸºçº¿
    sklearn_reg = MLPRegressor(hidden_layer_sizes=(64, 32), alpha=0.0)
    sklearn_reg.fit(X_train, y_train)
    sklearn_pred = sklearn_reg.predict(X_test)
    
    # CausalEngineç­‰ä»·å®ç°
    causal_reg = MLPCausalRegressor(mode='deterministic', 
                                   hidden_layer_sizes=(64, 32))
    causal_reg.fit(X_train, y_train)
    causal_pred = causal_reg.predict(X_test)
    
    # ç­‰ä»·æ€§éªŒè¯
    r2_diff = abs(r2_score(y_test, sklearn_pred) - r2_score(y_test, causal_pred))
    pred_mse = mean_squared_error(sklearn_pred, causal_pred)
    
    assert r2_diff < 0.001, "ç­‰ä»·æ€§éªŒè¯å¤±è´¥"
    assert pred_mse < 0.001, "é¢„æµ‹å·®å¼‚è¿‡å¤§"
```
Deterministicæ¨¡å¼ç­‰ä»·æ€§æ•°å­¦åŸç†**æ ¸å¿ƒæœºåˆ¶**ï¼šAbductionNetworkè®¾ä¸ºæ’ç­‰æ˜ å°„ $W_{loc} = I, b_{loc} = 0$ å¹¶å†»ç»“å‚æ•°

**æ•°å­¦ç­‰ä»·**ï¼š
- sklearn: $\hat{y} = W_{final} \cdot h + b_{final}$  
- CausalEngine: $\hat{y} = W_A \cdot h + b_A$ (å› ä¸º $\mu_U = h$)
- ç­‰ä»·æ¡ä»¶: $W_A = W_{final}, b_A = b_{final}$

Deterministicæ¨¡å¼åœ¨è®¡ç®—æŸå¤±çš„æ—¶å€™åªç”¨åˆ°äº† loc_S çš„ä¿¡æ¯ã€‚


### sklearnæ ‡å‡†æ¥å£å®ç°

```python
class MLPCausalRegressor(BaseEstimator, RegressorMixin):
    """å®Œæ•´çš„sklearnæ ‡å‡†æ¥å£"""
    
    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒæ¨¡å‹ - sklearnæ ‡å‡†ç­¾å"""
        # è‡ªåŠ¨æ•°æ®éªŒè¯å’Œé¢„å¤„ç†
        X, y = check_X_y(X, y)
        
        # è‡ªåŠ¨æ„å»ºç½‘ç»œæ¶æ„
        if self.hidden_layer_sizes == 'auto':
            self.hidden_layer_sizes_ = self._auto_hidden_layer_sizes(X.shape[1], X.shape[0])
        
        # è®­ç»ƒå¾ªç¯ (å«early stopping)
        return self._fit_with_mode(X, y)
    
    def predict(self, X):
        """é¢„æµ‹ - sklearnæ ‡å‡†ç­¾å"""
        check_is_fitted(self)
        X = check_array(X)
        return self._predict_with_mode(X, self.mode)
    
    def score(self, X, y, sample_weight=None):
        """è¯„åˆ† - sklearnæ ‡å‡†ç­¾å"""
        return r2_score(y, self.predict(X), sample_weight=sample_weight)
    
    # sklearnæ ‡å‡†å±æ€§
    @property
    def feature_importances_(self):
        """ç‰¹å¾é‡è¦æ€§"""
        return self._compute_feature_importance()
    
    @property 
    def loss_curve_(self):
        """è®­ç»ƒæŸå¤±æ›²çº¿"""
        return self.training_loss_history_
```

### æ¸è¿›å¼èƒ½åŠ›è®¿é—®

**åˆ†å±‚èƒ½åŠ›è®¾è®¡**ï¼š
```python
# ç¬¬1å±‚ï¼šsklearnå®Œå…¨å…¼å®¹
reg = MLPCausalRegressor()
predictions = reg.predict(X_test)  # è¿”å›æ•°å€¼ï¼Œå¦‚sklearn

# ç¬¬2å±‚ï¼šåˆ†å¸ƒä¿¡æ¯è®¿é—®
distributions = reg.predict(X_test, mode='standard')  # è¿”å›åˆ†å¸ƒå¯¹è±¡ï¼ˆä¸€ç»´æ—¶å®Œæ•´è”åˆåˆ†å¸ƒï¼Œé«˜ç»´æ—¶è¾¹é™…åˆ†å¸ƒï¼‰

# ç¬¬3å±‚ï¼šå› æœæ¨ç†æ¨¡å¼
causal_dists = reg.predict(X_test, mode='endogenous')    # çº¯å› æœ
sampling_dists = reg.predict(X_test, mode='sampling')   # æ¢ç´¢æ€§

# æ•°å­¦ä¸€è‡´æ€§ä¿è¯
assert np.allclose(predictions, distributions.mean(), atol=1e-6)
```

## å®è·µæŒ‡å—

### æ¨¡å¼é€‰æ‹©å†³ç­–æ ‘

**æŒ‰åº”ç”¨éœ€æ±‚é€‰æ‹©æ¨¡å¼**ï¼š

```mermaid
graph TD
    Start([å¼€å§‹é€‰æ‹©æ¨¡å¼]) --> Question1{éœ€è¦sklearnå…¼å®¹ï¼Ÿ}
    
    Question1 -->|æ˜¯| Deterministic[ğŸ¯ Deterministic Mode<br/>ç­‰ä»·sklearnåŸºçº¿éªŒè¯]
    Question1 -->|å¦| Question2{ä¸ªä½“è¡¨å¾ç¡®å®šï¼Ÿ}
    
    Question2 -->|å®Œå…¨ç¡®å®š| Question3{å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ}
    Question2 -->|æœ‰ä¸ç¡®å®šæ€§| Question4{å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ}
    
    Question3 -->|æ˜¯| Exogenous[ğŸŒ Exogenous Mode<br/>ç¡®å®šä¸ªä½“+å¤–ç”Ÿå™ªå£°]
    Question3 -->|å¦| Deterministic
    
    Question4 -->|å¦| Endogenous[ğŸ§  Endogenous Mode<br/>çº¯å†…ç”Ÿå› æœæ¨ç†]
    Question4 -->|æ˜¯| Question5{åº”ç”¨åœºæ™¯ï¼Ÿ}
    
    Question5 -->|ç”Ÿäº§ç¯å¢ƒ| Standard[âš¡ Standard Mode<br/>å™ªå£°å¢å¼ºä¸ç¡®å®šæ€§]
    Question5 -->|æ¢ç´¢ç ”ç©¶| Sampling[ğŸ² Sampling Mode<br/>å™ªå£°æ‰°åŠ¨èº«ä»½]
    
    classDef questionStyle fill:#f9f9f9,stroke:#666,stroke-width:2px
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:3px
    
    class Start,Question1,Question2,Question3,Question4,Question5 questionStyle
    class Deterministic deterministicStyle
    class Exogenous exogenousStyle
    class Endogenous endogenousStyle
    class Standard standardStyle
    class Sampling samplingStyle
```

### åº”ç”¨åœºæ™¯æŒ‡å¯¼

| æ•°æ®ç‰¹æ€§ | æ¨èæ¨¡å¼ | å…¸å‹åº”ç”¨ | æ•°å­¦åŸç† |
|----------|----------|----------|----------|
| **å®Œå…¨ç¡®å®šæ€§æ•°æ®** | Deterministic | åŸºçº¿éªŒè¯ã€è°ƒè¯•å¼€å‘ | $U' = \mu_U$ |
| **ä¼ æ„Ÿå™¨æ•°æ®** | Exogenous | IoTè®¾å¤‡ã€æµ‹é‡ç³»ç»Ÿ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ |
| **åŒ»ç–—è¯Šæ–­** | Endogenous | ä¸ªä½“å·®å¼‚å»ºæ¨¡ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ |
| **é‡‘èé£æ§** | Standard | ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ |
| **æ¨èç³»ç»Ÿ** | Sampling | å¤šæ ·æ€§ç”Ÿæˆ | $U' \sim \text{Cauchy}(\mu_U + b_{noise}e, \gamma_U)$ |

### æ¸è¿›å¼å¼€å‘æµç¨‹

**é˜¶æ®µåŒ–å®æ–½ç­–ç•¥**ï¼š

```python
# é˜¶æ®µ1: åŸºçº¿éªŒè¯ (Deterministic Mode)
reg = MLPCausalRegressor(mode='deterministic')
reg.fit(X_train, y_train)
baseline_score = reg.score(X_test, y_test)
print(f"åŸºçº¿RÂ²: {baseline_score:.4f}")

# é˜¶æ®µ2: å› æœå»ºæ¨¡ (Endogenous Mode) 
reg_causal = MLPCausalRegressor(mode='endogenous')
reg_causal.fit(X_train, y_train)
causal_score = reg_causal.score(X_test, y_test)
print(f"å› æœRÂ²: {causal_score:.4f}")

# é˜¶æ®µ3: ç”Ÿäº§ä¼˜åŒ– (Standard Mode)
reg_standard = MLPCausalRegressor(mode='standard')
reg_standard.fit(X_train, y_train)
standard_score = reg_standard.score(X_test, y_test)
distributions = reg_standard.predict(X_test, mode='standard')
uncertainty = distributions.scale.mean()
print(f"æ ‡å‡†RÂ²: {standard_score:.4f}, å¹³å‡ä¸ç¡®å®šæ€§: {uncertainty:.4f}")

# é˜¶æ®µ4: æ¢ç´¢åˆ†æ (Sampling Mode)
reg_sampling = MLPCausalRegressor(mode='sampling')
diverse_predictions = []
for _ in range(10):  # å¤šæ¬¡é‡‡æ ·è·å¾—é¢„æµ‹å¤šæ ·æ€§
    pred = reg_sampling.predict(X_test, mode='sampling')
    diverse_predictions.append(pred)

diversity = np.std(diverse_predictions, axis=0).mean()
print(f"é¢„æµ‹å¤šæ ·æ€§: {diversity:.4f}")
```

### å…³é”®å®è·µåŸåˆ™

1. **å§‹ç»ˆä»Deterministicå¼€å§‹**ï¼šç¡®ä¿ç®—æ³•æ­£ç¡®æ€§åå†æ·»åŠ å¤æ‚æ€§
2. **æ•°å­¦ç­‰ä»·æ€§éªŒè¯**ï¼šä¸sklearnåŸºçº¿å¯¹æ¯”éªŒè¯å®ç°æ­£ç¡®æ€§  
3. **æŸå¤±å‡½æ•°ç»Ÿä¸€**ï¼šæ¨¡å¼2-5å¿…é¡»ä½¿ç”¨ç›¸åŒçš„Cauchy NLLæŸå¤±
4. **æ¸è¿›å¼å¤æ‚åŒ–**ï¼šé€æ­¥å¼•å…¥ä¸ç¡®å®šæ€§å’Œå™ªå£°æœºåˆ¶
5. **å……åˆ†æµ‹è¯•éªŒè¯**ï¼šæ¯ä¸ªæ¨¡å¼éƒ½éœ€è¦ç‹¬ç«‹éªŒè¯æ•°å­¦æ­£ç¡®æ€§

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

**æ¨¡å¼ç‰¹å®šçš„è¶…å‚æ•°å»ºè®®**ï¼š

```python
# Deterministic Mode: ç­‰ä»·sklearnï¼Œä½¿ç”¨ä¼ ç»Ÿè°ƒä¼˜
deterministic_params = {
    'hidden_layer_sizes': [(64, 32), (128, 64)],
    'learning_rate_init': [0.001, 0.01],
    'alpha': [0.0001, 0.001]  # L2æ­£åˆ™åŒ–
}

# Standard Mode: å¹³è¡¡æ€§èƒ½ä¸ç¨³å®šæ€§
standard_params = {
    'hidden_layer_sizes': [(64, 32), (128, 64)],
    'b_noise_init': [0.1, 0.2, 0.5],     # å™ªå£°å¼ºåº¦
    'gamma_init': [0.5, 1.0, 2.0]        # åˆå§‹å°ºåº¦
}

# Sampling Mode: æ¢ç´¢æ€§è¾ƒå¼ºï¼Œéœ€è¦æ›´å¤§ç½‘ç»œ
sampling_params = {
    'hidden_layer_sizes': [(128, 64), (256, 128)],
    'b_noise_init': [0.2, 0.5, 1.0],     # æ›´å¤§å™ªå£°
    'max_iter': [1500, 2000]             # æ›´å¤šè®­ç»ƒè½®æ•°
}
```

## æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ï¼šæ ‡ç­¾å™ªå£°é²æ£’æ€§

### ç†è®ºåŸºç¡€ï¼šä¸ºä»€ä¹ˆCausalEngineå¤©ç„¶æŠ—å™ªå£°

**æ•°å­¦åŸç†**ï¼šCausalEngineå­¦ä¹ ä¸ªä½“å†…åœ¨å› æœè¡¨å¾ï¼Œè€Œéè¡¨é¢ç»Ÿè®¡å…³è”

$$U \sim \text{Cauchy}(\mu_U, \gamma_U) \quad \text{(å­¦ä¹ ä¸ªä½“å› æœæœ¬è´¨)}$$
$$Y = f(U, \varepsilon) \quad \text{(åº”ç”¨æ™®é€‚å› æœæœºåˆ¶)}$$

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬å·®å¼‚**ï¼š
```python
# ä¼ ç»ŸMLPRegressor/MLPClassifierï¼šå­¦ä¹ è¡¨é¢å…³è”
# X â†’ h â†’ Å· = Wh + b  (å®¹æ˜“è¢«å™ªå£°æ ‡ç­¾è¯¯å¯¼)

# MLPCausalRegressor/MLPClassifierï¼šå­¦ä¹ å› æœæœ¬è´¨  
# X â†’ h â†’ U â†’ S â†’ Y  (å­¦ä¹ æ·±å±‚å› æœç»“æ„ï¼ŒæŠ—å™ªå£°)
```

### åˆ†ç±»ä»»åŠ¡çš„OvRç­–ç•¥ä¼˜åŠ¿

**CausalEngine OvRçš„ç‹¬ç«‹æ€§**ï¼š
$$P_{k,i} = \frac{1}{2} + \frac{1}{\pi} \arctan\left(\frac{\mu_{S_k,i}}{\gamma_{S_k,i}}\right) \quad \text{(æ¯ä¸ªç±»åˆ«ç‹¬ç«‹åˆ¤æ–­)}$$

**ä¼ ç»ŸSoftmaxçš„ç«äº‰æ€§**ï¼š
$$P_k^{\text{softmax}} = \frac{\exp(z_k)}{\sum_{j=1}^K \exp(z_j)} \quad \text{(å¼ºåˆ¶å½’ä¸€åŒ–çº¦æŸ)}$$

**å…³é”®å·®å¼‚çš„å®ç”¨å½±å“**ï¼š
```python
# å™ªå£°åœºæ™¯ç¤ºä¾‹ï¼šçœŸå®æ ‡ç­¾[Cat]è¢«é”™è¯¯æ ‡è®°ä¸º[Dog]

# âŒ ä¼ ç»ŸSoftmaxï¼šå™ªå£°ä¼ æ’­åˆ°æ‰€æœ‰ç±»åˆ«
# é”™è¯¯è®­ç»ƒæ ·æœ¬å½±å“æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å½’ä¸€åŒ–
softmax_probs = [0.1, 0.7, 0.2]  # [Cat, Dog, Bird] - Dogæ¦‚ç‡è¢«é”™è¯¯æå‡

# âœ… CausalEngine OvRï¼šå™ªå£°å±€é™åœ¨å•ä¸ªç±»åˆ«  
# é”™è¯¯æ ‡ç­¾åªå½±å“å¯¹åº”ç±»åˆ«ï¼Œå…¶ä»–ç±»åˆ«ä¿æŒç‹¬ç«‹
ovr_probs = [0.8, 0.3, 0.2]  # [Cat, Dog, Bird] - Catæ¦‚ç‡ä¿æŒå‡†ç¡®
```

### å¼€ç®±å³ç”¨çš„å™ªå£°å¤„ç†

**å·¥ä½œæµç®€åŒ–å¯¹æ¯”**ï¼š

```python
# âŒ ä¼ ç»Ÿæ–¹æ³•ï¼šéœ€è¦å¤æ‚çš„æ•°æ®æ¸…æ´—æµç¨‹
from sklearn.neural_network import MLPClassifier

# ç¬¬1æ­¥ï¼šäººå·¥è¯†åˆ«å’Œå¤„ç†å™ªå£°ï¼ˆè€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼‰
X_clean, y_clean = manual_outlier_detection(X_raw, y_raw)
y_scaled = RobustScaler().fit_transform(y_clean.reshape(-1, 1))

# ç¬¬2æ­¥ï¼šè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
traditional_clf = MLPClassifier().fit(X_clean, y_scaled.ravel())

# âœ… CausalEngineï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
causal_clf = MLPCausalClassifier()
causal_clf.fit(X_raw, y_raw)  # æ— éœ€é¢„å¤„ç†ï¼

# æ€§èƒ½å¯¹æ¯”ï¼šåœ¨å¹²å‡€æµ‹è¯•é›†ä¸Šè¯„ä¼°
print(f"ä¼ ç»Ÿæ–¹æ³•ç²¾åº¦: {accuracy_score(y_test_clean, traditional_clf.predict(X_test)):.3f}")
print(f"CausalEngineç²¾åº¦: {accuracy_score(y_test_clean, causal_clf.predict(X_test)):.3f}")
```

### ç°å®å™ªå£°åœºæ™¯çš„ä¼˜åŠ¿

**é«˜ä»·å€¼åº”ç”¨åœºæ™¯**ï¼š
1. **åŒ»ç–—æ•°æ®**ï¼šè¯Šæ–­æ ‡ç­¾å­˜åœ¨ä¸»è§‚æ€§å’Œé”™è¯¯
2. **é‡‘èæ•°æ®**ï¼šæ•°æ®æºä¸ä¸€è‡´ï¼Œæ ‡ç­¾è´¨é‡å‚å·®ä¸é½  
3. **ä¼—åŒ…æ ‡æ³¨**ï¼šäººå·¥æ ‡æ³¨å­˜åœ¨ä¸»è§‚å·®å¼‚å’Œé”™è¯¯
4. **ä¼ æ„Ÿå™¨æ•°æ®**ï¼šç¯å¢ƒå¹²æ‰°å¯¼è‡´çš„æµ‹é‡è¯¯å·®

**é¢„æœŸæ€§èƒ½ä¼˜åŠ¿**ï¼š
- **æ•°é‡çº§é”™è¯¯**ï¼šCausalEngineåœ¨10x/100xé”™è¯¯ä¸‹ä»ä¿æŒ80%+æ€§èƒ½
- **æ ‡ç­¾ç¿»è½¬**ï¼š50%æ ‡ç­¾å™ªå£°ä¸‹ä»ä¿æŒ80%+åŸå§‹æ€§èƒ½
- **ç³»ç»Ÿåå·®**ï¼šé€šè¿‡å› æœè¡¨å¾å­¦ä¹ å¯ä»¥éƒ¨åˆ†æŠµæ¶ˆåå·®  
- **å¼‚å¸¸å€¼**ï¼šCauchyåˆ†å¸ƒçš„é‡å°¾ç‰¹æ€§å¤©ç„¶é€‚åˆå¤„ç†å¼‚å¸¸å€¼

### ç«äº‰ä¼˜åŠ¿æ€»ç»“

**æŠ€æœ¯å·®å¼‚åŒ–**ï¼š
- **æ•°å­¦åˆ›æ–°**ï¼šåŸºäºCauchyåˆ†å¸ƒçš„å› æœæ¨ç†æ¡†æ¶ï¼Œåœ¨ä¸€ç»´å›å½’ä¸­æä¾›å®Œæ•´çš„ä¸ç¡®å®šæ€§é‡åŒ–
- **è§£æä¼˜åŠ¿**ï¼šæ— é‡‡æ ·çš„åˆ†å¸ƒè®¡ç®—ï¼Œæé«˜çš„è®¡ç®—æ•ˆç‡
- **ç‹¬ç‰¹æ¶æ„**ï¼šOvRç­–ç•¥å¸¦æ¥çš„çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›

**ç”¨æˆ·ä½“éªŒä¼˜åŠ¿**ï¼š
- **é›¶å­¦ä¹ æˆæœ¬**ï¼šå®Œç¾çš„sklearnå…¼å®¹æ€§
- **æ¸è¿›å¼èƒ½åŠ›**ï¼šä»ç®€å•é¢„æµ‹åˆ°å¤æ‚åˆ†å¸ƒåˆ†æ
- **å·¥ä½œæµç®€åŒ–**ï¼šä»20+è¡Œé¢„å¤„ç†ä»£ç ç®€åŒ–ä¸º1è¡Œè®­ç»ƒä»£ç 
- **ä¸°å¯Œä¿¡æ¯**ï¼šä¸ä»…æœ‰é¢„æµ‹å€¼ï¼Œè¿˜æœ‰ä¸ç¡®å®šæ€§ä¿¡æ¯ï¼ˆä¸€ç»´å›å½’æ—¶æ•°å­¦å®Œå¤‡ï¼Œé«˜ç»´æ—¶åŸºäºç‹¬ç«‹å‡è®¾ï¼‰
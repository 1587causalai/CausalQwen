# Sklearn-Style CausalEngine è®¾è®¡æ–‡æ¡£

> **MLPCausalRegressor & MLPCausalClassifier æ ¸å¿ƒæ•°å­¦æ¡†æ¶**

CausalEngineåŸºäºå› æœç»“æ„æ–¹ç¨‹ $Y = f(U, E)$ æ„å»ºé¢„æµ‹æ¨¡å‹ï¼Œå…¶ä¸­ $U$ ä¸ºä¸ªä½“å› æœè¡¨å¾ï¼Œ$E \sim \text{Cauchy}(0, I)$ ä¸ºå¤–ç”Ÿå™ªå£°ã€‚æ ¸å¿ƒæ•°å­¦æ¡†æ¶åˆ©ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ï¼šè‹¥ $X \sim \text{Cauchy}(\mu, \gamma)$ï¼Œåˆ™ $aX + b \sim \text{Cauchy}(a\mu + b, |a|\gamma)$ã€‚

äº”ç§æ¨¡å¼éµå¾ªç»Ÿä¸€å…¬å¼ $U' = U + b_{noise} \cdot E$ï¼Œä½†åœ¨ActionNetworkä¸­äº§ç”Ÿä¸åŒçš„ $U'$ åˆ†å¸ƒï¼š

$$\begin{aligned}
\text{Deterministic:} \quad U' &= \mu_U \\
\text{Exogenous:} \quad U' &\sim \text{Cauchy}(\mu_U, |b_{noise}|) \\
\text{Endogenous:} \quad U' &\sim \text{Cauchy}(\mu_U, \gamma_U) \\
\text{Standard:} \quad U' &\sim \text{Cauchy}(\mu_U, \gamma_U + |b_{noise}|) \\
\text{Sampling:} \quad U' &\sim \text{Cauchy}(\mu_U + b_{noise} \cdot e, \gamma_U)
\end{aligned}$$

```mermaid
flowchart LR
    Input["è¾“å…¥ X"]
    MLP["ç‰¹å¾æå–<br/>H = MLP(X)"]
    Abduction["ä¸ªä½“æ¨æ–­<br/>Î¼_U = W_loc*H + b_loc<br/>Î³_U = softplus(W_scale*H + b_scale)"]
    Action["å™ªå£°è°ƒåˆ¶ & çº¿æ€§å› æœå¾‹<br/>ActionNetwork<br/>(æ¨¡å¼å·®å¼‚æ ¸å¿ƒ)"]
    Output["è¾“å‡ºç”Ÿæˆ<br/>å›å½’: Y = Î¼_S<br/>åˆ†ç±»: P(Y=k) via æŸ¯è¥¿CDF"]

    Input --> MLP --> Abduction --> Action --> Output

    classDef inputStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef coreStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef actionStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    classDef outputStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    class Input inputStyle
    class MLP,Abduction coreStyle
    class Action actionStyle
    class Output outputStyle
```

å‰å‘ä¼ æ’­æµç¨‹ä¸º $X \xrightarrow{MLP} H \xrightarrow{Abduction} (\mu_U, \gamma_U) \xrightarrow{Action} (\mu_S, \gamma_S) \xrightarrow{Output} Y$ï¼Œå…¶ä¸­å…³é”®æ­¥éª¤ï¼š

ä¸ªä½“æ¨æ–­ï¼š$\mu_U = W_{loc} \cdot H + b_{loc}$ï¼Œ$\gamma_U = \text{softplus}(W_{scale} \cdot H + b_{scale})$

çº¿æ€§å› æœå¾‹ï¼š$\mu_S = W_S \cdot \mu_{U'} + b_S$ï¼Œ$\gamma_S = |W_S| \cdot \gamma_{U'}$

```mermaid
flowchart LR
    Input["è¾“å…¥ (Î¼_U, Î³_U)"]
    
    Deterministic["ğŸ¯ Deterministic<br/>Î¼_U' = Î¼_U, Î³_U' = 0"]
    Endogenous["ğŸ§  Endogenous<br/>Î¼_U' = Î¼_U, Î³_U' = Î³_U"]
    Exogenous["ğŸŒ Exogenous<br/>Î¼_U' = Î¼_U, Î³_U' = |b_noise|"]
    Standard["âš¡ Standard<br/>Î¼_U' = Î¼_U, Î³_U' = Î³_U + |b_noise|"]
    Sampling["ğŸ² Sampling<br/>Î¼_U' = Î¼_U + b_noise*e, Î³_U' = Î³_U"]

    CausalLaw["çº¿æ€§å› æœå¾‹<br/>Î¼_S = W_S*Î¼_U' + b_S<br/>Î³_S = |W_S|*Î³_U'"]
    Output["è¾“å‡º (Î¼_S, Î³_S)"]

    Input --> Deterministic --> CausalLaw
    Input --> Endogenous --> CausalLaw
    Input --> Exogenous --> CausalLaw
    Input --> Standard --> CausalLaw
    Input --> Sampling --> CausalLaw
    CausalLaw --> Output

    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef commonStyle fill:#f5f5f5,stroke:#666,stroke-width:2px

    class Deterministic deterministicStyle
    class Endogenous endogenousStyle
    class Exogenous exogenousStyle
    class Standard standardStyle
    class Sampling samplingStyle
    class Input,CausalLaw,Output commonStyle
```

è¾“å‡ºç”Ÿæˆï¼šå›å½’ä»»åŠ¡ $Y = \mu_S$ï¼Œåˆ†ç±»ä»»åŠ¡ $$P(Y=k) = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S,k}}{\gamma_{S,k}}\right)$$

æŸå¤±å‡½æ•°è®¡ç®—ï¼š

**Deterministicæ¨¡å¼** ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°ï¼š

å›å½’ï¼š$$L_{MSE} = \frac{1}{N}\sum_i (y_i - \mu_{S,i})^2$$

åˆ†ç±»ï¼š$$L_{CE} = -\frac{1}{N}\sum_i \sum_k y_{i,k} \log \text{softmax}(\mu_{S,i})_k$$

**å› æœæ¨¡å¼** ç»Ÿä¸€ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒæŸå¤±ï¼š

å›å½’æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š$$L_{Cauchy} = -\sum_i \log \frac{1}{\pi\gamma_{S,i}[1 + ((y_i-\mu_{S,i})/\gamma_{S,i})^2]}$$

åˆ†ç±»OvRäºŒå…ƒäº¤å‰ç†µï¼š$$P_{i,k} = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S,i,k}}{\gamma_{S,i,k}}\right)$$

$$L_{OvR} = -\frac{1}{N}\sum_i \sum_k [y_{i,k} \log P_{i,k} + (1-y_{i,k}) \log (1-P_{i,k})]$$

äº”æ¨¡å¼ç³»ç»Ÿæœ¬è´¨æ˜¯ActionNetworkçš„äº”ç§ä¸åŒè®¡ç®—æ–¹å¼ï¼Œè¦†ç›–å‚æ•°ç©ºé—´ $(\gamma_U, b_{noise})$ çš„ä¸»è¦æœ‰æ„ä¹‰ç»„åˆï¼Œå®ç°ä»ç¡®å®šæ€§å»ºæ¨¡åˆ°éšæœºæ€§æ¢ç´¢çš„å®Œæ•´å› æœæ¨ç†å…‰è°±ã€‚

## ç»Ÿä¸€APIè®¾è®¡

### MLPCausalRegressorä¸MLPCausalClassifieræ ¸å¿ƒæ¥å£

```python
from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier

# å›å½’ä»»åŠ¡ - sklearné£æ ¼æ¥å£
reg = MLPCausalRegressor(
    hidden_layer_sizes=(64, 32),  # ç½‘ç»œç»“æ„
    mode='standard',              # äº”ç§æ¨¡å¼é€‰æ‹©
    max_iter=1000,               # è®­ç»ƒè½®æ•°
    random_state=42              # éšæœºç§å­
)
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)        # æ•°å€¼è¾“å‡º
distributions = reg.predict(X_test, mode='standard')  # åˆ†å¸ƒä¿¡æ¯

# åˆ†ç±»ä»»åŠ¡ - ç›¸åŒçš„è®¾è®¡æ¨¡å¼
clf = MLPCausalClassifier(
    hidden_layer_sizes=(64, 32),
    mode='standard',
    max_iter=1000,
    random_state=42
)
clf.fit(X_train, y_train)
labels = clf.predict(X_test)             # ç±»åˆ«æ ‡ç­¾
probabilities = clf.predict_proba(X_test)  # æ¿€æ´»æ¦‚ç‡
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼šä»…æ›¿æ¢è¾“å‡ºå±‚

**æ¶æ„å¯¹æ¯”**ï¼š
```python
# ä¼ ç»ŸMLPRegressor/MLPClassifieræ¶æ„
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ çº¿æ€§è¾“å‡ºå±‚ â†’ ç¡®å®šæ€§é¢„æµ‹å€¼
  X    â†’   MLPs   â†’  y = WÂ·h + b  â†’    Å·

# MLPCausalRegressor/MLPCausalClassifieræ¶æ„
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ CausalEngine â†’ åˆ†å¸ƒè¾“å‡º â†’ æ¦‚ç‡é¢„æµ‹
  X    â†’   MLPs   â†’ (å½’å› +è¡ŒåŠ¨+æ¿€æ´») â†’ S~Cauchy â†’ P(Y)
```

**å…³é”®ä¼˜åŠ¿**ï¼š
- ğŸš€ **è®­ç»ƒæ•ˆç‡**ï¼šå¤§éƒ¨åˆ†ç½‘ç»œç»“æ„å®Œå…¨ç›¸åŒ
- ğŸš€ **å‚æ•°è§„æ¨¡**ï¼šä»…CausalEngineéƒ¨åˆ†å¢åŠ å°‘é‡å‚æ•°  
- ğŸš€ **æ”¶ç›Šå·¨å¤§**ï¼šä»ç¡®å®šæ€§é¢„æµ‹å‡çº§åˆ°åˆ†å¸ƒå»ºæ¨¡å’Œå› æœæ¨ç†

### ç»Ÿä¸€predict()æ¥å£è®¾è®¡

ä¸¤ä¸ªç±»éƒ½æä¾›ç›¸åŒçš„æ¨¡å¼æ§åˆ¶æ¥å£ï¼š

```python
def predict(self, X, mode=None):
    """ç»Ÿä¸€é¢„æµ‹æ¥å£
    
    Parameters:
    -----------
    mode : str, optional
        é¢„æµ‹æ¨¡å¼ (å¯ä¸è®­ç»ƒæ¨¡å¼ä¸åŒ):
        - 'deterministic': ç¡®å®šæ€§å› æœ (ç­‰ä»·sklearn)
        - 'exogenous': å¤–ç”Ÿå™ªå£°å› æœ
        - 'endogenous': å†…ç”Ÿå› æœæ¨ç† 
        - 'standard': æ ‡å‡†å› æœæ¨ç† (é»˜è®¤)
        - 'sampling': æ¢ç´¢æ€§å› æœæ¨ç†
        
    Returns:
    --------
    predictions : array-like or dict
        - MLPCausalRegressor: æ•°å€¼æ•°ç»„
        - MLPCausalClassifier: ç±»åˆ«æ ‡ç­¾æ•°ç»„
        è‹¥mode != 'deterministic', è¿˜åŒ…å«åˆ†å¸ƒä¿¡æ¯
    """
    return predictions
```

### åˆ†ç±»ä»»åŠ¡çš„OvRç­–ç•¥

**æ•°å­¦åŸç†**ï¼šå„ç±»åˆ«ç‹¬ç«‹æ¿€æ´»åˆ¤æ–­
$$P_k = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\mu_{S,k}}{\gamma_{S,k}}\right)$$

**ä¼˜åŠ¿å¯¹æ¯”**ï¼š
- **ä¼ ç»ŸSoftmax**ï¼š$P_k = \frac{\exp(z_k)}{\sum_j \exp(z_j)}$ (å¼ºåˆ¶å½’ä¸€åŒ–çº¦æŸ)
- **CausalEngine OvR**ï¼š$P_k$ ç‹¬ç«‹è®¡ç®— (ç±»åˆ«é—´æ— ç«äº‰çº¦æŸ)

**åˆ†ç±»ä¸“ç”¨æ¥å£**ï¼š
```python
clf = MLPCausalClassifier()
labels = clf.predict(X_test)                    # ç±»åˆ«é¢„æµ‹
probabilities = clf.predict_proba(X_test)       # æ¦‚ç‡é¢„æµ‹  
ovr_dists = clf.predict(X_test, mode='standard') # OvRåˆ†å¸ƒä¿¡æ¯
```

## äº”æ¨¡å¼å‚æ•°æ§åˆ¶

### modeå‚æ•°çš„ç»Ÿä¸€æ§åˆ¶è®¾è®¡

**æ ¸å¿ƒåŸåˆ™**ï¼šmodeå‚æ•°è´¯ç©¿æ•´ä¸ªå»ºæ¨¡æµç¨‹ï¼Œæ§åˆ¶è®­ç»ƒã€æ¨ç†ã€æŸå¤±è®¡ç®—ï¼š

```python
class MLPCausalRegressor:
    def __init__(self, mode='standard', **kwargs):
        """äº”æ¨¡å¼ç»Ÿä¸€æ¥å£
        
        Parameters:
        -----------
        mode : str, default='standard'
            å»ºæ¨¡æ¨¡å¼é€‰æ‹©ï¼š
            - 'deterministic': Î³_U=0, b_noise=0 (ç­‰ä»·sklearn)
            - 'exogenous': Î³_U=0, b_noiseâ‰ 0 (å¤–ç”Ÿå™ªå£°)
            - 'endogenous': Î³_Uâ‰ 0, b_noise=0 (å†…ç”Ÿå› æœ)
            - 'standard': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’å°ºåº¦)
            - 'sampling': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’ä½ç½®)
        """
        self.mode = mode
        self._configure_mode_parameters()
    
    def _configure_mode_parameters(self):
        """æ ¹æ®æ¨¡å¼é…ç½®å†…éƒ¨å‚æ•°"""
        if self.mode == 'deterministic':
            self.gamma_U_enabled = False
            self.b_noise_enabled = False
            self.loss_type = 'traditional'  # MSE/CrossEntropy
        elif self.mode == 'exogenous':
            self.gamma_U_enabled = False
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode == 'endogenous':
            self.gamma_U_enabled = True
            self.b_noise_enabled = False
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode in ['standard', 'sampling']:
            self.gamma_U_enabled = True
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
```

### äº”æ¨¡å¼ActionNetworkå®ç°

**æ ¸å¿ƒè®¤çŸ¥**ï¼šäº”æ¨¡å¼çš„å·®å¼‚å°±æ˜¯ActionNetworkå¦‚ä½•è®¡ç®— $U'$ åˆ†å¸ƒï¼š

```python
class ActionNetwork(nn.Module):
    def forward(self, loc_U, scale_U, mode='standard'):
        """äº”æ¨¡å¼å·®å¼‚çš„æ ¸å¿ƒå®ç°"""
        
        if mode == 'deterministic':
            # U' = Î¼_U (ç¡®å®šæ€§)
            loc_U_final = loc_U
            scale_U_final = torch.zeros_like(scale_U)
        
        elif mode == 'exogenous':
            # U' ~ Cauchy(Î¼_U, |b_noise|)
            loc_U_final = loc_U
            scale_U_final = torch.full_like(scale_U, abs(self.b_noise))
        
        elif mode == 'endogenous':
            # U' ~ Cauchy(Î¼_U, Î³_U)
            loc_U_final = loc_U
            scale_U_final = scale_U
        
        elif mode == 'standard':
            # U' ~ Cauchy(Î¼_U, Î³_U + |b_noise|) - è§£æèåˆ
            loc_U_final = loc_U
            scale_U_final = scale_U + abs(self.b_noise)
        
        elif mode == 'sampling':
            # U' ~ Cauchy(Î¼_U + b_noise*Îµ, Î³_U) - ä½ç½®æ‰°åŠ¨
            epsilon = torch.tan(torch.pi * (torch.rand_like(loc_U) - 0.5))
            loc_U_final = loc_U + self.b_noise * epsilon
            scale_U_final = scale_U
        
        # çº¿æ€§å› æœå¾‹ (æ‰€æœ‰æ¨¡å¼ç»Ÿä¸€)
        loc_S = self.lm_head(loc_U_final)
        scale_S = scale_U_final @ torch.abs(self.lm_head.weight).T
        
        return loc_S, scale_S
```

### å‚æ•°ç©ºé—´å®Œå¤‡æ€§

äº”æ¨¡å¼è¦†ç›– $(\gamma_U, b_{noise})$ å‚æ•°ç©ºé—´çš„æ‰€æœ‰æœ‰æ„ä¹‰ç»„åˆï¼š

| æ¨¡å¼ | å‚æ•°é…ç½® | æ•°å­¦è¡¨è¿° | åº”ç”¨åœºæ™¯ |
|------|----------|----------|----------|
| **Deterministic** | $\gamma_U=0, b_{noise}=0$ | $U' = \mu_U$ | åŸºçº¿éªŒè¯ã€è°ƒè¯•å¼€å‘ |
| **Exogenous** | $\gamma_U=0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ | å¤–éƒ¨å†²å‡»å»ºæ¨¡ |
| **Endogenous** | $\gamma_U \neq 0, b_{noise}=0$ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ | é«˜å¯è§£é‡Šæ€§éœ€æ±‚ |
| **Standard** | $\gamma_U \neq 0, b_{noise} \neq 0$ (å°ºåº¦) | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ | ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² |
| **Sampling** | $\gamma_U \neq 0, b_{noise} \neq 0$ (ä½ç½®) | $U' \sim \text{Cauchy}(\mu_U + b_{noise}e, \gamma_U)$ | æ¢ç´¢æ€§ç ”ç©¶ |

### è®­ç»ƒä¸æ¨ç†çš„æ¨¡å¼çµæ´»æ€§

```python
# è®­ç»ƒæ—¶ä½¿ç”¨ä¸€ç§æ¨¡å¼
reg = MLPCausalRegressor(mode='standard')
reg.fit(X_train, y_train)

# æ¨ç†æ—¶å¯ä»¥åˆ‡æ¢æ¨¡å¼
deterministic_pred = reg.predict(X_test, mode='deterministic')  # sklearnå…¼å®¹
standard_pred = reg.predict(X_test, mode='standard')            # æ ‡å‡†å› æœ
causal_pred = reg.predict(X_test, mode='endogenous')           # çº¯å› æœ
sampling_pred = reg.predict(X_test, mode='sampling')           # æ¢ç´¢æ€§
```

## sklearnå…¼å®¹æ€§è®¾è®¡

### å®Œç¾çš„sklearnç”Ÿæ€é›†æˆ

```python
# ä¸sklearnç”Ÿæ€æ— ç¼é›†æˆ
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# äº¤å‰éªŒè¯
scores = cross_val_score(MLPCausalRegressor(), X, y, cv=5)

# ç½‘æ ¼æœç´¢
param_grid = {
    'hidden_layer_sizes': [(32,), (64,), (64, 32)],
    'mode': ['deterministic', 'standard', 'endogenous']
}
grid_search = GridSearchCV(MLPCausalRegressor(), param_grid, cv=3)

# ç®¡é“é›†æˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('causal', MLPCausalRegressor())
])
```

### æ™ºèƒ½é»˜è®¤é…ç½®ç­–ç•¥

**è‡ªåŠ¨ç½‘ç»œç»“æ„æ¨è**ï¼š
```python
def _auto_hidden_layer_sizes(n_features, n_samples):
    """æ ¹æ®ç‰¹å¾æ•°å’Œæ ·æœ¬æ•°æ™ºèƒ½æ¨èç½‘ç»œç»“æ„"""
    if n_features <= 10:
        return (32,)
    elif n_features <= 50:
        return (64, 32)
    elif n_features <= 100:
        return (128, 64)
    else:
        return (256, 128, 64)

# æ™ºèƒ½é»˜è®¤é…ç½®
AUTO_CONFIG = {
    'early_stopping': True,
    'patience': 20,
    'min_delta': 1e-4,
    'learning_rate_schedule': 'adaptive'
}
```

### æ•°å­¦ç­‰ä»·æ€§éªŒè¯

**Deterministicæ¨¡å¼çš„sklearnç­‰ä»·æ€§**ï¼š
```python
def test_sklearn_equivalence():
    """éªŒè¯Deterministicæ¨¡å¼ä¸sklearnçš„æ•°å­¦ç­‰ä»·æ€§"""
    # sklearnåŸºçº¿
    sklearn_reg = MLPRegressor(hidden_layer_sizes=(64, 32), alpha=0.0)
    sklearn_reg.fit(X_train, y_train)
    sklearn_pred = sklearn_reg.predict(X_test)
    
    # CausalEngineç­‰ä»·å®ç°
    causal_reg = MLPCausalRegressor(mode='deterministic', 
                                   hidden_layer_sizes=(64, 32))
    causal_reg.fit(X_train, y_train)
    causal_pred = causal_reg.predict(X_test)
    
    # ç­‰ä»·æ€§éªŒè¯
    r2_diff = abs(r2_score(y_test, sklearn_pred) - r2_score(y_test, causal_pred))
    pred_mse = mean_squared_error(sklearn_pred, causal_pred)
    
    assert r2_diff < 0.001, "ç­‰ä»·æ€§éªŒè¯å¤±è´¥"
    assert pred_mse < 0.001, "é¢„æµ‹å·®å¼‚è¿‡å¤§"
```

### sklearnæ ‡å‡†æ¥å£å®ç°

```python
class MLPCausalRegressor(BaseEstimator, RegressorMixin):
    """å®Œæ•´çš„sklearnæ ‡å‡†æ¥å£"""
    
    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒæ¨¡å‹ - sklearnæ ‡å‡†ç­¾å"""
        # è‡ªåŠ¨æ•°æ®éªŒè¯å’Œé¢„å¤„ç†
        X, y = check_X_y(X, y)
        
        # è‡ªåŠ¨æ„å»ºç½‘ç»œæ¶æ„
        if self.hidden_layer_sizes == 'auto':
            self.hidden_layer_sizes_ = self._auto_hidden_layer_sizes(X.shape[1], X.shape[0])
        
        # è®­ç»ƒå¾ªç¯ (å«early stopping)
        return self._fit_with_mode(X, y)
    
    def predict(self, X):
        """é¢„æµ‹ - sklearnæ ‡å‡†ç­¾å"""
        check_is_fitted(self)
        X = check_array(X)
        return self._predict_with_mode(X, self.mode)
    
    def score(self, X, y, sample_weight=None):
        """è¯„åˆ† - sklearnæ ‡å‡†ç­¾å"""
        return r2_score(y, self.predict(X), sample_weight=sample_weight)
    
    # sklearnæ ‡å‡†å±æ€§
    @property
    def feature_importances_(self):
        """ç‰¹å¾é‡è¦æ€§"""
        return self._compute_feature_importance()
    
    @property 
    def loss_curve_(self):
        """è®­ç»ƒæŸå¤±æ›²çº¿"""
        return self.training_loss_history_
```

### æ¸è¿›å¼èƒ½åŠ›è®¿é—®

**åˆ†å±‚èƒ½åŠ›è®¾è®¡**ï¼š
```python
# ç¬¬1å±‚ï¼šsklearnå®Œå…¨å…¼å®¹
reg = MLPCausalRegressor()
predictions = reg.predict(X_test)  # è¿”å›æ•°å€¼ï¼Œå¦‚sklearn

# ç¬¬2å±‚ï¼šåˆ†å¸ƒä¿¡æ¯è®¿é—®
distributions = reg.predict(X_test, mode='standard')  # è¿”å›åˆ†å¸ƒå¯¹è±¡

# ç¬¬3å±‚ï¼šå› æœæ¨ç†æ¨¡å¼
causal_dists = reg.predict(X_test, mode='endogenous')    # çº¯å› æœ
sampling_dists = reg.predict(X_test, mode='sampling')   # æ¢ç´¢æ€§

# æ•°å­¦ä¸€è‡´æ€§ä¿è¯
assert np.allclose(predictions, distributions.mean(), atol=1e-6)
```

## å®è·µæŒ‡å—

### æ¨¡å¼é€‰æ‹©å†³ç­–æ ‘

**æŒ‰åº”ç”¨éœ€æ±‚é€‰æ‹©æ¨¡å¼**ï¼š

```mermaid
graph TD
    Start([å¼€å§‹é€‰æ‹©æ¨¡å¼]) --> Question1{éœ€è¦sklearnå…¼å®¹ï¼Ÿ}
    
    Question1 -->|æ˜¯| Deterministic[ğŸ¯ Deterministic Mode<br/>ç­‰ä»·sklearnåŸºçº¿éªŒè¯]
    Question1 -->|å¦| Question2{ä¸ªä½“è¡¨å¾ç¡®å®šï¼Ÿ}
    
    Question2 -->|å®Œå…¨ç¡®å®š| Question3{å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ}
    Question2 -->|æœ‰ä¸ç¡®å®šæ€§| Question4{å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ}
    
    Question3 -->|æ˜¯| Exogenous[ğŸŒ Exogenous Mode<br/>ç¡®å®šä¸ªä½“+å¤–ç”Ÿå™ªå£°]
    Question3 -->|å¦| Deterministic
    
    Question4 -->|å¦| Endogenous[ğŸ§  Endogenous Mode<br/>çº¯å†…ç”Ÿå› æœæ¨ç†]
    Question4 -->|æ˜¯| Question5{åº”ç”¨åœºæ™¯ï¼Ÿ}
    
    Question5 -->|ç”Ÿäº§ç¯å¢ƒ| Standard[âš¡ Standard Mode<br/>å™ªå£°å¢å¼ºä¸ç¡®å®šæ€§]
    Question5 -->|æ¢ç´¢ç ”ç©¶| Sampling[ğŸ² Sampling Mode<br/>å™ªå£°æ‰°åŠ¨èº«ä»½]
    
    classDef questionStyle fill:#f9f9f9,stroke:#666,stroke-width:2px
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:3px
    
    class Start,Question1,Question2,Question3,Question4,Question5 questionStyle
    class Deterministic deterministicStyle
    class Exogenous exogenousStyle
    class Endogenous endogenousStyle
    class Standard standardStyle
    class Sampling samplingStyle
```

### åº”ç”¨åœºæ™¯æŒ‡å¯¼

| æ•°æ®ç‰¹æ€§ | æ¨èæ¨¡å¼ | å…¸å‹åº”ç”¨ | æ•°å­¦åŸç† |
|----------|----------|----------|----------|
| **å®Œå…¨ç¡®å®šæ€§æ•°æ®** | Deterministic | åŸºçº¿éªŒè¯ã€è°ƒè¯•å¼€å‘ | $U' = \mu_U$ |
| **ä¼ æ„Ÿå™¨æ•°æ®** | Exogenous | IoTè®¾å¤‡ã€æµ‹é‡ç³»ç»Ÿ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ |
| **åŒ»ç–—è¯Šæ–­** | Endogenous | ä¸ªä½“å·®å¼‚å»ºæ¨¡ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ |
| **é‡‘èé£æ§** | Standard | ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ |
| **æ¨èç³»ç»Ÿ** | Sampling | å¤šæ ·æ€§ç”Ÿæˆ | $U' \sim \text{Cauchy}(\mu_U + b_{noise}e, \gamma_U)$ |

### æ¸è¿›å¼å¼€å‘æµç¨‹

**é˜¶æ®µåŒ–å®æ–½ç­–ç•¥**ï¼š

```python
# é˜¶æ®µ1: åŸºçº¿éªŒè¯ (Deterministic Mode)
reg = MLPCausalRegressor(mode='deterministic')
reg.fit(X_train, y_train)
baseline_score = reg.score(X_test, y_test)
print(f"åŸºçº¿RÂ²: {baseline_score:.4f}")

# é˜¶æ®µ2: å› æœå»ºæ¨¡ (Endogenous Mode) 
reg_causal = MLPCausalRegressor(mode='endogenous')
reg_causal.fit(X_train, y_train)
causal_score = reg_causal.score(X_test, y_test)
print(f"å› æœRÂ²: {causal_score:.4f}")

# é˜¶æ®µ3: ç”Ÿäº§ä¼˜åŒ– (Standard Mode)
reg_standard = MLPCausalRegressor(mode='standard')
reg_standard.fit(X_train, y_train)
standard_score = reg_standard.score(X_test, y_test)
distributions = reg_standard.predict(X_test, mode='standard')
uncertainty = distributions.scale.mean()
print(f"æ ‡å‡†RÂ²: {standard_score:.4f}, å¹³å‡ä¸ç¡®å®šæ€§: {uncertainty:.4f}")

# é˜¶æ®µ4: æ¢ç´¢åˆ†æ (Sampling Mode)
reg_sampling = MLPCausalRegressor(mode='sampling')
diverse_predictions = []
for _ in range(10):  # å¤šæ¬¡é‡‡æ ·è·å¾—é¢„æµ‹å¤šæ ·æ€§
    pred = reg_sampling.predict(X_test, mode='sampling')
    diverse_predictions.append(pred)

diversity = np.std(diverse_predictions, axis=0).mean()
print(f"é¢„æµ‹å¤šæ ·æ€§: {diversity:.4f}")
```

### å…³é”®å®è·µåŸåˆ™

1. **å§‹ç»ˆä»Deterministicå¼€å§‹**ï¼šç¡®ä¿ç®—æ³•æ­£ç¡®æ€§åå†æ·»åŠ å¤æ‚æ€§
2. **æ•°å­¦ç­‰ä»·æ€§éªŒè¯**ï¼šä¸sklearnåŸºçº¿å¯¹æ¯”éªŒè¯å®ç°æ­£ç¡®æ€§  
3. **æŸå¤±å‡½æ•°ç»Ÿä¸€**ï¼šæ¨¡å¼2-5å¿…é¡»ä½¿ç”¨ç›¸åŒçš„Cauchy NLLæŸå¤±
4. **æ¸è¿›å¼å¤æ‚åŒ–**ï¼šé€æ­¥å¼•å…¥ä¸ç¡®å®šæ€§å’Œå™ªå£°æœºåˆ¶
5. **å……åˆ†æµ‹è¯•éªŒè¯**ï¼šæ¯ä¸ªæ¨¡å¼éƒ½éœ€è¦ç‹¬ç«‹éªŒè¯æ•°å­¦æ­£ç¡®æ€§

### æ€§èƒ½è°ƒä¼˜æŒ‡å—

**æ¨¡å¼ç‰¹å®šçš„è¶…å‚æ•°å»ºè®®**ï¼š

```python
# Deterministic Mode: ç­‰ä»·sklearnï¼Œä½¿ç”¨ä¼ ç»Ÿè°ƒä¼˜
deterministic_params = {
    'hidden_layer_sizes': [(64, 32), (128, 64)],
    'learning_rate_init': [0.001, 0.01],
    'alpha': [0.0001, 0.001]  # L2æ­£åˆ™åŒ–
}

# Standard Mode: å¹³è¡¡æ€§èƒ½ä¸ç¨³å®šæ€§
standard_params = {
    'hidden_layer_sizes': [(64, 32), (128, 64)],
    'b_noise_init': [0.1, 0.2, 0.5],     # å™ªå£°å¼ºåº¦
    'gamma_init': [0.5, 1.0, 2.0]        # åˆå§‹å°ºåº¦
}

# Sampling Mode: æ¢ç´¢æ€§è¾ƒå¼ºï¼Œéœ€è¦æ›´å¤§ç½‘ç»œ
sampling_params = {
    'hidden_layer_sizes': [(128, 64), (256, 128)],
    'b_noise_init': [0.2, 0.5, 1.0],     # æ›´å¤§å™ªå£°
    'max_iter': [1500, 2000]             # æ›´å¤šè®­ç»ƒè½®æ•°
}
```

## æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ï¼šæ ‡ç­¾å™ªå£°é²æ£’æ€§

### ç†è®ºåŸºç¡€ï¼šä¸ºä»€ä¹ˆCausalEngineå¤©ç„¶æŠ—å™ªå£°

**æ•°å­¦åŸç†**ï¼šCausalEngineå­¦ä¹ ä¸ªä½“å†…åœ¨å› æœè¡¨å¾ï¼Œè€Œéè¡¨é¢ç»Ÿè®¡å…³è”

$$U \sim \text{Cauchy}(\mu_U, \gamma_U) \quad \text{(å­¦ä¹ ä¸ªä½“å› æœæœ¬è´¨)}$$
$$Y = f(U, \varepsilon) \quad \text{(åº”ç”¨æ™®é€‚å› æœæœºåˆ¶)}$$

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬å·®å¼‚**ï¼š
```python
# ä¼ ç»ŸMLPRegressor/MLPClassifierï¼šå­¦ä¹ è¡¨é¢å…³è”
# X â†’ h â†’ Å· = Wh + b  (å®¹æ˜“è¢«å™ªå£°æ ‡ç­¾è¯¯å¯¼)

# MLPCausalRegressor/MLPClassifierï¼šå­¦ä¹ å› æœæœ¬è´¨  
# X â†’ h â†’ U â†’ S â†’ Y  (å­¦ä¹ æ·±å±‚å› æœç»“æ„ï¼ŒæŠ—å™ªå£°)
```

### åˆ†ç±»ä»»åŠ¡çš„OvRç­–ç•¥ä¼˜åŠ¿

**CausalEngine OvRçš„ç‹¬ç«‹æ€§**ï¼š
$$P_k = \frac{1}{2} + \frac{1}{\pi} \arctan\left(\frac{\text{loc}_{S_k}}{\text{scale}_{S_k}}\right) \quad \text{(æ¯ä¸ªç±»åˆ«ç‹¬ç«‹åˆ¤æ–­)}$$

**ä¼ ç»ŸSoftmaxçš„ç«äº‰æ€§**ï¼š
$$P_k^{\text{softmax}} = \frac{\exp(z_k)}{\sum_{j=1}^K \exp(z_j)} \quad \text{(å¼ºåˆ¶å½’ä¸€åŒ–çº¦æŸ)}$$

**å…³é”®å·®å¼‚çš„å®ç”¨å½±å“**ï¼š
```python
# å™ªå£°åœºæ™¯ç¤ºä¾‹ï¼šçœŸå®æ ‡ç­¾[Cat]è¢«é”™è¯¯æ ‡è®°ä¸º[Dog]

# âŒ ä¼ ç»ŸSoftmaxï¼šå™ªå£°ä¼ æ’­åˆ°æ‰€æœ‰ç±»åˆ«
# é”™è¯¯è®­ç»ƒæ ·æœ¬å½±å“æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å½’ä¸€åŒ–
softmax_probs = [0.1, 0.7, 0.2]  # [Cat, Dog, Bird] - Dogæ¦‚ç‡è¢«é”™è¯¯æå‡

# âœ… CausalEngine OvRï¼šå™ªå£°å±€é™åœ¨å•ä¸ªç±»åˆ«  
# é”™è¯¯æ ‡ç­¾åªå½±å“å¯¹åº”ç±»åˆ«ï¼Œå…¶ä»–ç±»åˆ«ä¿æŒç‹¬ç«‹
ovr_probs = [0.8, 0.3, 0.2]  # [Cat, Dog, Bird] - Catæ¦‚ç‡ä¿æŒå‡†ç¡®
```

### å¼€ç®±å³ç”¨çš„å™ªå£°å¤„ç†

**å·¥ä½œæµç®€åŒ–å¯¹æ¯”**ï¼š

```python
# âŒ ä¼ ç»Ÿæ–¹æ³•ï¼šéœ€è¦å¤æ‚çš„æ•°æ®æ¸…æ´—æµç¨‹
from sklearn.neural_network import MLPClassifier

# ç¬¬1æ­¥ï¼šäººå·¥è¯†åˆ«å’Œå¤„ç†å™ªå£°ï¼ˆè€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼‰
X_clean, y_clean = manual_outlier_detection(X_raw, y_raw)
y_scaled = RobustScaler().fit_transform(y_clean.reshape(-1, 1))

# ç¬¬2æ­¥ï¼šè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
traditional_clf = MLPClassifier().fit(X_clean, y_scaled.ravel())

# âœ… CausalEngineï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
causal_clf = MLPCausalClassifier()
causal_clf.fit(X_raw, y_raw)  # æ— éœ€é¢„å¤„ç†ï¼

# æ€§èƒ½å¯¹æ¯”ï¼šåœ¨å¹²å‡€æµ‹è¯•é›†ä¸Šè¯„ä¼°
print(f"ä¼ ç»Ÿæ–¹æ³•ç²¾åº¦: {accuracy_score(y_test_clean, traditional_clf.predict(X_test)):.3f}")
print(f"CausalEngineç²¾åº¦: {accuracy_score(y_test_clean, causal_clf.predict(X_test)):.3f}")
```

### ç°å®å™ªå£°åœºæ™¯çš„ä¼˜åŠ¿

**é«˜ä»·å€¼åº”ç”¨åœºæ™¯**ï¼š
1. **åŒ»ç–—æ•°æ®**ï¼šè¯Šæ–­æ ‡ç­¾å­˜åœ¨ä¸»è§‚æ€§å’Œé”™è¯¯
2. **é‡‘èæ•°æ®**ï¼šæ•°æ®æºä¸ä¸€è‡´ï¼Œæ ‡ç­¾è´¨é‡å‚å·®ä¸é½  
3. **ä¼—åŒ…æ ‡æ³¨**ï¼šäººå·¥æ ‡æ³¨å­˜åœ¨ä¸»è§‚å·®å¼‚å’Œé”™è¯¯
4. **ä¼ æ„Ÿå™¨æ•°æ®**ï¼šç¯å¢ƒå¹²æ‰°å¯¼è‡´çš„æµ‹é‡è¯¯å·®

**é¢„æœŸæ€§èƒ½ä¼˜åŠ¿**ï¼š
- **æ•°é‡çº§é”™è¯¯**ï¼šCausalEngineåœ¨10x/100xé”™è¯¯ä¸‹ä»ä¿æŒ80%+æ€§èƒ½
- **æ ‡ç­¾ç¿»è½¬**ï¼š50%æ ‡ç­¾å™ªå£°ä¸‹ä»ä¿æŒ80%+åŸå§‹æ€§èƒ½
- **ç³»ç»Ÿåå·®**ï¼šé€šè¿‡å› æœè¡¨å¾å­¦ä¹ å¯ä»¥éƒ¨åˆ†æŠµæ¶ˆåå·®  
- **å¼‚å¸¸å€¼**ï¼šCauchyåˆ†å¸ƒçš„é‡å°¾ç‰¹æ€§å¤©ç„¶é€‚åˆå¤„ç†å¼‚å¸¸å€¼

### ç«äº‰ä¼˜åŠ¿æ€»ç»“

**æŠ€æœ¯å·®å¼‚åŒ–**ï¼š
- **æ•°å­¦åˆ›æ–°**ï¼šç¬¬ä¸€ä¸ªåŸºäºCauchyåˆ†å¸ƒçš„ç”Ÿäº§çº§åˆ†ç±»å™¨/å›å½’å™¨
- **è§£æä¼˜åŠ¿**ï¼šæ— é‡‡æ ·çš„åˆ†å¸ƒè®¡ç®—ï¼Œæé«˜çš„è®¡ç®—æ•ˆç‡
- **ç‹¬ç‰¹æ¶æ„**ï¼šOvRç­–ç•¥å¸¦æ¥çš„çµæ´»æ€§å’Œè¡¨è¾¾èƒ½åŠ›

**ç”¨æˆ·ä½“éªŒä¼˜åŠ¿**ï¼š
- **é›¶å­¦ä¹ æˆæœ¬**ï¼šå®Œç¾çš„sklearnå…¼å®¹æ€§
- **æ¸è¿›å¼èƒ½åŠ›**ï¼šä»ç®€å•é¢„æµ‹åˆ°å¤æ‚åˆ†å¸ƒåˆ†æ
- **å·¥ä½œæµç®€åŒ–**ï¼šä»20+è¡Œé¢„å¤„ç†ä»£ç ç®€åŒ–ä¸º1è¡Œè®­ç»ƒä»£ç 
- **ä¸°å¯Œä¿¡æ¯**ï¼šä¸ä»…æœ‰é¢„æµ‹å€¼ï¼Œè¿˜æœ‰å®Œæ•´çš„ä¸ç¡®å®šæ€§ä¿¡æ¯
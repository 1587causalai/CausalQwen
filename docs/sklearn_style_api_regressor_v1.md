# MLPCausalRegressor Sklearn-Style API è®¾è®¡æ–¹æ¡ˆ V1

> **ç›®æ ‡**: ä¸“æ³¨å›å½’ä»»åŠ¡ï¼Œå°†CausalEngineæ‰“åŒ…æˆç±»ä¼¼sklearn MLPRegressoré‚£æ ·æ˜“ç”¨çš„å›å½’å™¨ï¼Œæä¾›ç»Ÿä¸€çš„APIæ¥å£å’Œæ™ºèƒ½é»˜è®¤é…ç½®ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¿›è¡Œå›å½’é¢„æµ‹ã€‚

## 1. çµæ„Ÿæ¥æºï¼šsklearnç¥ç»ç½‘ç»œæ¨¡å—åˆ†æ

### 1.1 sklearn MLPRegressor/MLPClassifier çš„æˆåŠŸä¹‹å¤„

```python
# sklearnç¥ç»ç½‘ç»œçš„ç»å…¸ç”¨æ³•
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# åˆ†ç±»ä»»åŠ¡
clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
probabilities = clf.predict_proba(X_test)

# å›å½’ä»»åŠ¡  
reg = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
```

### 1.2 sklearnè®¾è®¡çš„æ ¸å¿ƒä¼˜åŠ¿

- âœ… **ç»Ÿä¸€API**: fit/predict/score ä¸‰æ­¥èµ°
- âœ… **æ™ºèƒ½é»˜è®¤**: å¤§å¤šæ•°å‚æ•°æœ‰åˆç†é»˜è®¤å€¼
- âœ… **ä»»åŠ¡åˆ†ç¦»**: Regressor vs Classifier æ¸…æ™°åˆ†å·¥
- âœ… **æ ‡å‡†åŒ–é›†æˆ**: ä¸preprocessing, model_selectionæ— ç¼é…åˆ
- âœ… **ä¸°å¯Œå±æ€§**: è®­ç»ƒåå¯æŸ¥çœ‹æƒé‡ã€æŸå¤±å†å²ç­‰
- âœ… **é”™è¯¯å¤„ç†**: å‹å¥½çš„é”™è¯¯ä¿¡æ¯å’Œè­¦å‘Š

## 2. CausalEngine å½“å‰æ¶æ„åˆ†æ

### 2.1 ç°æœ‰ç»„ä»¶ç»“æ„
```python
# å½“å‰çš„ä½¿ç”¨æ–¹å¼ï¼ˆç›¸å¯¹å¤æ‚ï¼‰
from causal_engine import CausalEngine, AbductionNetwork, ActionNetwork

# éœ€è¦ç”¨æˆ·æ‰‹åŠ¨é…ç½®å¾ˆå¤šå‚æ•°
abduction_net = AbductionNetwork(
    input_size=X.shape[1], 
    causal_size=64
)
action_net = ActionNetwork(
    causal_size=64,
    output_size=1
)

engine = CausalEngine(
    hidden_size=X.shape[1],
    vocab_size=1,
    causal_size=64
)

# è®­ç»ƒè¿‡ç¨‹éœ€è¦æ‰‹åŠ¨ç®¡ç†
engine.train()
for epoch in range(num_epochs):
    # æ‰‹åŠ¨è®­ç»ƒå¾ªç¯...
```

### 2.2 ç”¨æˆ·ç—›ç‚¹
- ğŸš« éœ€è¦æ‰‹åŠ¨æ„å»ºç½‘ç»œç»“æ„
- ğŸš« éœ€è¦äº†è§£å†…éƒ¨æ¶æ„ç»†èŠ‚ 
- ğŸš« ç¼ºä¹ç»Ÿä¸€çš„è®­ç»ƒæ¥å£
- ğŸš« å‚æ•°é…ç½®å¤æ‚
- ğŸš« æ²¡æœ‰æ ‡å‡†çš„é¢„æµ‹æ¥å£

## 3. CausalEngineå›å½’å·¥ä½œæµç¨‹å›¾

### 3.1 æ ¸å¿ƒç®—æ³•æµç¨‹

```mermaid
graph LR
    %% è¾“å…¥æ•°æ®
    Input["ğŸ“Š è¾“å…¥æ•°æ®<br/>X, y"]
    
    %% è®­ç»ƒé˜¶æ®µ - æ°´å¹³å¸ƒå±€
    subgraph Training ["ğŸ”§ è®­ç»ƒé˜¶æ®µ (.fitæ–¹æ³•)"]
        direction LR
        Stage1["ğŸ” å½’å› æ¨æ–­<br/>Abduction<br/>E â†’ U"]
        Stage2["âš¡ è¡ŒåŠ¨å†³ç­–<br/>Action<br/>U â†’ S"]
        Stage3["ğŸ¯ æ¿€æ´»è¾“å‡º<br/>Activation<br/>S â†’ Y"]
        Loss["ğŸ“‰ æŸå¤±è®¡ç®—<br/>Cauchy NLL"]
        
        Stage1 --> Stage2 --> Stage3 --> Loss
    end
    
    %% é¢„æµ‹é˜¶æ®µ - ç´§å‡‘å¸ƒå±€
    subgraph Prediction ["ğŸ”® é¢„æµ‹é˜¶æ®µ (.predictæ–¹æ³•)"]
        direction TB
        ModeSelect["ğŸ›ï¸ æ¨ç†æ¨¡å¼"]
        
        subgraph Modes ["å››ç§é¢„æµ‹æ¨¡å¼"]
            direction LR
            Compatible["ğŸ”„ Compatible<br/>Î¼áµ§"]
            Standard["ğŸ“Š Standard<br/>Cauchy(Î¼áµ§,Î³áµ§)"]  
            Causal["âš–ï¸ Causal<br/>çº¯å› æœ"]
            Sampling["ğŸ² Sampling<br/>æ¢ç´¢æ€§"]
        end
        
        ModeSelect --> Modes
    end
    
    %% æ•°æ®æµ
    Input --> Training
    Training --> Prediction
    
    %% æ ·å¼ä¼˜åŒ–
    style Input fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    style Training fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style Prediction fill:#e1f5fe,stroke:#0277bd,stroke-width:3px
    style Modes fill:#f5f5f5,stroke:#757575,stroke-width:2px
    
    style Stage1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    style Stage2 fill:#fff8e1,stroke:#ffa000,stroke-width:2px  
    style Stage3 fill:#e0f2f1,stroke:#00796b,stroke-width:2px
    style Loss fill:#ffebee,stroke:#d32f2f,stroke-width:2px
    
    style Compatible fill:#e3f2fd,stroke:#1976d2
    style Standard fill:#e8f5e9,stroke:#388e3c
    style Causal fill:#fce4ec,stroke:#c2185b
    style Sampling fill:#f3e5f5,stroke:#7b1fa2
```

### 3.2 æ•°å­¦å…¬å¼æ€»è§ˆ

**é˜¶æ®µ1 - å½’å› æ¨æ–­**ï¼š
$$\mu_U = \text{loc\_net}(E), \quad \gamma_U = \text{softplus}(\text{scale\_net}(E))$$
$$U \sim \text{Cauchy}(\mu_U, \gamma_U)$$

**é˜¶æ®µ2 - è¡ŒåŠ¨å†³ç­–**ï¼š
$$S = W \cdot U + b, \quad S \sim \text{Cauchy}(\mu_S, \gamma_S)$$

**é˜¶æ®µ3 - æ¿€æ´»è¾“å‡º**ï¼š
$$Y = w_k \cdot S + b_k, \quad Y \sim \text{Cauchy}(\mu_Y, \gamma_Y)$$

**æŸå¤±å‡½æ•°**ï¼š
$$\mathcal{L} = \log(\pi \cdot \gamma_Y) + \log\left(1 + \left(\frac{y_{\text{true}} - \mu_Y}{\gamma_Y}\right)^2\right)$$

## 4. è®¾è®¡ç›®æ ‡ï¼šç†æƒ³çš„CausalEngine API

### 4.1 ç›®æ ‡ä½¿ç”¨ä½“éªŒ

```python
# ç†æƒ³çš„ä½¿ç”¨æ–¹å¼ - ç®€å•å¦‚sklearn
from causal_engine.sklearn import MLPCausalRegressor

# å›å½’ä»»åŠ¡ - 3è¡Œä»£ç æå®š
reg = MLPCausalRegressor()  # æ™ºèƒ½é»˜è®¤é…ç½®
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)

# é«˜çº§ç”¨æ³• - ä»ç„¶ç®€æ´
reg = MLPCausalRegressor(
    hidden_layer_sizes=(64, 32),  # ç½‘ç»œç»“æ„ï¼ˆä¸sklearnå…¼å®¹ï¼‰
    max_iter=1000,          # è®­ç»ƒè½®æ•°
    default_mode='compatible', # é»˜è®¤é¢„æµ‹æ¨¡å¼
    random_state=42         # éšæœºç§å­
)
```

### 3.2 ä¸sklearnå®Œå…¨å…¼å®¹

```python
# ä¸sklearnç”Ÿæ€æ— ç¼é›†æˆ
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# äº¤å‰éªŒè¯
scores = cross_val_score(MLPCausalRegressor(), X, y, cv=5)

# ç½‘æ ¼æœç´¢
param_grid = {
    'hidden_layer_sizes': [(32,), (64,), (64, 32)],
    'default_mode': ['compatible', 'standard']
}
grid_search = GridSearchCV(MLPCausalRegressor(), param_grid, cv=3)

# ç®¡é“é›†æˆ
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('causal', MLPCausalRegressor())
])
```

## 4. æ ¸å¿ƒè®¾è®¡å·®å¼‚ï¼šMLPCausalRegressor vs MLPRegressor ğŸ§®

### 4.1 è®¾è®¡å“²å­¦ï¼šä»…æ›¿æ¢è¾“å‡ºå±‚ âœ¨

**æ ¸å¿ƒæ´å¯Ÿ**: MLPCausalRegressor å’Œ MLPRegressor çš„**å”¯ä¸€åŒºåˆ«**å°±æ˜¯æœ€åä¸€ä¸ªè¾“å‡ºå±‚ï¼
- **MLPRegressor**: çº¿æ€§è¾“å‡ºå±‚ `y = WÂ·h + b`
- **MLPCausalRegressor**: CausalEngineè¾“å‡ºå±‚ï¼ˆå½’å› â†’è¡ŒåŠ¨â†’æ¿€æ´»ï¼‰

è¿™ç§è®¾è®¡çš„ä¼˜é›…ä¹‹å¤„ï¼š
- âœ… **æœ€å°åŒ–æ”¹åŠ¨**: ä¿æŒsklearnçš„æ‰€æœ‰ä¼˜ç§€ç‰¹æ€§
- âœ… **æœ€å¤§åŒ–æ”¶ç›Š**: è·å¾—å®Œæ•´çš„å› æœæ¨ç†èƒ½åŠ›
- âœ… **æ— ç¼æ›¿æ¢**: å¯ä»¥ç›´æ¥æ›¿ä»£MLPRegressorä½¿ç”¨

### 4.2 ç½‘ç»œç»“æ„å¯¹æ¯”

```python
# ä¼ ç»ŸMLPRegressoræ¶æ„
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ çº¿æ€§è¾“å‡ºå±‚ â†’ ç¡®å®šæ€§é¢„æµ‹å€¼
  X    â†’   MLPs   â†’  y = WÂ·h + b  â†’    Å·

# MLPCausalRegressoræ¶æ„ï¼ˆä»…æœ€åä¸€å±‚ä¸åŒï¼ï¼‰  
è¾“å…¥å±‚ â†’ éšè—å±‚ä»¬ â†’ CausalEngine â†’ åˆ†å¸ƒè¾“å‡º â†’ æ¦‚ç‡é¢„æµ‹
  X    â†’   MLPs   â†’ (å½’å› +è¡ŒåŠ¨+æ¿€æ´») â†’ S~Cauchy â†’ P(Y)
```

**å…³é”®ä¼˜åŠ¿**: 
- ğŸš€ **è®­ç»ƒæ•ˆç‡**: å¤§éƒ¨åˆ†ç½‘ç»œç»“æ„å®Œå…¨ç›¸åŒï¼Œè®­ç»ƒå¤æ‚åº¦ç›¸å½“
- ğŸš€ **å‚æ•°è§„æ¨¡**: ä»…CausalEngineéƒ¨åˆ†å¢åŠ å°‘é‡å‚æ•°
- ğŸš€ **æ”¶ç›Šå·¨å¤§**: ä»ç¡®å®šæ€§é¢„æµ‹å‡çº§åˆ°åˆ†å¸ƒå»ºæ¨¡å’Œå› æœæ¨ç†

### 4.3 å®ç°è¦ç‚¹ï¼šCausalEngineé›†æˆ

**æ ¸å¿ƒåŸç†**ï¼šå°†CausalEngineä½œä¸ºMLPRegressorçš„æœ€ç»ˆè¾“å‡ºå±‚ï¼Œå®ç°ä»ç¡®å®šæ€§é¢„æµ‹åˆ°æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡çš„å‡çº§

```python
# æ ‡å‡†sklearnæ¶æ„ + CausalEngineè¾“å‡ºå±‚
class MLPCausalRegressor:
    def __init__(self):
        # 1. æ ‡å‡†MLPéšè—å±‚ï¼ˆä¸sklearnç›¸åŒï¼‰
        self.hidden_layers = MLPLayers(hidden_layer_sizes)
        
        # 2. CausalEngineè¾“å‡ºå±‚ï¼ˆæ›¿ä»£çº¿æ€§è¾“å‡ºå±‚ï¼‰
        self.causal_engine = CausalEngine(
            abduction_net=AbductionNetwork(),    # è¯æ®â†’ä¸ªä½“å› æœè¡¨å¾
            action_net=ActionNetwork(),          # ä¸ªä½“â†’å†³ç­–å¾—åˆ†
            activation_head=RegressionHead()     # å¾—åˆ†â†’é¢„æµ‹åˆ†å¸ƒ
        )
    
    def forward(self, X):
        # å‰å‘ä¼ æ’­ï¼šéšè—å±‚ç‰¹å¾ â†’ CausalEngineä¸‰é˜¶æ®µ
        hidden_features = self.hidden_layers(X)
        predictions = self.causal_engine(hidden_features)
        return predictions
```

**æ¶æ„å¯¹æ¯”ä¸æ•°å­¦å·®å¼‚**ï¼š

| ç»„ä»¶ | MLPRegressor | MLPCausalRegressor |
|------|-------------|-------------------|
| **è¾“å…¥å±‚** | X âˆˆ â„â¿Ë£áµˆ | X âˆˆ â„â¿Ë£áµˆ (ç›¸åŒ) |
| **éšè—å±‚** | h = Ïƒ(Wx + b) | h = Ïƒ(Wx + b) (ç›¸åŒ) |
| **è¾“å‡ºå±‚** | Å· = Wh + b | Y ~ Cauchy(Î¼,Î³) (æ ¸å¿ƒå·®å¼‚) |
| **æŸå¤±å‡½æ•°** | MSE: Â½(y-Å·)Â² | Cauchy NLL (æ ¸å¿ƒå·®å¼‚) |
| **é¢„æµ‹ç»“æœ** | ç¡®å®šå€¼Å· | åˆ†å¸ƒY + å…¼å®¹æ¨¡å¼Î¼ |

**æ•°å­¦åˆ›æ–°çš„å®ç”¨ä»·å€¼**ï¼š
$$\text{ä¼ ç»Ÿ}: \quad \hat{y} = \mathbf{W}\mathbf{h} + b$$
$$\text{CausalEngine}: \quad Y \sim \text{Cauchy}(\mu_Y, \gamma_Y) \text{ with } \hat{y} = \mu_Y$$

è¿™ç§è®¾è®¡è®©ç”¨æˆ·æ—¢èƒ½äº«å—sklearnçš„ç®€å•æ€§ï¼ˆé€šè¿‡compatibleæ¨¡å¼ï¼‰ï¼Œåˆèƒ½è·å¾—å®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒä¿¡æ¯ï¼ˆé€šè¿‡standardæ¨¡å¼ï¼‰ã€‚

## 5. APIæ¥å£è®¾è®¡ - V1.0 ä¸“æ³¨å›å½’

### 5.1 MLPCausalRegressor æ ¸å¿ƒæ¥å£

```python
class MLPCausalRegressor(BaseEstimator, RegressorMixin):
    """MLPå› æœå›å½’å™¨ - sklearné£æ ¼æ¥å£"""
    
    def __init__(self, 
                 hidden_layer_sizes=(64, 32),    # ç½‘ç»œç»“æ„ï¼ˆä¸sklearnå…¼å®¹ï¼‰
                 max_iter=1000,                  # æœ€å¤§è¿­ä»£æ¬¡æ•°
                 learning_rate=0.001,            # å­¦ä¹ ç‡
                 default_mode='compatible',      # é»˜è®¤é¢„æµ‹æ¨¡å¼
                 early_stopping=True,            # æ—©åœ
                 validation_fraction=0.1,        # éªŒè¯é›†æ¯”ä¾‹
                 random_state=None,              # éšæœºç§å­
                 verbose=False):                 # è®­ç»ƒæ—¥å¿—
        pass
    
    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒæ¨¡å‹"""
        # 1. è‡ªåŠ¨æ•°æ®é¢„å¤„ç†å’ŒéªŒè¯
        # 2. è‡ªåŠ¨æ„å»ºMLPç‰¹å¾æå–å±‚
        # 3. è‡ªåŠ¨æ„å»ºCausalEngineè¾“å‡ºå±‚
        # 4. è‡ªåŠ¨è®­ç»ƒå¾ªç¯ï¼ˆå«early stoppingï¼‰
        return self
    
    def predict(self, X, mode='compatible'):
        """ç»Ÿä¸€é¢„æµ‹æ¥å£
        
        Parameters:
        -----------
        X : array-like
            è¾“å…¥ç‰¹å¾
        mode : str, default='compatible'
            é¢„æµ‹æ¨¡å¼:
            - 'compatible': æå–åˆ†å¸ƒä¸­å¿ƒå€¼ - sklearnå…¼å®¹çš„æ•°å€¼è¾“å‡º
            - 'standard': å®Œæ•´Cauchyåˆ†å¸ƒ - æ ‡å‡†CausalEngineæ¨ç†  
            - 'causal': çº¯å› æœCauchyåˆ†å¸ƒ - æ— å¤–ç”Ÿå™ªå£°
            - 'sampling': æ¢ç´¢æ€§Cauchyåˆ†å¸ƒ - ä¸ªä½“å¤šæ ·æ€§è¾¹ç•Œ
            
        Returns:
        --------
        predictions : array-like or list of distributions
            å†…éƒ¨ç»Ÿä¸€è¿”å› Cauchy(Î¼, Î³) åˆ†å¸ƒå‚æ•°:
            - mode='compatible': è¿”å›Î¼å€¼ï¼ˆæ•°å€¼æ•°ç»„ï¼‰ï¼Œéšå¼è®¾ç½®Î³=0
            - å…¶ä»–mode: è¿”å›(Î¼, Î³)å®Œæ•´åˆ†å¸ƒå¯¹è±¡åˆ—è¡¨
        """
        return predictions
    
    def score(self, X, y, sample_weight=None):
        """è¯„åˆ† (RÂ²)"""
        return r2_score(y, self.predict(X))
    
    # sklearnæ ‡å‡†å±æ€§
    @property
    def feature_importances_(self):
        """ç‰¹å¾é‡è¦æ€§"""
        pass
    
    @property
    def loss_curve_(self):
        """è®­ç»ƒæŸå¤±æ›²çº¿"""
        pass
```

### 5.2 æ™ºèƒ½é»˜è®¤é…ç½®ç­–ç•¥

```python
# æ ¹æ®æ•°æ®è§„æ¨¡è‡ªåŠ¨è°ƒæ•´ç½‘ç»œç»“æ„
def _auto_hidden_layer_sizes(n_features, n_samples):
    """æ ¹æ®ç‰¹å¾æ•°å’Œæ ·æœ¬æ•°æ™ºèƒ½æ¨èç½‘ç»œç»“æ„"""
    if n_features <= 10:
        return (32,)
    elif n_features <= 50:
        return (64, 32)
    elif n_features <= 100:
        return (128, 64)
    else:
        return (256, 128, 64)

# è‡ªåŠ¨æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´
AUTO_CONFIG = {
    'early_stopping': True,
    'patience': 20,
    'min_delta': 1e-4,
    'learning_rate_schedule': 'adaptive'
}
```

### 5.3 MLPCausalRegressorçš„æ ¸å¿ƒç«äº‰ä¼˜åŠ¿ï¼šæ ‡ç­¾å™ªå£°é²æ£’æ€§ ğŸ›¡ï¸

**ä¸ºä»€ä¹ˆè¿™å¯¹sklearné£æ ¼æ¨¡å—åŒ–å¾ˆé‡è¦ï¼Ÿ**ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®è´¨é‡å¾€å¾€æ˜¯æœºå™¨å­¦ä¹ é¡¹ç›®æˆè´¥çš„å…³é”®ã€‚ä¼ ç»ŸMLPRegressoråœ¨é¢å¯¹å™ªå£°æ ‡ç­¾æ—¶æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œéœ€è¦å¤æ‚çš„æ•°æ®æ¸…æ´—æµç¨‹ã€‚MLPCausalRegressoræä¾›äº†å¼€ç®±å³ç”¨çš„å™ªå£°é²æ£’æ€§ï¼Œè®©ç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒï¼Œå¤§å¤§ç®€åŒ–äº†å·¥ä½œæµç¨‹ã€‚

#### 5.3.1 ç†è®ºåŸºç¡€ï¼šä¸ºä»€ä¹ˆCausalEngineå¤©ç„¶æŠ—å™ªå£°

**æ•°å­¦åŸç†**: CausalEngineå­¦ä¹ ä¸ªä½“å†…åœ¨å› æœè¡¨å¾ï¼Œè€Œéè¡¨é¢ç»Ÿè®¡å…³è”

$$U \sim \text{Cauchy}(\mu_U, \gamma_U) \quad \text{(å­¦ä¹ ä¸ªä½“å› æœæœ¬è´¨)}$$
$$Y = f(U, \varepsilon) \quad \text{(åº”ç”¨æ™®é€‚å› æœæœºåˆ¶)}$$

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„æ ¹æœ¬å·®å¼‚**:
```python
# ä¼ ç»ŸMLPRegressorï¼šå­¦ä¹ è¡¨é¢å…³è”
# X â†’ h â†’ Å· = Wh + b  (å®¹æ˜“è¢«å™ªå£°æ ‡ç­¾è¯¯å¯¼)

# MLPCausalRegressorï¼šå­¦ä¹ å› æœæœ¬è´¨  
# X â†’ h â†’ U â†’ S â†’ Y  (å­¦ä¹ æ·±å±‚å› æœç»“æ„ï¼ŒæŠ—å™ªå£°)
```

**æ•°å­¦ä¿è¯çš„é²æ£’æ€§**:
- **å™ªå£°æ ‡ç­¾**: $\tilde{y} = y + \eta$ (å¤–éƒ¨å™ªå£°)
- **å› æœä¸å˜æ€§**: å†…åœ¨å› æœæœºåˆ¶ $f(U, \varepsilon)$ ä¸å—è¡¨å±‚å™ªå£°å½±å“
- **å­¦ä¹ ç¨³å®šæ€§**: å½’å› æ¨æ–­ä¸“æ³¨äºå­¦ä¹ æ·±å±‚ $U$ åˆ†å¸ƒ

#### 5.3.2 å¼€ç®±å³ç”¨çš„å™ªå£°å¤„ç†ï¼šæ— éœ€æ•°æ®æ¸…æ´—çš„sklearnå·¥ä½œæµ

**ä¼ ç»Ÿsklearnå·¥ä½œæµ vs CausalEngineå·¥ä½œæµå¯¹æ¯”**:

```python
# âŒ ä¼ ç»ŸMLPRegressorï¼šéœ€è¦å¤æ‚çš„æ•°æ®æ¸…æ´—æµç¨‹
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import cross_val_score

# ç¬¬1æ­¥ï¼šäººå·¥è¯†åˆ«å’Œå¤„ç†å™ªå£°ï¼ˆè€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼‰
X_clean, y_clean = manual_outlier_detection(X_raw, y_raw)
y_scaled = RobustScaler().fit_transform(y_clean.reshape(-1, 1))

# ç¬¬2æ­¥ï¼šè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
traditional_reg = MLPRegressor().fit(X_clean, y_scaled.ravel())

# âœ… MLPCausalRegressorï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
causal_reg = MLPCausalRegressor()
causal_reg.fit(X_raw, y_raw)  # æ— éœ€é¢„å¤„ç†ï¼

# æ€§èƒ½å¯¹æ¯”ï¼šåœ¨å¹²å‡€æµ‹è¯•é›†ä¸Šè¯„ä¼°
print(f"ä¼ ç»Ÿæ–¹æ³• RÂ²: {r2_score(y_test_clean, traditional_reg.predict(X_test)):.3f}")
print(f"CausalEngine RÂ²: {r2_score(y_test_clean, causal_reg.predict(X_test)):.3f}")
```

**ç°å®å™ªå£°åœºæ™¯çš„æ ‡å‡†æµ‹è¯•åè®®**:
```python
def sklearn_noise_robustness_benchmark(X, y_clean):
    """sklearné£æ ¼çš„å™ªå£°é²æ£’æ€§åŸºå‡†æµ‹è¯•"""
    noise_scenarios = {
        'magnitude_errors': [10, 100, 0.1, 0.01],      # æ•°é‡çº§é”™è¯¯
        'unit_conversion': [1000, 0.001, 3.28, 0.305], # å•ä½è½¬æ¢é”™è¯¯  
        'systematic_bias': [0.5, 1.0, 2.0, 5.0],       # ç³»ç»Ÿæ€§åå·®
        'outlier_contamination': [0.05, 0.1, 0.2, 0.3] # å¼‚å¸¸å€¼æ±¡æŸ“
    }
    
    results = {}
    for scenario, levels in noise_scenarios.items():
        results[scenario] = []
        for level in levels:
            # åº”ç”¨ç°å®å™ªå£°
            y_noisy = apply_realistic_noise(y_clean, scenario, level)
            
            # ç›´æ¥å¯¹æ¯”ï¼šæ— é¢„å¤„ç† vs å¤æ‚é¢„å¤„ç†
            causal_reg = MLPCausalRegressor().fit(X, y_noisy)
            traditional_reg = get_best_preprocessed_mlp(X, y_noisy)  # æœ€ä½³é¢„å¤„ç†åçš„ä¼ ç»Ÿæ–¹æ³•
            
            results[scenario].append({
                'noise_level': level,
                'causal_r2': causal_reg.score(X_test, y_clean),
                'traditional_r2': traditional_reg.score(X_test, y_clean),
                'workflow_simplicity': 'CausalEngine: 1è¡Œä»£ç  vs ä¼ ç»Ÿ: 10+è¡Œé¢„å¤„ç†'
            })
    
    return results
```

def apply_magnitude_noise(y, magnitude_factor):
    """æ•°é‡çº§é”™è¯¯ï¼šéƒ¨åˆ†æ•°æ®è¢«ä¹˜ä»¥é”™è¯¯çš„æ•°é‡çº§"""
    y_noisy = y.copy()
    n_errors = int(0.1 * len(y))  # 10%çš„æ•°æ®æœ‰é”™è¯¯
    error_indices = np.random.choice(len(y), n_errors, replace=False)
    y_noisy[error_indices] *= magnitude_factor  # ä¹˜ä»¥10ã€100æˆ–é™¤ä»¥10ã€100
    return y_noisy

def apply_unit_conversion_noise(y, conversion_factor):
    """å•ä½é”™è¯¯ï¼šå•ä½è½¬æ¢é”™è¯¯"""
    y_noisy = y.copy()
    n_errors = int(0.2 * len(y))  # 20%çš„æ•°æ®æœ‰å•ä½é”™è¯¯
    error_indices = np.random.choice(len(y), n_errors, replace=False)
    y_noisy[error_indices] *= conversion_factor  # 1000ï¼ˆç±³â†’æ¯«ç±³ï¼‰æˆ– 0.001ï¼ˆæ¯«ç±³â†’ç±³ï¼‰
    return y_noisy

def apply_systematic_bias(y, bias_std_ratio):
    """ç³»ç»Ÿæ€§åå·®ï¼šæ‰€æœ‰æ•°æ®éƒ½æœ‰åŒä¸€æ–¹å‘çš„åç§»"""
    bias = bias_std_ratio * np.std(y)  # åå·®ä¸ºæ ‡å‡†å·®çš„å€æ•°
    return y + bias

def apply_outlier_contamination(y, contamination_ratio):
    """å¼‚å¸¸å€¼æ±¡æŸ“ï¼šéšæœºæ›¿æ¢ä¸ºæç«¯å€¼"""
    y_noisy = y.copy()
    n_outliers = int(contamination_ratio * len(y))
    outlier_indices = np.random.choice(len(y), n_outliers, replace=False)
    
    # ç”Ÿæˆæç«¯å¼‚å¸¸å€¼ï¼šè·ç¦»ä¸­ä½æ•° 5-10 å€æ ‡å‡†å·®
    outlier_values = np.median(y) + np.random.choice([-1, 1], n_outliers) * \
                    np.random.uniform(5, 10, n_outliers) * np.std(y)
    y_noisy[outlier_indices] = outlier_values
    return y_noisy
```

**é¢„æœŸç»“æœ**:
- **æ•°é‡çº§é”™è¯¯**: CausalEngineåœ¨ 10x/100x é”™è¯¯ä¸‹ä»ä¿æŒ 80%+ æ€§èƒ½
- **å•ä½é”™è¯¯**: ä¼ ç»Ÿæ–¹æ³•åœ¨å•ä½è½¬æ¢é”™è¯¯ä¸‹å´©æºƒï¼ŒCausalEngineç›¸å¯¹ç¨³å®š
- **ç³»ç»Ÿåå·®**: CausalEngineé€šè¿‡å› æœè¡¨å¾å­¦ä¹ å¯ä»¥éƒ¨åˆ†æŠµæ¶ˆåå·®
- **å¼‚å¸¸å€¼**: Cauchyåˆ†å¸ƒçš„é‡å°¾ç‰¹æ€§å¤©ç„¶é€‚åˆå¤„ç†å¼‚å¸¸å€¼

#### 5.3.3 åº”ç”¨ä»·å€¼ä¸åœºæ™¯

**é«˜ä»·å€¼åœºæ™¯**:
1. **åŒ»ç–—æ•°æ®**: è¯Šæ–­æ ‡ç­¾å­˜åœ¨ä¸»è§‚æ€§å’Œé”™è¯¯
2. **é‡‘èæ•°æ®**: æ•°æ®æºä¸ä¸€è‡´ï¼Œæ ‡ç­¾è´¨é‡å‚å·®ä¸é½
3. **ä¼—åŒ…æ ‡æ³¨**: äººå·¥æ ‡æ³¨å­˜åœ¨ä¸»è§‚å·®å¼‚å’Œé”™è¯¯
4. **æ—¶é—´åºåˆ—**: æ•°æ®é‡‡é›†å»¶è¿Ÿå¯¼è‡´æ ‡ç­¾åç§»

**ç«äº‰ä¼˜åŠ¿**:
```python
# ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤æ‚çš„æ•°æ®æ¸…æ´—
from sklearn.preprocessing import robust_scale
from sklearn.model_selection import cross_val_score

# CausalEngine: ç›´æ¥ä½¿ç”¨å™ªå£°æ•°æ®
reg_causal = MLPCausalRegressor()
reg_causal.fit(X_train, y_noisy)  # ç›´æ¥ä½¿ç”¨å™ªå£°æ ‡ç­¾
performance = reg_causal.score(X_test, y_clean)  # åœ¨å¹²å‡€æ•°æ®ä¸Šä»ç„¶è¡¨ç°ä¼˜å¼‚
```

### 5.4 CausalEngineçš„ç‹¬ç‰¹ä»·å€¼ï¼šç»Ÿä¸€é¢„æµ‹æ¥å£ ğŸš€

**æ ¸å¿ƒåˆ›æ–°**: å•ä¸€ `predict()` æ–¹æ³•ï¼ŒåŸºäºç»Ÿä¸€çš„åˆ†å¸ƒè¾“å‡ºï¼Œé€šè¿‡ `mode` å‚æ•°æ§åˆ¶è¡¨ç°å½¢å¼

**æ•°å­¦ç»Ÿä¸€æ€§**: æ‰€æœ‰æ¨¡å¼éƒ½åŸºäºç»Ÿä¸€çš„Cauchyåˆ†å¸ƒï¼š
$$Y \sim \text{Cauchy}(\mu_Y, \gamma_Y)$$

#### 5.4.1 æ•°å­¦ç»Ÿä¸€æ€§ - ä¸€ä¸ªåˆ†å¸ƒï¼Œå¤šç§è¡¨ç°
```python
# å†…éƒ¨å§‹ç»ˆä¿æŒç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶
reg = MLPCausalRegressor()
reg.fit(X_train, y_train)  # å­¦ä¹ å®Œæ•´åˆ†å¸ƒ Y ~ Cauchy(Î¼, Î³)

# ä¸åŒæ¨¡å¼åªæ˜¯åŒä¸€åˆ†å¸ƒçš„ä¸åŒä½¿ç”¨æ–¹å¼
predictions = reg.predict(X_test)  # æå–ä¸­å¿ƒå€¼ Î¼
distributions = reg.predict(X_test, mode='standard')  # å®Œæ•´åˆ†å¸ƒ Cauchy(Î¼, Î³)

# sklearnç”Ÿæ€è‡ªç„¶å…¼å®¹
scores = cross_val_score(reg, X, y, cv=5)  # è‡ªåŠ¨ä½¿ç”¨compatibleæ¨¡å¼
```

#### 5.4.2 åˆ†å±‚è®¿é—® - ä»ç®€å•åˆ°å¼ºå¤§
```python
# åŒä¸€ä¸ªæ¨¡å‹ï¼Œä¸åŒå±‚æ¬¡çš„ä¿¡æ¯è®¿é—®
reg = MLPCausalRegressor()
reg.fit(X_train, y_train)  # ä¸€æ¬¡è®­ç»ƒï¼Œå­¦ä¹ å®Œæ•´åˆ†å¸ƒ

# ç¬¬1å±‚ï¼šæ•°å€¼æ¨¡å¼ - Cauchy(Î¼, Î³=0) çš„æå–
predictions = reg.predict(X_test)  # å†…éƒ¨: Cauchy(Î¼, 0), è¿”å›: Î¼

# ç¬¬2å±‚ï¼šåˆ†å¸ƒæ¨¡å¼ - Cauchy(Î¼, Î³) çš„å®Œæ•´ä¿¡æ¯
distributions = reg.predict(X_test, mode='standard')
print(f"ç›¸åŒçš„ä½ç½®å‚æ•°: {predictions[0]:.3f} vs {distributions[0].loc:.3f}")
print(f"å­¦ä¹ åˆ°çš„å°ºåº¦å‚æ•°: {distributions[0].scale:.3f}")  # compatibleæ¨¡å¼ä¸‹éšè—

# ç¬¬3å±‚ï¼šå› æœåˆ†æ - ä¸åŒçš„åˆ†å¸ƒå½¢æ€
causal_dists = reg.predict(X_test, mode='causal')     # çº¯å› æœ
sampling_dists = reg.predict(X_test, mode='sampling') # æ¢ç´¢æ€§
```

#### 5.4.3 è®¾è®¡å“²å­¦ - ç»Ÿä¸€æ•°å­¦ï¼Œåˆ†å±‚ä½“éªŒ
**æ ¸å¿ƒç†å¿µ**: å†…éƒ¨æ•°å­¦æ¡†æ¶å§‹ç»ˆç»Ÿä¸€ï¼Œç”¨æˆ·ä½“éªŒåˆ†å±‚é€’è¿›

```python
# æ•°å­¦ç»Ÿä¸€æ€§ï¼šæ‰€æœ‰æ¨¡å¼éƒ½è®¡ç®— Cauchy(Î¼, Î³) å‚æ•°
reg = MLPCausalRegressor()
reg.fit(X_train, y_train)  # å­¦ä¹ å®Œæ•´ Y ~ Cauchy(Î¼_Y, Î³_Y) åˆ†å¸ƒ

# åˆ†å±‚è®¿é—®ï¼šä¸åŒæ¨¡å¼å¯¹åº”ä¸åŒçš„(Î¼, Î³)ç»„åˆ
compatible = reg.predict(X)                 # è¿”å› Î¼_Yï¼Œéšå¼è®¾ç½® Î³_Y=0
standard = reg.predict(X, mode='standard')  # è¿”å› Cauchy(Î¼_Y, Î³_Y)
causal = reg.predict(X, mode='causal')      # è¿”å› Cauchy(Î¼_Y', Î³_Y')
sampling = reg.predict(X, mode='sampling')  # è¿”å› Cauchy(Î¼_Y'', Î³_Y'')

# æ•°å­¦ä¸€è‡´æ€§ï¼šä½ç½®å‚æ•°å§‹ç»ˆä¸€è‡´
assert abs(compatible[0] - standard[0].loc) < 1e-10  # Î¼_Y å€¼ç›¸åŒ

# compatible æ¨¡å¼çš„æ•°å­¦æœ¬è´¨ï¼š
lim_{Î³ â†’ 0} Cauchy(Î¼, Î³) = Î´(Î¼)  # ç‚¹è´¨é‡åˆ†å¸ƒ
```

**æ¨¡å¼å®šä½** - åŸºäºç»Ÿä¸€åˆ†å¸ƒ $Y \sim \text{Cauchy}(\mu_Y, \gamma_Y)$ çš„ä¸åŒå‚æ•°è®¿é—®:

- **`compatible`**: $\hat{y} = \mu_Y$ï¼Œéšå¼ $\gamma_Y = 0$ï¼Œsklearnç”Ÿæ€å…¼å®¹
- **`standard`**: $Y \sim \text{Cauchy}(\mu_Y, \gamma_Y)$ï¼Œå®Œæ•´åˆ†å¸ƒä¿¡æ¯
- **`causal`**: $Y \sim \text{Cauchy}(\mu_Y', \gamma_Y')$ where $T=0$ï¼Œçº¯å› æœå‚æ•°
- **`sampling`**: $Y \sim \text{Cauchy}(\mu_Y'', \gamma_Y'')$ where $T>0, \text{do\_sample}=\text{True}$ï¼Œæ¢ç´¢æ€§å‚æ•°

## 6. å®ç°è·¯çº¿å›¾

### 6.1 V1.0 å½“å‰ç‰ˆæœ¬ï¼šMLPCausalRegressor æ ¸å¿ƒå®ç°
**é‡ç‚¹**ï¼šä¸“æ³¨å›å½’ä»»åŠ¡ï¼Œæ‰“é€ å®Œæ•´å¯ç”¨çš„sklearné£æ ¼æ¥å£

- [ ] åˆ›å»º `causal_engine.sklearn` å­æ¨¡å—
- [ ] å®ç° `MLPCausalRegressor` åŸºç¡€ç±»
- [ ] é›†æˆç°æœ‰CausalEngineæ ¸å¿ƒåŠŸèƒ½ï¼ˆAbductionNetwork + ActionNetworkï¼‰
- [ ] **ç®€åŒ–è®¾è®¡**: ä½¿ç”¨æ’ç­‰æ¿€æ´»ï¼Œç›´æ¥è¾“å‡ºå†³ç­–å¾—åˆ†åˆ†å¸ƒ
- [ ] å®ç°è‡ªåŠ¨è®­ç»ƒå¾ªç¯å’Œæ ‡å‡†sklearnæ¥å£
- [ ] åŸºç¡€å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†
- [ ] ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹å’Œæ–‡æ¡£

### 6.2 V1.1 ä¼˜åŒ–å¢å¼º
- [ ] å®ç°è‡ªåŠ¨ç½‘ç»œç»“æ„æ¨è
- [ ] æ·»åŠ early stoppingå’Œvalidation
- [ ] æä¾›causalæ¨ç†æ¨¡å¼ä½œä¸ºstandardæ¨¡å¼çš„å¯¹æ¯”é€‰é¡¹
- [ ] å®Œå–„é”™è¯¯å¤„ç†å’Œè­¦å‘Š
- [ ] sklearnå…¼å®¹æ€§æµ‹è¯•

### 6.3 V1.1 å›å½’å™¨å¢å¼ºåŠŸèƒ½
- [ ] **å¯å­¦ä¹ ä»»åŠ¡æ¿€æ´»å¤´**: å®ç° `w_kÂ·S_k + b_k` çº¿æ€§å˜æ¢
- [ ] æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ
- [ ] è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
- [ ] **å®éªŒæ€§**: æ¢ç´¢è®­ç»ƒé˜¶æ®µä¸åŒæ¨ç†æ¨¡å¼çš„æ•ˆæœå·®å¼‚

### 6.4 V1.2 ç”Ÿæ€é›†æˆ
- [ ] ä¸pandas DataFrameæ·±åº¦é›†æˆ
- [ ] é›†æˆæ¨¡å‹è§£é‡Šå·¥å…·
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œå¤§è§„æ¨¡æ•°æ®æ”¯æŒ

---

**ğŸ“ æ³¨æ„**: MLPCausalClassifieråˆ†ç±»å™¨å°†åœ¨ç‹¬ç«‹æ–‡æ¡£ `sklearn_style_api_classifier_v1.md` ä¸­è¯¦ç»†è®¾è®¡

## 7. åç»­å¼€å‘è®¡åˆ’ä¸è€ƒè™‘

### 7.1 V1.1 å¢å¼ºåŠŸèƒ½å¼€å‘åŠ¨æœº

#### 7.1.1 å¯å­¦ä¹ ä»»åŠ¡æ¿€æ´»å¤´
**å¼€å‘åŠ¨æœº**: å½“å‰V1.0ä½¿ç”¨æ’ç­‰æ¿€æ´» $Y = S$ï¼Œè™½ç„¶ç®€æ´ä½†å¯èƒ½é™åˆ¶è¡¨è¾¾èƒ½åŠ›
**æŠ€æœ¯æ–¹æ¡ˆ**: å®ç°çº¿æ€§å˜æ¢ $Y_k = w_k S_k + b_k$ï¼Œå…¶ä¸­ $w_k, b_k$ æ˜¯å¯å­¦ä¹ å‚æ•°
**æ•°å­¦æ¡†æ¶**: 
$$Y_k \sim \text{Cauchy}(w_k \cdot \text{loc}_{S_k} + b_k, |w_k| \cdot \text{scale}_{S_k})$$
**é¢„æœŸæ”¶ç›Š**: 
- å¢å¼ºå›å½’ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒä¸Š
- æä¾›æ›´çµæ´»çš„è¾“å‡ºèŒƒå›´æ§åˆ¶
- ä¿æŒCauchyåˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ä¼˜åŠ¿

#### 7.1.2 ç‰¹å¾é‡è¦æ€§åˆ†æ
**å¼€å‘åŠ¨æœº**: ç”¨æˆ·éœ€è¦ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å“ªäº›ç‰¹å¾æœ€å½±å“é¢„æµ‹
**æŠ€æœ¯æŒ‘æˆ˜**: ä¼ ç»Ÿæ¢¯åº¦æ–¹æ³•åœ¨Cauchyåˆ†å¸ƒä¸‹å¯èƒ½ä¸å¤Ÿç¨³å®š
**åˆ›æ–°æ–¹æ¡ˆ**: 
- åŸºäºå†³ç­–å¾—åˆ†åˆ†å¸ƒæ–¹å·®çš„é‡è¦æ€§åˆ†æ
- åˆ©ç”¨å½’å› ç½‘ç»œçš„ä½ç½®å’Œå°ºåº¦å‚æ•°å˜åŒ–
- å¼€å‘CausalEngineç‰¹æœ‰çš„è§£é‡Šæ€§æ–¹æ³•

#### 7.1.3 æ ‡ç­¾å™ªå£°é²æ£’æ€§éªŒè¯å’Œå±•ç¤º
**å¼€å‘åŠ¨æœº**: éªŒè¯å’Œå±•ç¤ºCausalEngineçš„æ ¸å¿ƒç†è®ºä¼˜åŠ¿ï¼Œå»ºç«‹ç«äº‰å£å’
**æŠ€æœ¯å®ç°**: 
- å»ºç«‹æ ‡å‡†åŒ–çš„å™ªå£°é²æ£’æ€§æµ‹è¯•å¥—ä»¶
- å¼€å‘å¤šç§å™ªå£°ç±»å‹çš„æ¨¡æ‹Ÿå·¥å…·
- å®ç°ä¸ä¼ ç»Ÿæ–¹æ³•çš„å…¨é¢å¯¹æ¯”åŸºå‡†
**é¢„æœŸæ”¶ç›Š**: 
- æä¾›å¼ºæœ‰åŠ›çš„å®éªŒè¯æ®æ”¯æŒå¸‚åœºæ¨å¹¿
- å¸å¼•é¢ä¸´å™ªå£°æ•°æ®æŒ‘æˆ˜çš„ä¼ä¸šç”¨æˆ·
- å»ºç«‹åœ¨é²æ£’æ€§æ–¹é¢çš„æŠ€æœ¯é¢†å…ˆåœ°ä½

#### 7.1.4 è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
**å¼€å‘åŠ¨æœº**: å¸®åŠ©ç”¨æˆ·ç†è§£CausalEngineçš„ç‹¬ç‰¹è®­ç»ƒåŠ¨æ€
**å…³é”®æŒ‡æ ‡**: 
- Cauchyåˆ†å¸ƒå‚æ•° $\mu_Y, \gamma_Y$ çš„æ¼”åŒ–
- å½’å› ç½‘ç»œçš„æ”¶æ•›è¿‡ç¨‹
- ä¸åŒæ¨ç†æ¨¡å¼çš„æ•ˆæœå¯¹æ¯”
- å¤–ç”Ÿå™ªå£°å‚æ•° $\mathbf{b}_{\text{noise}}$ çš„å­¦ä¹ è½¨è¿¹

### 7.2 V1.2 ç”Ÿæ€é›†æˆçš„æˆ˜ç•¥æ„ä¹‰

#### 7.2.1 Pandas DataFrameæ·±åº¦é›†æˆ
**æˆ˜ç•¥ä»·å€¼**: é™ä½æ•°æ®ç§‘å­¦å®¶çš„ä½¿ç”¨é—¨æ§›ï¼Œæä¾›åŸç”Ÿçš„è¡¨æ ¼æ•°æ®æ”¯æŒ
**æŠ€æœ¯å®ç°**: 
- è‡ªåŠ¨ç‰¹å¾ç±»å‹æ¨æ–­å’Œé¢„å¤„ç†
- åˆ—åä¿æŒå’Œç´¢å¼•å¯¹é½
- ç¼ºå¤±å€¼çš„Cauchyåˆ†å¸ƒå»ºæ¨¡
**ç‹¬ç‰¹ä¼˜åŠ¿**: åˆ©ç”¨CausalEngineå¯¹ä¸ç¡®å®šæ€§çš„åŸç”Ÿæ”¯æŒï¼Œæ›´å¥½åœ°å¤„ç†ç¼ºå¤±æ•°æ®

#### 7.2.2 æ¨¡å‹è§£é‡Šå·¥å…·é›†æˆ
**ç›®æ ‡å·¥å…·**: SHAP, LIME, Captum
**æŠ€æœ¯æŒ‘æˆ˜**: ç°æœ‰è§£é‡Šå·¥å…·ä¸»è¦é’ˆå¯¹ç¡®å®šæ€§é¢„æµ‹ï¼Œéœ€è¦é€‚é…åˆ†å¸ƒè¾“å‡º
**åˆ›æ–°æœºä¼š**: 
- å¼€å‘åŸºäºCauchyåˆ†å¸ƒçš„SHAPå€¼è®¡ç®—
- å®ç°åˆ†å¸ƒçº§åˆ«çš„ç‰¹å¾é‡è¦æ€§è§£é‡Š
- æä¾›å› æœæ¨ç†æ¨¡å¼çš„ä¸“é—¨è§£é‡Šæ–¹æ³•

#### 7.2.3 æ€§èƒ½ä¼˜åŒ–å’Œå¤§è§„æ¨¡æ•°æ®æ”¯æŒ
**ä¼˜åŒ–ç›®æ ‡**: 
- GPUåŠ é€Ÿçš„Cauchyåˆ†å¸ƒè®¡ç®—
- æ‰¹é‡åŒ–çš„ $\arctan$ å‡½æ•°è®¡ç®—
- å†…å­˜é«˜æ•ˆçš„åˆ†å¸ƒå‚æ•°å­˜å‚¨
**æ‰©å±•æ€§è€ƒè™‘**: 
- åˆ†å¸ƒå¼è®­ç»ƒçš„æ•°å­¦ä¸€è‡´æ€§ä¿è¯
- å¤§è§„æ¨¡æ•°æ®ä¸‹çš„é‡‡æ ·ç­–ç•¥ä¼˜åŒ–
- æµå¼æ•°æ®çš„åœ¨çº¿å­¦ä¹ é€‚é…

### 7.3 å®éªŒæ€§åŠŸèƒ½æ¢ç´¢

#### 7.3.1 è®­ç»ƒé˜¶æ®µæ¨ç†æ¨¡å¼ç ”ç©¶
**ç ”ç©¶é—®é¢˜**: è®­ç»ƒæ—¶ä½¿ç”¨ä¸åŒæ¨ç†æ¨¡å¼æ˜¯å¦èƒ½æå‡æœ€ç»ˆæ€§èƒ½ï¼Ÿ
**å®éªŒè®¾è®¡**: 
- å¯¹æ¯”æ ‡å‡†è®­ç»ƒ vs å› æœæ¨¡å¼è®­ç»ƒ vs é‡‡æ ·æ¨¡å¼è®­ç»ƒ
- ç ”ç©¶ä¸åŒ temperature è°ƒåº¦ç­–ç•¥
- åˆ†ææ¨ç†æ¨¡å¼å¯¹æ³›åŒ–èƒ½åŠ›çš„å½±å“
**ç†è®ºæ„ä¹‰**: æ¢ç´¢å› æœæ¨ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½œç”¨æœºåˆ¶

#### 7.3.2 å¤šä»»åŠ¡å›å½’æ¶æ„
**æŠ€æœ¯æ„¿æ™¯**: å•ä¸ªCausalEngineåŒæ—¶å¤„ç†å¤šä¸ªå›å½’ä»»åŠ¡
**æ•°å­¦æ¡†æ¶**: 
$$\mathbf{Y} = [Y_1, Y_2, \ldots, Y_T], \quad Y_t \sim \text{Cauchy}(\mu_{Y_t}, \gamma_{Y_t})$$
**åº”ç”¨åœºæ™¯**: 
- æ—¶é—´åºåˆ—çš„å¤šæ­¥é¢„æµ‹
- å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜
- ç›¸å…³æ€§å›å½’ä»»åŠ¡çš„è”åˆå»ºæ¨¡

#### 7.3.3 è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
**åŠ¨æœº**: CausalEngineçš„è¶…å‚æ•°ç©ºé—´å¤æ‚ï¼Œéœ€è¦æ™ºèƒ½è°ƒä¼˜
**æŠ€æœ¯è·¯çº¿**: 
- åˆ©ç”¨åˆ†å¸ƒè¾“å‡ºçš„ä¸ç¡®å®šæ€§æŒ‡å¯¼è¶…å‚æ•°æœç´¢
- å¼€å‘CausalEngineä¸“ç”¨çš„è·å–å‡½æ•°
- å®ç°å¤šä¿çœŸåº¦ä¼˜åŒ–ç­–ç•¥

### 7.4 é•¿æœŸæŠ€æœ¯è·¯çº¿å›¾

#### 7.4.1 ç†è®ºç ”ç©¶æ–¹å‘
1. **æ•°å­¦åŸºç¡€æ‹“å±•**: æ¢ç´¢å…¶ä»–é‡å°¾åˆ†å¸ƒçš„å¯èƒ½æ€§
2. **å› æœæ¨ç†æ·±åŒ–**: ä¸å› æœå‘ç°ç®—æ³•çš„ç»“åˆ
3. **ä¸ç¡®å®šæ€§é‡åŒ–**: ä¸æ¦‚ç‡æœºå™¨å­¦ä¹ çš„æ·±åº¦èåˆ

#### 7.4.2 å·¥ç¨‹å®ç°ä¼˜åŒ–
1. **ç¼–è¯‘ä¼˜åŒ–**: å¼€å‘CausalEngineä¸“ç”¨çš„ç®—å­
2. **ç¡¬ä»¶é€‚é…**: é’ˆå¯¹ä¸åŒç¡¬ä»¶å¹³å°çš„ä¼˜åŒ–
3. **éƒ¨ç½²å·¥å…·**: ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹æœåŠ¡å·¥å…·

#### 7.4.3 ç”Ÿæ€ç³»ç»Ÿå»ºè®¾
1. **ç¤¾åŒºå»ºè®¾**: å¼€æºç¤¾åŒºå’Œè´¡çŒ®è€…ç”Ÿæ€
2. **æ•™è‚²èµ„æº**: æ•™ç¨‹ã€è¯¾ç¨‹å’Œæœ€ä½³å®è·µ
3. **è¡Œä¸šåº”ç”¨**: å‚ç›´é¢†åŸŸçš„ä¸“é—¨è§£å†³æ–¹æ¡ˆ

## 8. ä½¿ç”¨åœºæ™¯å¯¹æ¯”

### 8.1 ç°åœ¨ vs æœªæ¥

| åœºæ™¯ | ç°åœ¨çš„æ–¹å¼ | ç†æƒ³çš„æ–¹å¼ |
|------|-----------|-----------|
| **å¿«é€ŸåŸå‹** | 20+è¡Œä»£ç ï¼Œéœ€äº†è§£æ¶æ„ | 3è¡Œä»£ç ï¼Œé›¶é…ç½® |
| **å‚æ•°è°ƒä¼˜** | æ‰‹åŠ¨è¯•éªŒå„ç§ç»„åˆ | GridSearchCVè‡ªåŠ¨æœç´¢ |
| **æ¨¡å‹è¯„ä¼°** | æ‰‹å†™è¯„ä¼°ä»£ç  | cross_val_scoreä¸€è¡Œæå®š |
| **ç”Ÿäº§éƒ¨ç½²** | éœ€è¦è‡ªå·±å¤„ç†åºåˆ—åŒ– | joblibç›´æ¥ä¿å­˜åŠ è½½ |
| **ç‰¹å¾åˆ†æ** | éœ€è¦è‡ªå·±å®ç° | feature_importances_å±æ€§ |

### 8.2 ç«äº‰å¯¹æ¯”

```python
# XGBoosté£æ ¼
import xgboost as xgb
reg = xgb.XGBRegressor()
reg.fit(X_train, y_train)

# LightGBMé£æ ¼  
import lightgbm as lgb
reg = lgb.LGBMRegressor()
reg.fit(X_train, y_train)

# CausalEngineé£æ ¼ (ç›®æ ‡)
from causal_engine.sklearn import MLPCausalRegressor
reg = MLPCausalRegressor()  # åŒæ ·ç®€æ´ï¼Œä½†æ˜¯å› æœæ¨ç†ï¼
reg.fit(X_train, y_train)

# åŸºç¡€é¢„æµ‹ï¼ˆsklearnå…¼å®¹ï¼‰
predictions = reg.predict(X_test)  # æ•°å€¼è¾“å‡º

# é«˜çº§é¢„æµ‹ï¼ˆç‹¬ç‰¹çš„åˆ†å¸ƒè¾“å‡ºï¼‰
distributions = reg.predict(X_test, mode='standard')  # åˆ†å¸ƒè¾“å‡º
```

## 9. æŠ€æœ¯å®ç°è¦ç‚¹

### 9.1 å…³é”®æŒ‘æˆ˜
1. **åˆ†å¸ƒæŸå¤±è®¡ç®—**: å®ç°æŸ¯è¥¿åˆ†å¸ƒçš„é«˜æ•ˆä¼¼ç„¶è®¡ç®—
2. **å‚æ•°æ˜ å°„**: sklearnå‚æ•° â†’ CausalEngineå†…éƒ¨å‚æ•°
3. **è®­ç»ƒå¾ªç¯**: å°è£…å¤æ‚çš„å› æœæ¨ç†è®­ç»ƒé€»è¾‘
4. **çŠ¶æ€ç®¡ç†**: æ¨¡å‹è®­ç»ƒçŠ¶æ€çš„ä¿å­˜å’Œæ¢å¤
5. **é”™è¯¯å¤„ç†**: å‹å¥½çš„é”™è¯¯ä¿¡æ¯
6. **æ€§èƒ½ä¼˜åŒ–**: ä¿æŒè§£æè®¡ç®—çš„æ€§èƒ½ä¼˜åŠ¿

### 9.2 æ¶æ„è®¾è®¡ - V1.0 ç®€åŒ–ç‰ˆæœ¬

```python
# V1.0 å†…éƒ¨æ¶æ„ï¼ˆä¸“æ³¨å›å½’ï¼‰
causal_engine/sklearn/
â”œâ”€â”€ __init__.py       # å¯¼å‡ºMLPCausalRegressor
â”œâ”€â”€ regressor.py      # MLPCausalRegressoræ ¸å¿ƒå®ç°
â”œâ”€â”€ _base.py          # åŸºç¡€å·¥å…·å‡½æ•°å’ŒéªŒè¯
â””â”€â”€ _config.py        # é»˜è®¤é…ç½®å’Œè‡ªåŠ¨æ¨è

# V2.0+ æ‰©å±•æ¶æ„
causal_engine/sklearn/
â”œâ”€â”€ __init__.py       # å¯¼å‡ºæ‰€æœ‰æ¥å£
â”œâ”€â”€ regressor.py      # MLPCausalRegressorå®ç°
â”œâ”€â”€ classifier.py     # MLPCausalClassifierå®ç°ï¼ˆV2.0+ï¼‰
â”œâ”€â”€ _base.py          # åŸºç¡€ç±»å’Œæ¥å£
â”œâ”€â”€ _utils.py         # å·¥å…·å‡½æ•°
â”œâ”€â”€ _validation.py    # å‚æ•°éªŒè¯
â””â”€â”€ _config.py        # é»˜è®¤é…ç½®
```

## 10. V1.0 å¼€å‘é‡ç‚¹ç¡®è®¤ âœ…

### 10.1 å½“å‰ç‰ˆæœ¬æ˜ç¡®ç›®æ ‡
- **ä¸“æ³¨ä»»åŠ¡**: å›å½’ä»»åŠ¡ (`MLPCausalRegressor`)
- **æ ¸å¿ƒåŠŸèƒ½**: å®ç°å®Œæ•´çš„sklearné£æ ¼æ¥å£
- **è®¾è®¡åŸåˆ™**: ç®€å•ã€å®ç”¨ã€å¯æ‰©å±•

### 10.2 V1.0 æœ€å°å¯è¡Œäº§å“ï¼ˆMVPï¼‰
```python
# V1.0 ç›®æ ‡ä½“éªŒ - å®Œç¾çš„sklearnå…¼å®¹æ€§
from causal_engine.sklearn import MLPCausalRegressor

# é›¶é…ç½®ä½¿ç”¨ - å°±åƒMLPRegressorä¸€æ ·ç®€å•
reg = MLPCausalRegressor()
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)  # mode='compatible'ï¼Œè¿”å›æ•°å€¼

# sklearnç”Ÿæ€å®Œç¾é›†æˆ
from sklearn.model_selection import cross_val_score
scores = cross_val_score(MLPCausalRegressor(), X, y, cv=5)
print(f"CV Score: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")

# æŒ‰éœ€è§£é”CausalEngineç‹¬ç‰¹èƒ½åŠ›
distributions = reg.predict(X_test, mode='standard')  # è·å–åˆ†å¸ƒä¿¡æ¯
print(f"ç¬¬ä¸€ä¸ªé¢„æµ‹: å‡å€¼={distributions[0].mean():.2f}, æ ‡å‡†å·®={distributions[0].scale:.3f}")
```

### 10.3 åç»­ç‰ˆæœ¬è§„åˆ’
- **V1.1**: ä¼˜åŒ–å’Œå¢å¼ºåŠŸèƒ½
- **V2.0**: æ·»åŠ  `MLPCausalClassifier` åˆ†ç±»æ”¯æŒ
- **V3.0**: ç”Ÿæ€é›†æˆå’Œé«˜çº§åŠŸèƒ½

### 10.4 å¼€å‘ç­–ç•¥
- ğŸ¯ **æ¸è¿›å¼å¼€å‘**: å…ˆåšå¥½å›å½’ï¼Œå†æ‰©å±•åˆ†ç±»
- ğŸ¯ **ç”¨æˆ·é©±åŠ¨**: åŸºäºå®é™…ä½¿ç”¨åé¦ˆè¿›è¡Œè¿­ä»£
- ğŸ¯ **è´¨é‡ä¼˜å…ˆ**: ç¡®ä¿æ¯ä¸ªç‰ˆæœ¬éƒ½æ˜¯å®Œæ•´å¯ç”¨çš„

---

**ğŸ’¡ è¿™ä¸ªæ–¹æ¡ˆçš„ä»·å€¼**:
- å¤§å¹…é™ä½ä½¿ç”¨é—¨æ§›ï¼Œè®©æ›´å¤šäººèƒ½ç”¨ä¸Šå› æœæ¨ç†
- ä¸ç°æœ‰MLå·¥ä½œæµæ— ç¼é›†æˆ
- ä¿æŒCausalEngineçš„æŠ€æœ¯ä¼˜åŠ¿ï¼ŒåŒ…è£…æˆç”¨æˆ·å‹å¥½çš„æ¥å£
- ä¸ºCausalEngineçš„å¹¿æ³›é‡‡ç”¨å¥ å®šåŸºç¡€

**ğŸ¯ æœŸå¾…åé¦ˆ**:
è¯·æä¾›æ‚¨å¯¹è¿™ä¸ªè®¾è®¡æ–¹æ¡ˆçš„å…·ä½“æƒ³æ³•ã€éœ€æ±‚å’Œå»ºè®®ï¼
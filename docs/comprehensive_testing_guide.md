# CausalQwen æ•°å­¦å®ç°å…¨é¢æµ‹è¯•æŒ‡å—

æœ¬æ–‡æ¡£æä¾›ä¸€ä¸ªå…¨æ–°çš„ã€æ›´åŠ ä¸¥æ ¼çš„æµ‹è¯•æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºéªŒè¯ CausalQwen ä¸­æ¯ä¸ªæ•°å­¦ç»„ä»¶çš„æ­£ç¡®å®ç°ã€‚

## ğŸ¯ æ ¸å¿ƒæµ‹è¯•åŸåˆ™

1. **æ•°å­¦ç²¾ç¡®æ€§ä¼˜å…ˆ**ï¼šæ¯ä¸ªæµ‹è¯•å¿…é¡»ç›´æ¥å¯¹åº”æ•°å­¦å…¬å¼
2. **éš”ç¦»æµ‹è¯•**ï¼šæ¯ä¸ªç»„ä»¶ç‹¬ç«‹æµ‹è¯•ï¼Œé¿å…çº§è”é”™è¯¯
3. **è¾¹ç•Œæ¡ä»¶å…¨è¦†ç›–**ï¼šç‰¹åˆ«å…³æ³¨é›¶å€¼ã€æå€¼ã€ç‰¹æ®Šæƒ…å†µ
4. **æ•°å€¼ç¨³å®šæ€§éªŒè¯**ï¼šæ£€æŸ¥æµ®ç‚¹è¿ç®—çš„ç²¾åº¦å’Œç¨³å®šæ€§
5. **ç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥**ï¼šç¡®ä¿å¼ é‡å½¢çŠ¶åœ¨æ•´ä¸ªæµç¨‹ä¸­ä¿æŒæ­£ç¡®

## ğŸ“‹ æµ‹è¯•æ¸…å•æ¦‚è§ˆ

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç»„ä»¶æµ‹è¯•
- [ ] æ•°å€¼ç¼–ç å‡½æ•° Ï†(v) çš„å®Œæ•´éªŒè¯
- [ ] å¢å¼ºåµŒå…¥è®¡ç®—çš„ç²¾ç¡®æ€§æµ‹è¯•
- [ ] è¯æ±‡è¡¨æ‰©å±•çš„æ­£ç¡®æ€§éªŒè¯

### ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†å¸ƒè®¡ç®—æµ‹è¯•
- [ ] æŸ¯è¥¿åˆ†å¸ƒå‚æ•°çš„æœ‰æ•ˆæ€§éªŒè¯
- [ ] çº¿æ€§å˜æ¢çš„æ•°å­¦ç²¾ç¡®æ€§æµ‹è¯•
- [ ] CDF è®¡ç®—çš„æ•°å€¼å‡†ç¡®æ€§

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šç½‘ç»œç»„ä»¶æµ‹è¯•
- [ ] å½’å› ç½‘ç»œçš„åˆå§‹åŒ–éªŒè¯
- [ ] è¡ŒåŠ¨ç½‘ç»œçš„æƒé‡è¿ç§»æµ‹è¯•
- [ ] OvR é˜ˆå€¼çš„å½±å“åˆ†æ

### ç¬¬å››éƒ¨åˆ†ï¼šæŸå¤±å‡½æ•°æµ‹è¯•
- [ ] æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶çš„ç²¾ç¡®è®¡ç®—
- [ ] é—¨æ§æœºåˆ¶çš„è¾¹ç•Œæƒ…å†µ
- [ ] æ¢¯åº¦æµçš„æ­£ç¡®æ€§éªŒè¯

### ç¬¬äº”éƒ¨åˆ†ï¼šç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
- [ ] å®Œæ•´å‰å‘ä¼ æ’­çš„æ•°å€¼éªŒè¯
- [ ] é‡‡æ ·æ¨¡å¼çš„ä¸€è‡´æ€§æµ‹è¯•
- [ ] æ‰¹å¤„ç†çš„æ­£ç¡®æ€§éªŒè¯

## ğŸ”¬ è¯¦ç»†æµ‹è¯•è§„èŒƒ

### 1. æ•°å€¼ç¼–ç å‡½æ•° Ï†(v) æµ‹è¯•

#### 1.1 åŸºç¡€æ€§è´¨æµ‹è¯•

**æµ‹è¯•ç›®æ ‡**ï¼šéªŒè¯æ•°å€¼ç¼–ç å‡½æ•°çš„æ ¸å¿ƒæ•°å­¦æ€§è´¨

```python
import torch
import numpy as np

class TestNumericalEncodingFunction:
    def test_phi_zero_exact(self):
        """Ï†(0) å¿…é¡»ç²¾ç¡®ç­‰äºé›¶å‘é‡"""
        # æµ‹è¯•å•ä¸ªé›¶å€¼
        v = 0.0
        direction = torch.randn(768)
        direction = direction / direction.norm()
        
        phi_v = compute_phi(v, direction)
        
        # ä½¿ç”¨æå°çš„å®¹å·®
        assert torch.allclose(phi_v, torch.zeros_like(phi_v), atol=1e-10)
        assert phi_v.abs().max().item() == 0.0  # ç²¾ç¡®ä¸ºé›¶
        
    def test_phi_sign_symmetry(self):
        """éªŒè¯ Ï†(-v) = -Ï†(v)"""
        for v in [0.1, 1.0, 10.0, 100.0, 1e6]:
            direction = torch.randn(768)
            direction = direction / direction.norm()
            
            phi_pos = compute_phi(v, direction)
            phi_neg = compute_phi(-v, direction)
            
            # ç¬¦å·å¯¹ç§°æ€§å¿…é¡»ç²¾ç¡®
            assert torch.allclose(phi_pos, -phi_neg, rtol=1e-7)
            
    def test_phi_logarithmic_growth(self):
        """éªŒè¯å¯¹æ•°å¢é•¿ç‰¹æ€§"""
        direction = torch.randn(768)
        direction = direction / direction.norm()
        
        values = [1.0, 10.0, 100.0, 1000.0]
        norms = []
        
        for v in values:
            phi_v = compute_phi(v, direction)
            norms.append(phi_v.norm().item())
        
        # éªŒè¯å¢é•¿ç‡é€’å‡
        growth_rates = []
        for i in range(1, len(norms)):
            growth_rate = norms[i] / norms[i-1]
            growth_rates.append(growth_rate)
            
        # å¢é•¿ç‡åº”è¯¥é€’å‡
        for i in range(1, len(growth_rates)):
            assert growth_rates[i] < growth_rates[i-1]
            
    def test_phi_numerical_stability(self):
        """æµ‹è¯•æç«¯å€¼çš„æ•°å€¼ç¨³å®šæ€§"""
        direction = torch.randn(768)
        direction = direction / direction.norm()
        
        extreme_values = [
            1e-10,  # æå°å€¼
            1e-5,
            1e10,   # å¤§å€¼
            1e20,   # æå¤§å€¼
        ]
        
        for v in extreme_values:
            phi_v = compute_phi(v, direction)
            
            # ä¸åº”è¯¥æœ‰ NaN æˆ– Inf
            assert not torch.isnan(phi_v).any()
            assert not torch.isinf(phi_v).any()
            
            # éªŒè¯å…¬å¼ï¼š|Ï†(v)| = |ln(1 + |v|)|
            expected_norm = abs(np.log(1 + abs(v)))
            actual_norm = phi_v.norm().item()
            
            # å¯¹äºæç«¯å€¼å…è®¸ç¨å¤§çš„ç›¸å¯¹è¯¯å·®
            rtol = 1e-5 if abs(v) < 1e10 else 1e-3
            assert np.isclose(actual_norm, expected_norm, rtol=rtol)
```

#### 1.2 æ‰¹é‡è®¡ç®—æµ‹è¯•

```python
def test_phi_batch_computation(self):
    """éªŒè¯æ‰¹é‡è®¡ç®—çš„æ­£ç¡®æ€§"""
    batch_size = 32
    seq_len = 128
    hidden_dim = 768
    
    # åˆ›å»ºæ‰¹é‡æ•°å€¼æ•°æ®
    numeric_values = torch.randn(batch_size, seq_len)
    # æ·»åŠ ç‰¹æ®Šå€¼
    numeric_values[0, 0] = 0.0  # é›¶å€¼
    numeric_values[1, 1] = 1e10  # å¤§å€¼
    numeric_values[2, 2] = -1e-5  # å°è´Ÿå€¼
    
    direction = torch.randn(hidden_dim)
    direction = direction / direction.norm()
    
    # æ‰¹é‡è®¡ç®—
    phi_batch = compute_phi_batch(numeric_values, direction)
    
    # éªŒè¯å½¢çŠ¶
    assert phi_batch.shape == (batch_size, seq_len, hidden_dim)
    
    # éªŒè¯ç‰¹æ®Šå€¼
    assert torch.allclose(phi_batch[0, 0], torch.zeros(hidden_dim), atol=1e-10)
    
    # é€ä¸ªéªŒè¯ä¸å•ç‹¬è®¡ç®—çš„ä¸€è‡´æ€§
    for i in range(batch_size):
        for j in range(seq_len):
            v = numeric_values[i, j].item()
            phi_single = compute_phi(v, direction)
            assert torch.allclose(phi_batch[i, j], phi_single, rtol=1e-6)
```

### 2. å¢å¼ºåµŒå…¥æµ‹è¯•

#### 2.1 åµŒå…¥èåˆæµ‹è¯•

```python
class TestEnhancedEmbedding:
    def test_embedding_fusion_non_numeric(self):
        """éæ•°å€¼ä½ç½®çš„åµŒå…¥ä¿æŒä¸å˜"""
        vocab_size = 151937  # Qwenè¯æ±‡è¡¨å¤§å°
        hidden_dim = 768
        
        # åˆ›å»ºåµŒå…¥å±‚
        embedding = torch.nn.Embedding(vocab_size, hidden_dim)
        
        # éæ•°å€¼è¯å…ƒ
        input_ids = torch.randint(0, vocab_size-1, (4, 16))  # é¿å…<NUM>
        numeric_values = torch.zeros(4, 16)
        
        # è®¡ç®—åŸºç¡€åµŒå…¥
        base_embeddings = embedding(input_ids)
        
        # è®¡ç®—å¢å¼ºåµŒå…¥
        enhanced = compute_enhanced_embedding(
            input_ids, numeric_values, embedding
        )
        
        # éæ•°å€¼ä½ç½®åº”è¯¥å®Œå…¨ç›¸åŒ
        assert torch.allclose(enhanced, base_embeddings, atol=1e-8)
        
    def test_embedding_fusion_numeric(self):
        """æ•°å€¼ä½ç½®çš„åµŒå…¥æ­£ç¡®å¢å¼º"""
        num_token_id = 151936  # <NUM> token ID
        
        # åˆ›å»ºåŒ…å«æ•°å€¼çš„è¾“å…¥
        input_ids = torch.tensor([[100, num_token_id, 200]])
        numeric_values = torch.tensor([[0.0, 99.9, 0.0]])
        
        # è·å–å¢å¼ºåµŒå…¥
        enhanced = compute_enhanced_embedding(
            input_ids, numeric_values, embedding
        )
        
        # éªŒè¯æ•°å€¼ä½ç½®è¢«å¢å¼º
        base_num_embedding = embedding(torch.tensor([num_token_id]))
        phi_value = compute_phi(99.9, direction)
        expected = base_num_embedding + phi_value
        
        assert torch.allclose(enhanced[0, 1], expected, rtol=1e-6)
```

#### 2.2 è¾¹ç•Œæ¡ä»¶æµ‹è¯•

```python
def test_embedding_edge_cases(self):
    """æµ‹è¯•è¾¹ç•Œå’Œç‰¹æ®Šæƒ…å†µ"""
    # æƒ…å†µ1ï¼šå…¨æ˜¯æ•°å€¼
    input_ids = torch.full((2, 8), num_token_id)
    numeric_values = torch.randn(2, 8) * 100
    
    enhanced = compute_enhanced_embedding(
        input_ids, numeric_values, embedding
    )
    
    # æ¯ä¸ªä½ç½®éƒ½åº”è¯¥è¢«å¢å¼º
    base = embedding(input_ids)
    for i in range(2):
        for j in range(8):
            phi_v = compute_phi(numeric_values[i, j].item(), direction)
            expected = base[i, j] + phi_v
            assert torch.allclose(enhanced[i, j], expected, rtol=1e-6)
    
    # æƒ…å†µ2ï¼šæ•°å€¼ä¸ºé›¶çš„<NUM>ä½ç½®
    input_ids = torch.tensor([[num_token_id]])
    numeric_values = torch.tensor([[0.0]])
    
    enhanced = compute_enhanced_embedding(
        input_ids, numeric_values, embedding
    )
    
    # åº”è¯¥ç­‰äºåŸºç¡€åµŒå…¥ï¼ˆå› ä¸ºÏ†(0) = 0ï¼‰
    base = embedding(torch.tensor([num_token_id]))
    assert torch.allclose(enhanced[0, 0], base[0], atol=1e-8)
```

### 3. æŸ¯è¥¿åˆ†å¸ƒè®¡ç®—æµ‹è¯•

#### 3.1 åˆ†å¸ƒå‚æ•°éªŒè¯

```python
class TestCauchyDistribution:
    def test_cauchy_parameter_constraints(self):
        """éªŒè¯æŸ¯è¥¿åˆ†å¸ƒå‚æ•°çš„æœ‰æ•ˆæ€§"""
        batch_size = 16
        seq_len = 32
        hidden_dim = 768
        
        # åˆ›å»ºéšæœºç‰¹å¾
        features = torch.randn(batch_size, seq_len, hidden_dim)
        
        # é€šè¿‡å½’å› ç½‘ç»œ
        loc_u, scale_u = abduction_network(features)
        
        # éªŒè¯å½¢çŠ¶
        assert loc_u.shape == (batch_size, seq_len, hidden_dim)
        assert scale_u.shape == (batch_size, seq_len, hidden_dim)
        
        # éªŒè¯ scale > 0
        assert (scale_u > 0).all()
        
        # éªŒè¯åˆå§‹åŒ–åçš„å¤§è‡´èŒƒå›´
        if not model.is_trained:
            # åˆå§‹åŒ–æ—¶ scale åº”è¯¥è¾ƒå¤§ï¼ˆçº¦10ï¼‰
            assert scale_u.mean().item() > 5.0
            assert scale_u.mean().item() < 20.0
            
    def test_cauchy_linear_transformation(self):
        """éªŒè¯æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§å˜æ¢æ€§è´¨"""
        # U ~ Cauchy(Î¼, Î³)
        loc_u = torch.tensor([2.0])
        scale_u = torch.tensor([3.0])
        
        # çº¿æ€§å˜æ¢ Y = aU + b
        a = 2.5
        b = -1.0
        
        # è®¡ç®—å˜æ¢åçš„å‚æ•°
        loc_y = a * loc_u + b
        scale_y = abs(a) * scale_u
        
        # éªŒè¯å…¬å¼æ­£ç¡®æ€§
        expected_loc = 2.5 * 2.0 - 1.0  # = 4.0
        expected_scale = 2.5 * 3.0  # = 7.5
        
        assert torch.isclose(loc_y, torch.tensor([expected_loc]))
        assert torch.isclose(scale_y, torch.tensor([expected_scale]))
```

#### 3.2 CDF è®¡ç®—ç²¾åº¦æµ‹è¯•

```python
def test_cauchy_cdf_accuracy(self):
    """éªŒè¯æŸ¯è¥¿CDFè®¡ç®—çš„æ•°å€¼ç²¾åº¦"""
    # æ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒçš„å·²çŸ¥å€¼
    test_cases = [
        # (x, loc, scale, expected_cdf)
        (0.0, 0.0, 1.0, 0.5),  # ä¸­ä½æ•°
        (1.0, 0.0, 1.0, 0.75),  # ç¬¬ä¸‰å››åˆ†ä½æ•°
        (-1.0, 0.0, 1.0, 0.25),  # ç¬¬ä¸€å››åˆ†ä½æ•°
    ]
    
    for x, loc, scale, expected in test_cases:
        # è®¡ç®— P(X > x)
        prob = compute_cauchy_survival(x, loc, scale)
        expected_survival = 1 - expected
        
        # é«˜ç²¾åº¦éªŒè¯
        assert abs(prob - expected_survival) < 1e-10
        
def test_cauchy_cdf_vectorized(self):
    """éªŒè¯å‘é‡åŒ–CDFè®¡ç®—"""
    batch_size = 8
    vocab_size = 151937
    
    # åˆ›å»ºæ‰¹é‡æ•°æ®
    loc_s = torch.randn(batch_size, vocab_size)
    scale_s = torch.rand(batch_size, vocab_size) * 5 + 0.1
    thresholds = torch.randn(vocab_size)
    
    # æ‰¹é‡è®¡ç®—
    probs = compute_ovr_probabilities(loc_s, scale_s, thresholds)
    
    # éªŒè¯èŒƒå›´
    assert (probs >= 0).all()
    assert (probs <= 1).all()
    
    # éªŒè¯ç‰¹æ®Šæƒ…å†µ
    # å½“ loc >> threshold æ—¶ï¼Œæ¦‚ç‡åº”æ¥è¿‘1
    loc_s[0, 0] = 1000.0
    scale_s[0, 0] = 1.0
    thresholds[0] = 0.0
    
    probs_special = compute_ovr_probabilities(loc_s, scale_s, thresholds)
    assert probs_special[0, 0] > 0.999
```

### 4. ç½‘ç»œåˆå§‹åŒ–æµ‹è¯•

#### 4.1 å½’å› ç½‘ç»œåˆå§‹åŒ–éªŒè¯

```python
class TestNetworkInitialization:
    def test_abduction_network_initialization(self):
        """éªŒè¯å½’å› ç½‘ç»œçš„æ’ç­‰æ˜ å°„åˆå§‹åŒ–"""
        hidden_dim = 768
        
        # åˆ›å»ºå½’å› ç½‘ç»œ
        abduction_net = AbductionNetwork(hidden_dim)
        
        # æµ‹è¯•è¾“å…¥
        z = torch.randn(4, 16, hidden_dim)
        
        # è·å–è¾“å‡º
        loc_u, scale_u = abduction_net(z)
        
        # éªŒè¯ä½ç½®å‚æ•°çš„æ’ç­‰æ˜ å°„
        if isinstance(abduction_net.loc_net, torch.nn.Linear):
            # æ£€æŸ¥æƒé‡çŸ©é˜µæ˜¯å¦ä¸ºæ’ç­‰çŸ©é˜µ
            weight = abduction_net.loc_net.weight
            eye = torch.eye(hidden_dim)
            assert torch.allclose(weight, eye, atol=1e-6)
            
            # æ£€æŸ¥åç½®ä¸ºé›¶
            bias = abduction_net.loc_net.bias
            assert torch.allclose(bias, torch.zeros_like(bias), atol=1e-6)
        
        # éªŒè¯ loc_u â‰ˆ z
        assert torch.allclose(loc_u, z, rtol=1e-6)
        
        # éªŒè¯ scale_u æ˜¯å¤§å€¼
        assert scale_u.mean().item() > 5.0
        
    def test_action_network_weight_transfer(self):
        """éªŒè¯è¡ŒåŠ¨ç½‘ç»œçš„æƒé‡è¿ç§»"""
        # æ¨¡æ‹ŸQwençš„lm_headæƒé‡
        vocab_size = 151936
        hidden_dim = 768
        qwen_lm_head = torch.nn.Linear(hidden_dim, vocab_size)
        qwen_lm_head.weight.data = torch.randn(vocab_size, hidden_dim)
        qwen_lm_head.bias.data = torch.randn(vocab_size)
        
        # åˆ›å»ºè¡ŒåŠ¨ç½‘ç»œ
        action_net = ActionNetwork(hidden_dim, vocab_size + 1)
        
        # æ‰§è¡Œæƒé‡è¿ç§»
        transfer_classification_weights(qwen_lm_head, action_net)
        
        # éªŒè¯æƒé‡å¤åˆ¶
        # å‰vocab_sizeä¸ªæƒé‡åº”è¯¥ç›¸åŒ
        transferred_weight = action_net.cls_net.weight[:vocab_size]
        assert torch.allclose(transferred_weight, qwen_lm_head.weight)
        
        # <NUM> tokençš„æƒé‡åº”è¯¥è¢«æ­£ç¡®åˆå§‹åŒ–ï¼ˆä¸æ˜¯é›¶ï¼‰
        num_weight = action_net.cls_net.weight[vocab_size]
        assert num_weight.norm() > 0
```

#### 4.2 OvR é˜ˆå€¼åˆå§‹åŒ–æµ‹è¯•

```python
def test_ovr_threshold_initialization(self):
    """éªŒè¯OvRé˜ˆå€¼çš„åˆå§‹åŒ–ç­–ç•¥"""
    vocab_size = 151937
    
    # æƒ…å†µ1ï¼šç»Ÿä¸€é˜ˆå€¼
    thresholds = initialize_ovr_thresholds(vocab_size, uniform=True, value=100.0)
    assert thresholds.shape == (vocab_size,)
    assert (thresholds == 100.0).all()
    
    # æƒ…å†µ2ï¼šå¯å­¦ä¹ å‚æ•°
    thresholds = torch.nn.Parameter(torch.full((vocab_size,), 100.0))
    assert thresholds.requires_grad
    
    # éªŒè¯é˜ˆå€¼å¯¹æ¦‚ç‡çš„å½±å“
    loc = torch.zeros(vocab_size)
    scale = torch.ones(vocab_size)
    
    # é«˜é˜ˆå€¼ -> ä½æ¦‚ç‡
    probs_high = compute_ovr_probabilities(loc, scale, thresholds)
    assert probs_high.mean() < 0.1
    
    # é›¶é˜ˆå€¼ -> 0.5æ¦‚ç‡
    zero_thresholds = torch.zeros(vocab_size)
    probs_zero = compute_ovr_probabilities(loc, scale, zero_thresholds)
    assert torch.allclose(probs_zero, torch.full_like(probs_zero, 0.5), atol=1e-6)
```

### 5. æŸå¤±å‡½æ•°æµ‹è¯•

#### 5.1 æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶æµ‹è¯•

```python
class TestLossFunctions:
    def test_cauchy_nll_formula(self):
        """éªŒè¯æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶çš„ç²¾ç¡®å…¬å¼"""
        # æµ‹è¯•æ¡ˆä¾‹
        test_cases = [
            # (y_true, loc, scale, expected_nll)
            (0.0, 0.0, 1.0, np.log(np.pi)),  # æœ€å°æŸå¤±æƒ…å†µ
            (1.0, 0.0, 1.0, np.log(np.pi) + np.log(2)),  # æ ‡å‡†åå·®
        ]
        
        for y_true, loc, scale, expected in test_cases:
            y_true_t = torch.tensor([y_true])
            loc_t = torch.tensor([loc])
            scale_t = torch.tensor([scale])
            
            # è®¡ç®—NLL
            nll = compute_cauchy_nll(y_true_t, loc_t, scale_t)
            
            # éªŒè¯å…¬å¼ï¼šlog(Ï€Â·scale) + log(1 + ((y-loc)/scale)Â²)
            z = (y_true - loc) / scale
            expected_computed = np.log(np.pi * scale) + np.log(1 + z**2)
            
            assert abs(nll.item() - expected) < 1e-10
            assert abs(nll.item() - expected_computed) < 1e-10
            
    def test_cauchy_nll_gradient(self):
        """éªŒè¯æŸ¯è¥¿NLLçš„æ¢¯åº¦æ­£ç¡®æ€§"""
        y_true = torch.tensor([2.0], requires_grad=False)
        loc = torch.tensor([1.0], requires_grad=True)
        scale = torch.tensor([0.5], requires_grad=True)
        
        # è®¡ç®—æŸå¤±
        nll = compute_cauchy_nll(y_true, loc, scale)
        nll.backward()
        
        # æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦
        z = (y_true - loc) / scale
        expected_grad_loc = 2 * z / (scale * (1 + z**2))
        expected_grad_scale = 1/scale - 2*z**2 / (scale * (1 + z**2))
        
        # éªŒè¯æ¢¯åº¦
        assert torch.isclose(loc.grad, expected_grad_loc, rtol=1e-5)
        assert torch.isclose(scale.grad, expected_grad_scale, rtol=1e-5)
```

#### 5.2 é—¨æ§æœºåˆ¶æµ‹è¯•

```python
def test_gated_loss_mechanism(self):
    """æµ‹è¯•é—¨æ§æŸå¤±çš„å„ç§æƒ…å†µ"""
    batch_size = 4
    seq_len = 8
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    # ä½ç½®0,2,5æ˜¯æ•°å€¼ä½ç½®
    mask = torch.zeros(batch_size, seq_len)
    mask[:, [0, 2, 5]] = 1.0
    
    # NUM tokençš„é¢„æµ‹æ¦‚ç‡
    p_num = torch.rand(batch_size, seq_len)
    
    # åŸºç¡€å›å½’æŸå¤±
    base_loss = torch.rand(batch_size, seq_len) + 0.1
    
    # æµ‹è¯•ä¸åŒçš„alphaå€¼
    for alpha in [0.0, 0.1, 0.5, 1.0]:
        gated_loss = compute_gated_regression_loss(
            base_loss, mask, p_num, alpha
        )
        
        # éªŒè¯å½¢çŠ¶
        assert gated_loss.shape == (batch_size, seq_len)
        
        # éæ•°å€¼ä½ç½®çš„æŸå¤±åº”è¯¥ä¸º0
        assert (gated_loss[:, mask[0] == 0] == 0).all()
        
        # æ•°å€¼ä½ç½®çš„æŸå¤±éªŒè¯
        for i in [0, 2, 5]:
            if alpha == 1.0:
                # å®Œå…¨å¿½ç•¥æ¨¡å‹ç½®ä¿¡åº¦
                assert torch.allclose(gated_loss[:, i], base_loss[:, i])
            elif alpha == 0.0:
                # å®Œå…¨ä¾èµ–æ¨¡å‹ç½®ä¿¡åº¦
                expected = base_loss[:, i] * p_num[:, i]
                assert torch.allclose(gated_loss[:, i], expected)
            else:
                # æ··åˆæƒ…å†µ
                gate = alpha + (1 - alpha) * p_num[:, i]
                expected = base_loss[:, i] * gate
                assert torch.allclose(gated_loss[:, i], expected)
```

### 6. ç«¯åˆ°ç«¯æ•°å€¼éªŒè¯

#### 6.1 å®Œæ•´å‰å‘ä¼ æ’­æµ‹è¯•

```python
class TestEndToEnd:
    def test_complete_forward_pass(self):
        """æµ‹è¯•å®Œæ•´çš„å‰å‘ä¼ æ’­æ•°å€¼æ­£ç¡®æ€§"""
        model = CausalQwen(config)
        model.eval()
        
        # å‡†å¤‡è¾“å…¥
        input_text = "ä»·æ ¼æ˜¯ <NUM> å…ƒï¼Œæ¶¨å¹… <NUM> %"
        input_ids = torch.tensor([[1234, 5678, num_token_id, 9012, 3456, num_token_id, 7890]])
        numeric_values = torch.tensor([[0.0, 0.0, 99.9, 0.0, 0.0, 3.5, 0.0]])
        
        with torch.no_grad():
            # æ‰§è¡Œå‰å‘ä¼ æ’­
            output = model(input_ids, numeric_values)
            
        # éªŒè¯è¾“å‡ºç»“æ„
        assert 'loc_S' in output
        assert 'scale_S' in output
        assert 'loc_Y' in output
        assert 'scale_Y' in output
        
        # éªŒè¯ç»´åº¦
        batch_size, seq_len = input_ids.shape
        vocab_size = model.config.vocab_size
        
        assert output['loc_S'].shape == (batch_size, seq_len, vocab_size)
        assert output['scale_S'].shape == (batch_size, seq_len, vocab_size)
        assert output['loc_Y'].shape == (batch_size, seq_len)
        assert output['scale_Y'].shape == (batch_size, seq_len)
        
        # éªŒè¯æ•°å€¼èŒƒå›´
        assert (output['scale_S'] > 0).all()
        assert (output['scale_Y'] > 0).all()
        
    def test_gradient_flow(self):
        """éªŒè¯æ¢¯åº¦å¯ä»¥æ­£ç¡®å›ä¼ """
        model = CausalQwen(config)
        
        # å‡†å¤‡æ•°æ®
        input_ids = torch.randint(0, 151936, (2, 16))
        numeric_values = torch.zeros(2, 16)
        labels = torch.randint(0, 151936, (2, 16))
        
        # æ·»åŠ ä¸€äº›æ•°å€¼ä½ç½®
        input_ids[0, 5] = num_token_id
        numeric_values[0, 5] = 10.5
        labels[0, 5] = num_token_id
        
        # å‰å‘ä¼ æ’­
        output = model(input_ids, numeric_values)
        
        # è®¡ç®—æŸå¤±
        loss = compute_total_loss(output, labels, numeric_values)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # éªŒè¯æ¢¯åº¦å­˜åœ¨
        for name, param in model.named_parameters():
            if param.requires_grad:
                assert param.grad is not None
                assert not torch.isnan(param.grad).any()
                assert not torch.isinf(param.grad).any()
```

#### 6.2 æ•°å€¼ç¨³å®šæ€§å‹åŠ›æµ‹è¯•

```python
def test_numerical_stability_stress(self):
    """å‹åŠ›æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
    model = CausalQwen(config)
    model.eval()
    
    # æç«¯è¾“å…¥
    extreme_cases = [
        # (description, numeric_values)
        ("æå°å€¼", torch.tensor([[1e-10, 1e-8, 1e-6]])),
        ("æå¤§å€¼", torch.tensor([[1e10, 1e12, 1e15]])),
        ("æ··åˆå€¼", torch.tensor([[-1e10, 0.0, 1e10]])),
        ("é«˜ç²¾åº¦", torch.tensor([[3.141592653589793, 2.718281828459045, 1.414213562373095]])),
    ]
    
    for desc, values in extreme_cases:
        print(f"æµ‹è¯•: {desc}")
        
        batch_size, seq_len = values.shape
        input_ids = torch.full((batch_size, seq_len), num_token_id)
        
        with torch.no_grad():
            output = model(input_ids, values)
        
        # éªŒè¯è¾“å‡ºæœ‰æ•ˆæ€§
        for key in ['loc_S', 'scale_S', 'loc_Y', 'scale_Y']:
            tensor = output[key]
            assert not torch.isnan(tensor).any(), f"{key} åŒ…å« NaN"
            assert not torch.isinf(tensor).any(), f"{key} åŒ…å« Inf"
            
        # éªŒè¯æ¦‚ç‡è®¡ç®—
        probs = compute_ovr_probabilities(
            output['loc_S'], 
            output['scale_S'], 
            torch.zeros(vocab_size)
        )
        assert (probs >= 0).all()
        assert (probs <= 1).all()
```

### 7. é‡‡æ ·ä¸€è‡´æ€§æµ‹è¯•

```python
class TestSamplingConsistency:
    def test_deterministic_vs_sampling_mode(self):
        """éªŒè¯ç¡®å®šæ€§æ¨¡å¼ä¸é‡‡æ ·æ¨¡å¼çš„ä¸€è‡´æ€§"""
        model = CausalQwen(config)
        model.eval()
        
        input_ids = torch.tensor([[1234, 5678, num_token_id]])
        numeric_values = torch.tensor([[0.0, 0.0, 50.0]])
        
        with torch.no_grad():
            output = model(input_ids, numeric_values)
        
        # ç¡®å®šæ€§é¢„æµ‹
        det_cls = output['loc_S'].argmax(dim=-1)
        det_reg = output['loc_Y']
        
        # é‡‡æ ·å¤šæ¬¡
        n_samples = 10000
        sampled_cls_counts = torch.zeros_like(output['loc_S'])
        sampled_reg_sum = torch.zeros_like(output['loc_Y'])
        
        for _ in range(n_samples):
            # å› æœé‡‡æ ·
            u_sample = sample_cauchy(output['loc_U'], output['scale_U'])
            cls_scores = model.action_network.classify(u_sample)
            reg_values = model.action_network.regress(u_sample)
            
            # ç´¯è®¡
            cls_pred = cls_scores.argmax(dim=-1)
            for i in range(batch_size):
                for j in range(seq_len):
                    sampled_cls_counts[i, j, cls_pred[i, j]] += 1
            sampled_reg_sum += reg_values
        
        # éªŒè¯åˆ†ç±»æ¨¡å¼
        sampled_cls_mode = sampled_cls_counts.argmax(dim=-1)
        assert (sampled_cls_mode == det_cls).float().mean() > 0.9
        
        # éªŒè¯å›å½’ä¸­ä½æ•°
        sampled_reg_median = sampled_reg_sum / n_samples
        assert torch.allclose(sampled_reg_median, det_reg, rtol=0.1)
```

## ğŸ”§ æµ‹è¯•å·¥å…·å‡½æ•°

### è¾…åŠ©å‡½æ•°å®ç°

```python
# test_utils.py

def compute_phi(v, direction):
    """è®¡ç®—å•ä¸ªæ•°å€¼çš„ç¼–ç """
    if v == 0:
        return torch.zeros_like(direction)
    return np.sign(v) * np.log(1 + abs(v)) * direction

def compute_cauchy_nll(y_true, loc, scale):
    """è®¡ç®—æŸ¯è¥¿åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶"""
    z = (y_true - loc) / scale
    return torch.log(np.pi * scale) + torch.log(1 + z**2)

def compute_ovr_probabilities(loc_s, scale_s, thresholds):
    """è®¡ç®—OvRæ¦‚ç‡"""
    # P(S > C) = 1/2 + (1/Ï€)arctan((loc-C)/scale)
    z = (loc_s - thresholds) / scale_s
    return 0.5 + torch.atan(z) / np.pi

def sample_cauchy(loc, scale):
    """ä»æŸ¯è¥¿åˆ†å¸ƒé‡‡æ ·"""
    u = torch.rand_like(loc)
    return loc + scale * torch.tan(np.pi * (u - 0.5))
```

## ğŸ“Š æµ‹è¯•æ‰§è¡Œç­–ç•¥

### åˆ†é˜¶æ®µæµ‹è¯•

```bash
# ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç»„ä»¶
pytest tests/test_numerical_encoding.py -v
pytest tests/test_enhanced_embedding.py -v

# ç¬¬äºŒé˜¶æ®µï¼šåˆ†å¸ƒè®¡ç®—
pytest tests/test_cauchy_distribution.py -v
pytest tests/test_linear_transformation.py -v

# ç¬¬ä¸‰é˜¶æ®µï¼šç½‘ç»œç»„ä»¶
pytest tests/test_network_initialization.py -v
pytest tests/test_weight_transfer.py -v

# ç¬¬å››é˜¶æ®µï¼šæŸå¤±å‡½æ•°
pytest tests/test_loss_functions.py -v
pytest tests/test_gated_mechanism.py -v

# ç¬¬äº”é˜¶æ®µï¼šé›†æˆæµ‹è¯•
pytest tests/test_end_to_end.py -v
pytest tests/test_numerical_stability.py -v
```

### æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```bash
# ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
pytest tests/ --html=report.html --self-contained-html

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=src --cov-report=html --cov-report=term
```

## ğŸ¯ å…³é”®éªŒè¯ç‚¹æ£€æŸ¥æ¸…å•

- [ ] Ï†(0) ç²¾ç¡®ç­‰äºé›¶å‘é‡ï¼ˆè¯¯å·® < 1e-10ï¼‰
- [ ] Ï†(-v) = -Ï†(v) å¯¹æ‰€æœ‰æµ‹è¯•å€¼æˆç«‹
- [ ] å¢å¼ºåµŒå…¥åœ¨éæ•°å€¼ä½ç½®ç²¾ç¡®ç­‰äºåŸºç¡€åµŒå…¥
- [ ] æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§å˜æ¢å…¬å¼ç²¾ç¡®æˆç«‹
- [ ] OvRæ¦‚ç‡ä¸¥æ ¼åœ¨ [0,1] åŒºé—´
- [ ] æŸ¯è¥¿NLLå…¬å¼ä¸æ‰‹åŠ¨è®¡ç®—å®Œå…¨ä¸€è‡´
- [ ] é—¨æ§æœºåˆ¶åœ¨Î±=0å’ŒÎ±=1æ—¶çš„è¡Œä¸ºæ­£ç¡®
- [ ] æ¢¯åº¦å¯ä»¥æ­£ç¡®å›ä¼ åˆ°æ‰€æœ‰å‚æ•°
- [ ] æç«¯æ•°å€¼è¾“å…¥ä¸äº§ç”ŸNaNæˆ–Inf
- [ ] æ‰¹å¤„ç†ç»“æœä¸é€ä¸ªå¤„ç†å®Œå…¨ä¸€è‡´

## ğŸš¨ å¸¸è§å®ç°é”™è¯¯

1. **æ•°å€¼ç¼–ç çš„ç¬¦å·å¤„ç†é”™è¯¯**
   ```python
   # âŒ é”™è¯¯ï¼šå¿˜è®°å¤„ç†è´Ÿæ•°
   phi = torch.log(1 + v) * direction
   
   # âœ… æ­£ç¡®ï¼šåŒ…å«ç¬¦å·
   phi = torch.sign(v) * torch.log(1 + torch.abs(v)) * direction
   ```

2. **æŸ¯è¥¿åˆ†å¸ƒscaleå‚æ•°çº¦æŸ**
   ```python
   # âŒ é”™è¯¯ï¼šscaleå¯èƒ½ä¸ºè´Ÿ
   scale = linear(features)
   
   # âœ… æ­£ç¡®ï¼šç¡®ä¿scale > 0
   scale = F.softplus(linear(features)) + eps
   ```

3. **OvRæ¦‚ç‡è®¡ç®—é”™è¯¯**
   ```python
   # âŒ é”™è¯¯ï¼šä½¿ç”¨sigmoid
   prob = torch.sigmoid((loc - threshold) / scale)
   
   # âœ… æ­£ç¡®ï¼šä½¿ç”¨æŸ¯è¥¿CDF
   prob = 0.5 + torch.atan((loc - threshold) / scale) / np.pi
   ```

## ğŸ“ æµ‹è¯•æ—¥å¿—æ¨¡æ¿

```python
# æ¯ä¸ªæµ‹è¯•åº”è¯¥è®°å½•
def test_something():
    """æµ‹è¯•æè¿°"""
    logger.info("="*50)
    logger.info(f"æµ‹è¯•: {test_name}")
    logger.info(f"è¾“å…¥: {input_description}")
    
    # æ‰§è¡Œæµ‹è¯•
    result = perform_test()
    
    logger.info(f"æœŸæœ›: {expected}")
    logger.info(f"å®é™…: {actual}")
    logger.info(f"è¯¯å·®: {error}")
    logger.info(f"ç»“æœ: {'é€šè¿‡' if passed else 'å¤±è´¥'}")
    logger.info("="*50)
```

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³æ‰§è¡Œ**ï¼šæŒ‰ç…§æµ‹è¯•æ¸…å•é€é¡¹éªŒè¯æ¯ä¸ªæ•°å­¦ç»„ä»¶
2. **è®°å½•é—®é¢˜**ï¼šå‘ç°çš„ä»»ä½•åå·®éƒ½è¦è¯¦ç»†è®°å½•
3. **ä¿®å¤éªŒè¯**ï¼šä¿®å¤åå¿…é¡»é‡æ–°è¿è¡Œç›¸å…³æµ‹è¯•
4. **å›å½’æµ‹è¯•**ï¼šä»»ä½•ä¿®æ”¹éƒ½è¦è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶

è®°ä½ï¼š**æ²¡æœ‰é€šè¿‡æµ‹è¯•çš„ä»£ç å°±æ˜¯é”™è¯¯çš„ä»£ç ï¼**

# CausalEngine äº”æ¨¡å¼ç³»ç»Ÿè®¾è®¡

> **æ–‡æ¡£ç›®æ ‡**: å®šä¹‰çº¯å›å½’å’Œåˆ†ç±»ä»»åŠ¡çš„äº”ç§å»ºæ¨¡æ¨¡å¼ï¼Œä½œä¸ºå»ºæ¨¡å±‚é¢çš„ç»Ÿä¸€å‚æ•°  
> **é€‚ç”¨èŒƒå›´**: MLPCausalRegressor, MLPCausalClassifierï¼ˆä¸æ¶‰åŠLLM+CausalEngineï¼‰  
> **æ ¸å¿ƒè®¾è®¡**: mode å‚æ•°åŒæ—¶æ§åˆ¶è®­ç»ƒã€æ¨ç†ã€æŸå¤±è®¡ç®—çš„ç»Ÿä¸€æ¡†æ¶

---

## 1. äº”æ¨¡å¼ç³»ç»Ÿæ¦‚è¿°

### 1.1 è®¾è®¡å“²å­¦

CausalEngine çš„äº”æ¨¡å¼ç³»ç»ŸåŸºäºå¯¹éšæœºæ€§æ¥æºçš„ä¸åŒå»ºæ¨¡å‡è®¾ï¼š

```mermaid
graph TB
    subgraph Philosophy["å»ºæ¨¡å“²å­¦æ¡†æ¶"]
        direction TB
        
        subgraph Source["éšæœºæ€§æ¥æº"]
            direction LR
            S1["ä¸ªä½“é€‰æ‹©å·®å¼‚<br/>ï¼ˆå†…åœ¨ï¼‰"]
            S2["ç¯å¢ƒå™ªå£°æ‰°åŠ¨<br/>ï¼ˆå¤–åœ¨ï¼‰"]
        end
        
        subgraph Effects["å™ªå£°ä½œç”¨æ–¹å¼"]
            direction LR
            E1["ä½ç½®å‚æ•°æ‰°åŠ¨<br/>æ”¹å˜ä¸ªä½“èº«ä»½"]
            E2["å°ºåº¦å‚æ•°å¢å¼º<br/>æ‰©å¤§å†³ç­–ä¸ç¡®å®šæ€§"]
            E3["å®Œå…¨æ¶ˆé™¤å™ªå£°<br/>çº¯å› æœç¡®å®šæ€§"]
        end
        
        Source --> Effects
    end
    
    subgraph Modes["äº”ç§å»ºæ¨¡æ¨¡å¼"]
        direction TB
        M1["Deterministic Mode<br/>Î³_U=0, b_noise=0<br/>ç¡®å®šæ€§å› æœ"]
        M2["Exogenous Mode<br/>Î³_U=0, b_noiseâ‰ 0<br/>å¤–ç”Ÿå™ªå£°å› æœ"]
        M3["Endogenous Mode<br/>Î³_Uâ‰ 0, b_noise=0<br/>å†…ç”Ÿå› æœæ¨ç†"]
        M4["Standard Mode<br/>b_noiseâ†’scale<br/>æ··åˆå› æœæ¨ç†"]
        M5["Sampling Mode<br/>b_noiseâ†’location<br/>éšæœºæ¢ç´¢å› æœ"]
    end
    
    Philosophy --> Modes
    
    classDef philosophyStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef modeStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    
    class Philosophy,Source,Effects philosophyStyle
    class Modes,M1,M2,M3,M4,M5 modeStyle
```

### 1.2 äº”æ¨¡å¼æ•°å­¦å®šä¹‰

| æ¨¡å¼ | å‚æ•°è®¾ç½® | æ•°å­¦è¡¨è¿° | æŸå¤±è®¡ç®— | å“²å­¦å«ä¹‰ |
|------|----------|----------|----------|----------|
| **Deterministic** | $\gamma_U=0, b_{noise}=0$ | $U' = \mu_U$ (ç¡®å®šæ€§) | MSE/CrossEntropy (ç­‰ä»·æ€§éªŒè¯) | æ•°å­¦è®¡ç®—ä¸Šç­‰ä»·äºä¼ ç»ŸMLP |
| **Exogenous** | $\gamma_U=0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ | Cauchy NLL / OvRæ¦‚ç‡ | å¤–ç”Ÿå™ªå£°é©±åŠ¨çš„å› æœæ¨ç† |
| **Endogenous** | $\gamma_U \neq 0, b_{noise}=0$ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ | Cauchy NLL / OvRæ¦‚ç‡ | å†…ç”Ÿä¸ªä½“ä¸ç¡®å®šæ€§é©±åŠ¨çš„å› æœæ¨ç† |
| **Standard** | $\gamma_U \neq 0, b_{noise} \neq 0$ (ä½œç”¨äºå°ºåº¦) | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ | Cauchy NLL / OvRæ¦‚ç‡ | å†…ç”Ÿ+å¤–ç”Ÿæ··åˆï¼Œå™ªå£°å¢å¼ºå†³ç­–ä¸ç¡®å®šæ€§ |
| **Sampling** | $\gamma_U \neq 0, b_{noise} \neq 0$ (ä½œç”¨äºä½ç½®) | $U' \sim \text{Cauchy}(\mu_U + b_{noise}\varepsilon, \gamma_U)$ | Cauchy NLL / OvRæ¦‚ç‡ | éšæœºæ€§æ‰°åŠ¨ä¸ªä½“èº«ä»½ï¼Œæ¢ç´¢æ€§å› æœæ¨ç† |


äº”ç§æ¨¡å¼çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºå¦‚ä½•è®¡ç®— $U'$ çš„åˆ†å¸ƒã€‚åŸºäºæŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¯ä»¥è§£æåœ°æ¨å¯¼å‡ºæ¯ç§æ¨¡å¼ä¸‹ $U'$ çš„åˆ†å¸ƒï¼š

**åŸºç¡€è®¾å®š**ï¼š
- ä¸ªä½“è¡¨å¾ï¼š$U \sim \text{Cauchy}(\mu_U, \gamma_U)$
- å¤–ç”Ÿå™ªå£°ï¼š$\varepsilon \sim \text{Cauchy}(0, 1)$
- ç»Ÿä¸€å…¬å¼ï¼š$U' = U + b_{noise} \varepsilon$

**å„æ¨¡å¼çš„åˆ†å¸ƒæ¨å¯¼**ï¼š

1. **Deterministic Mode** ($\gamma_U=0, b_{noise}=0$)ï¼š
   $$U' = U + 0 \cdot \varepsilon = \mu_U \quad \text{(ç¡®å®šæ€§)}$$

2. **Exogenous Mode** ($\gamma_U=0, b_{noise} \neq 0$)ï¼š
   $$U' = \mu_U + b_{noise} \varepsilon \sim \text{Cauchy}(\mu_U, |b_{noise}|)$$
   
3. **Endogenous Mode** ($\gamma_U \neq 0, b_{noise}=0$)ï¼š
   $$U' = U + 0 \cdot \varepsilon = U \sim \text{Cauchy}(\mu_U, \gamma_U)$$

4. **Standard Mode** ($\gamma_U \neq 0, b_{noise} \neq 0$ï¼Œå™ªå£°ä½œç”¨äºå°ºåº¦)ï¼š
   
   è™½ç„¶ç»Ÿä¸€å…¬å¼æ˜¯ $U' = U + b_{noise} \varepsilon$ï¼Œä½†åœ¨å®ç°ä¸­å™ªå£°è¢«èåˆåˆ°å°ºåº¦å‚æ•°ï¼š
   $$U' \sim \text{Cauchy}(\mu_U, \gamma_U + |b_{noise}|)$$
   
   è¿™åˆ©ç”¨äº†æŸ¯è¥¿åˆ†å¸ƒçš„åŠ æ³•ç¨³å®šæ€§ï¼šå¦‚æœ $X \sim \text{Cauchy}(\mu, \gamma)$ ä¸” $Y \sim \text{Cauchy}(0, |b|)$ï¼Œåˆ™ $X + Y \sim \text{Cauchy}(\mu, \gamma + |b|)$ã€‚

5. **Sampling Mode** ($\gamma_U \neq 0, b_{noise} \neq 0$ï¼Œå™ªå£°ä½œç”¨äºä½ç½®)ï¼š
   
   é¦–å…ˆé‡‡æ ·å™ªå£° $\varepsilon$ï¼Œç„¶åè®¡ç®—ï¼š
   $$U' \sim \text{Cauchy}(\mu_U + b_{noise}\varepsilon, \gamma_U)$$
   
   å…¶ä¸­ $\varepsilon$ æ˜¯ä»æ ‡å‡†æŸ¯è¥¿åˆ†å¸ƒé‡‡æ ·çš„å…·ä½“å€¼ã€‚

**å…³é”®æ´å¯Ÿ**ï¼šStandard å’Œ Sampling æ¨¡å¼ä½¿ç”¨ç›¸åŒçš„ç»Ÿä¸€å…¬å¼ $U' = U + b_{noise} \varepsilon$ï¼Œä½†é€šè¿‡ä¸åŒçš„å®ç°æ–¹å¼è¾¾åˆ°ä¸åŒçš„æ•°å­¦æ•ˆæœï¼š
- **Standard**ï¼šè§£æåœ°å°†å™ªå£°èåˆåˆ°å°ºåº¦å‚æ•°ï¼Œé¿å…é‡‡æ ·
- **Sampling**ï¼šæ˜¾å¼é‡‡æ ·å™ªå£°ï¼Œæ‰°åŠ¨ä½ç½®å‚æ•°

### 1.3 æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼šä¸ºä»€ä¹ˆä¸å¯¹Ué‡‡æ ·ï¼Ÿ

**è®¾è®¡åŸåˆ™**: å¯¹å™ªå£°é‡‡æ ·æ˜¯æ·±åº¦å­¦ä¹ ä¸­å¹¿æ³›å­˜åœ¨çš„å®è·µï¼Œç¬¦åˆé¢†åŸŸæƒ¯ä¾‹ã€‚æ›´å¤šçš„é‡‡æ ·æ–¹å¼ï¼Œæˆ‘ä»¬ä¼šåç»­è¿›è¡Œæ¢ç´¢ã€‚

**å½“å‰è®¾è®¡**: CausalEngineä¿æŒä¸ªä½“è¡¨å¾åˆ†å¸ƒ $U \sim \text{Cauchy}(\mu_U, \gamma_U)$ çš„å®Œæ•´ä¿¡æ¯ï¼Œä»…å¯¹å¤–ç”Ÿå™ªå£° $\epsilon$ è¿›è¡Œé‡‡æ ·ï¼ˆåœ¨Samplingæ¨¡å¼ä¸­ï¼‰ï¼Œä»¥å¹³è¡¡ä¿¡æ¯ä¿å­˜ä¸è®¡ç®—æ•ˆç‡ã€‚

---

## 2. å»ºæ¨¡å±‚é¢çš„modeå‚æ•°è®¾è®¡

### 2.1 modeä½œä¸ºç»Ÿä¸€æ§åˆ¶å‚æ•°

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**ï¼šmodeå‚æ•°ä¸ä»…ä»…æ˜¯æ¨ç†æ—¶çš„é…ç½®ï¼Œè€Œæ˜¯è´¯ç©¿æ•´ä¸ªå»ºæ¨¡è¿‡ç¨‹çš„ç»Ÿä¸€å‚æ•°ï¼š

```mermaid
graph LR
    subgraph ModelingProcess["å»ºæ¨¡å…¨æµç¨‹"]
        direction TB
        
        subgraph Training["è®­ç»ƒé˜¶æ®µ"]
            direction TB
            T1["å‰å‘ä¼ æ’­<br/>æ ¹æ®modeè°ƒæ•´å™ªå£°"]
            T2["æŸå¤±è®¡ç®—<br/>modeå½±å“æŸå¤±å‡½æ•°é€‰æ‹©"]
            T3["åå‘ä¼ æ’­<br/>å‚æ•°æ›´æ–°ç­–ç•¥"]
        end
        
        subgraph Inference["æ¨ç†é˜¶æ®µ"]
            direction TB
            I1["ç‰¹å¾æå–<br/>ç›¸åŒçš„MLP backbone"]
            I2["å› æœæ¨ç†<br/>modeæ§åˆ¶å™ªå£°æœºåˆ¶"]
            I3["è¾“å‡ºç”Ÿæˆ<br/>æ¦‚ç‡è®¡ç®—æ–¹å¼"]
        end
        
        subgraph Loss["æŸå¤±è®¡ç®—"]
            direction TB
            L1["Traditional: MSE/CrossE"]
            L2["Causal: æŸ¯è¥¿ä¼¼ç„¶"]
            L3["Standard: æ··åˆæŸå¤±"]
            L4["Sampling: æ¢ç´¢æ€§æŸå¤±"]
        end
    end
    
    Mode[["modeå‚æ•°<br/>(ç»Ÿä¸€æ§åˆ¶)"]] --> Training
    Mode --> Inference  
    Mode --> Loss
    
    classDef processStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef modeStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    
    class Training,Inference,Loss,T1,T2,T3,I1,I2,I3,L1,L2,L3,L4 processStyle
    class Mode modeStyle
```

### 2.2 modeå‚æ•°çš„APIè®¾è®¡

```python
class MLPCausalRegressor:
    def __init__(self, mode='standard', **kwargs):
        """
        åˆå§‹åŒ–æ—¶è®¾å®šå»ºæ¨¡æ¨¡å¼ï¼Œå½±å“æ•´ä¸ªå»ºæ¨¡æµç¨‹
        
        Parameters:
        -----------
        mode : str, default='standard'
            å»ºæ¨¡æ¨¡å¼é€‰æ‹©ï¼š
            - 'deterministic': ç¡®å®šæ€§å› æœï¼ˆç­‰ä»·äºsklearn MLPï¼‰
            - 'exogenous': å¤–ç”Ÿå™ªå£°å› æœå»ºæ¨¡
            - 'endogenous': å†…ç”Ÿå› æœå»ºæ¨¡
            - 'standard': æ ‡å‡†å› æœå»ºæ¨¡ï¼ˆé»˜è®¤ï¼‰
            - 'sampling': æ¢ç´¢æ€§å› æœå»ºæ¨¡
        """
        self.mode = mode
        self._setup_mode_configuration()
    
    def fit(self, X, y, mode=None):
        """
        è®­ç»ƒæ¨¡å‹ï¼Œå¯è¦†ç›–åˆå§‹åŒ–æ—¶çš„modeè®¾ç½®
        
        Parameters:
        -----------
        mode : str, optional
            ä¸´æ—¶è¦†ç›–å»ºæ¨¡æ¨¡å¼ï¼ˆä»…å¯¹å½“å‰è®­ç»ƒæœ‰æ•ˆï¼‰
        """
        effective_mode = mode or self.mode
        return self._fit_with_mode(X, y, effective_mode)
    
    def predict(self, X, mode=None, enable_flexibility=True):
        """
        é¢„æµ‹ï¼Œæ”¯æŒæ¨ç†æ—¶çš„æ¨¡å¼çµæ´»åˆ‡æ¢
        
        Parameters:
        -----------
        mode : str, optional
            æ¨ç†æ¨¡å¼ï¼ˆå¯ä¸è®­ç»ƒæ¨¡å¼ä¸åŒï¼‰
        enable_flexibility : bool, default=True
            æ˜¯å¦å…è®¸æ¨ç†æ—¶åˆ‡æ¢æ¨¡å¼
        """
        if enable_flexibility:
            inference_mode = mode or self.mode
        else:
            inference_mode = self.mode  # å¼ºåˆ¶ä½¿ç”¨è®­ç»ƒæ—¶çš„æ¨¡å¼
            
        return self._predict_with_mode(X, inference_mode)
```

---

## 3. äº”æ¨¡å¼è¯¦ç»†è®¾è®¡

### 3.1 Deterministic Mode (ç¡®å®šæ€§æ¨¡å¼)

**è®¾è®¡ç›®æ ‡**: å®Œå…¨ç­‰ä»·äºsklearn MLPï¼Œæä¾›åŸºçº¿æ¯”è¾ƒ

```mermaid
graph TB
    subgraph Deterministic["Deterministic Mode æ•°å­¦æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥ X"]
        MLP["MLPç‰¹å¾æå–<br/>H = MLP(X)"]
        Direct["ç›´æ¥çº¿æ€§æ˜ å°„<br/>y = WÂ·H + b"]
        Output["è¾“å‡º y"]
        
        Input --> MLP --> Direct --> Output
        
        subgraph Config["é…ç½®è¦æ±‚"]
            direction TB
            C1["Î³_U = 0 (æ— å°ºåº¦å‚æ•°)"]
            C2["b_noise = 0 (æ— å¤–ç”Ÿå™ªå£°)"]
            C3["ç»•è¿‡ AbductionNetwork"]
            C4["ä½¿ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°"]
        end
    end
    
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef configStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    
    class Input,MLP,Direct,Output deterministicStyle
    class Config,C1,C2,C3,C4 configStyle
```

**æ•°å­¦è¡¨è¿°**:
$$y = W \cdot \text{MLP}(X) + b$$

**å®ç°å…³é”®ç‚¹**:
- AbductionNetwork: è®¾ç½®ä¸ºæ’ç­‰æ˜ å°„æˆ–å®Œå…¨ç»•è¿‡
- ActionNetwork: ç®€åŒ–ä¸ºçº¿æ€§å±‚
- ActivationHead: æ’ç­‰æ˜ å°„
- æŸå¤±å‡½æ•°: MSE (å›å½’) / CrossEntropy (åˆ†ç±»)

### 3.2 Exogenous Mode (å¤–ç”Ÿæ¨¡å¼)

**è®¾è®¡ç›®æ ‡**: ç¡®å®šæ€§ä¸ªä½“æ¨æ–­ï¼Œå¤–ç”Ÿå™ªå£°é©±åŠ¨éšæœºæ€§

```mermaid
graph TB
    subgraph Exogenous["Exogenous Mode æ•°å­¦æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥ X"]
        MLP["MLPç‰¹å¾æå–<br/>H = MLP(X)"]
        Deterministic["ç¡®å®šæ€§ä¸ªä½“æ¨æ–­<br/>U = Î¼_U = H (Î³_U = 0)"]
        Noise["å™ªå£°æ³¨å…¥<br/>U' = U + b_noise*Îµ"]
        Action["è¡ŒåŠ¨å†³ç­–<br/>ActionNetwork(U')"]
        Activation["ä»»åŠ¡æ¿€æ´»<br/>y = f(S)"]
        Output["è¾“å‡º y"]
        
        Input --> MLP --> Deterministic --> Noise --> Action --> Activation --> Output
        
        subgraph Config["é…ç½®è¦æ±‚"]
            direction TB
            C1["Î³_U = 0 (æ— ä¸ªä½“ä¸ç¡®å®šæ€§)"]
            C2["b_noise â‰  0 (å¤–ç”Ÿå™ªå£°)"]
            C3["å®Œå…¨ç¡®å®šæ€§æ¨æ–­ä¸ªä½“"]
            C4["å™ªå£°ç›´æ¥ä½œç”¨äºè¾“å‡º"]
        end
    end
    
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef configStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class Input,MLP,Deterministic,Noise,Action,Activation,Output exogenousStyle
    class Config,C1,C2,C3,C4 configStyle
```

**æ•°å­¦è¡¨è¿°**:
- **ç¡®å®šæ€§æ¨æ–­**: $U = \mu_U = \text{MLP}(X), \gamma_U = 0$
- **å™ªå£°æ³¨å…¥**: $U' = \mu_U + b_{noise} \varepsilon$ï¼Œå…¶ä¸­ $\varepsilon \sim \text{Cauchy}(0,1)$
- **è¡ŒåŠ¨ç½‘ç»œ**: ActionNetwork æ¥æ”¶ $U'$ ä½œä¸ºè¾“å…¥

**å“²å­¦å«ä¹‰**: é€šè¿‡è§‚å¯Ÿè¯æ®å¯ä»¥å®Œå…¨ç¡®å®šä¸ªä½“æ˜¯è°ï¼ˆ$\gamma_U = 0$ï¼‰ï¼Œä½†ç¯å¢ƒä¸­å­˜åœ¨æ— æ³•é¢„æµ‹çš„å¤–ç”Ÿéšæœºå› ç´ 

**å…³é”®ç‰¹æ€§**:
- ä¸ªä½“è¡¨å¾å®Œå…¨ç¡®å®šæ€§
- å¤–ç”Ÿå™ªå£°ç‹¬ç«‹äºä¸ªä½“ç‰¹å¾
- é€‚ç”¨äº"èƒ½åŠ›ç¡®å®šä½†ç»“æœéšæœº"çš„åœºæ™¯

### 3.3 Endogenous Mode (å†…ç”Ÿæ¨¡å¼)

**è®¾è®¡ç›®æ ‡**: çº¯å†…ç”Ÿå› æœæ¨ç†ï¼Œæ— å¤–ç”Ÿéšæœºæ‰°åŠ¨

```mermaid
graph TB
    subgraph Endogenous["Endogenous Mode æ•°å­¦æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥ X"]
        MLP["MLPç‰¹å¾æå–<br/>H = MLP(X)"]
        Abduction["å½’å› æ¨æ–­<br/>U ~ Cauchy(Î¼_U, Î³_U)"]
        NoiseApply["å™ªå£°è®¡ç®—<br/>U' = U + 0 = U"]
        Action["è¡ŒåŠ¨å†³ç­–<br/>ActionNetwork(U')"]
        Activation["ä»»åŠ¡æ¿€æ´»<br/>y = f(S)"]
        Output["è¾“å‡º y"]
        
        Input --> MLP --> Abduction --> NoiseApply --> Action --> Activation --> Output
        
        subgraph Config["é…ç½®è¦æ±‚"]
            direction TB
            C1["b_noise = 0 (æ— å¤–ç”Ÿå™ªå£°)"]
            C2["Î³_U > 0 (ä¿æŒå°ºåº¦å‚æ•°)"]
            C3["ä½¿ç”¨æŸ¯è¥¿CDFæ¿€æ´»"]
            C4["å¯é€‰æ‹©ä¼ ç»Ÿæˆ–æŸ¯è¥¿æŸå¤±"]
        end
    end
    
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef configStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    
    class Input,MLP,Abduction,NoiseApply,Action,Activation,Output endogenousStyle
    class Config,C1,C2,C3,C4 configStyle
```

**æ•°å­¦è¡¨è¿°**:
- **å½’å› **: $U \sim \text{Cauchy}(\mu_U(H), \gamma_U(H))$
- **å™ªå£°è®¡ç®—**: $U' = U + 0 = U$ï¼ˆæ— å¤–ç”Ÿå™ªå£°ï¼‰
- **è¡ŒåŠ¨ç½‘ç»œ**: ActionNetwork æ¥æ”¶ $U'$ ä½œä¸ºè¾“å…¥
- **æ¿€æ´»**: $P_k = \frac{1}{2} + \frac{1}{\pi}\arctan\left(\frac{\text{loc}_{S_k}}{\text{scale}_{S_k}}\right)$ (åˆ†ç±»)

**å…³é”®ç‰¹æ€§**:
- å®Œå…¨ç¡®å®šæ€§çš„å› æœæ¨ç†
- ä¿ç•™æŸ¯è¥¿åˆ†å¸ƒçš„è§£ææ€§è´¨
- é€‚ç”¨äºé«˜ä¸€è‡´æ€§éœ€æ±‚åœºæ™¯

### 3.4 Standard Mode (æ ‡å‡†æ¨¡å¼)

**è®¾è®¡ç›®æ ‡**: å™ªå£°ä½œç”¨äºå°ºåº¦å‚æ•°ï¼Œæ‰©å¤§å†³ç­–ä¸ç¡®å®šæ€§

```mermaid
graph TB
    subgraph Standard["Standard Mode æ•°å­¦æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥ X"]
        MLP["MLPç‰¹å¾æå–<br/>H = MLP(X)"]
        Abduction["å½’å› æ¨æ–­<br/>U ~ Cauchy(Î¼_U, Î³_U)"]
        NoiseScale["å™ªå£°è®¡ç®—<br/>U' = U + b_noise*Îµ"]
        Action["è¡ŒåŠ¨å†³ç­–<br/>ActionNetwork(U')"]
        Note["æ³¨ï¼šå°ºåº¦æ¨¡å¼ä¸‹<br/>å™ªå£°ä½œç”¨äºå°ºåº¦å‚æ•°"]
        Activation["ä»»åŠ¡æ¿€æ´»<br/>y = f(S)"]
        Output["è¾“å‡º y"]
        
        Input --> MLP --> Abduction --> NoiseScale --> Action --> Activation --> Output
        NoiseScale -.-> Note
        
        subgraph Config["é…ç½®è¦æ±‚"]
            direction TB
            C1["b_noise â‰  0 (å¤–ç”Ÿå™ªå£°)"]
            C2["å™ªå£°ä½œç”¨äºå°ºåº¦å‚æ•°"]
            C3["ä¿æŒä¸ªä½“èº«ä»½ä¸å˜"]
            C4["æ··åˆæŸå¤±å‡½æ•°"]
        end
    end
    
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef configStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    
    class Input,MLP,Abduction,NoiseScale,Action,Activation,Output standardStyle
    class Note configStyle
    class Config,C1,C2,C3,C4 configStyle
```

**æ•°å­¦è¡¨è¿°**:
- **ç»Ÿä¸€å…¬å¼**: $U' = U + b_{noise} \varepsilon$ï¼Œå…¶ä¸­ $\varepsilon \sim \text{Cauchy}(0,1)$
- **å°ºåº¦æ¨¡å¼**: å™ªå£°ä½œç”¨äºå°ºåº¦å‚æ•°ï¼Œæ‰©å¤§å†³ç­–ä¸ç¡®å®šæ€§
- **è¡ŒåŠ¨ç½‘ç»œ**: ActionNetwork æ¥æ”¶ $U'$ ä½œä¸ºè¾“å…¥

**å“²å­¦å«ä¹‰**: ç¯å¢ƒå™ªå£°ä½¿ä¸ªä½“å†³ç­–æ›´åŠ æ¨¡ç³Šï¼Œä½†æ ¸å¿ƒèº«ä»½ï¼ˆä½ç½®å‚æ•°ï¼‰ä¿æŒä¸å˜

### 3.5 Sampling Mode (é‡‡æ ·æ¨¡å¼)

**è®¾è®¡ç›®æ ‡**: å™ªå£°ä½œç”¨äºä½ç½®å‚æ•°ï¼Œæ‰°åŠ¨ä¸ªä½“èº«ä»½

```mermaid
graph TB
    subgraph Sampling["Sampling Mode æ•°å­¦æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥ X"]
        MLP["MLPç‰¹å¾æå–<br/>H = MLP(X)"]
        Abduction["å½’å› æ¨æ–­<br/>U ~ Cauchy(Î¼_U, Î³_U)"]
        NoiseSample["å™ªå£°é‡‡æ ·<br/>Îµ ~ Cauchy(0, 1)"]
        NoiseLocation["å™ªå£°è®¡ç®—<br/>U' = U + b_noise*Îµ"]
        Action["è¡ŒåŠ¨å†³ç­–<br/>ActionNetwork(U')"]
        Note2["æ³¨ï¼šé‡‡æ ·æ¨¡å¼ä¸‹<br/>å™ªå£°ä½œç”¨äºä½ç½®å‚æ•°"]
        Activation["ä»»åŠ¡æ¿€æ´»<br/>y = f(S)"]
        Output["è¾“å‡º y"]
        
        Input --> MLP --> Abduction --> NoiseSample --> NoiseLocation --> Action --> Activation --> Output
        NoiseLocation -.-> Note2
        
        subgraph Config["é…ç½®è¦æ±‚"]
            direction TB
            C1["b_noise â‰  0 (å¤–ç”Ÿå™ªå£°)"]
            C2["å™ªå£°ä½œç”¨äºä½ç½®å‚æ•°"]
            C3["éšæœºæ‰°åŠ¨ä¸ªä½“èº«ä»½"]
            C4["æ¢ç´¢æ€§æŸå¤±å‡½æ•°"]
        end
    end
    
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef configStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    
    class Input,MLP,Abduction,NoiseSample,NoiseLocation,Action,Activation,Output samplingStyle
    class Note2 configStyle
    class Config,C1,C2,C3,C4 configStyle
```

**æ•°å­¦è¡¨è¿°**:
- **ç»Ÿä¸€å…¬å¼**: $U' = U + b_{noise} \varepsilon$ï¼Œå…¶ä¸­ $\varepsilon \sim \text{Cauchy}(0,1)$
- **é‡‡æ ·æ¨¡å¼**: å™ªå£°ä½œç”¨äºä½ç½®å‚æ•°ï¼Œæ‰°åŠ¨ä¸ªä½“èº«ä»½
- **è¡ŒåŠ¨ç½‘ç»œ**: ActionNetwork æ¥æ”¶ $U'$ ä½œä¸ºè¾“å…¥

**å“²å­¦å«ä¹‰**: æ¢ç´¢ä¸ªä½“åœ¨éšæœºæ‰°åŠ¨ä¸‹çš„éå…¸å‹è¡Œä¸ºï¼Œç”¨äºå¤šæ ·æ€§ç”Ÿæˆ

---

## 4. æŸå¤±å‡½æ•°è®¾è®¡

### 4.1 modeç›¸å…³çš„æŸå¤±å‡½æ•°ç­–ç•¥

æ¯ç§æ¨¡å¼é‡‡ç”¨æœ€é€‚åˆå…¶å“²å­¦å«ä¹‰çš„æŸå¤±å‡½æ•°ï¼š

```mermaid
graph TB
    subgraph LossStrategy["æŸå¤±å‡½æ•°ç­–ç•¥"]
        direction 
        
        subgraph Deterministic["Deterministic Mode"]
            direction TB
            TL1["å›å½’: MSE Loss"]
            TL2["åˆ†ç±»: CrossEntropy Loss"]
            TL3["å®Œå…¨ç­‰ä»·äºsklearn"]
        end
        
        subgraph Exogenous["Exogenous Mode"]
            direction TB
            NL1["å›å½’: Cauchy NLL"]
            NL2["åˆ†ç±»: OvRæ¦‚ç‡"]
            NL3["å¤–ç”Ÿå™ªå£°å› æœæ¨ç†"]
        end
        
        subgraph Endogenous["Endogenous Mode"]
            direction TB
            CL1["å›å½’: Cauchy NLL"]
            CL2["åˆ†ç±»: OvRæ¦‚ç‡"]
            CL3["å†…ç”Ÿå› æœæ¨ç†"]
        end
        
        subgraph Standard["Standard Mode"]
            direction TB
            SL1["å›å½’: Cauchy NLL"]
            SL2["åˆ†ç±»: OvRæ¦‚ç‡"]
            SL3["æ··åˆå› æœæ¨ç†"]
        end
        
        subgraph Sampling["Sampling Mode"]
            direction TB
            SML1["å›å½’: Cauchy NLL"]
            SML2["åˆ†ç±»: OvRæ¦‚ç‡"]
            SML3["æ¢ç´¢æ€§å› æœæ¨ç†"]
        end
    end
    
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    
    class Deterministic,TL1,TL2,TL3 deterministicStyle
    class Exogenous,NL1,NL2,NL3 exogenousStyle
    class Endogenous,CL1,CL2,CL3 endogenousStyle
    class Standard,SL1,SL2,SL3 standardStyle
    class Sampling,SML1,SML2,SML3 samplingStyle
```

### 4.2 æŸå¤±å‡½æ•°æ•°å­¦å®šä¹‰

#### Deterministic ModeæŸå¤± (ç­‰ä»·æ€§éªŒè¯ç”¨)
```python
def deterministic_loss(predictions, targets, task_type):
    """ä¸ sklearn å®Œå…¨ç­‰ä»·çš„æŸå¤±å‡½æ•°"""
    if task_type == 'regression':
        return F.mse_loss(predictions, targets)
    elif task_type == 'classification':
        return F.cross_entropy(predictions, targets)
```

#### Exogenous/Endogenous/Standard/Sampling Modeç»Ÿä¸€æŸå¤±
```python
def causal_loss(loc_S, scale_S, targets, task_type):
    """æ¨¡å¼2-5çš„ç»Ÿä¸€æŸå¤±å‡½æ•°ï¼ˆå®Œå…¨ç›¸åŒï¼‰"""
    if task_type == 'regression':
        # Cauchyè´Ÿå¯¹æ•°ä¼¼ç„¶
        return -cauchy_log_pdf(targets, loc_S, scale_S).mean()
    elif task_type == 'classification':
        # OvR Cauchyæ¦‚ç‡
        probs = cauchy_cdf(0, loc_S, scale_S)  # P(S > 0)
        return F.binary_cross_entropy(probs, targets_one_hot)
```

#### é‡è¦è¯´æ˜ï¼šæŸå¤±å‡½æ•°ç»Ÿä¸€æ€§

ç”¨æˆ·éœ€è¦æ˜ç¡®çš„æ˜¯ï¼Œ**ç¬¬2ç§æ¨¡å¼åˆ°ç¬¬5ç§æ¨¡å¼ï¼Œå®ƒçš„æŸå¤±å‡½æ•°éƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„**ã€‚å®ƒä»¬çš„åŒºåˆ«ä»…åœ¨äº $U'$ çš„è®¡ç®—æ–¹å¼ï¼š

- **Exogenous Mode**: $U' = \mu_U + b_{noise} \varepsilon$
- **Endogenous Mode**: $U' = U + 0 = U \sim \text{Cauchy}(\mu_U, \gamma_U)$
- **Standard Mode**: $U' = U + b_{noise} \varepsilon$ (å™ªå£°ä½œç”¨äºå°ºåº¦)
- **Sampling Mode**: $U' = U + b_{noise} \varepsilon$ (å™ªå£°ä½œç”¨äºä½ç½®)

ä½†å®ƒä»¬éƒ½ä½¿ç”¨ç›¸åŒçš„ Cauchy NLL / OvR æ¦‚ç‡æŸå¤±å‡½æ•°ï¼Œç„¶åè¡ŒåŠ¨ç½‘ç»œçš„è¾“å…¥æ˜¯ $U'$ã€‚

---

## 5. æ¨ç†ä¸åº”ç”¨ç­–ç•¥

### 5.1 äº”æ¨¡å¼çš„åº”ç”¨åœºæ™¯

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**: æ¯ç§æ¨¡å¼éƒ½æœ‰å…¶ç‰¹å®šçš„åº”ç”¨åœºæ™¯å’Œç†è®ºæ„ä¹‰

```mermaid
graph TB
    subgraph Applications["äº”æ¨¡å¼åº”ç”¨åœºæ™¯"]
        
        subgraph Deterministic["ğŸ¯ Deterministic Mode"]
            direction TB
            D1["ç­‰ä»·æ€§éªŒè¯<br/>ä¸sklearnåŸºçº¿å¯¹æ¯”"]
            D2["è°ƒè¯•ä¸å¼€å‘<br/>ç¡®å®šæ€§è¡Œä¸º"]
            D3["åŸºç¡€åŠŸèƒ½éªŒè¯<br/>ç®—æ³•æ­£ç¡®æ€§æ£€æŸ¥"]
        end
        
        subgraph Exogenous["ğŸŒ Exogenous Mode"]
            direction TB
            E1["å¤–ç”Ÿå†²å‡»å»ºæ¨¡<br/>å¸‚åœºæ³¢åŠ¨ã€è‡ªç„¶ç¾å®³"]
            E2["ç¯å¢ƒå™ªå£°åœºæ™¯<br/>ä¼ æ„Ÿå™¨è¯¯å·®ã€æµ‹é‡å™ªå£°"]
            E3["ç¡®å®šæ€§ä¸ªä½“<br/>ä½†å¤–éƒ¨éšæœºå¹²æ‰°"]
        end
        
        subgraph Endogenous["ğŸ§  Endogenous Mode"]
            direction TB
            EN1["çº¯å› æœæ¨ç†<br/>å†…åœ¨ä¸ç¡®å®šæ€§å»ºæ¨¡"]
            EN2["ä¸ªä½“å·®å¼‚åˆ†æ<br/>è®¤çŸ¥èƒ½åŠ›åˆ†å¸ƒ"]
            EN3["é«˜å¯è§£é‡Šæ€§éœ€æ±‚<br/>åŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§"]
        end
        
        subgraph Standard["âš¡ Standard Mode"]
            direction TB
            S1["ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²<br/>å¹³è¡¡æ€§èƒ½ä¸ç†è®º"]
            S2["ä¸€èˆ¬åº”ç”¨åœºæ™¯<br/>é»˜è®¤æ¨èæ¨¡å¼"]
            S3["æ··åˆå› æœå»ºæ¨¡<br/>å†…ç”Ÿ+å¤–ç”Ÿå¹¶å­˜"]
        end
        
        subgraph Sampling["ğŸ² Sampling Mode"]
            direction TB
            SA1["æ¢ç´¢æ€§æ•°æ®åˆ†æ<br/>å‘ç°éšè—æ¨¡å¼"]
            SA2["å¤šæ ·æ€§ç”Ÿæˆ<br/>åˆ›æ„æ¨èç³»ç»Ÿ"]
            SA3["ç ”ç©¶ä¸å®éªŒ<br/>å› æœå‘ç°"]
        end
    end
    
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    
    class Deterministic,D1,D2,D3 deterministicStyle
    class Exogenous,E1,E2,E3 exogenousStyle
    class Endogenous,EN1,EN2,EN3 endogenousStyle
    class Standard,S1,S2,S3 standardStyle
    class Sampling,SA1,SA2,SA3 samplingStyle
```

### 5.2 æ¨¡å¼é€‰æ‹©å†³ç­–æ ‘

```mermaid
graph TD
    Start(["å¼€å§‹é€‰æ‹©æ¨¡å¼"]) --> Question1{"éœ€è¦ç­‰ä»·æ€§éªŒè¯ï¼Ÿ"}
    
    Question1 -->|æ˜¯| Deterministic["ğŸ¯ Deterministic Mode<br/>ä¸sklearnå®Œå…¨ç­‰ä»·"]
    Question1 -->|å¦| Question2{"ä¸ªä½“è¡¨å¾ç¡®å®šæ€§ï¼Ÿ"}
    
    Question2 -->|å®Œå…¨ç¡®å®š| Question3{"å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ"}
    Question2 -->|æœ‰ä¸ç¡®å®šæ€§| Question4{"å­˜åœ¨å¤–ç”Ÿå™ªå£°ï¼Ÿ"}
    
    Question3 -->|æ˜¯| Exogenous["ğŸŒ Exogenous Mode<br/>ç¡®å®šä¸ªä½“+å¤–ç”Ÿå™ªå£°"]
    Question3 -->|å¦| Deterministic
    
    Question4 -->|å¦| Endogenous["ğŸ§  Endogenous Mode<br/>çº¯å†…ç”Ÿå› æœæ¨ç†"]
    Question4 -->|æ˜¯| Question5{"åº”ç”¨åœºæ™¯ï¼Ÿ"}
    
    Question5 -->|ç”Ÿäº§ç¯å¢ƒ| Standard["âš¡ Standard Mode<br/>å™ªå£°å¢å¼ºä¸ç¡®å®šæ€§"]
    Question5 -->|æ¢ç´¢ç ”ç©¶| Sampling["ğŸ² Sampling Mode<br/>å™ªå£°æ‰°åŠ¨èº«ä»½"]
    
    classDef questionStyle fill:#f9f9f9,stroke:#666,stroke-width:2px
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    classDef standardStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px
    classDef samplingStyle fill:#fff8e1,stroke:#f9a825,stroke-width:3px
    
    class Start,Question1,Question2,Question3,Question4,Question5 questionStyle
    class Deterministic deterministicStyle
    class Exogenous exogenousStyle
    class Endogenous endogenousStyle
    class Standard standardStyle
    class Sampling samplingStyle
```

### 5.3 ç»Ÿä¸€APIè®¾è®¡

```python
class MLPCausalRegressor:
    def __init__(self, mode='standard', **kwargs):
        """
        äº”æ¨¡å¼ç»Ÿä¸€æ¥å£
        
        Parameters:
        -----------
        mode : str, default='standard'
            å»ºæ¨¡æ¨¡å¼é€‰æ‹©ï¼š
            - 'deterministic': Î³_U=0, b_noise=0 (ç­‰ä»·sklearn)
            - 'exogenous': Î³_U=0, b_noiseâ‰ 0 (å¤–ç”Ÿå™ªå£°)
            - 'endogenous': Î³_Uâ‰ 0, b_noise=0 (å†…ç”Ÿå› æœ)
            - 'standard': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’å°ºåº¦)
            - 'sampling': Î³_Uâ‰ 0, b_noiseâ‰ 0 (å™ªå£°â†’ä½ç½®)
        """
        self.mode = mode
        self._configure_mode_parameters()
    
    def _configure_mode_parameters(self):
        """æ ¹æ®æ¨¡å¼é…ç½®å‚æ•°"""
        if self.mode == 'deterministic':
            self.gamma_U_enabled = False
            self.b_noise_enabled = False
            self.loss_type = 'traditional'  # MSE/CrossEntropy
        elif self.mode == 'exogenous':
            self.gamma_U_enabled = False
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode == 'endogenous':
            self.gamma_U_enabled = True
            self.b_noise_enabled = False
            self.loss_type = 'causal'  # Cauchy NLL
        elif self.mode in ['standard', 'sampling']:
            self.gamma_U_enabled = True
            self.b_noise_enabled = True
            self.loss_type = 'causal'  # Cauchy NLL
    
    def predict(self, X, return_uncertainty=False):
        """
        ç»Ÿä¸€é¢„æµ‹æ¥å£
        
        æ ¹æ®è®­ç»ƒæ—¶çš„æ¨¡å¼è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„U'è®¡ç®—æ–¹å¼
        """
        # 1. ç‰¹å¾æå–ï¼ˆæ‰€æœ‰æ¨¡å¼ç»Ÿä¸€ï¼‰
        H = self.mlp_backbone(X)
        
        # 2. ä¸ªä½“æ¨æ–­ï¼ˆæ ¹æ®æ¨¡å¼è°ƒæ•´ï¼‰
        if self.gamma_U_enabled:
            U = self.abduction(H)  # U ~ Cauchy(Î¼_U, Î³_U)
        else:
            U = H  # U = Î¼_U (ç¡®å®šæ€§)
        
        # 3. è®¡ç®—U'ï¼ˆæ ¸å¿ƒå·®å¼‚ï¼‰
        if self.mode == 'deterministic':
            U_prime = U  # U' = Î¼_U
        elif self.mode == 'exogenous':
            epsilon = self._sample_cauchy_noise()
            U_prime = U + self.b_noise * epsilon  # U' ~ Cauchy(Î¼_U, |b_noise|)
        elif self.mode == 'endogenous':
            U_prime = U  # U' = U ~ Cauchy(Î¼_U, Î³_U)
        elif self.mode == 'standard':
            # è§£æåœ°èåˆå™ªå£°åˆ°å°ºåº¦å‚æ•°
            U_prime = U  # ä½†å°ºåº¦å‚æ•°ä¼šåœ¨ActionNetworkä¸­è°ƒæ•´
        elif self.mode == 'sampling':
            epsilon = self._sample_cauchy_noise()
            U_prime = U + self.b_noise * epsilon  # ä½ç½®æ‰°åŠ¨
        
        # 4. è¡ŒåŠ¨å†³ç­–ï¼ˆActionNetworkæ¥æ”¶U'ï¼‰
        predictions = self.action_network(U_prime)
        
        if return_uncertainty:
            return predictions, self._estimate_uncertainty(U_prime)
        return predictions
```

---

## 6. å®éªŒéªŒè¯ä¸åŸºå‡†æµ‹è¯•

### 6.1 ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•è®¾è®¡

**éªŒè¯ç›®æ ‡**: è¯æ˜äº”æ¨¡å¼ç³»ç»Ÿçš„æ•°å­¦æ­£ç¡®æ€§ã€åº”ç”¨æœ‰æ•ˆæ€§å’Œè®¡ç®—æ•ˆç‡

```mermaid
graph TB
    subgraph Benchmark["äº”æ¨¡å¼åŸºå‡†æµ‹è¯•ä½“ç³»"]
        direction TB
        
        subgraph MathValidation["æ•°å­¦éªŒè¯"]
            direction TB
            MV1["ç­‰ä»·æ€§éªŒè¯<br/>Deterministic vs sklearn"]
            MV2["åˆ†å¸ƒæ­£ç¡®æ€§<br/>U'åˆ†å¸ƒè®¡ç®—"]
            MV3["æŸå¤±å‡½æ•°ç»Ÿä¸€æ€§<br/>æ¨¡å¼2-5ä¸€è‡´æ€§"]
        end
        
        subgraph PerformanceTest["æ€§èƒ½æµ‹è¯•"]
            direction TB
            PT1["å›å½’åŸºå‡†<br/>4ä¸ªçœŸå®æ•°æ®é›†"]
            PT2["åˆ†ç±»åŸºå‡†<br/>5ä¸ªçœŸå®æ•°æ®é›†"]
            PT3["å¯è§£é‡Šæ€§å¯¹æ¯”<br/>ä¸ç¡®å®šæ€§é‡åŒ–"]
        end
        
        subgraph EfficiencyTest["æ•ˆç‡æµ‹è¯•"]
            direction TB
            ET1["è®¡ç®—å¤æ‚åº¦<br/>æ—¶é—´ä¸å†…å­˜"]
            ET2["å‚æ•°æ•°é‡å¯¹æ¯”<br/>æ¨¡å‹è§„æ¨¡"]
            ET3["æ”¶æ•›æ€§åˆ†æ<br/>è®­ç»ƒç¨³å®šæ€§"]
        end
        
        subgraph ApplicationTest["åº”ç”¨æµ‹è¯•"]
            direction TB
            AT1["å› æœå‘ç°èƒ½åŠ›<br/>åˆæˆå› æœæ•°æ®"]
            AT2["é²æ£’æ€§æµ‹è¯•<br/>å™ªå£°ç¯å¢ƒé€‚åº”"]
            AT3["æ¨¡å¼é€‰æ‹©æŒ‡å¯¼<br/>åœºæ™¯åŒ¹é…éªŒè¯"]
        end
    end
    
    MathValidation --> PerformanceTest --> EfficiencyTest --> ApplicationTest
    
    classDef mathStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef perfStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef effStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef appStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class MathValidation,MV1,MV2,MV3 mathStyle
    class PerformanceTest,PT1,PT2,PT3 perfStyle
    class EfficiencyTest,ET1,ET2,ET3 effStyle
    class ApplicationTest,AT1,AT2,AT3 appStyle
```

### 6.2 æ ¸å¿ƒéªŒè¯å®éªŒ

#### 6.2.1 æ•°å­¦ç­‰ä»·æ€§éªŒè¯

```python
def test_mathematical_equivalence():
    """éªŒè¯Deterministicæ¨¡å¼ä¸sklearnçš„æ•°å­¦ç­‰ä»·æ€§"""
    
    # æ•°æ®å‡†å¤‡
    X, y = make_regression(n_samples=500, n_features=10, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # sklearnåŸºçº¿
    sklearn_model = MLPRegressor(
        hidden_layer_sizes=(64, 32),
        random_state=42,
        max_iter=500,
        alpha=0.0
    )
    sklearn_model.fit(X_train, y_train)
    sklearn_pred = sklearn_model.predict(X_test)
    sklearn_r2 = r2_score(y_test, sklearn_pred)
    
    # CausalEngine Deterministicæ¨¡å¼
    causal_model = MLPCausalRegressor(
        mode='deterministic',
        hidden_layer_sizes=(64, 32),
        random_state=42
    )
    # å…³é”®ï¼šé…ç½®ç­‰ä»·æ€§å‚æ•°
    causal_model._setup_mathematical_equivalence()
    causal_model.fit(X_train, y_train)
    causal_pred = causal_model.predict(X_test)
    causal_r2 = r2_score(y_test, causal_pred)
    
    # ç­‰ä»·æ€§æ£€éªŒ
    r2_diff = abs(sklearn_r2 - causal_r2)
    pred_mse = mean_squared_error(sklearn_pred, causal_pred)
    
    print(f"sklearn RÂ²: {sklearn_r2:.6f}")
    print(f"CausalEngine RÂ²: {causal_r2:.6f}")
    print(f"RÂ²å·®å¼‚: {r2_diff:.6f} (<0.001: âœ“)")
    print(f"é¢„æµ‹å·®å¼‚MSE: {pred_mse:.6f} (<0.001: âœ“)")
    
    assert r2_diff < 0.001, f"ç­‰ä»·æ€§éªŒè¯å¤±è´¥: RÂ²å·®å¼‚ {r2_diff}"
    assert pred_mse < 0.001, f"ç­‰ä»·æ€§éªŒè¯å¤±è´¥: é¢„æµ‹å·®å¼‚ {pred_mse}"
    
    return True

def test_distribution_correctness():
    """éªŒè¯U'åˆ†å¸ƒè®¡ç®—çš„æ•°å­¦æ­£ç¡®æ€§"""
    
    # æ¨¡æ‹Ÿå‚æ•°
    mu_U = torch.tensor([1.0, -0.5, 2.0])
    gamma_U = torch.tensor([0.5, 1.0, 0.3])
    b_noise = torch.tensor(0.2)
    
    # æµ‹è¯•å„æ¨¡å¼çš„U'åˆ†å¸ƒ
    test_cases = {
        'deterministic': {
            'gamma_U': 0, 'b_noise': 0,
            'expected_loc': mu_U, 'expected_scale': torch.zeros_like(mu_U)
        },
        'exogenous': {
            'gamma_U': 0, 'b_noise': b_noise,
            'expected_loc': mu_U, 'expected_scale': torch.full_like(mu_U, abs(b_noise))
        },
        'endogenous': {
            'gamma_U': gamma_U, 'b_noise': 0,
            'expected_loc': mu_U, 'expected_scale': gamma_U
        },
        'standard': {
            'gamma_U': gamma_U, 'b_noise': b_noise,
            'expected_loc': mu_U, 'expected_scale': gamma_U + abs(b_noise)
        }
    }
    
    for mode, params in test_cases.items():
        loc_U_prime, scale_U_prime = compute_U_prime_distribution(
            mu_U, params['gamma_U'], params['b_noise'], mode
        )
        
        loc_close = torch.allclose(loc_U_prime, params['expected_loc'], atol=1e-6)
        scale_close = torch.allclose(scale_U_prime, params['expected_scale'], atol=1e-6)
        
        print(f"{mode} æ¨¡å¼ U' åˆ†å¸ƒ: ä½ç½®å‚æ•°âœ“={loc_close}, å°ºåº¦å‚æ•°âœ“={scale_close}")
        
        assert loc_close and scale_close, f"{mode}æ¨¡å¼ U'åˆ†å¸ƒè®¡ç®—é”™è¯¯"
    
    return True

def test_loss_function_unity():
    """éªŒè¯æ¨¡å¼2-5ä½¿ç”¨ç›¸åŒçš„æŸå¤±å‡½æ•°"""
    
    # æ¨¡æ‹Ÿæ•°æ®
    loc_S = torch.randn(10, 5)
    scale_S = torch.abs(torch.randn(10, 5)) + 0.1
    targets = torch.randint(0, 5, (10,))
    
    # è®¡ç®—æ¨¡å¼2-5çš„æŸå¤±
    losses = {}
    for mode in ['exogenous', 'endogenous', 'standard', 'sampling']:
        loss = compute_causal_loss(loc_S, scale_S, targets, mode)
        losses[mode] = loss
    
    # éªŒè¯æŸå¤±å‡½æ•°å®Œå…¨ç›¸åŒ
    base_loss = losses['exogenous']
    for mode, loss in losses.items():
        if mode != 'exogenous':
            assert torch.allclose(loss, base_loss, atol=1e-8), f"{mode}æ¨¡å¼æŸå¤±ä¸åŸºå‡†ä¸ä¸€è‡´"
    
    print("âœ“ æ¨¡å¼2-5æŸå¤±å‡½æ•°ç»Ÿä¸€æ€§éªŒè¯é€šè¿‡")
    return True
```

#### 6.2.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def benchmark_five_modes():
    """äº”æ¨¡å¼ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    # çœŸå®æ•°æ®é›†
    regression_datasets = [
        load_boston(), load_diabetes(), 
        load_california_housing(), make_regression(n_samples=1000, n_features=20)
    ]
    
    classification_datasets = [
        load_iris(), load_wine(), load_breast_cancer(),
        make_classification(n_samples=1000, n_features=20, n_classes=3)
    ]
    
    results = {'regression': {}, 'classification': {}}
    
    # å›å½’åŸºå‡†æµ‹è¯•
    for i, (X, y) in enumerate(regression_datasets):
        print(f"\n=== å›å½’æ•°æ®é›† {i+1} ===")
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        dataset_results = {}
        for mode in ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']:
            model = MLPCausalRegressor(mode=mode, random_state=42)
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            
            r2 = r2_score(y_test, pred)
            rmse = np.sqrt(mean_squared_error(y_test, pred))
            
            dataset_results[mode] = {'r2': r2, 'rmse': rmse}
            print(f"{mode:12s}: RÂ²={r2:.4f}, RMSE={rmse:.4f}")
        
        results['regression'][f'dataset_{i+1}'] = dataset_results
    
    # åˆ†ç±»åŸºå‡†æµ‹è¯• (ç±»ä¼¼ç»“æ„)
    # ...
    
    return results
```

---

## 7. æ€»ç»“ä¸å®è·µæŒ‡å—

### 7.1 åº”ç”¨åœºæ™¯ä¸æ¨¡å¼é€‰æ‹©

#### 7.1.1 æŒ‰åº”ç”¨éœ€æ±‚é€‰æ‹©

```mermaid
graph LR
    subgraph Scenarios["åº”ç”¨åœºæ™¯åˆ†ç±»"]
        direction TB
        
        subgraph Development["ğŸ”§ å¼€å‘ä¸éªŒè¯"]
            D1["åŸºçº¿å¯¹æ¯” â†’ Deterministic"]
            D2["ç®—æ³•éªŒè¯ â†’ Deterministic"]
            D3["åŠŸèƒ½è°ƒè¯• â†’ Deterministic"]
        end
        
        subgraph Production["ğŸ¢ ç”Ÿäº§éƒ¨ç½²"]
            P1["é€šç”¨åº”ç”¨ â†’ Standard"]
            P2["é«˜å¯è§£é‡Šæ€§ â†’ Endogenous"]
            P3["åŒ»ç–—é‡‘è â†’ Endogenous"]
        end
        
        subgraph Research["ğŸ”¬ ç ”ç©¶åˆ†æ"]
            R1["å› æœå‘ç° â†’ Sampling"]
            R2["æ¢ç´¢æ€§åˆ†æ â†’ Sampling"]
            R3["å¤šæ ·æ€§ç”Ÿæˆ â†’ Sampling"]
        end
        
        subgraph Environment["ğŸŒ ç‰¹æ®Šç¯å¢ƒ"]
            E1["å¤–ç”Ÿå†²å‡» â†’ Exogenous"]
            E2["å™ªå£°ç¯å¢ƒ â†’ Exogenous"]
            E3["ä¼ æ„Ÿå™¨è¯¯å·® â†’ Exogenous"]
        end
    end
    
    classDef devStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef prodStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef resStyle fill:#fff8e1,stroke:#f9a825,stroke-width:2px
    classDef envStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    
    class Development,D1,D2,D3 devStyle
    class Production,P1,P2,P3 prodStyle
    class Research,R1,R2,R3 resStyle
    class Environment,E1,E2,E3 envStyle
```

#### 7.1.2 æŒ‰æ•°æ®ç‰¹æ€§é€‰æ‹©

| æ•°æ®ç‰¹æ€§ | æ¨èæ¨¡å¼ | æ•°å­¦åŸç† | é€‚ç”¨æƒ…å†µ |
|------------|----------|----------|----------|
| **å®Œå…¨ç¡®å®šæ€§** | Deterministic | $U' = \mu_U$ | æ— å™ªå£°ã€é«˜è´¨é‡æ•°æ® |
| **ä¸ªä½“ç¡®å®š+å¤–éƒ¨å™ªå£°** | Exogenous | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ | ä¼ æ„Ÿå™¨æ•°æ®ã€å¸‚åœºæ³¢åŠ¨ |
| **ä¸ªä½“ä¸ç¡®å®šæ€§** | Endogenous | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ | è®¤çŸ¥å·®å¼‚ã€ä¸ªæ€§åŒ– |
| **æ··åˆä¸ç¡®å®šæ€§** | Standard | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ | å¸¸è§åº”ç”¨åœºæ™¯ |
| **æ¢ç´¢æ€§éœ€æ±‚** | Sampling | $U' \sim \text{Cauchy}(\mu_U + b_{noise}\varepsilon, \gamma_U)$ | åˆ›æ„ä»»åŠ¡ã€ç ”ç©¶åˆ†æ |

### 7.2 å®è·µå¼€å‘æµç¨‹

#### 7.2.1 æ¸è¿›å¼å¼€å‘è·¯å¾„

```mermaid
graph TD
    Start(["é¡¹ç›®å¼€å§‹"]) --> Phase1["ğŸ¯ é˜¶æ®µ1: åŸºçº¿éªŒè¯<br/>Deterministic Mode"]
    
    Phase1 --> Check1{"åŸºç¡€åŠŸèƒ½æ­£å¸¸ï¼Ÿ"}
    Check1 -->|å¦| Debug1["è°ƒè¯•ä¿®å¤"]
    Debug1 --> Phase1
    
    Check1 -->|æ˜¯| Phase2["ğŸŒ é˜¶æ®µ2: ç¯å¢ƒé€‚åº”<br/>Exogenous/Endogenous"]
    
    Phase2 --> Check2{"é€‚åº”æ€§æ»¡è¶³ï¼Ÿ"}
    Check2 -->|å¦| Tune2["æ¨¡å¼è°ƒæ•´"]
    Tune2 --> Phase2
    
    Check2 -->|æ˜¯| Phase3["âš¡ é˜¶æ®µ3: æ€§èƒ½ä¼˜åŒ–<br/>Standard Mode"]
    
    Phase3 --> Check3{"æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ï¼Ÿ"}
    Check3 -->|å¦| Optimize3["å‚æ•°ä¼˜åŒ–"]
    Optimize3 --> Phase3
    
    Check3 -->|æ˜¯| Phase4["ğŸ² é˜¶æ®µ4: æ¢ç´¢æ‰©å±•<br/>Sampling Mode"]
    
    Phase4 --> Deploy["ğŸš€ ç”Ÿäº§éƒ¨ç½²"]
    
    classDef phaseStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef checkStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef actionStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    
    class Phase1,Phase2,Phase3,Phase4 phaseStyle
    class Check1,Check2,Check3 checkStyle
    class Debug1,Tune2,Optimize3 actionStyle
    class Start,Deploy actionStyle
```

#### 7.2.2 å…³é”®å®è·µåŸåˆ™

1. **å§‹ç»ˆä» Deterministic å¼€å§‹**: ç¡®ä¿ç®—æ³•æ­£ç¡®æ€§åå†æ·»åŠ å¤æ‚æ€§
2. **æ•°å­¦ç­‰ä»·æ€§éªŒè¯**: ä½¿ç”¨ sklearn åŸºçº¿å¯¹æ¯”éªŒè¯å®ç°æ­£ç¡®æ€§
3. **æŸå¤±å‡½æ•°ç»Ÿä¸€**: æ¨¡å¼ 2-5 å¿…é¡»ä½¿ç”¨ç›¸åŒçš„ Cauchy NLL æŸå¤±
4. **æ¸è¿›å¼å¤æ‚åŒ–**: é€æ­¥å¼•å…¥ä¸ç¡®å®šæ€§å’Œå™ªå£°æœºåˆ¶
5. **å……åˆ†æµ‹è¯•éªŒè¯**: æ¯ä¸ªæ¨¡å¼éƒ½éœ€è¦ç‹¬ç«‹éªŒè¯æ•°å­¦æ­£ç¡®æ€§

### 7.3 æŠ€æœ¯å‚è€ƒå¯¹ç…§è¡¨

#### 7.3.1 å®Œæ•´æ•°å­¦å®šä¹‰

| æ¨¡å¼ | å‚æ•°è®¾ç½® | $U'$ åˆ†å¸ƒ | æŸå¤±å‡½æ•° | å®ç°ç‰¹ç‚¹ |
|------|----------|-----------|----------|----------|
| **Deterministic** | $\gamma_U=0, b_{noise}=0$ | $U' = \mu_U$ (ç¡®å®šæ€§) | MSE/CrossEntropy | ç­‰ä»·sklearnï¼Œç¦ç”¨AbductionNetwork |
| **Exogenous** | $\gamma_U=0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U, \|b_{noise}\|)$ | Cauchy NLL | ç¡®å®šæ€§ä¸ªä½“ï¼Œå¤–ç”Ÿå™ªå£°é‡‡æ · |
| **Endogenous** | $\gamma_U \neq 0, b_{noise}=0$ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U)$ | Cauchy NLL | çº¯å†…ç”Ÿå› æœï¼Œæ— å¤–éƒ¨å™ªå£° |
| **Standard** | $\gamma_U \neq 0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U, \gamma_U + \|b_{noise}\|)$ | Cauchy NLL | å™ªå£°èåˆåˆ°å°ºåº¦ï¼Œè§£æè®¡ç®— |
| **Sampling** | $\gamma_U \neq 0, b_{noise} \neq 0$ | $U' \sim \text{Cauchy}(\mu_U + b_{noise}\varepsilon, \gamma_U)$ | Cauchy NLL | å™ªå£°æ‰°åŠ¨ä½ç½®ï¼Œé‡‡æ ·è®¡ç®— |

#### 7.3.2 API å‚æ•°å¯¹ç…§

```python
# æ ¸å¿ƒæ´å¯Ÿï¼šäº”æ¨¡å¼æœ¬è´¨ä¸Šéƒ½æ˜¯ ActionNetwork çš„ä¸åŒè®¡ç®—æ–¹å¼ï¼
class ActionNetwork(nn.Module):
    def forward(self, loc_U, scale_U, mode='standard', temperature=1.0):
        """
        äº”æ¨¡å¼çš„å·®å¼‚å°±åœ¨è¿™é‡Œï¼šActionNetwork å¦‚ä½•å¤„ç†è¾“å…¥çš„ (loc_U, scale_U)
        """
        # æ­¥éª¤1: æ ¹æ®æ¨¡å¼è®¡ç®— U' çš„åˆ†å¸ƒå‚æ•°
        if mode == 'deterministic':
            # U' = Î¼_U (ç¡®å®šæ€§)
            loc_U_final = loc_U
            scale_U_final = torch.zeros_like(scale_U)
        
        elif mode == 'exogenous':
            # U' ~ Cauchy(Î¼_U, |b_noise|)
            loc_U_final = loc_U
            scale_U_final = torch.full_like(scale_U, abs(self.b_noise))
        
        elif mode == 'endogenous':
            # U' ~ Cauchy(Î¼_U, Î³_U)
            loc_U_final = loc_U
            scale_U_final = scale_U
        
        elif mode == 'standard':
            # U' ~ Cauchy(Î¼_U, Î³_U + |b_noise|) - è§£æèåˆ
            loc_U_final = loc_U
            scale_U_final = scale_U + temperature * abs(self.b_noise)
        
        elif mode == 'sampling':
            # U' ~ Cauchy(Î¼_U + b_noise*Îµ, Î³_U) - ä½ç½®æ‰°åŠ¨
            epsilon = torch.tan(torch.pi * (torch.rand_like(loc_U) - 0.5))
            loc_U_final = loc_U + temperature * self.b_noise * epsilon
            scale_U_final = scale_U
        
        # æ­¥éª¤2: çº¿æ€§å˜æ¢ (ActionNetwork çš„æ ¸å¿ƒåŠŸèƒ½)
        # åˆ©ç”¨æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§ç¨³å®šæ€§ï¼šY = WX + b
        loc_S = self.lm_head(loc_U_final)  # W * Î¼ + b
        scale_S = scale_U_final @ torch.abs(self.lm_head.weight).T  # |W| * Î³
        
        return loc_S, scale_S

# æ€»ç»“ï¼šäº”æ¨¡å¼çš„ç»Ÿä¸€æµç¨‹
def unified_causal_pipeline(X, mode='standard'):
    """
    æ ¸å¿ƒè®¤çŸ¥ï¼šäº”æ¨¡å¼æœ¬è´¨ä¸Šéƒ½æ˜¯ ActionNetwork çš„ä¸åŒè®¡ç®—æ–¹å¼ï¼
    
    ç»Ÿä¸€æµç¨‹ï¼š
    1. AbductionNetwork: X â†’ (loc_U, scale_U)
    2. ActionNetwork(æ¨¡å¼å·®å¼‚): (loc_U, scale_U) â†’ (loc_S, scale_S)
    3. æŸå¤±è®¡ç®—: ç»Ÿä¸€çš„ Cauchy NLL (é™¤äº† Deterministic)
    """
    # æ­¥éª¤1: ä¸ªä½“æ¨æ–­ (æ‰€æœ‰æ¨¡å¼ç›¸åŒ)
    loc_U, scale_U = abduction_network(X)
    
    # æ­¥éª¤2: è¡ŒåŠ¨å†³ç­– (æ¨¡å¼å·®å¼‚çš„æ ¸å¿ƒ)
    loc_S, scale_S = action_network(loc_U, scale_U, mode=mode)
    
    # æ­¥éª¤3: æŸå¤±è®¡ç®— (ç®€å•äºŒé€‰ä¸€)
    if mode == 'deterministic':
        loss = mse_loss(loc_S, targets)  # ä¼ ç»ŸæŸå¤±
    else:
        loss = cauchy_nll_loss(loc_S, scale_S, targets)  # ç»Ÿä¸€å› æœæŸå¤±
    
    return predictions, loss
```

#### é‡è¦è®¤çŸ¥

äº”ç§æ¨¡å¼çš„æœ¬è´¨å·®å¼‚å°±æ˜¯ **ActionNetwork çš„è®¡ç®—æ–¹å¼ä¸åŒ**ï¼

- **AbductionNetwork**: æ‰€æœ‰æ¨¡å¼å®Œå…¨ç›¸åŒ
- **ActionNetwork**: æ¨¡å¼å·®å¼‚çš„æ ¸å¿ƒæ‰€åœ¨ï¼Œå¦‚ä½•ä» (loc_U, scale_U) è®¡ç®— (loc_S, scale_S)
- **æŸå¤±è®¡ç®—**: åªæœ‰ Deterministic vs å…¶ä»–æ¨¡å¼çš„åŒºåˆ«

### 7.4 ç³»ç»Ÿè®¾è®¡å“²å­¦

#### 7.4.1 å‚æ•°ç©ºé—´å®Œå¤‡æ€§

äº”æ¨¡å¼ç³»ç»Ÿè¦†ç›–äº† $(\gamma_U, b_{noise})$ å‚æ•°ç©ºé—´çš„æ‰€æœ‰æœ‰æ„ä¹‰ç»„åˆï¼š

```mermaid
graph TB
    subgraph ParameterSpace["å‚æ•°ç©ºé—´å®Œå¤‡æ€§ (Î³_U, b_noise)"]
        direction TB
        
        subgraph Q1["Î³_U = 0, b_noise = 0"]
            D["ğŸ¯ Deterministic<br/>ç­‰ä»·sklearn MLP"]
        end
        
        subgraph Q2["Î³_U = 0, b_noise â‰  0"]
            E["ğŸŒ Exogenous<br/>ç¡®å®šä¸ªä½“+å¤–ç”Ÿå™ªå£°"]
        end
        
        subgraph Q3["Î³_U â‰  0, b_noise = 0"]
            EN["ğŸ§  Endogenous<br/>çº¯å†…ç”Ÿå› æœæ¨ç†"]
        end
        
        subgraph Q4["Î³_U â‰  0, b_noise â‰  0"]
            direction LR
            S["âš¡ Standard<br/>å™ªå£°â†’å°ºåº¦"]
            SA["ğŸ² Sampling<br/>å™ªå£°â†’ä½ç½®"]
        end
    end
    
    classDef deterministicStyle fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef exogenousStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef endogenousStyle fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef mixedStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class D deterministicStyle
    class E exogenousStyle
    class EN endogenousStyle
    class S,SA mixedStyle
```

#### 7.4.2 æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **æ•°å­¦ä¸¥è°¨æ€§**: æ¯ç§æ¨¡å¼éƒ½æœ‰æ˜ç¡®çš„æ•°å­¦å®šä¹‰å’Œç†è®ºåŸºç¡€
2. **å®ç°ç»Ÿä¸€æ€§**: æ‰€æœ‰æ¨¡å¼å…±äº«ç›¸åŒçš„æ ¸å¿ƒæ¶æ„å’Œç»Ÿä¸€å…¬å¼
3. **æŸå¤±å‡½æ•°ä¸€è‡´æ€§**: æ¨¡å¼2-5ä½¿ç”¨ç›¸åŒçš„Cauchy NLLæŸå¤±
4. **å‚æ•°ç©ºé—´å®Œå¤‡æ€§**: è¦†ç›–æ‰€æœ‰æœ‰æ„ä¹‰çš„$(\gamma_U, b_{noise})$ç»„åˆ
5. **åº”ç”¨åœºæ™¯äº’è¡¥æ€§**: æ¯ç§æ¨¡å¼éƒ½æœ‰æ˜ç¡®çš„åº”ç”¨é¢†åŸŸ

#### 7.4.3 ç†è®ºæ„ä¹‰

- **è½´1 ($\gamma_U$)**: ä¸ªä½“å†…åœ¨ä¸ç¡®å®šæ€§ï¼Œä½“ç°è®¤çŸ¥å·®å¼‚å’Œå†³ç­–æ¨¡ç³Šæ€§
- **è½´2 ($b_{noise}$)**: å¤–éƒ¨ç¯å¢ƒå™ªå£°ï¼Œä½“ç°ä¸å¯æ§çš„å¤–ç”Ÿå› ç´ 
- **äº¤äº’æ•ˆåº”**: Standardå’ŒSamplingæ¨¡å¼åœ¨Q4è±¡é™ä¸­æä¾›ä¸åŒçš„æ··åˆç­–ç•¥
- **å› æœå¯è§£é‡Šæ€§**: æ¯ç§æ¨¡å¼éƒ½æœ‰æ˜ç¡®çš„å› æœè§£é‡Šå’Œç°å®æ„ä¹‰

### 7.5 æœªæ¥æ‰©å±•ä¸ç ”ç©¶æ–¹å‘

...
#!/usr/bin/env python3
"""
ğŸ  æ‰©å±•ç‰ˆçœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ï¼šåŠ å·æˆ¿ä»·é¢„æµ‹ - Sklearn-Styleç‰ˆæœ¬
============================================================

è¿™ä¸ªæ•™ç¨‹æ¼”ç¤ºCausalEngineä¸å¤šç§å¼ºåŠ›ä¼ ç»Ÿæ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„æ€§èƒ½å¯¹æ¯”ã€‚

æ•°æ®é›†ï¼šåŠ å·æˆ¿ä»·æ•°æ®é›†ï¼ˆCalifornia Housing Datasetï¼‰
- 20,640ä¸ªæ ·æœ¬
- 8ä¸ªç‰¹å¾ï¼ˆæˆ¿å±‹å¹´é¾„ã€æ”¶å…¥ã€äººå£ç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹æˆ¿ä»·ä¸­ä½æ•°

æˆ‘ä»¬å°†æ¯”è¾ƒ13ç§æ–¹æ³•ï¼š
1. sklearn MLPRegressorï¼ˆä¼ ç»Ÿç¥ç»ç½‘ç»œï¼‰
2. PyTorch MLPï¼ˆä¼ ç»Ÿæ·±åº¦å­¦ä¹ ï¼‰
3. MLP Huberï¼ˆHuberæŸå¤±ç¨³å¥å›å½’ï¼‰
4. MLP Pinballï¼ˆPinballæŸå¤±ç¨³å¥å›å½’ï¼‰
5. MLP Cauchyï¼ˆCauchyæŸå¤±ç¨³å¥å›å½’ï¼‰
6. Random Forestï¼ˆéšæœºæ£®æ—ï¼‰
7. XGBoostï¼ˆæ¢¯åº¦æå‡ï¼‰
8. LightGBMï¼ˆè½»é‡æ¢¯åº¦æå‡ï¼‰
9. CatBoostï¼ˆå¼ºåŠ›æ¢¯åº¦æå‡ï¼‰
10. CausalEngine - deterministicï¼ˆç¡®å®šæ€§ï¼‰
11. CausalEngine - exogenousï¼ˆå¤–ç”Ÿå™ªå£°ä¸»å¯¼ï¼‰
12. CausalEngine - endogenousï¼ˆå†…ç”Ÿä¸ç¡®å®šæ€§ä¸»å¯¼ï¼‰
13. CausalEngine - standardï¼ˆå†…ç”Ÿ+å¤–ç”Ÿæ··åˆï¼‰

å…³é”®äº®ç‚¹ï¼š
- çœŸå®ä¸–ç•Œæ•°æ®çš„é²æ£’æ€§æµ‹è¯•
- 6ç§å¼ºåŠ›ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ¯”
- 3ç§ç¨³å¥ç¥ç»ç½‘ç»œå›å½’æ–¹æ³•ï¼ˆHuberã€Pinballã€Cauchyï¼‰
- 4ç§CausalEngineæ¨¡å¼å®Œæ•´å¯¹æ¯”
- ç»Ÿä¸€ç¥ç»ç½‘ç»œå‚æ•°é…ç½®ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
- å› æœæ¨ç†vsä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½å·®å¼‚åˆ†æ
- ä½¿ç”¨sklearn-style regressorå®ç°

å®éªŒè®¾è®¡è¯´æ˜
==================================================================
æœ¬è„šæœ¬åŒ…å«ä¸¤ç»„æ ¸å¿ƒå®éªŒï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°CausalEngineåœ¨çœŸå®å›å½’ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚
æ‰€æœ‰å®éªŒå‚æ•°å‡å¯åœ¨ä¸‹æ–¹çš„ `TutorialConfig` ç±»ä¸­è¿›è¡Œä¿®æ”¹ã€‚

å®éªŒä¸€ï¼šæ ¸å¿ƒæ€§èƒ½å¯¹æ¯” (åœ¨40%æ ‡ç­¾å™ªå£°ä¸‹)
--------------------------------------------------
- **ç›®æ ‡**: æ¯”è¾ƒCausalEngineå’Œ9ç§ä¼ ç»Ÿæ–¹æ³•åœ¨å«æœ‰å›ºå®šå™ªå£°æ•°æ®ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚
- **è®¾ç½®**: é»˜è®¤è®¾ç½®40%çš„æ ‡ç­¾å™ªå£°ï¼ˆ`ANOMALY_RATIO = 0.4`ï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­å¸¸è§çš„æ•°æ®è´¨é‡é—®é¢˜ã€‚
- **å¯¹æ¯”æ¨¡å‹**: 
  - ä¼ ç»Ÿæ–¹æ³•: sklearn MLP, PyTorch MLP, MLP Huber, MLP Pinball, MLP Cauchy, Random Forest, XGBoost, LightGBM, CatBoost
  - CausalEngine: deterministic, exogenous, endogenous, standardç­‰æ¨¡å¼

å®éªŒäºŒï¼šé²æ£’æ€§åˆ†æ (è·¨è¶Šä¸åŒå™ªå£°æ°´å¹³)
--------------------------------------------------
- **ç›®æ ‡**: æ¢ç©¶æ¨¡å‹æ€§èƒ½éšæ ‡ç­¾å™ªå£°æ°´å¹³å¢åŠ æ—¶çš„å˜åŒ–æƒ…å†µï¼Œè¯„ä¼°å…¶ç¨³å®šæ€§ã€‚
- **è®¾ç½®**: åœ¨ä¸€ç³»åˆ—å™ªå£°æ¯”ä¾‹ï¼ˆå¦‚0%, 10%, 20%, 30%, 40%, 50%ï¼‰ä¸‹åˆ†åˆ«è¿è¡Œæµ‹è¯•ã€‚
- **å¯¹æ¯”æ¨¡å‹**: æ‰€æœ‰13ç§æ–¹æ³•åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡¨ç°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
import warnings
import os
import sys
import time

# è®¾ç½®matplotlibåç«¯ä¸ºéäº¤äº’å¼ï¼Œé¿å…å¼¹å‡ºçª—å£
plt.switch_backend('Agg')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥sklearn-styleå®ç°
from causal_sklearn.regressor import (
    MLPCausalRegressor, MLPPytorchRegressor, 
    MLPHuberRegressor, MLPPinballRegressor, MLPCauchyRegressor
)
from causal_sklearn.data_processing import inject_shuffle_noise

# å¯¼å…¥æ ‘æ¨¡å‹ - å¤„ç†å¯èƒ½çš„å¯¼å…¥é”™è¯¯
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False

warnings.filterwarnings('ignore')


class TutorialConfig:
    """
    æ‰©å±•æ•™ç¨‹é…ç½®ç±» - æ–¹ä¾¿è°ƒæ•´å„ç§å‚æ•°
    
    ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°æ¥è‡ªå®šä¹‰å®éªŒè®¾ç½®ï¼
    """
    
    # ğŸ¯ æ•°æ®åˆ†å‰²å‚æ•°
    TEST_SIZE = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹
    VAL_SIZE = 0.2           # éªŒè¯é›†æ¯”ä¾‹ (ç›¸å¯¹äºè®­ç»ƒé›†)
    RANDOM_STATE = 42        # éšæœºç§å­
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•ä½¿ç”¨ç›¸åŒå‚æ•°ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
    # =========================================================================
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ‰€æœ‰ç¥ç»ç½‘ç»œæ–¹æ³•çš„å…±åŒå‚æ•°ï¼
    NN_HIDDEN_SIZES = (128, 64, 32)                 # ç¥ç»ç½‘ç»œéšè—å±‚ç»“æ„
    NN_MAX_EPOCHS = 3000                            # æœ€å¤§è®­ç»ƒè½®æ•°
    NN_LEARNING_RATE = 0.01                         # å­¦ä¹ ç‡
    NN_PATIENCE = 50                                # æ—©åœpatience
    NN_TOLERANCE = 1e-4                             # æ—©åœtolerance
    # =========================================================================
    
    # ğŸ¤– CausalEngineå‚æ•° - ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MODES = ['deterministic', 'exogenous', 'endogenous', 'standard']  # å¯é€‰: ['deterministic', 'exogenous', 'endogenous', 'standard']
    CAUSAL_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_MAX_EPOCHS = NN_MAX_EPOCHS              # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_TOL = NN_TOLERANCE                      # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    CAUSAL_GAMMA_INIT = 1.0                      # gammaåˆå§‹åŒ–
    CAUSAL_B_NOISE_INIT = 1.0                    # b_noiseåˆå§‹åŒ–
    CAUSAL_B_NOISE_TRAINABLE = True              # b_noiseæ˜¯å¦å¯è®­ç»ƒ
    
    # ğŸ§  ä¼ ç»Ÿç¥ç»ç½‘ç»œæ–¹æ³•å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    SKLEARN_HIDDEN_LAYERS = NN_HIDDEN_SIZES         # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_MAX_ITER = NN_MAX_EPOCHS                # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    SKLEARN_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    PYTORCH_HIDDEN_SIZES = NN_HIDDEN_SIZES          # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_EPOCHS = NN_MAX_EPOCHS                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_LR = NN_LEARNING_RATE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    PYTORCH_PATIENCE = NN_PATIENCE                  # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸ¯ åŸºå‡†æ–¹æ³•é…ç½® - æ‰©å±•ç‰ˆåŒ…å«æ›´å¤šå¼ºåŠ›æ–¹æ³•
    BASELINE_METHODS = [
        'sklearn_mlp',       # sklearnç¥ç»ç½‘ç»œ  
        'pytorch_mlp',       # PyTorchç¥ç»ç½‘ç»œ
        'mlp_huber',         # HuberæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_pinball_median',# PinballæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'mlp_cauchy',        # CauchyæŸå¤±MLPï¼ˆç¨³å¥å›å½’ï¼‰
        'random_forest',     # éšæœºæ£®æ—
        'xgboost',           # XGBoost - å¼ºåŠ›æ¢¯åº¦æå‡
        'lightgbm',          # LightGBM - è½»é‡æ¢¯åº¦æå‡
        'catboost'           # CatBoost - å¼ºåŠ›æ¢¯åº¦æå‡
    ]
    
    # ğŸ›¡ï¸ ç¨³å¥å›å½’å™¨å‚æ•° - ä½¿ç”¨ç»Ÿä¸€é…ç½®
    ROBUST_HIDDEN_SIZES = NN_HIDDEN_SIZES           # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_MAX_EPOCHS = NN_MAX_EPOCHS               # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_LR = NN_LEARNING_RATE                    # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    ROBUST_PATIENCE = NN_PATIENCE                   # ä½¿ç”¨ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½®
    
    # ğŸŒ² æ ‘æ¨¡å‹å‚æ•°
    TREE_N_ESTIMATORS = 100                         # æ ‘çš„æ•°é‡
    TREE_RANDOM_STATE = RANDOM_STATE                # éšæœºç§å­
    TREE_MAX_DEPTH = None                           # æœ€å¤§æ·±åº¦ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    
    # XGBoost å‚æ•°
    XGBOOST_N_ESTIMATORS = 100
    XGBOOST_LEARNING_RATE = 0.1
    XGBOOST_MAX_DEPTH = 6
    
    # LightGBM å‚æ•°
    LIGHTGBM_N_ESTIMATORS = 100
    LIGHTGBM_LEARNING_RATE = 0.1
    LIGHTGBM_MAX_DEPTH = -1
    
    # CatBoost å‚æ•°
    CATBOOST_ITERATIONS = 100
    CATBOOST_LEARNING_RATE = 0.1
    CATBOOST_DEPTH = 6
    
    # ğŸ“Š å®éªŒå‚æ•°
    ANOMALY_RATIO = 0.4                          # æ ‡ç­¾å¼‚å¸¸æ¯”ä¾‹ (æ ¸å¿ƒå®éªŒé»˜è®¤å€¼: 40%å™ªå£°æŒ‘æˆ˜)
    SAVE_PLOTS = True                            # æ˜¯å¦ä¿å­˜å›¾è¡¨
    VERBOSE = True                               # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
    
    # ğŸ›¡ï¸ é²æ£’æ€§æµ‹è¯•å‚æ•° - éªŒè¯"CausalEngineé²æ£’æ€§ä¼˜åŠ¿"çš„å‡è®¾
    ROBUSTNESS_ANOMALY_RATIOS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]  # å™ªå£°æ°´å¹³
    RUN_ROBUSTNESS_TEST = True                      # æ˜¯å¦è¿è¡Œé²æ£’æ€§æµ‹è¯•
    
    # ğŸ“ˆ å¯è§†åŒ–å‚æ•°
    FIGURE_DPI = 300                             # å›¾è¡¨åˆ†è¾¨ç‡
    FIGURE_SIZE_ANALYSIS = (24, 20)              # æ•°æ®åˆ†æå›¾è¡¨å¤§å°
    FIGURE_SIZE_PERFORMANCE = (30, 25)           # æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¤§å° (æ›´å¤§ä»¥å®¹çº³æ›´å¤šæ–¹æ³•)
    FIGURE_SIZE_ROBUSTNESS = (30, 25)            # é²æ£’æ€§æµ‹è¯•å›¾è¡¨å¤§å° (æ›´å¤§ä»¥å®¹çº³æ›´å¤šæ–¹æ³•)
    
    # ğŸ“ è¾“å‡ºç›®å½•å‚æ•°
    OUTPUT_DIR = "results/california_housing_regression_extended_sklearn_style"  # è¾“å‡ºç›®å½•åç§°


class ExtendedCaliforniaHousingTutorialSklearnStyle:
    """
    æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹ç±» - Sklearn-Styleç‰ˆæœ¬
    
    æ¼”ç¤ºCausalEngineåœ¨çœŸå®ä¸–ç•Œå›å½’ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½
    """
    
    def __init__(self, config=None):
        self.config = config if config is not None else TutorialConfig()
        self.X = None
        self.y = None
        self.feature_names = None
        self.results = {}
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.config.OUTPUT_DIR):
            os.makedirs(self.config.OUTPUT_DIR)
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}/")
    
    def _get_output_path(self, filename):
        """è·å–è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
        return os.path.join(self.config.OUTPUT_DIR, filename)
        
    def load_and_explore_data(self, verbose=True):
        """åŠ è½½å¹¶æ¢ç´¢åŠ å·æˆ¿ä»·æ•°æ®é›†"""
        if verbose:
            print("ğŸ  æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·é¢„æµ‹ - çœŸå®ä¸–ç•Œå›å½’æ•™ç¨‹ (Sklearn-Style)")
            print("=" * 80)
            print("ğŸ“Š æ­£åœ¨åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†...")
        
        # åŠ è½½æ•°æ®
        housing = fetch_california_housing()
        self.X, self.y = housing.data, housing.target
        self.feature_names = housing.feature_names
        
        if verbose:
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   - æ ·æœ¬æ•°é‡: {self.X.shape[0]:,}")
            print(f"   - ç‰¹å¾æ•°é‡: {self.X.shape[1]}")
            print(f"   - ç‰¹å¾åç§°: {', '.join(self.feature_names)}")
            print(f"   - ç›®æ ‡èŒƒå›´: ${self.y.min():.2f} - ${self.y.max():.2f} (ç™¾ä¸‡ç¾å…ƒ)")
            print(f"   - ç›®æ ‡å‡å€¼: ${self.y.mean():.2f}")
            print(f"   - ç›®æ ‡æ ‡å‡†å·®: ${self.y.std():.2f}")
            print(f"   - å°†æ¯”è¾ƒ {len(self.config.CAUSAL_MODES) + 9} ç§æ–¹æ³•")
        
        return self.X, self.y
    
    def visualize_data(self, save_plots=None):
        """æ•°æ®å¯è§†åŒ–åˆ†æ"""
        if save_plots is None:
            save_plots = self.config.SAVE_PLOTS
            
        print("\\nğŸ“ˆ æ•°æ®åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_ANALYSIS)
        fig.suptitle('California Housing Dataset Analysis - Extended Sklearn-Style Tutorial', fontsize=16, fontweight='bold')
        
        # 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ
        axes[0, 0].hist(self.y, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('House Price Distribution')
        axes[0, 0].set_xlabel('House Price ($100k)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(self.y.mean(), color='red', linestyle='--', label=f'Mean: ${self.y.mean():.2f}')
        axes[0, 0].legend()
        
        # 2. ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾
        df = pd.DataFrame(self.X, columns=self.feature_names)
        df['MedHouseVal'] = self.y
        corr_matrix = df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=axes[0, 1], cbar_kws={'shrink': 0.8})
        axes[0, 1].set_title('Feature Correlation Matrix')
        
        # 3. ç‰¹å¾åˆ†å¸ƒç®±çº¿å›¾
        df_features = pd.DataFrame(self.X, columns=self.feature_names)
        df_features_normalized = (df_features - df_features.mean()) / df_features.std()
        df_features_normalized.boxplot(ax=axes[1, 0])
        axes[1, 0].set_title('Feature Distribution (Standardized)')
        axes[1, 0].set_xlabel('Features')
        axes[1, 0].set_ylabel('Standardized Values')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æœ€é‡è¦ç‰¹å¾ä¸ç›®æ ‡çš„æ•£ç‚¹å›¾
        most_corr_feature = corr_matrix['MedHouseVal'].abs().sort_values(ascending=False).index[1]
        most_corr_idx = list(self.feature_names).index(most_corr_feature)
        axes[1, 1].scatter(self.X[:, most_corr_idx], self.y, alpha=0.5, s=1)
        axes[1, 1].set_title(f'Most Correlated Feature: {most_corr_feature}')
        axes[1, 1].set_xlabel(most_corr_feature)
        axes[1, 1].set_ylabel('House Price ($100k)')
        
        plt.tight_layout()
        
        if save_plots:
            output_path = self._get_output_path('california_housing_analysis_extended_sklearn_style.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ•°æ®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
        
        # æ•°æ®ç»Ÿè®¡æ‘˜è¦
        print("\\nğŸ“‹ æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"  - æœ€ç›¸å…³ç‰¹å¾: {most_corr_feature} (ç›¸å…³ç³»æ•°: {corr_matrix.loc[most_corr_feature, 'MedHouseVal']:.3f})")
        print(f"  - å¼‚å¸¸å€¼æ£€æµ‹: {np.sum(np.abs(self.y - self.y.mean()) > 3 * self.y.std())} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")
        print(f"  - æ•°æ®å®Œæ•´æ€§: æ— ç¼ºå¤±å€¼" if not np.any(np.isnan(self.X)) else "  - è­¦å‘Š: å­˜åœ¨ç¼ºå¤±å€¼")
    
    def _prepare_data(self, test_size, val_size, anomaly_ratio, random_state):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        # æ•°æ®åˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size, random_state=random_state
        )
        
        # æ³¨å…¥å™ªå£°
        if anomaly_ratio > 0:
            y_train_noisy, noise_indices = inject_shuffle_noise(
                y_train, noise_ratio=anomaly_ratio, random_state=random_state
            )
            y_train = y_train_noisy
            if self.config.VERBOSE:
                print(f"   å¼‚å¸¸æ³¨å…¥: {anomaly_ratio:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
        
        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_val_scaled = scaler_X.transform(X_val)
        X_test_scaled = scaler_X.transform(X_test)
        
        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
        
        return {
            'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,
            'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test,
            'scaler_X': scaler_X, 'scaler_y': scaler_y
        }
    
    def _train_sklearn_model(self, data):
        """è®­ç»ƒsklearnæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
        
        model = MLPRegressor(
            hidden_layer_sizes=self.config.SKLEARN_HIDDEN_LAYERS,
            max_iter=self.config.SKLEARN_MAX_ITER,
            learning_rate_init=self.config.SKLEARN_LR,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=self.config.NN_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            random_state=self.config.RANDOM_STATE
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _train_pytorch_model(self, data):
        """è®­ç»ƒPyTorchæ¨¡å‹"""
        print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
        
        model = MLPPytorchRegressor(
            hidden_layer_sizes=self.config.PYTORCH_HIDDEN_SIZES,
            max_iter=self.config.PYTORCH_EPOCHS,
            learning_rate=self.config.PYTORCH_LR,
            early_stopping=True,
            n_iter_no_change=self.config.PYTORCH_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        return model
    
    def _train_robust_regressor(self, data, robust_type):
        """è®­ç»ƒç¨³å¥å›å½’å™¨"""
        print(f"ğŸ”§ è®­ç»ƒ {robust_type} Regressor...")
        
        # é€‰æ‹©ç¨³å¥å›å½’å™¨ç±»å‹
        if robust_type == 'huber':
            model_class = MLPHuberRegressor
        elif robust_type == 'pinball':
            model_class = MLPPinballRegressor
        elif robust_type == 'cauchy':
            model_class = MLPCauchyRegressor
        else:
            raise ValueError(f"Unknown robust type: {robust_type}")
        
        model = model_class(
            hidden_layer_sizes=self.config.ROBUST_HIDDEN_SIZES,
            max_iter=self.config.ROBUST_MAX_EPOCHS,
            learning_rate=self.config.ROBUST_LR,
            early_stopping=True,
            n_iter_no_change=self.config.ROBUST_PATIENCE,
            tol=self.config.NN_TOLERANCE,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if hasattr(model, 'n_iter_'):
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        else:
            print(f"   è®­ç»ƒå®Œæˆ")
        
        return model
    
    def _train_tree_model(self, data, tree_type):
        """è®­ç»ƒæ ‘æ¨¡å‹"""
        print(f"ğŸ”§ è®­ç»ƒ {tree_type}...")
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæœªæ ‡å‡†åŒ–çš„ç‰¹å¾ï¼‰è¿›è¡Œæ ‘æ¨¡å‹è®­ç»ƒ
        X_train_original = data['scaler_X'].inverse_transform(data['X_train'])
        y_train_original = data['scaler_y'].inverse_transform(data['y_train'].reshape(-1, 1)).flatten()
        
        if tree_type == 'random_forest':
            model = RandomForestRegressor(
                n_estimators=self.config.TREE_N_ESTIMATORS,
                max_depth=self.config.TREE_MAX_DEPTH,
                random_state=self.config.TREE_RANDOM_STATE,
                n_jobs=-1
            )
            
        elif tree_type == 'xgboost':
            if not XGBOOST_AVAILABLE:
                print("   âš ï¸ XGBoost ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = xgb.XGBRegressor(
                n_estimators=self.config.XGBOOST_N_ESTIMATORS,
                learning_rate=self.config.XGBOOST_LEARNING_RATE,
                max_depth=self.config.XGBOOST_MAX_DEPTH,
                random_state=self.config.RANDOM_STATE,
                n_jobs=-1,
                verbosity=0
            )
            
        elif tree_type == 'lightgbm':
            if not LIGHTGBM_AVAILABLE:
                print("   âš ï¸ LightGBM ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = lgb.LGBMRegressor(
                n_estimators=self.config.LIGHTGBM_N_ESTIMATORS,
                learning_rate=self.config.LIGHTGBM_LEARNING_RATE,
                max_depth=self.config.LIGHTGBM_MAX_DEPTH,
                random_state=self.config.RANDOM_STATE,
                n_jobs=-1,
                verbosity=-1
            )
            
        elif tree_type == 'catboost':
            if not CATBOOST_AVAILABLE:
                print("   âš ï¸ CatBoost ä¸å¯ç”¨ï¼Œè·³è¿‡...")
                return None
            model = cb.CatBoostRegressor(
                iterations=self.config.CATBOOST_ITERATIONS,
                learning_rate=self.config.CATBOOST_LEARNING_RATE,
                depth=self.config.CATBOOST_DEPTH,
                random_state=self.config.RANDOM_STATE,
                verbose=False
            )
            
        else:
            raise ValueError(f"Unknown tree type: {tree_type}")
        
        model.fit(X_train_original, y_train_original)
        print(f"   è®­ç»ƒå®Œæˆ")
        
        return model
    
    def _train_causal_model(self, data, mode):
        """è®­ç»ƒCausalEngineæ¨¡å‹"""
        print(f"ğŸ”§ è®­ç»ƒ CausalEngine ({mode})...")
        
        model = MLPCausalRegressor(
            perception_hidden_layers=self.config.CAUSAL_HIDDEN_SIZES,
            mode=mode,
            max_iter=self.config.CAUSAL_MAX_EPOCHS,
            learning_rate=self.config.CAUSAL_LR,
            early_stopping=True,
            n_iter_no_change=self.config.CAUSAL_PATIENCE,
            tol=self.config.CAUSAL_TOL,
            gamma_init=self.config.CAUSAL_GAMMA_INIT,
            b_noise_init=self.config.CAUSAL_B_NOISE_INIT,
            b_noise_trainable=self.config.CAUSAL_B_NOISE_TRAINABLE,
            random_state=self.config.RANDOM_STATE,
            verbose=False
        )
        
        model.fit(data['X_train'], data['y_train'])
        
        if self.config.VERBOSE:
            print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
        
        return model
    
    def _evaluate_model(self, model, data, model_name):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        is_tree_model = model_name in ['random_forest', 'xgboost', 'lightgbm', 'catboost']
        
        if is_tree_model:
            # æ ‘æ¨¡å‹ä½¿ç”¨åŸå§‹ç‰¹å¾è¿›è¡Œé¢„æµ‹
            X_val_for_pred = data['scaler_X'].inverse_transform(data['X_val'])
            X_test_for_pred = data['scaler_X'].inverse_transform(data['X_test'])
            
            val_pred = model.predict(X_val_for_pred)
            test_pred = model.predict(X_test_for_pred)
            
            # è·å–éªŒè¯é›†çš„åŸå§‹ç›®æ ‡å€¼
            y_val_original = data['scaler_y'].inverse_transform(data['y_val'].reshape(-1, 1)).flatten()
        else:
            # ç¥ç»ç½‘ç»œæ¨¡å‹ä½¿ç”¨æ ‡å‡†åŒ–ç‰¹å¾è¿›è¡Œé¢„æµ‹
            val_pred_scaled = model.predict(data['X_val'])
            test_pred_scaled = model.predict(data['X_test'])
            
            # è½¬æ¢å›åŸå§‹å°ºåº¦
            val_pred = data['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
            test_pred = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
            y_val_original = data['scaler_y'].inverse_transform(data['y_val'].reshape(-1, 1)).flatten()
        
        # è®¡ç®—æŒ‡æ ‡
        val_metrics = {
            'MAE': mean_absolute_error(y_val_original, val_pred),
            'MdAE': median_absolute_error(y_val_original, val_pred),
            'RMSE': np.sqrt(mean_squared_error(y_val_original, val_pred)),
            'RÂ²': r2_score(y_val_original, val_pred)
        }
        
        test_metrics = {
            'MAE': mean_absolute_error(data['y_test'], test_pred),
            'MdAE': median_absolute_error(data['y_test'], test_pred),
            'RMSE': np.sqrt(mean_squared_error(data['y_test'], test_pred)),
            'RÂ²': r2_score(data['y_test'], test_pred)
        }
        
        return {'val': val_metrics, 'test': test_metrics}
    
    def run_comprehensive_benchmark(self, test_size=None, val_size=None, anomaly_ratio=None, verbose=None):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        # ä½¿ç”¨é…ç½®å‚æ•°ä½œä¸ºé»˜è®¤å€¼
        if test_size is None:
            test_size = self.config.TEST_SIZE
        if val_size is None:
            val_size = self.config.VAL_SIZE
        if anomaly_ratio is None:
            anomaly_ratio = self.config.ANOMALY_RATIO
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\\nğŸš€ å¼€å§‹ç»¼åˆåŸºå‡†æµ‹è¯• (Extended Sklearn-Style)")
            print("=" * 80)
            print(f"ğŸ”§ å®éªŒé…ç½®:")
            print(f"   - æµ‹è¯•é›†æ¯”ä¾‹: {test_size:.1%}")
            print(f"   - éªŒè¯é›†æ¯”ä¾‹: {val_size:.1%}")
            print(f"   - å¼‚å¸¸æ ‡ç­¾æ¯”ä¾‹: {anomaly_ratio:.1%}")
            print(f"   - éšæœºç§å­: {self.config.RANDOM_STATE}")
            print(f"   - CausalEngineæ¨¡å¼: {', '.join(self.config.CAUSAL_MODES)}")
            print(f"   - ç½‘ç»œæ¶æ„: {self.config.CAUSAL_HIDDEN_SIZES}")
            print(f"   - æœ€å¤§è®­ç»ƒè½®æ•°: {self.config.CAUSAL_MAX_EPOCHS}")
            print(f"   - æ—©åœpatience: {self.config.CAUSAL_PATIENCE}")
        
        # åŠ è½½æ•°æ®
        if self.X is None or self.y is None:
            self.load_and_explore_data(verbose=verbose)
        
        # å‡†å¤‡æ•°æ®
        data = self._prepare_data(test_size, val_size, anomaly_ratio, self.config.RANDOM_STATE)
        
        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        self.results = {}
        
        # 1. sklearnæ¨¡å‹
        sklearn_model = self._train_sklearn_model(data)
        self.results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
        
        # 2. PyTorchæ¨¡å‹
        pytorch_model = self._train_pytorch_model(data)
        self.results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
        
        # 3. ç¨³å¥å›å½’å™¨
        for robust_type in ['huber', 'pinball', 'cauchy']:
            robust_model = self._train_robust_regressor(data, robust_type)
            if robust_model is not None:
                # ä½¿ç”¨ä¸Legacyç‰ˆæœ¬ä¸€è‡´çš„é”®å
                result_key = f'mlp_{robust_type}_median' if robust_type == 'pinball' else f'mlp_{robust_type}'
                self.results[result_key] = self._evaluate_model(robust_model, data, result_key)
        
        # 4. æ ‘æ¨¡å‹
        for tree_type in ['random_forest', 'xgboost', 'lightgbm', 'catboost']:
            tree_model = self._train_tree_model(data, tree_type)
            if tree_model is not None:
                self.results[tree_type] = self._evaluate_model(tree_model, data, tree_type)
        
        # 5. CausalEngineæ¨¡å‹
        for mode in self.config.CAUSAL_MODES:
            causal_model = self._train_causal_model(data, mode)
            self.results[mode] = self._evaluate_model(causal_model, data, mode)
        
        if verbose:
            self._print_results(anomaly_ratio)
        
        return self.results
    
    def _print_results(self, anomaly_ratio):
        """æ‰“å°ç»“æœ"""
        print(f"\\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ (å¼‚å¸¸æ¯”ä¾‹: {anomaly_ratio:.0%})")
        print("=" * 140)
        print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
        print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
        print("-" * 140)
        
        for method, metrics in self.results.items():
            val_m = metrics['val']
            test_m = metrics['test']
            print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
                  f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
        
        print("=" * 140)
    
    def analyze_performance(self, verbose=True):
        """åˆ†ææ€§èƒ½ç»“æœ"""
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        if verbose:
            print("\\nğŸ” æ€§èƒ½åˆ†æ")
            print("=" * 60)
        
        # æå–æµ‹è¯•é›†RÂ²åˆ†æ•°
        test_r2_scores = {}
        for method, metrics in self.results.items():
            test_r2_scores[method] = metrics['test']['RÂ²']
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        best_method = max(test_r2_scores.keys(), key=lambda x: test_r2_scores[x])
        best_r2 = test_r2_scores[best_method]
        
        if verbose:
            print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
            print(f"   RÂ² = {best_r2:.4f}")
            print()
            print("ğŸ“Š æ€§èƒ½æ’å (æŒ‰RÂ²åˆ†æ•°):")
            
            sorted_methods = sorted(test_r2_scores.items(), key=lambda x: x[1], reverse=True)
            for i, (method, r2) in enumerate(sorted_methods, 1):
                improvement = ((r2 - sorted_methods[-1][1]) / abs(sorted_methods[-1][1])) * 100
                print(f"   {i}. {method:<15} RÂ² = {r2:.4f} (+ {improvement:+.1f}%)")
        
        # CausalEngineæ€§èƒ½åˆ†æ
        causal_methods = [m for m in self.results.keys() if m in ['deterministic', 'standard', 'sampling', 'exogenous', 'endogenous']]
        if causal_methods:
            best_causal = max(causal_methods, key=lambda x: test_r2_scores[x])
            traditional_methods = [m for m in self.results.keys() if m not in causal_methods]
            
            if traditional_methods and verbose:
                best_traditional = max(traditional_methods, key=lambda x: test_r2_scores[x])
                causal_improvement = ((test_r2_scores[best_causal] - test_r2_scores[best_traditional]) 
                                    / abs(test_r2_scores[best_traditional])) * 100
                
                print(f"\\nğŸ¯ CausalEngineä¼˜åŠ¿åˆ†æ:")
                print(f"   æœ€ä½³CausalEngineæ¨¡å¼: {best_causal} (RÂ² = {test_r2_scores[best_causal]:.4f})")
                print(f"   æœ€ä½³ä¼ ç»Ÿæ–¹æ³•: {best_traditional} (RÂ² = {test_r2_scores[best_traditional]:.4f})")
                print(f"   æ€§èƒ½æå‡: {causal_improvement:+.2f}%")
                
                if causal_improvement > 0:
                    print(f"   âœ… CausalEngineæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼")
                else:
                    print(f"   âš ï¸ åœ¨æ­¤æ•°æ®é›†ä¸Šä¼ ç»Ÿæ–¹æ³•è¡¨ç°æ›´å¥½")
        
        return test_r2_scores
    
    def create_performance_visualization(self, save_plot=None):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        if not self.results:
            print("âŒ è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•")
            return
        
        print("\\nğŸ“Š åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨")
        print("-" * 30)
        
        # å‡†å¤‡æ•°æ®
        methods = list(self.results.keys())
        metrics = ['MAE', 'MdAE', 'RMSE', 'RÂ²']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=self.config.FIGURE_SIZE_PERFORMANCE)
        fig.suptitle('Extended CausalEngine vs Traditional Methods: California Housing Performance (40% Label Noise) - Sklearn-Style', 
                    fontsize=16, fontweight='bold')
        axes = axes.flatten()  # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ä¾¿äºè®¿é—®
        
        # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
        def get_method_color(method):
            if method in ['deterministic', 'exogenous', 'endogenous', 'standard']:
                return 'gold'  # CausalEngineç”¨é‡‘è‰²
            elif any(robust in method for robust in ['huber', 'pinball', 'cauchy']):
                return 'lightgreen'  # ç¨³å¥æ–¹æ³•ç”¨æµ…ç»¿
            elif method in ['random_forest', 'xgboost', 'lightgbm', 'catboost']:
                return 'sandybrown'  # æ ‘æ¨¡å‹ç”¨æ£•è‰²
            else:
                return 'lightblue'  # ç¥ç»ç½‘ç»œç”¨æµ…è“
        
        colors = [get_method_color(method) for method in methods]
        
        for i, metric in enumerate(metrics):
            values = [self.results[method]['test'][metric] for method in methods]
            
            bars = axes[i].bar(methods, values, color=colors, alpha=0.8, edgecolor='black')
            axes[i].set_title(f'{metric} (Test Set)', fontweight='bold')
            axes[i].set_ylabel(metric)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
            
            # é«˜äº®æœ€ä½³ç»“æœ
            if metric == 'RÂ²':
                best_idx = values.index(max(values))
            else:
                best_idx = values.index(min(values))
            bars[best_idx].set_color('red')
            bars[best_idx].set_edgecolor('darkred')
            bars[best_idx].set_linewidth(3)
            
            axes[i].tick_params(axis='x', rotation=45)
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('california_housing_performance_extended_sklearn_style.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()  # å…³é—­å›¾å½¢ï¼Œé¿å…å†…å­˜æ³„æ¼
    
    def run_robustness_analysis(self, noise_levels=None, verbose=None):
        """è¿è¡Œé²æ£’æ€§åˆ†æ"""
        if noise_levels is None:
            noise_levels = self.config.ROBUSTNESS_ANOMALY_RATIOS
        if verbose is None:
            verbose = self.config.VERBOSE
            
        if verbose:
            print("\nğŸ”¬ å¼€å§‹é²æ£’æ€§åˆ†æ")
            print("=" * 60)
            print(f"ğŸ¯ æµ‹è¯•å™ªå£°æ°´å¹³: {[f'{level:.0%}' for level in noise_levels]}")
            print(f"   æ¯”è¾ƒæ–¹æ³•: é€‰å–ä¸»è¦æ–¹æ³•è¿›è¡Œé²æ£’æ€§æµ‹è¯•")
        
        robustness_results = {}
        
        for noise_level in noise_levels:
            if verbose:
                print(f"\nğŸ“Š æµ‹è¯•å™ªå£°æ°´å¹³: {noise_level:.0%}")
                print("-" * 30)
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®ä»¥æµ‹è¯•ç‰¹å®šå™ªå£°æ°´å¹³
            original_config = self.config.ANOMALY_RATIO
            self.config.ANOMALY_RATIO = noise_level
            
            # é€‰å–ä¸»è¦æ–¹æ³•è¿›è¡Œé²æ£’æ€§æµ‹è¯•ä»¥èŠ‚çœæ—¶é—´
            original_causal_modes = self.config.CAUSAL_MODES
            robustness_methods = ['sklearn_mlp', 'pytorch_mlp', 'random_forest', 'standard', 'endogenous']
            self.config.CAUSAL_MODES = [mode for mode in self.config.CAUSAL_MODES 
                                      if mode in robustness_methods]
            
            try:
                # åŠ è½½æ•°æ®ï¼ˆå¦‚æœå°šæœªåŠ è½½ï¼‰
                if self.X is None or self.y is None:
                    self.load_and_explore_data(verbose=False)
                
                # å‡†å¤‡æ•°æ®
                data = self._prepare_data(
                    self.config.TEST_SIZE, 
                    self.config.VAL_SIZE, 
                    noise_level, 
                    self.config.RANDOM_STATE
                )
                
                noise_results = {}
                
                # æµ‹è¯•sklearn MLP
                if 'sklearn_mlp' in robustness_methods:
                    sklearn_model = self._train_sklearn_model(data)
                    noise_results['sklearn_mlp'] = self._evaluate_model(sklearn_model, data, 'sklearn_mlp')
                
                # æµ‹è¯•PyTorch MLP
                if 'pytorch_mlp' in robustness_methods:
                    pytorch_model = self._train_pytorch_model(data)
                    noise_results['pytorch_mlp'] = self._evaluate_model(pytorch_model, data, 'pytorch_mlp')
                
                # æµ‹è¯•CausalEngine
                for mode in self.config.CAUSAL_MODES:
                    if not mode in robustness_methods:
                        continue
                    causal_model = self._train_causal_model(data, mode)
                    noise_results[mode] = self._evaluate_model(causal_model, data, mode)
                
                # æµ‹è¯•æœ€ä½³æ ‘æ¨¡å‹ï¼ˆRandom Forestä½œä¸ºä»£è¡¨ï¼‰
                if 'random_forest' in robustness_methods:
                    rf_model = self._train_tree_model(data, 'random_forest')
                    if rf_model is not None:
                        noise_results['random_forest'] = self._evaluate_model(rf_model, data, 'random_forest')
                
                robustness_results[noise_level] = noise_results
                
            finally:
                # æ¢å¤åŸå§‹é…ç½®
                self.config.ANOMALY_RATIO = original_config
                self.config.CAUSAL_MODES = original_causal_modes
        
        if verbose:
            self._print_robustness_results(robustness_results, noise_levels)
        
        return robustness_results
    
    def _print_robustness_results(self, robustness_results, noise_levels):
        """æ‰“å°é²æ£’æ€§åˆ†æç»“æœ"""
        print("\nğŸ“Š é²æ£’æ€§åˆ†æç»“æœ")
        print("=" * 100)
        
        methods = list(robustness_results[noise_levels[0]].keys())
        
        # æ‰“å°è¡¨å¤´
        header = f"{'æ–¹æ³•':<20}"
        for noise_level in noise_levels:
            header += f"{'å™ªå£°' + f'{noise_level:.0%}':<15}"
        print(header)
        print("-" * len(header))
        
        # æ‰“å°æ¯ä¸ªæ–¹æ³•çš„ç»“æœï¼ˆä½¿ç”¨RÂ²ä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
        for method in methods:
            row = f"{method:<20}"
            for noise_level in noise_levels:
                if method in robustness_results[noise_level]:
                    r2 = robustness_results[noise_level][method]['test']['RÂ²']
                    row += f"{r2:<15.4f}"
                else:
                    row += f"{'N/A':<15}"
            print(row)
        
        print("=" * 100)
        
        # åˆ†ææœ€ç¨³å®šçš„æ–¹æ³•
        print("\nğŸ¯ ç¨³å®šæ€§åˆ†æ:")
        stability_scores = {}
        
        for method in methods:
            r2_scores = []
            for noise_level in noise_levels:
                if method in robustness_results[noise_level]:
                    r2_scores.append(robustness_results[noise_level][method]['test']['RÂ²'])
            
            if r2_scores:
                # è®¡ç®—æ ‡å‡†å·®ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰
                stability_scores[method] = np.std(r2_scores)
        
        # æ‰¾åˆ°æœ€ç¨³å®šçš„æ–¹æ³•
        most_stable = min(stability_scores.keys(), key=lambda x: stability_scores[x])
        print(f"   æœ€ç¨³å®šæ–¹æ³•: {most_stable} (æ ‡å‡†å·®: {stability_scores[most_stable]:.4f})")
        
        # æŒ‰ç¨³å®šæ€§æ’åº
        print("   ç¨³å®šæ€§æ’å:")
        sorted_stability = sorted(stability_scores.items(), key=lambda x: x[1])
        for i, (method, std) in enumerate(sorted_stability, 1):
            print(f"   {i}. {method:<15} æ ‡å‡†å·®: {std:.4f}")
    
    def create_robustness_visualization(self, robustness_results, save_plot=None):
        """åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–å›¾è¡¨"""
        if save_plot is None:
            save_plot = self.config.SAVE_PLOTS
            
        print("\nğŸ“Š åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–å›¾è¡¨")
        print("-" * 30)
        
        noise_levels = list(robustness_results.keys())
        methods = list(robustness_results[noise_levels[0]].keys())
        
        fig, ax = plt.subplots(figsize=self.config.FIGURE_SIZE_ROBUSTNESS)
        
        # ä¸ºæ¯ä¸ªæ–¹æ³•ç»˜åˆ¶çº¿å›¾
        for method in methods:
            r2_scores = []
            valid_noise_levels = []
            
            for noise_level in noise_levels:
                if method in robustness_results[noise_level]:
                    r2_scores.append(robustness_results[noise_level][method]['test']['RÂ²'])
                    valid_noise_levels.append(noise_level * 100)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            
            if r2_scores:
                # è®¾ç½®çº¿æ¡æ ·å¼
                if method in ['deterministic', 'exogenous', 'endogenous', 'standard']:
                    linestyle = '-'
                    linewidth = 3
                    marker = 'o'
                    markersize = 8
                else:
                    linestyle = '--'
                    linewidth = 2
                    marker = 's'
                    markersize = 6
                
                ax.plot(valid_noise_levels, r2_scores, 
                       label=method, linestyle=linestyle, linewidth=linewidth,
                       marker=marker, markersize=markersize)
        
        ax.set_xlabel('å™ªå£°æ°´å¹³ (%)', fontweight='bold')
        ax.set_ylabel('RÂ² åˆ†æ•°', fontweight='bold')
        ax.set_title('Extended Methods Robustness Analysis: Performance vs Noise Level - Sklearn-Style', 
                    fontweight='bold')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plot:
            output_path = self._get_output_path('california_housing_robustness_extended_sklearn_style.png')
            plt.savefig(output_path, dpi=self.config.FIGURE_DPI, bbox_inches='tight')
            print(f"ğŸ“Š é²æ£’æ€§å›¾è¡¨å·²ä¿å­˜ä¸º {output_path}")
        
        plt.close()
    
    def _get_output_path(self, filename):
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        output_dir = os.path.join(os.path.dirname(__file__), self.config.OUTPUT_DIR)
        os.makedirs(output_dir, exist_ok=True)
        return os.path.join(output_dir, filename)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰©å±•ç‰ˆæ•™ç¨‹ (Sklearn-Styleç‰ˆæœ¬)"""
    print("ğŸš€ æ‰©å±•ç‰ˆåŠ å·æˆ¿ä»·å›å½’æ•™ç¨‹ - Sklearn-Styleç‰ˆæœ¬")
    print("=" * 60)
    
    # åˆ›å»ºæ•™ç¨‹å®ä¾‹
    tutorial = ExtendedCaliforniaHousingTutorialSklearnStyle()
    
    # 1. åŠ è½½å’Œåˆ†ææ•°æ®
    tutorial.load_and_explore_data()
    
    # 2. è¿è¡Œæ ¸å¿ƒæ€§èƒ½æµ‹è¯•
    core_results = tutorial.run_comprehensive_benchmark()
    
    # 3. è¿è¡Œé²æ£’æ€§æµ‹è¯•
    if tutorial.config.RUN_ROBUSTNESS_TEST:
        robustness_results = tutorial.run_robustness_analysis()
        
        # åˆ›å»ºé²æ£’æ€§å¯è§†åŒ–
        tutorial.create_robustness_visualization(robustness_results)
    
    if tutorial.config.VERBOSE:
        print("\nğŸ‰ æ‰©å±•ç‰ˆæ•™ç¨‹è¿è¡Œå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {tutorial.config.OUTPUT_DIR}")
        print("\nä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print("- california_housing_performance_extended_sklearn_style.png: æ ¸å¿ƒæ€§èƒ½å¯¹æ¯”å›¾è¡¨")
        if tutorial.config.RUN_ROBUSTNESS_TEST:
            print("- california_housing_robustness_extended_sklearn_style.png: é²æ£’æ€§åˆ†æå›¾è¡¨")
        print("\nğŸ’¡ å…³é”®å‘ç°:")
        print("   1. 13ç§æ–¹æ³•çš„å…¨é¢æ€§èƒ½å¯¹æ¯”å·²å®Œæˆ")
        print("   2. CausalEngineåœ¨å¤šç§å™ªå£°æ°´å¹³ä¸‹çš„é²æ£’æ€§å·²éªŒè¯")
        print("   3. ç¨³å¥å›å½’å’Œæ ‘æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„åŸºå‡†å¯¹æ¯”")
        print("   4. Sklearn-styleå®ç°ç¡®ä¿äº†å®éªŒçš„å¯é‡ç°æ€§")


if __name__ == "__main__":
    main()
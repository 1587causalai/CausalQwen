#!/usr/bin/env python3
"""
PyTorch vs Sklearn MLP å¯¹æ¯”è„šæœ¬

æœ¬è„šæœ¬å®ç°äº†ä¸‰ä¸ªç‰ˆæœ¬çš„ MLP:
1. ä»é›¶å¼€å§‹æ‰‹åŠ¨å®ç°çš„ PyTorch ç‰ˆæœ¬
2. Sklearn çš„ MLPRegressor
3. Causal-Sklearn åº“ä¸­å°è£…çš„ MLPPytorchRegressor

ç›®æ ‡æ˜¯æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½ï¼Œå¹¶è§‚å¯Ÿç®—æ³•å›ºæœ‰çš„å¯å˜æ€§ã€‚
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression, fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error
import matplotlib.pyplot as plt
import time
import warnings
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from causal_sklearn.regressor import MLPPytorchRegressor
from causal_sklearn.data_processing import inject_shuffle_noise
warnings.filterwarnings('ignore')

# å…¨å±€éšæœºç§å­å°†åœ¨æ¯æ¬¡è¿è¡Œæ—¶è®¾ç½®ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ†æå¯å˜æ€§
# è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè§‚å¯Ÿåˆ°çœŸå®çš„ç®—æ³•æ–¹å·®

class PyTorchMLP(nn.Module):
    """æ‰‹åŠ¨å®ç°çš„ PyTorch MLP"""
    
    def __init__(self, input_size, hidden_sizes=[100, 50], output_size=1, random_state=None):
        super(PyTorchMLP, self).__init__()
        
        # ä¸ºå¯å¤ç°çš„æƒé‡åˆå§‹åŒ–è®¾ç½®éšæœºç§å­
        if random_state is not None:
            torch.manual_seed(random_state)
        
        layers = []
        prev_size = input_size
        
        # æ·»åŠ éšè—å±‚
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            prev_size = hidden_size
        
        # æ·»åŠ è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))
        
        self.network = nn.Sequential(*layers)
        # ç§»é™¤å†…ç½®çš„ç¼©æ”¾å™¨ï¼Œä»¥ä½¿ç”¨å¤–éƒ¨é¢„å¤„ç†
        self.n_iter_ = 0
        
    def fit(self, X, y, epochs=1000, lr=0.001, batch_size=None, random_state=None,
            early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, tol=1e-4):
        """è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å®ç°æ—©åœé€»è¾‘"""
        # ä¸ºè®­ç»ƒçš„å¯å¤ç°æ€§è®¾ç½®éšæœºç§å­
        if random_state is not None:
            torch.manual_seed(random_state)
            np.random.seed(random_state)
        
        # è½¬æ¢ä¸ºå¼ é‡ (X å’Œ y åº”è¯¥å·²ç»è¢«é¢„å¤„ç†)
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.FloatTensor(y).reshape(-1, 1) # ç¡®ä¿yæ˜¯2D
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(self.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        # å¦‚æœå¯ç”¨æ—©åœï¼Œåˆ™åˆ†å‰²å‡ºéªŒè¯é›†
        if early_stopping:
            X_train, X_val, y_train, y_val = train_test_split(
                X_tensor, y_tensor,
                test_size=validation_fraction,
                random_state=random_state
            )
        else:
            X_train, y_train = X_tensor, y_tensor
            X_val, y_val = None, None
            
        # è®­ç»ƒå¾ªç¯
        self.train()
        
        best_val_loss = float('inf')
        no_improve_count = 0
        best_state_dict = None

        # å…¨æ‰¹é‡è®­ç»ƒ
        for epoch in range(epochs):
            # è®­ç»ƒæ­¥éª¤
            outputs = self.network(X_train).squeeze()
            loss = criterion(outputs, y_train.squeeze())
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # éªŒè¯æ­¥éª¤
            if early_stopping and X_val is not None:
                self.eval()
                with torch.no_grad():
                    val_outputs = self.network(X_val).squeeze()
                    val_loss = criterion(val_outputs, y_val.squeeze()).item()
                    
                    if val_loss < best_val_loss - tol:
                        best_val_loss = val_loss
                        no_improve_count = 0
                        best_state_dict = self.state_dict().copy()
                    else:
                        no_improve_count += 1
                
                self.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
                        
                if no_improve_count >= n_iter_no_change:
                    if self.n_iter_ == 0: self.n_iter_ = epoch + 1 # è®°å½•åœæ­¢çš„è¿­ä»£æ¬¡æ•°
                    print(f"   (æ‰‹åŠ¨PyTorch: åœ¨ {epoch + 1} æ¬¡è¿­ä»£åæ—©åœ)")
                    break
        
        self.n_iter_ = epoch + 1

        # å¦‚æœä½¿ç”¨æ—©åœï¼Œæ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€
        if early_stopping and best_state_dict is not None:
            self.load_state_dict(best_state_dict)
    
    def predict(self, X):
        """è¿›è¡Œé¢„æµ‹"""
        self.eval()
        with torch.no_grad():
            # X åº”è¯¥å·²ç»è¢«é¢„å¤„ç†
            X_tensor = torch.FloatTensor(X)
            predictions = self.network(X_tensor).squeeze().numpy()
        return predictions

def run_pytorch_mlp(X_train, X_test, y_train, y_test_original, run_id, random_state=None, early_stopping_config=None):
    """è¿è¡Œæ‰‹åŠ¨ PyTorch MLP å¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡"""
    print(f"æ­£åœ¨è¿è¡Œæ‰‹åŠ¨ PyTorch MLP - ç¬¬ {run_id} æ¬¡ (éšæœºç§å­: {random_state})")
    
    start_time = time.time()
    
    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    # æ³¨æ„: y_train æ˜¯å¸¦å™ªä¸”æ ‡å‡†åŒ–çš„ï¼Œæ¨¡å‹åœ¨æ­¤åŸºç¡€ä¸Šå­¦ä¹ 
    model = PyTorchMLP(input_size=X_train.shape[1], hidden_sizes=[100, 50], random_state=random_state)
    
    fit_params = {
        'epochs': 1000, 
        'lr': 0.001, 
        'batch_size': None, 
        'random_state': random_state
    }
    if early_stopping_config:
        fit_params.update(early_stopping_config)

    model.fit(X_train, y_train, **fit_params)
    
    # è¿›è¡Œé¢„æµ‹ (é¢„æµ‹ç»“æœæ˜¯æ ‡å‡†åŒ–çš„)
    y_pred_scaled = model.predict(X_test)
    
    training_time = time.time() - start_time
    
    # ä¿®æ­£ç­–ç•¥ï¼šè®©è¿è¡Œå‡½æ•°åªè¿”å›é¢„æµ‹å€¼ï¼Œè¯„ä¼°åœ¨ä¸»å‡½æ•°ä¸­ç»Ÿä¸€è¿›è¡Œã€‚
    return {
        'training_time': training_time,
        'predictions_scaled': y_pred_scaled, # è¿”å›æ ‡å‡†åŒ–é¢„æµ‹
        'n_iter': model.n_iter_
    }

def run_sklearn_mlp(X_train, X_test, y_train, y_test_original, run_id, random_state=None, early_stopping_config=None):
    """è¿è¡Œ Sklearn MLP å¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡"""
    print(f"æ­£åœ¨è¿è¡Œ Sklearn MLP - ç¬¬ {run_id} æ¬¡ (éšæœºç§å­: {random_state})")
    
    start_time = time.time()
    
    # åˆ›å»ºå¹¶è®­ç»ƒå‚æ•°åŒ¹é…çš„æ¨¡å‹
    model_params = {
        'hidden_layer_sizes': (100, 50),
        'max_iter': 1000,
        'random_state': random_state,
        'alpha': 0.0001,
        'learning_rate_init': 0.001,
        'solver': 'adam',
        'batch_size': X_train.shape[0] # æ˜¾å¼è®¾ç½®ä¸ºå…¨æ‰¹é‡ä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒ
    }
    if early_stopping_config:
        model_params.update({
            'early_stopping': True,
            'validation_fraction': early_stopping_config['validation_fraction'],
            'n_iter_no_change': early_stopping_config['n_iter_no_change'],
            'tol': early_stopping_config['tol']
        })
    else:
        model_params['early_stopping'] = False

    model = MLPRegressor(**model_params)
    print(model_params)

    # æ¨¡å‹åœ¨æ ‡å‡†åŒ–çš„ y_train ä¸Šè®­ç»ƒ
    model.fit(X_train, y_train)
    
    # è¿›è¡Œé¢„æµ‹ (é¢„æµ‹ç»“æœæ˜¯æ ‡å‡†åŒ–çš„)
    y_pred_scaled = model.predict(X_test)
    
    training_time = time.time() - start_time
    
    return {
        'training_time': training_time,
        'predictions_scaled': y_pred_scaled, # è¿”å›æ ‡å‡†åŒ–é¢„æµ‹
        'n_iter': model.n_iter_
    }

def run_causal_sklearn_mlp(X_train, X_test, y_train, y_test_original, run_id, random_state=None, early_stopping_config=None):
    """è¿è¡Œ Causal-Sklearn MLP å¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡"""
    print(f"æ­£åœ¨è¿è¡Œ Causal-Sklearn MLP - ç¬¬ {run_id} æ¬¡ (éšæœºç§å­: {random_state})")

    start_time = time.time()

    # åˆ›å»ºå¹¶è®­ç»ƒå‚æ•°åŒ¹é…çš„æ¨¡å‹
    model_params = {
        'hidden_layer_sizes': (100, 50),
        'max_iter': 1000,
        'random_state': random_state,
        'alpha': 0.0001,
        'learning_rate': 0.001,
        'batch_size': None,  # None è¡¨ç¤ºå…¨æ‰¹é‡
        'verbose': False
    }
    if early_stopping_config:
        model_params.update({
            'early_stopping': True,
            **early_stopping_config
        })
    else:
        model_params['early_stopping'] = False

    model = MLPPytorchRegressor(**model_params)

    model.fit(X_train, y_train)

    # è¿›è¡Œé¢„æµ‹
    y_pred_scaled = model.predict(X_test)

    training_time = time.time() - start_time

    return {
        'training_time': training_time,
        'predictions_scaled': y_pred_scaled, # è¿”å›æ ‡å‡†åŒ–é¢„æµ‹
        'n_iter': model.n_iter_
    }


def compare_mlp_implementations(n_runs=3, base_seed=42):
    """æ¯”è¾ƒ PyTorch å’Œ Sklearn MLP çš„å®ç°"""
    
    print("=" * 60)
    print("PyTorch vs Sklearn vs Causal-Sklearn MLP å®ç°å¯¹æ¯”")
    print("=" * 60)
    
    # 1. åŠ è½½çœŸå®æ•°æ®é›†
    print("\nğŸ“Š æ­¥éª¤ 1: åŠ è½½ California Housing æ•°æ®é›†")
    housing = fetch_california_housing()
    X, y = housing.data, housing.target
    print(f"   - æ•°æ®é›†åŠ è½½å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # 2. åˆ†å‰²æ•°æ® (å¾—åˆ°åŸå§‹çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†)
    print("\nğŸ“Š æ­¥éª¤ 2: åˆ†å‰²æ•°æ®")
    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
        X, y, test_size=0.2, random_state=base_seed
    )
    print(f"   - è®­ç»ƒé›†: {X_train_orig.shape[0]}, æµ‹è¯•é›†: {X_test_orig.shape[0]}")
    
    # 3. å¯¹è®­ç»ƒæ•°æ®æ³¨å…¥å™ªå£°
    anomaly_ratio = 0.25
    print(f"\nğŸ“Š æ­¥éª¤ 3: å¯¹è®­ç»ƒé›†æ³¨å…¥ {anomaly_ratio:.0%} çš„å¼‚å¸¸å€¼")
    y_train_noisy, noise_indices = inject_shuffle_noise(
        y_train_orig, 
        noise_ratio=anomaly_ratio,
        random_state=base_seed
    )
    print(f"   - {len(noise_indices)}/{len(y_train_orig)} ä¸ªè®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾è¢«æ±¡æŸ“")
    
    # 4. æ ‡å‡†åŒ–æ•°æ®ï¼ˆæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†ï¼‰
    print("\nğŸ“Š æ­¥éª¤ 4: ä½¿ç”¨ StandardScaler æ ‡å‡†åŒ–æ•°æ®")
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_X.fit_transform(X_train_orig)
    X_test_scaled = scaler_X.transform(X_test_orig)
    
    # ä½¿ç”¨å¸¦å™ªå£°çš„y_trainæ¥æ‹Ÿåˆyçš„scaler
    y_train_scaled = scaler_y.fit_transform(y_train_noisy.reshape(-1, 1)).flatten()
    print("   - ç‰¹å¾ (X) å’Œç›®æ ‡ (y) å‡å·²æ ‡å‡†åŒ–")
    
    # å®šä¹‰ç»Ÿä¸€çš„æ—©åœé…ç½®
    early_stopping_config = {
        'early_stopping': True,
        'validation_fraction': 0.1,
        'n_iter_no_change': 20,
        'tol': 1e-4
    }
    print("\nğŸ“Š æ­¥éª¤ 5: é…ç½®ç»Ÿä¸€çš„æ—©åœç­–ç•¥")
    print(f"   - Patience: {early_stopping_config['n_iter_no_change']}, Validation Fraction: {early_stopping_config['validation_fraction']}")

    # å­˜å‚¨ç»“æœ
    pytorch_results = []
    sklearn_results = []
    causal_sklearn_results = []
    
    print(f"\næ­£åœ¨è¿›è¡Œ {n_runs} æ¬¡å®éªŒï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒéšæœºç§å­...")
    
    for i in range(n_runs):
        run_seed = base_seed + i * 100  # æ¯æ¬¡è¿è¡Œä½¿ç”¨ä¸åŒçš„ç§å­
        print(f"\n--- å®éªŒ {i+1}/{n_runs} (éšæœºç§å­: {run_seed}) ---")
        
        # è¿è¡Œæ¨¡å‹å¾—åˆ°æ ‡å‡†åŒ–çš„é¢„æµ‹
        pytorch_run = run_pytorch_mlp(X_train_scaled, X_test_scaled, y_train_scaled, y_test_orig, i+1, run_seed, early_stopping_config)
        sklearn_run = run_sklearn_mlp(X_train_scaled, X_test_scaled, y_train_scaled, y_test_orig, i+1, run_seed, early_stopping_config)
        causal_sklearn_run = run_causal_sklearn_mlp(X_train_scaled, X_test_scaled, y_train_scaled, y_test_orig, i+1, run_seed, early_stopping_config)
        
        # ç»Ÿä¸€åœ¨ä¸»å‡½æ•°ä¸­è¿›è¡Œè¯„ä¼°
        runs = [pytorch_run, sklearn_run, causal_sklearn_run]
        results_list = [pytorch_results, sklearn_results, causal_sklearn_results]

        for run_result, result_container in zip(runs, results_list):
            # é€†æ ‡å‡†åŒ–é¢„æµ‹å€¼ä»¥è¿›è¡Œè¯„ä¼°
            y_pred_orig_scale = scaler_y.inverse_transform(run_result['predictions_scaled'].reshape(-1, 1)).flatten()
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mse = mean_squared_error(y_test_orig, y_pred_orig_scale)
            
            # å°†æŒ‡æ ‡å­˜å…¥å®¹å™¨
            result_container.append({
                'mse': mse,
                'r2': r2_score(y_test_orig, y_pred_orig_scale),
                'mae': mean_absolute_error(y_test_orig, y_pred_orig_scale),
                'mdae': median_absolute_error(y_test_orig, y_pred_orig_scale),
                'rmse': np.sqrt(mse),
                'training_time': run_result['training_time'],
                'n_iter': run_result['n_iter']
            })

        print(f"PyTorch        - MSE: {pytorch_results[-1]['mse']:.4f}, R2: {pytorch_results[-1]['r2']:.4f}, MAE: {pytorch_results[-1]['mae']:.4f}, "
              f"RMSE: {pytorch_results[-1]['rmse']:.4f}, MdAE: {pytorch_results[-1]['mdae']:.4f}, Time: {pytorch_results[-1]['training_time']:.2f}s")
        print(f"Sklearn        - MSE: {sklearn_results[-1]['mse']:.4f}, R2: {sklearn_results[-1]['r2']:.4f}, MAE: {sklearn_results[-1]['mae']:.4f}, "
              f"RMSE: {sklearn_results[-1]['rmse']:.4f}, MdAE: {sklearn_results[-1]['mdae']:.4f}, Time: {sklearn_results[-1]['training_time']:.2f}s, Iters: {sklearn_results[-1]['n_iter']}")
        print(f"Causal-Sklearn - MSE: {causal_sklearn_results[-1]['mse']:.4f}, R2: {causal_sklearn_results[-1]['r2']:.4f}, MAE: {causal_sklearn_results[-1]['mae']:.4f}, "
              f"RMSE: {causal_sklearn_results[-1]['rmse']:.4f}, MdAE: {causal_sklearn_results[-1]['mdae']:.4f}, Time: {causal_sklearn_results[-1]['training_time']:.2f}s, Iters: {causal_sklearn_results[-1]['n_iter']}")
    
    # åˆ†æç»“æœ
    print("\n" + "=" * 60)
    print("ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    # æå–æŒ‡æ ‡
    metrics_to_extract = ['mse', 'r2', 'mae', 'mdae', 'rmse', 'training_time']
    
    pytorch_metrics = {m: [r[m] for r in pytorch_results] for m in metrics_to_extract}
    sklearn_metrics = {m: [r[m] for r in sklearn_results] for m in metrics_to_extract}
    causal_sklearn_metrics = {m: [r[m] for r in causal_sklearn_results] for m in metrics_to_extract}
    
    # æ‰“å°ç»Ÿè®¡æ•°æ®
    sklearn_iters = [r['n_iter'] for r in sklearn_results]
    causal_sklearn_iters = [r['n_iter'] for r in causal_sklearn_results]
    
    def print_stats(name, metrics):
        print(f"\n{name} ç»Ÿè®¡ç»“æœ:")
        print(f"MSE  - å¹³å‡å€¼: {np.mean(metrics['mse']):.4f}, æ ‡å‡†å·®: {np.std(metrics['mse']):.4f}, èŒƒå›´: [{np.min(metrics['mse']):.4f}, {np.max(metrics['mse']):.4f}]")
        print(f"RMSE - å¹³å‡å€¼: {np.mean(metrics['rmse']):.4f}, æ ‡å‡†å·®: {np.std(metrics['rmse']):.4f}, èŒƒå›´: [{np.min(metrics['rmse']):.4f}, {np.max(metrics['rmse']):.4f}]")
        print(f"MAE  - å¹³å‡å€¼: {np.mean(metrics['mae']):.4f}, æ ‡å‡†å·®: {np.std(metrics['mae']):.4f}, èŒƒå›´: [{np.min(metrics['mae']):.4f}, {np.max(metrics['mae']):.4f}]")
        print(f"MdAE - å¹³å‡å€¼: {np.mean(metrics['mdae']):.4f}, æ ‡å‡†å·®: {np.std(metrics['mdae']):.4f}, èŒƒå›´: [{np.min(metrics['mdae']):.4f}, {np.max(metrics['mdae']):.4f}]")
        print(f"R2   - å¹³å‡å€¼: {np.mean(metrics['r2']):.4f}, æ ‡å‡†å·®: {np.std(metrics['r2']):.4f}, èŒƒå›´: [{np.min(metrics['r2']):.4f}, {np.max(metrics['r2']):.4f}]")
        print(f"æ—¶é—´ - å¹³å‡å€¼: {np.mean(metrics['training_time']):.2f}s, æ ‡å‡†å·®: {np.std(metrics['training_time']):.2f}s")

    print_stats("æ‰‹åŠ¨ PyTorch MLP", pytorch_metrics)
    print_stats("Sklearn MLP", sklearn_metrics)
    print_stats("Causal-Sklearn MLP", causal_sklearn_metrics)

    print("\nè¿­ä»£æ¬¡æ•°ç»Ÿè®¡:")
    print(f"Sklearn        - å¹³å‡å€¼: {np.mean(sklearn_iters):.1f}, æ ‡å‡†å·®: {np.std(sklearn_iters):.1f}")
    print(f"Causal-Sklearn - å¹³å‡å€¼: {np.mean(causal_sklearn_iters):.1f}, æ ‡å‡†å·®: {np.std(causal_sklearn_iters):.1f}")

    # ç»˜åˆ¶ç»“æœå›¾
    save_path = plot_comparison_results(pytorch_metrics, sklearn_metrics, causal_sklearn_metrics)
    
    # æ‰“å°å‚æ•°é…ç½®ä»¥ä¾›éªŒè¯
    print("\n" + "=" * 60)
    print("å‚æ•°é…ç½®éªŒè¯")
    print("=" * 60)
    print("ä¸‰ä¸ªæ¨¡å‹å‡ä½¿ç”¨:")
    print("- æ•°æ®é›†: California Housing, 25% è®­ç»ƒæ ‡ç­¾å™ªå£°")
    print("- ç½‘ç»œç»“æ„: [input_size, 100, 50, 1]")
    print("- æ—©åœç­–ç•¥: å¯ç”¨ (Patience=20, Val Fraction=0.1)")
    print("- æ¿€æ´»å‡½æ•°: ReLU")
    print("- ä¼˜åŒ–å™¨: Adam")
    print("- å­¦ä¹ ç‡: 0.001")
    print("- L2æ­£åˆ™åŒ– (alpha): 0.0001")
    print("- æœ€å¤§è¿­ä»£æ¬¡æ•°: 1000")
    print("- æ‰¹å¤„ç†å¤§å°: å…¨é‡æ‰¹å¤„ç† (ä¸ºå…¬å¹³æ¯”è¾ƒè€Œç»Ÿä¸€è®¾ç½®)")
    print("- æ•°æ®é¢„å¤„ç†: StandardScaler (Xå’Œyå‡ä½¿ç”¨)")
    print("- éšæœºç§å­: æ¯æ¬¡è¿è¡Œä¸åŒï¼Œä½†ä¸‰ç§æ¨¡å‹åœ¨åŒä¸€æ¬¡è¿è¡Œä¸­ä½¿ç”¨ç›¸åŒç§å­")
    
    return pytorch_results, sklearn_results, causal_sklearn_results, save_path

def plot_comparison_results(pytorch_metrics, sklearn_metrics, causal_sklearn_metrics):
    """ç»˜åˆ¶å¯¹æ¯”ç»“æœå›¾"""
    
    fig, axes = plt.subplots(3, 2, figsize=(15, 18))
    fig.suptitle('Model Performance Metrics Comparison (Boxplot)\nDataset: California Housing with 25% training noise', fontsize=16)
    
    all_metrics = {
        'MSE': ('mse', axes[0, 0]),
        'RMSE': ('rmse', axes[0, 1]),
        'MAE': ('mae', axes[1, 0]),
        'MdAE': ('mdae', axes[1, 1]),
        'RÂ²': ('r2', axes[2, 0]),
        'Training Time (s)': ('training_time', axes[2, 1])
    }
    
    labels = ['PyTorch', 'Sklearn', 'Causal-Sklearn']
    
    for title, (metric_key, ax) in all_metrics.items():
        data_to_plot = [
            pytorch_metrics[metric_key],
            sklearn_metrics[metric_key],
            causal_sklearn_metrics[metric_key]
        ]
        
        ax.boxplot(data_to_plot, labels=labels)
        ax.set_title(title)
        ax.set_ylabel(title)
        ax.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # å®šä¹‰å¹¶åˆ›å»ºç»“æœç›®å½•
    # ä½¿ç”¨ __file__ æ¥å®šä½é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿è·¯å¾„çš„å¥å£®æ€§
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    save_dir = os.path.join(project_root, 'results', 'tmp')
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, 'pytorch_vs_sklearn_mlp_comparison.png')

    plt.savefig(save_path, dpi=300)
    plt.show()
    return save_path

if __name__ == "__main__":
    # è¿è¡Œå¯¹æ¯”
    pytorch_results, sklearn_results, causal_sklearn_results, save_path = compare_mlp_implementations(n_runs=3, base_seed=42)
    
    print("\n" + "=" * 60)
    print("åˆ†æå®Œæˆ")
    print("=" * 60)
    print(f"ç»“æœå·²ä¿å­˜è‡³: {save_path}")
    print("\næ­¤å¯¹æ¯”å±•ç¤ºäº†åœ¨å¸¦å™ªå£°çš„çœŸå®æ•°æ®é›†ä¸Šï¼Œä¸åŒMLPå®ç°çš„æ€§èƒ½å·®å¼‚ã€‚")
    print("å®ƒæ¯”è¾ƒäº†æ‰‹åŠ¨PyTorchå®ç°ã€æ ‡å‡†Sklearn MLPä»¥åŠä¸€ä¸ªSklearnå…¼å®¹çš„PyTorchå°è£…å™¨ã€‚")
    print("\nå…³é”®æ´è§:")
    print("1. ä¸‰ç§å®ç°ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†å’Œç½‘ç»œç»“æ„ï¼Œå¹¶åœ¨å¸¦å™ªå£°çš„æ•°æ®ä¸Šè®­ç»ƒã€‚")
    print("2. ç»“æœçš„å·®å¼‚åæ˜ äº†ç®—æ³•å®ç°ï¼ˆå¦‚ä¼˜åŒ–å™¨ç»†èŠ‚ã€æƒé‡åˆå§‹åŒ–ï¼‰ä¸Šçš„å¾®å¦™ä¸åŒã€‚")
    print("3. æ¯ç§æ–¹æ³•å†…éƒ¨çš„å˜å¼‚æ€§æ˜¾ç¤ºäº†ä¼˜åŒ–è¿‡ç¨‹ä¸­å›ºæœ‰çš„éšæœºæ€§ã€‚")
    print("4. è¿™ä¸ºä¸‰ç§ä¸åŒçš„MLPå®ç°æä¾›äº†ä¸€ä¸ªåœ¨æ›´çœŸå®åœºæ™¯ä¸‹çš„å…¬å¹³æ€§èƒ½æ¯”è¾ƒã€‚")
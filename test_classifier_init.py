#!/usr/bin/env python3
"""
æµ‹è¯•CausalEngineåˆ†ç±»å™¨çš„é»˜è®¤åˆå§‹åŒ–å€¼
"""

import numpy as np
import torch
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalClassifier

def test_classifier_default_initialization():
    """æµ‹è¯•åˆ†ç±»å™¨é»˜è®¤åˆå§‹åŒ–æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
    print("ğŸ” æµ‹è¯•CausalEngineåˆ†ç±»å™¨é»˜è®¤åˆå§‹åŒ–å€¼...")
    
    # åˆ›å»ºé»˜è®¤é…ç½®çš„åˆ†ç±»å™¨
    print("\n1. åˆ›å»ºé»˜è®¤åˆ†ç±»å™¨:")
    classifier = MLPCausalClassifier()
    
    print(f"  æœŸæœ›: b_noise_init=0.1, gamma_init=10.0, ovr_threshold_init=0.0")
    print(f"  å®é™…: b_noise_init={classifier.b_noise_init}, gamma_init={classifier.gamma_init}, ovr_threshold_init={classifier.ovr_threshold_init}")
    
    # éªŒè¯é»˜è®¤å€¼
    assert classifier.b_noise_init == 0.1, f"b_noise_inité”™è¯¯: æœŸæœ›0.1, å®é™…{classifier.b_noise_init}"
    assert classifier.gamma_init == 10.0, f"gamma_inité”™è¯¯: æœŸæœ›10.0, å®é™…{classifier.gamma_init}"
    assert classifier.ovr_threshold_init == 0.0, f"ovr_threshold_inité”™è¯¯: æœŸæœ›0.0, å®é™…{classifier.ovr_threshold_init}"
    
    # åˆ›å»ºåˆ†ç±»æ•°æ®é›†æ¥è§¦å‘æ¨¡å‹æ„å»º
    print("\n2. æ„å»ºæ¨¡å‹éªŒè¯å‚æ•°ä¼ é€’:")
    np.random.seed(42)
    X = np.random.randn(100, 5)
    y = np.random.randint(0, 3, 100)  # 3ç±»åˆ†ç±»
    
    # åªè®­ç»ƒ1è½®æ¥è§¦å‘æ¨¡å‹æ„å»º
    classifier.max_iter = 1
    classifier.verbose = False
    classifier.fit(X, y)
    
    # æ£€æŸ¥CausalEngineå†…éƒ¨å‚æ•°
    print("\n3. æ£€æŸ¥CausalEngineå†…éƒ¨å‚æ•°:")
    causal_engine = classifier.model['causal_engine']
    
    # æ£€æŸ¥b_noise
    b_noise = causal_engine.action.b_noise
    print(f"  b_noiseå½¢çŠ¶: {b_noise.shape}, å€¼: {b_noise}")
    print(f"  b_noiseå¹³å‡å€¼: {b_noise.mean().item():.6f} (æœŸæœ›çº¦0.1)")
    
    # æ£€æŸ¥gamma_Uçš„åˆå§‹åŒ–
    actual_causal_size = causal_engine.causal_size
    print(f"  å®é™…causal_size: {actual_causal_size}")
    test_input = torch.randn(1, 1, actual_causal_size, dtype=torch.double)
    with torch.no_grad():
        loc_U, scale_U = causal_engine.abduction(test_input)
    
    gamma_U = scale_U.squeeze()
    print(f"  gamma_Uå½¢çŠ¶: {gamma_U.shape}, å€¼: {gamma_U}")
    print(f"  gamma_UèŒƒå›´: [{gamma_U.min().item():.3f}, {gamma_U.max().item():.3f}]")
    print(f"  gamma_Uå¹³å‡å€¼: {gamma_U.mean().item():.3f}")
    
    # æ£€æŸ¥ovr_threshold (å¦‚æœå­˜åœ¨)
    if hasattr(causal_engine, 'activation') and hasattr(causal_engine.activation, 'classification_thresholds'):
        ovr_thresholds = causal_engine.activation.classification_thresholds
        print(f"  ovr_thresholdå½¢çŠ¶: {ovr_thresholds.shape}, å€¼: {ovr_thresholds}")
        print(f"  ovr_thresholdå¹³å‡å€¼: {ovr_thresholds.mean().item():.6f} (æœŸæœ›çº¦0.0)")
    
    # éªŒè¯gamma_Uæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    print(f"\n4. åˆ†ægamma_Uåˆå§‹åŒ–:")
    print(f"  é…ç½®çš„gamma_init: {classifier.gamma_init}")
    print(f"  å®é™…gamma_UèŒƒå›´: [{gamma_U.min().item():.3f}, {gamma_U.max().item():.3f}]")
    
    # æ£€æŸ¥scale_netçš„biasæ¥ç†è§£åˆå§‹åŒ–
    abduction = causal_engine.abduction
    linear_modules = [m for m in abduction.scale_net.modules() if isinstance(m, torch.nn.Linear)]
    if linear_modules:
        last_layer = linear_modules[-1]
        bias_values = last_layer.bias.data
        print(f"  scale_netæœ€åå±‚bias: {bias_values}")
        print(f"  softplus(bias): {torch.nn.functional.softplus(bias_values)}")
    
    print(f"\nâœ… åˆ†ç±»å™¨é»˜è®¤åˆå§‹åŒ–éªŒè¯å®Œæˆ!")
    return classifier

def test_classification_prediction():
    """æµ‹è¯•åˆ†ç±»å™¨çš„é¢„æµ‹åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯•åˆ†ç±»å™¨é¢„æµ‹åŠŸèƒ½")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = MLPCausalClassifier(max_iter=50, verbose=False)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    X_train = np.random.randn(200, 8)
    y_train = np.random.randint(0, 3, 200)
    X_test = np.random.randn(50, 8)
    
    # è®­ç»ƒ
    print("è®­ç»ƒåˆ†ç±»å™¨...")
    classifier.fit(X_train, y_train)
    
    # é¢„æµ‹
    print("è¿›è¡Œé¢„æµ‹...")
    predictions = classifier.predict(X_test)
    probabilities = classifier.predict_proba(X_test)
    
    # å¤„ç†é¢„æµ‹ç»“æœ (å¯èƒ½æ˜¯å­—å…¸æ ¼å¼)
    if isinstance(predictions, dict):
        pred_array = predictions['predictions']
        print(f"é¢„æµ‹ç»“æœæ ¼å¼: å­—å…¸")
        print(f"é¢„æµ‹å½¢çŠ¶: {pred_array.shape}")
        print(f"ç±»åˆ«èŒƒå›´: [{pred_array.min()}, {pred_array.max()}]")
    else:
        pred_array = predictions
        print(f"é¢„æµ‹ç»“æœæ ¼å¼: æ•°ç»„")
        print(f"é¢„æµ‹å½¢çŠ¶: {pred_array.shape}")
        print(f"ç±»åˆ«èŒƒå›´: [{pred_array.min()}, {pred_array.max()}]")
    
    print(f"æ¦‚ç‡å½¢çŠ¶: {probabilities.shape}")
    print(f"æ¦‚ç‡èŒƒå›´: [{probabilities.min():.3f}, {probabilities.max():.3f}]")
    
    # éªŒè¯é¢„æµ‹ä¸€è‡´æ€§
    predicted_classes = np.argmax(probabilities, axis=1)
    consistency = np.mean(pred_array == predicted_classes)
    print(f"é¢„æµ‹ä¸€è‡´æ€§: {consistency:.3f} (æœŸæœ›1.0)")
    
    print("âœ… åˆ†ç±»å™¨é¢„æµ‹åŠŸèƒ½éªŒè¯å®Œæˆ!")

if __name__ == "__main__":
    classifier = test_classifier_default_initialization()
    test_classification_prediction()
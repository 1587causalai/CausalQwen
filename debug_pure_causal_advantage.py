#!/usr/bin/env python3
"""
çº¯ç²¹çš„CausalEngineç»„ä»¶ä¼˜è¶Šæ€§éªŒè¯

ä¸¥æ ¼æ§åˆ¶æ‰€æœ‰å˜é‡ï¼Œç¡®ä¿ï¼š
1. ç›¸åŒçš„ä¼˜åŒ–å™¨ï¼šAdam
2. ç›¸åŒçš„å­¦ä¹ ç‡
3. ç›¸åŒçš„æ•°å€¼ç²¾åº¦ï¼šfloat64
4. ç›¸åŒçš„åˆå§‹åŒ–ç­–ç•¥ï¼šXavier uniform
5. ç›¸åŒçš„æ—©åœç­–ç•¥
6. ç›¸åŒçš„ç½‘ç»œç»“æ„

å”¯ä¸€çš„å·®å¼‚ï¼šæ˜¯å¦ä½¿ç”¨CausalEngineçš„å› æœæ¨ç†ç»„ä»¶
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalClassifier

class StandardMLP(nn.Module):
    """æ ‡å‡†MLPåŸºçº¿æ¨¡å‹ - ä¸CausalEngineå®Œå…¨ä¸€è‡´çš„é…ç½®"""
    
    def __init__(self, input_size, output_size, hidden_sizes):
        super().__init__()
        self.layers = nn.ModuleList()
        
        # æ„å»ºä¸CausalEngineç›¸åŒçš„MLPç»“æ„
        prev_size = input_size
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            prev_size = hidden_size
        
        # æœ€åçš„è¾“å‡ºå±‚
        self.output_layer = nn.Linear(prev_size, output_size)
        
        # ä½¿ç”¨ä¸CausalEngineç›¸åŒçš„åˆå§‹åŒ–
        self._init_weights_xavier_uniform()
    
    def _init_weights_xavier_uniform(self):
        """ä¸CausalEngineå®Œå…¨ç›¸åŒçš„Xavieråˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        # é€šè¿‡éšè—å±‚
        for layer in self.layers:
            x = F.relu(layer(x))
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        return x


def train_standard_mlp(model, X_train, y_train, X_val, y_val, 
                      epochs=1000, lr=0.001, patience=50, tol=1e-4, verbose=False):
    """è®­ç»ƒæ ‡å‡†MLP - ä¸CausalEngineå®Œå…¨ç›¸åŒçš„è®­ç»ƒç­–ç•¥"""
    
    # ç¡®ä¿ä½¿ç”¨float64ç²¾åº¦ï¼ˆä¸CausalEngineä¸€è‡´ï¼‰
    model = model.double()
    X_train_tensor = torch.tensor(X_train, dtype=torch.float64)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float64)
    y_val_tensor = torch.tensor(y_val, dtype=torch.long)
    
    # ä½¿ç”¨ä¸CausalEngineå®Œå…¨ç›¸åŒçš„é…ç½®
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # ç»Ÿä¸€ä½¿ç”¨Adam
    
    # ä¸CausalEngineç›¸åŒçš„æ—©åœç­–ç•¥
    best_loss = float('inf')
    no_improvement_count = 0
    best_model_state = None
    
    for epoch in range(epochs):
        # è®­ç»ƒæ­¥éª¤
        model.train()
        optimizer.zero_grad()
        
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§ï¼ˆä¸CausalEngineä¸€è‡´ï¼‰
        if torch.isnan(loss) or torch.isinf(loss):
            if verbose:
                print(f"è­¦å‘Šï¼šæŸå¤±å‡½æ•°å‡ºç°æ•°å€¼é—®é¢˜ (loss={loss.item()})")
            return model, float('inf')
        
        loss.backward()
        
        # ç®€åŒ–ï¼šç›´æ¥ä¼˜åŒ–å™¨æ­¥éª¤ï¼Œä¸åŠ é¢å¤–é€»è¾‘
        optimizer.step()
        
        # éªŒè¯æ­¥éª¤
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor).item()
        
        # æ—©åœæ£€æŸ¥ï¼ˆä¸CausalEngineå®Œå…¨ç›¸åŒçš„é€»è¾‘ï¼‰
        if val_loss < best_loss - tol:
            best_loss = val_loss
            no_improvement_count = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
            import copy
            best_model_state = copy.deepcopy(model.state_dict())
            if verbose and epoch == 0:
                print(f"New best validation loss: {val_loss:.6f} at epoch {epoch+1}")
        else:
            no_improvement_count += 1
        
        if no_improvement_count >= patience:
            if verbose:
                print(f"Early stopping at epoch {epoch+1}")
            break
    
    # æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        if verbose:
            print(f"Restored best model from validation loss: {best_loss:.6f}")
    
    model.n_iter_ = epoch + 1
    model.best_loss_ = best_loss
    return model, best_loss


def run_pure_comparison():
    """è¿è¡Œçº¯ç²¹çš„CausalEngineç»„ä»¶å¯¹æ¯”å®éªŒ"""
    print("ğŸ”¬ çº¯ç²¹çš„CausalEngineç»„ä»¶ä¼˜è¶Šæ€§éªŒè¯")
    print("=" * 80)
    print("ä¸¥æ ¼æ§åˆ¶å˜é‡ï¼šç›¸åŒä¼˜åŒ–å™¨(Adam)ã€å­¦ä¹ ç‡ã€ç²¾åº¦(float64)ã€åˆå§‹åŒ–(Xavier)ã€æ—©åœç­–ç•¥")
    print("å”¯ä¸€å·®å¼‚ï¼šæ˜¯å¦ä½¿ç”¨CausalEngineçš„å› æœæ¨ç†ç»„ä»¶")
    print()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    n_samples, n_features, n_classes = 2000, 15, 3
    X, y = make_classification(
        n_samples=n_samples, 
        n_features=n_features, 
        n_classes=n_classes,
        n_informative=10,
        n_redundant=0,
        class_sep=0.8,
        random_state=42
    )
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    
    # ç»Ÿä¸€å‚æ•°
    hidden_sizes = (128, 64)
    lr = 0.001
    max_iter = 2000
    patience = 50
    tol = 1e-4
    verbose = True
    
    print(f"æ•°æ®: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾, {n_classes}ç±»åˆ«")
    print(f"ç½‘ç»œ: MLP{hidden_sizes} â†’ Linear({n_classes})")
    print(f"è®­ç»ƒ: Adam(lr={lr}), max_iter={max_iter}, patience={patience}, tol={tol}")
    print(f"ç²¾åº¦: float64, åˆå§‹åŒ–: Xavier uniform")
    print()
    
    results = {}
    
    # 1. æ ‡å‡†MLPåŸºçº¿
    print("1ï¸âƒ£ æ ‡å‡†MLPåŸºçº¿ (ç›¸åŒé…ç½®ï¼Œæ— CausalEngine)")
    standard_model = StandardMLP(n_features, n_classes, hidden_sizes)
    standard_model, final_loss = train_standard_mlp(
        standard_model, X_train, y_train, X_val, y_val,
        epochs=max_iter, lr=lr, patience=patience, tol=tol, verbose=verbose
    )
    
    # æµ‹è¯•æ ‡å‡†MLP
    standard_model.eval()
    with torch.no_grad():
        test_outputs = standard_model(torch.tensor(X_test, dtype=torch.float64))
        standard_pred = torch.argmax(test_outputs, dim=1).numpy()
    
    standard_acc = accuracy_score(y_test, standard_pred)
    results['æ ‡å‡†MLP'] = {
        'accuracy': standard_acc,
        'n_iter': standard_model.n_iter_,
        'final_loss': final_loss
    }
    print(f"   å‡†ç¡®ç‡: {standard_acc:.4f}")
    print(f"   è®­ç»ƒè½®æ•°: {standard_model.n_iter_}")
    print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {final_loss:.6f}")
    print()
    
    # 2. CausalEngine deterministicæ¨¡å¼
    print("2ï¸âƒ£ CausalEngine deterministicæ¨¡å¼ (ç›¸åŒé…ç½®ï¼Œä½¿ç”¨å› æœæ¨ç†ç»„ä»¶)")
    
    # ç¡®ä¿CausalEngineä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨
    causal_model = MLPCausalClassifier(
        hidden_layer_sizes=hidden_sizes,
        mode='deterministic',
        max_iter=max_iter,
        learning_rate=lr,
        early_stopping=True,
        n_iter_no_change=patience,
        tol=tol,
        validation_fraction=0.2,  # ä¸æ‰‹åŠ¨åˆ†å‰²ä¸€è‡´
        random_state=42,
        verbose=verbose
    )
    
    # æ£€æŸ¥CausalEngineæ˜¯å¦çœŸçš„ä½¿ç”¨Adamï¼ˆé€šè¿‡ä¿®æ”¹æºç ç¡®è®¤ï¼‰
    causal_model.fit(X_train, y_train)
    
    # æµ‹è¯•CausalEngine
    causal_pred = causal_model.predict(X_test, mode='deterministic')
    if isinstance(causal_pred, dict):
        causal_pred = causal_pred['predictions']
    
    causal_acc = accuracy_score(y_test, causal_pred)
    results['CausalEngine'] = {
        'accuracy': causal_acc,
        'n_iter': causal_model.n_iter_,
        'final_loss': causal_model.best_loss_ if hasattr(causal_model, 'best_loss_') else 'N/A'
    }
    print(f"   å‡†ç¡®ç‡: {causal_acc:.4f}")
    print(f"   è®­ç»ƒè½®æ•°: {causal_model.n_iter_}")
    if hasattr(causal_model, 'best_loss_'):
        print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {causal_model.best_loss_:.6f}")
    print()
    
    # 3. åˆ†æç»“æœ
    print("ğŸ“Š çº¯ç²¹ç»„ä»¶å¯¹æ¯”ç»“æœ:")
    print("=" * 60)
    print(f"{'æ–¹æ³•':<20} {'å‡†ç¡®ç‡':<10} {'è®­ç»ƒè½®æ•°':<10} {'éªŒè¯æŸå¤±':<12}")
    print("-" * 60)
    
    for method, metrics in results.items():
        loss_str = f"{metrics['final_loss']:.6f}" if isinstance(metrics['final_loss'], float) else str(metrics['final_loss'])
        print(f"{method:<20} {metrics['accuracy']:<10.4f} {metrics['n_iter']:<10} {loss_str:<12}")
    
    # è®¡ç®—çº¯ç²¹çš„å› æœæ¨ç†ä¼˜åŠ¿
    causal_advantage = results['CausalEngine']['accuracy'] - results['æ ‡å‡†MLP']['accuracy']
    
    print(f"\nğŸ¯ çº¯ç²¹çš„å› æœæ¨ç†ç»„ä»¶ä¼˜åŠ¿: {causal_advantage:+.4f}")
    
    if abs(causal_advantage) < 0.005:
        print("âœ… å‡ ä¹æ— å·®å¼‚ - deterministicæ¨¡å¼åœ¨ç›¸åŒé…ç½®ä¸‹ä¸æ ‡å‡†MLPç­‰ä»·")
    elif causal_advantage > 0.005:
        print("ğŸš€ CausalEngineç»„ä»¶æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼å¯èƒ½åŸå› ï¼š")
        print("   - AbductionNetworkçš„æ’ç­‰åˆå§‹åŒ–ç­–ç•¥")
        print("   - æ›´ç²¾ç»†çš„ç½‘ç»œæ¶æ„è®¾è®¡")
        print("   - å› æœæ¨ç†æ¡†æ¶çš„éšå«æ­£åˆ™åŒ–æ•ˆåº”")
    else:
        print("ğŸ“‰ æ ‡å‡†MLPç•¥èƒœï¼Œå¯èƒ½éœ€è¦è°ƒæ•´CausalEngineçš„é…ç½®")
    
    # 4. è®­ç»ƒæ•ˆç‡å¯¹æ¯”
    iter_diff = results['CausalEngine']['n_iter'] - results['æ ‡å‡†MLP']['n_iter']
    print(f"\nâ±ï¸ è®­ç»ƒæ•ˆç‡å¯¹æ¯”:")
    print(f"   è®­ç»ƒè½®æ•°å·®å¼‚: {iter_diff:+d}")
    if iter_diff < -10:
        print("   CausalEngineæ”¶æ•›æ›´å¿«")
    elif iter_diff > 10:
        print("   CausalEngineæ”¶æ•›æ›´æ…¢")
    else:
        print("   æ”¶æ•›é€Ÿåº¦ç›¸è¿‘")
    
    return results


if __name__ == "__main__":
    # é¦–å…ˆæ£€æŸ¥CausalEngineåˆ†ç±»å™¨å½“å‰ä½¿ç”¨çš„ä¼˜åŒ–å™¨
    print("ğŸ“‹ æ£€æŸ¥CausalEngineå½“å‰é…ç½®:")
    import inspect
    from causal_engine.sklearn.classifier import MLPCausalClassifier
    
    # è¯»å–fitæ–¹æ³•æŸ¥çœ‹ä¼˜åŒ–å™¨è®¾ç½®
    lines = inspect.getsource(MLPCausalClassifier.fit).split('\n')
    for i, line in enumerate(lines):
        if 'optimizer' in line.lower() and ('torch.optim' in line or 'SGD' in line or 'Adam' in line):
            print(f"   å‘ç°ä¼˜åŒ–å™¨è®¾ç½®: {line.strip()}")
    
    print("\nâš ï¸  å¦‚æœCausalEngineä½¿ç”¨çš„ä¸æ˜¯Adamï¼Œéœ€è¦å…ˆä¿®æ”¹æºç ç»Ÿä¸€ä¼˜åŒ–å™¨ï¼")
    print("   å½“å‰CausalEngineåˆ†ç±»å™¨ä½¿ç”¨: SGD(lr, momentum=0.9)")
    print("   éœ€è¦æ”¹ä¸º: Adam(lr)")
    print()
    
    print("âœ… ä¼˜åŒ–å™¨å·²ç»Ÿä¸€ä¸ºAdamï¼Œå¼€å§‹çº¯ç²¹ç»„ä»¶å¯¹æ¯”å®éªŒ...")
    print()
    
    run_pure_comparison()
#!/usr/bin/env python3
"""
è°ƒè¯•standardæ¨¡å¼è®­ç»ƒé—®é¢˜
æ£€æŸ¥æ˜¯å¦æ˜¯è®­ç»ƒä¸å……åˆ†å¯¼è‡´çš„æ€§èƒ½å·®å¼‚
"""

import numpy as np
import torch
import sys
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

from causal_engine.sklearn import MLPCausalClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def debug_standard_mode_training():
    """è¯¦ç»†è°ƒè¯•standardæ¨¡å¼çš„è®­ç»ƒè¿‡ç¨‹"""
    print("ğŸ” è°ƒè¯•CausalEngine standardæ¨¡å¼è®­ç»ƒ")
    print("=" * 60)
    
    # ç”Ÿæˆç›¸åŒçš„æ•°æ®ï¼ˆä¸å¿«é€Ÿæµ‹è¯•ä¸€è‡´ï¼‰
    np.random.seed(42)
    X, y = make_classification(
        n_samples=800, n_features=15, n_classes=3,
        n_informative=7, n_redundant=0, n_clusters_per_class=1,
        class_sep=1.0, random_state=42
    )
    
    # æ·»åŠ 10%æ ‡ç­¾å™ªå£°
    n_flip = int(len(y) * 0.1)
    flip_indices = np.random.choice(len(y), n_flip, replace=False)
    unique_labels = np.unique(y)
    
    for idx in flip_indices:
        other_labels = unique_labels[unique_labels != y[idx]]
        if len(other_labels) > 0:
            y[idx] = np.random.choice(other_labels)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)
    
    print(f"æ•°æ®å½¢çŠ¶: è®­ç»ƒ{X_train.shape}, æµ‹è¯•{X_test.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_train)}")
    
    # æµ‹è¯•ä¸åŒçš„è®­ç»ƒé…ç½®
    configs = [
        {"max_iter": 800, "learning_rate": 0.001, "description": "é»˜è®¤é…ç½®"},
        {"max_iter": 1500, "learning_rate": 0.001, "description": "æ›´å¤šè½®æ•°"},
        {"max_iter": 800, "learning_rate": 0.0005, "description": "æ›´å°å­¦ä¹ ç‡"},
        {"max_iter": 1500, "learning_rate": 0.0005, "description": "æ›´å¤šè½®æ•°+å°å­¦ä¹ ç‡"},
        {"max_iter": 800, "learning_rate": 0.001, "early_stopping": False, "description": "å…³é—­æ—©åœ"},
    ]
    
    results = []
    
    for i, config in enumerate(configs):
        print(f"\n{i+1}ï¸âƒ£ æµ‹è¯•é…ç½®: {config['description']}")
        print(f"   å‚æ•°: {config}")
        
        # deterministicæ¨¡å¼ä½œä¸ºå¯¹ç…§
        clf_det = MLPCausalClassifier(
            hidden_layer_sizes=(128, 64),
            mode='deterministic',
            random_state=42,
            verbose=True,  # å¼€å¯è¯¦ç»†è¾“å‡º
            **{k: v for k, v in config.items() if k != 'description'}
        )
        
        clf_det.fit(X_train, y_train)
        pred_det = clf_det.predict(X_test)
        acc_det = accuracy_score(y_test, pred_det['predictions'] if isinstance(pred_det, dict) else pred_det)
        
        # standardæ¨¡å¼
        clf_std = MLPCausalClassifier(
            hidden_layer_sizes=(128, 64),
            mode='standard',
            gamma_init=15.0,
            b_noise_init=0.1,
            random_state=42,
            verbose=True,  # å¼€å¯è¯¦ç»†è¾“å‡º
            **{k: v for k, v in config.items() if k != 'description'}
        )
        
        clf_std.fit(X_train, y_train)
        pred_std = clf_std.predict(X_test)
        acc_std = accuracy_score(y_test, pred_std['predictions'] if isinstance(pred_std, dict) else pred_std)
        
        print(f"   deterministicå‡†ç¡®ç‡: {acc_det:.4f}")
        print(f"   standardå‡†ç¡®ç‡:      {acc_std:.4f}")
        print(f"   å·®å¼‚: {abs(acc_det - acc_std):.4f}")
        
        results.append({
            'config': config['description'],
            'det_acc': acc_det,
            'std_acc': acc_std,
            'diff': abs(acc_det - acc_std)
        })
        
        # å¦‚æœstandardæ¨¡å¼æ€§èƒ½å¤ªå·®ï¼Œæ£€æŸ¥æŸå¤±æ”¶æ•›
        if acc_std < 0.5:
            print(f"   âš ï¸  standardæ¨¡å¼æ€§èƒ½å¼‚å¸¸ä½ï¼")
            
            # æ£€æŸ¥è®­ç»ƒæŸå¤±å†å²
            if hasattr(clf_std, 'loss_curve_') and clf_std.loss_curve_:
                final_loss = clf_std.loss_curve_[-1] if clf_std.loss_curve_ else "N/A"
                print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_loss}")
                print(f"   è®­ç»ƒè½®æ•°: {len(clf_std.loss_curve_) if clf_std.loss_curve_ else 'N/A'}")
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°
            causal_engine = clf_std.model['causal_engine']
            b_noise = causal_engine.action.b_noise
            print(f"   b_noiseå€¼: {b_noise.data[:5]}... (å‰5ä¸ª)")
            print(f"   b_noiseèŒƒå›´: [{b_noise.min().item():.4f}, {b_noise.max().item():.4f}]")
            
    # æ€»ç»“ç»“æœ
    print(f"\nğŸ“Š é…ç½®å¯¹æ¯”æ€»ç»“:")
    print(f"{'é…ç½®':<20} {'deterministic':<15} {'standard':<15} {'å·®å¼‚':<10}")
    print("-" * 65)
    for r in results:
        print(f"{r['config']:<20} {r['det_acc']:<15.4f} {r['std_acc']:<15.4f} {r['diff']:<10.4f}")
    
    return results

def test_loss_function():
    """æµ‹è¯•ä¸åŒæ¨¡å¼çš„æŸå¤±å‡½æ•°è®¡ç®—"""
    print(f"\nğŸ” æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—")
    print("=" * 40)
    
    from causal_engine.sklearn import MLPCausalClassifier
    
    # åˆ›å»ºç®€å•æµ‹è¯•æ¡ˆä¾‹
    X = np.random.randn(50, 5)
    y = np.random.randint(0, 3, 50)
    
    clf = MLPCausalClassifier(
        hidden_layer_sizes=(16,),
        mode='standard',
        max_iter=5,  # åªè®­ç»ƒå‡ è½®
        verbose=True
    )
    
    print("è®­ç»ƒstandardæ¨¡å¼åˆ†ç±»å™¨...")
    clf.fit(X, y)
    
    # æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹
    if hasattr(clf, 'loss_curve_') and clf.loss_curve_:
        print(f"æŸå¤±å˜åŒ–: {clf.loss_curve_}")
    
    # æµ‹è¯•é¢„æµ‹
    pred = clf.predict(X[:10])
    print(f"å‰10ä¸ªé¢„æµ‹: {pred}")
    
    return clf

if __name__ == "__main__":
    print("ğŸ› CausalEngine Standardæ¨¡å¼è°ƒè¯•")
    print("=" * 70)
    
    # è¿è¡Œè°ƒè¯•
    results = debug_standard_mode_training()
    test_clf = test_loss_function()
    
    print(f"\nğŸ¯ è°ƒè¯•å»ºè®®:")
    print(f"   1. æ£€æŸ¥æ˜¯å¦early stoppingè¿‡æ—©")
    print(f"   2. å°è¯•é™ä½å­¦ä¹ ç‡") 
    print(f"   3. å¢åŠ è®­ç»ƒè½®æ•°")
    print(f"   4. æ£€æŸ¥æŸå¤±å‡½æ•°è®¡ç®—æ˜¯å¦æ­£ç¡®")
    print(f"   5. éªŒè¯æ¨¡å¼åˆ‡æ¢é€»è¾‘")
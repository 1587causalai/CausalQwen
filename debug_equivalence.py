"""
è°ƒè¯•æ•°å­¦ç­‰ä»·æ€§é—®é¢˜çš„å®éªŒè„šæœ¬

æ‰¾å‡ºå†»ç»“CausalEngineä¸ä¼ ç»ŸMLPæ€§èƒ½å·®å¼‚çš„æ ¹æœ¬åŸå› 
"""

import numpy as np
import torch
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error
import sys
import os

sys.path.append('/Users/gongqian/DailyLog/CausalQwen')
from causal_engine.sklearn import MLPCausalRegressor

def freeze_abduction_to_identity(model):
    """å†»ç»“AbductionNetworkä¸ºæ’ç­‰æ˜ å°„"""
    abduction = model.causal_engine.abduction
    
    if hasattr(abduction, '_loc_is_identity_candidate') and abduction._loc_is_identity_candidate:
        with torch.no_grad():
            causal_size = abduction.causal_size
            abduction.loc_net.weight.copy_(torch.eye(causal_size))
            abduction.loc_net.bias.zero_()
            
        abduction.loc_net.weight.requires_grad = False
        abduction.loc_net.bias.requires_grad = False
        return True
    return False

def enable_traditional_loss_mode(model, task_type='regression'):
    """ä¸ºå†»ç»“çš„æ¨¡å‹å¯ç”¨ä¼ ç»ŸæŸå¤±å‡½æ•°æ¨¡å¼"""
    if task_type == 'regression':
        def mse_loss(predictions, targets):
            if isinstance(predictions, dict):
                if 'activation_output' in predictions and 'regression_values' in predictions['activation_output']:
                    pred_values = predictions['activation_output']['regression_values'].squeeze()
                elif 'loc_S' in predictions:
                    pred_values = predictions['loc_S'].squeeze()
                else:
                    raise ValueError("Cannot extract predictions for MSE loss")
            else:
                pred_values = predictions.squeeze()
            
            targets = targets.squeeze()
            return torch.nn.functional.mse_loss(pred_values, targets)
        
        model._traditional_loss = mse_loss
        model._use_traditional_loss = True

def debug_equivalence():
    """è°ƒè¯•ç­‰ä»·æ€§é—®é¢˜"""
    print("ğŸ” è°ƒè¯•æ•°å­¦ç­‰ä»·æ€§é—®é¢˜")
    print("="*50)
    
    # ç”Ÿæˆå›å½’æ•°æ®
    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    print(f"æ•°æ®ç»´åº¦: X_train {X_train.shape}, y_train {y_train.shape}")
    
    # æ–¹æ¡ˆ1: åŸå§‹æ–¹æ³•ï¼ˆæœ‰é—®é¢˜ï¼‰
    print("\n1ï¸âƒ£ åŸå§‹æ–¹æ³• - åˆ†ä¸¤æ­¥è®­ç»ƒï¼ˆæœ‰é—®é¢˜ï¼‰:")
    frozen_reg_v1 = MLPCausalRegressor(
        hidden_layer_sizes=(64, 32),
        causal_size=32,
        max_iter=500,
        random_state=42,
        verbose=False
    )
    
    # åˆ†ä¸¤æ­¥è®­ç»ƒ
    frozen_reg_v1.fit(X_train[:50], y_train[:50])  # å°æ‰¹é‡åˆå§‹åŒ–
    freeze_abduction_to_identity(frozen_reg_v1)
    enable_traditional_loss_mode(frozen_reg_v1, 'regression')
    original_compute_loss = frozen_reg_v1._compute_loss
    frozen_reg_v1._compute_loss = lambda predictions, targets: frozen_reg_v1._traditional_loss(predictions, targets)
    frozen_reg_v1.fit(X_train, y_train)  # é‡æ–°è®­ç»ƒ
    
    pred_v1 = frozen_reg_v1.predict(X_test, mode='compatible')
    r2_v1 = r2_score(y_test, pred_v1)
    
    # æ–¹æ¡ˆ2: æ”¹è¿›æ–¹æ³• - ä¸€æ¬¡æ€§è®­ç»ƒ + L2æ­£åˆ™åŒ–
    print("\n2ï¸âƒ£ æ”¹è¿›æ–¹æ³• - ä¸€æ¬¡æ€§è®­ç»ƒ:")
    frozen_reg_v2 = MLPCausalRegressor(
        hidden_layer_sizes=(64, 32),
        causal_size=32,
        max_iter=500,
        random_state=42,
        verbose=False
    )
    
    # å…ˆå†»ç»“ï¼Œå†è®­ç»ƒï¼ˆä¸€æ¬¡æ€§ï¼‰
    # é¦–å…ˆè¿›è¡Œæœ€å°åˆå§‹åŒ–
    frozen_reg_v2._build_model(X_train.shape[1])
    freeze_abduction_to_identity(frozen_reg_v2)
    enable_traditional_loss_mode(frozen_reg_v2, 'regression')
    frozen_reg_v2._compute_loss = lambda predictions, targets: frozen_reg_v2._traditional_loss(predictions, targets)
    
    # æ·»åŠ L2æ­£åˆ™åŒ– (æ¨¡æ‹Ÿsklearnçš„alpha=0.0001)
    optimizer_v2 = torch.optim.Adam(frozen_reg_v2.model.parameters(), lr=0.001, weight_decay=0.0001)
    
    # æ‰‹åŠ¨è®­ç»ƒè¿‡ç¨‹
    frozen_reg_v2.model.train()
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    
    for epoch in range(500):
        optimizer_v2.zero_grad()
        predictions = frozen_reg_v2._forward(X_train_tensor)
        loss = frozen_reg_v2._compute_loss(predictions, y_train_tensor)
        loss.backward()
        optimizer_v2.step()
        
        if epoch % 100 == 0:
            print(f"    Epoch {epoch}: Loss = {loss.item():.6f}")
    
    pred_v2 = frozen_reg_v2.predict(X_test, mode='compatible')
    r2_v2 = r2_score(y_test, pred_v2)
    
    # ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    print("\n3ï¸âƒ£ ä¼ ç»Ÿsklearnæ–¹æ³•:")
    traditional_reg = MLPRegressor(
        hidden_layer_sizes=(64, 32), 
        max_iter=500, 
        random_state=42,
        alpha=0.0001  # L2æ­£åˆ™åŒ–
    )
    traditional_reg.fit(X_train, y_train)
    pred_trad = traditional_reg.predict(X_test)
    r2_trad = r2_score(y_test, pred_trad)
    
    # ç»“æœå¯¹æ¯”
    print("\nğŸ“Š ç»“æœå¯¹æ¯”:")
    print(f"ä¼ ç»Ÿsklearn:     RÂ² = {r2_trad:.6f}")
    print(f"åŸå§‹æ–¹æ³•(åˆ†æ­¥):   RÂ² = {r2_v1:.6f}, å·®å¼‚ = {abs(r2_trad - r2_v1):.6f}")
    print(f"æ”¹è¿›æ–¹æ³•(ä¸€æ­¥):   RÂ² = {r2_v2:.6f}, å·®å¼‚ = {abs(r2_trad - r2_v2):.6f}")
    
    print("\nğŸ¯ åˆ†æ:")
    if abs(r2_trad - r2_v2) < abs(r2_trad - r2_v1):
        print("âœ… æ”¹è¿›æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å·®å¼‚!")
        if abs(r2_trad - r2_v2) < 0.001:
            print("ğŸ‰ å‡ ä¹å®Œå…¨ç­‰ä»·!")
        else:
            print("âš ï¸ ä»æœ‰å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    else:
        print("âŒ æ”¹è¿›æ–¹æ³•æ²¡æœ‰æ˜æ˜¾æ•ˆæœï¼Œéœ€è¦å…¶ä»–è§£å†³æ–¹æ¡ˆ")

if __name__ == "__main__":
    debug_equivalence()
"""
CausalEngine Sklearn Interface Demo v2.0 - äº”æ¨¡å¼ç³»ç»Ÿæ¼”ç¤º

å±•ç¤ºMLPCausalRegressorå’ŒMLPCausalClassifierçš„äº”æ¨¡å¼ç³»ç»Ÿï¼š
1. Deterministic: Î³_U=0, b_noise=0 (ç­‰ä»·sklearn)
2. Exogenous: Î³_U=0, b_noiseâ‰ 0 (å¤–ç”Ÿå™ªå£°æ¨ç†)
3. Endogenous: Î³_Uâ‰ 0, b_noise=0 (å†…ç”Ÿå› æœæ¨ç†)
4. Standard: Î³_Uâ‰ 0, b_noiseâ†’scale (æ ‡å‡†å› æœæ¨ç†)
5. Sampling: Î³_Uâ‰ 0, b_noiseâ†’location (æ¢ç´¢æ€§å› æœæ¨ç†)

ä¸»è¦å‡çº§:
- å®Œæ•´çš„äº”æ¨¡å¼ç³»ç»Ÿæ¼”ç¤º
- æ–°çš„predict_dist()æ–¹æ³•æ¼”ç¤º
- æ¨¡å¼é—´æ€§èƒ½å¯¹æ¯”åˆ†æ
- ä¸ç¡®å®šæ€§é‡åŒ–å±•ç¤º
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import r2_score, accuracy_score, mean_squared_error
import sys
import os
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/gongqian/DailyLog/CausalQwen')

# å¯¼å…¥CausalEngine sklearnæ¥å£
try:
    from causal_engine.sklearn import MLPCausalRegressor, MLPCausalClassifier
    print("âœ… CausalEngine sklearnæ¥å£å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def demo_five_modes_regression():
    """æ¼”ç¤ºäº”æ¨¡å¼å›å½’ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("ğŸ”§ äº”æ¨¡å¼å›å½’ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆå›å½’æ•°æ®
    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    print(f"æ•°æ®ç»´åº¦: X_train {X_train.shape}, y_train {y_train.shape}")
    
    # ä¼ ç»ŸsklearnåŸºçº¿
    print("\nğŸ“Š ä¼ ç»ŸsklearnåŸºçº¿:")
    sklearn_reg = MLPRegressor(
        hidden_layer_sizes=(64, 32), 
        max_iter=200, 
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1
    )
    sklearn_reg.fit(X_train, y_train)
    sklearn_pred = sklearn_reg.predict(X_test)
    sklearn_r2 = r2_score(y_test, sklearn_pred)
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)
    print(f"sklearn MLPRegressor - RÂ²: {sklearn_r2:.4f}, MSE: {sklearn_mse:.4f}")
    
    # äº”æ¨¡å¼æµ‹è¯•
    modes = ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    results = {}
    
    print(f"\nğŸš€ CausalEngineäº”æ¨¡å¼æµ‹è¯•:")
    
    for mode in modes:
        print(f"\n--- {mode.upper()} æ¨¡å¼ ---")
        
        # åˆ›å»ºå¯¹åº”æ¨¡å¼çš„å›å½’å™¨
        causal_reg = MLPCausalRegressor(
            hidden_layer_sizes=(64, 32),
            mode=mode,
            max_iter=200,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            verbose=False
        )
        
        # è®­ç»ƒ
        causal_reg.fit(X_train, y_train)
        
        # é¢„æµ‹
        causal_pred = causal_reg.predict(X_test)
        if isinstance(causal_pred, dict):
            causal_pred = causal_pred['predictions']
            
        # è®¡ç®—æŒ‡æ ‡
        causal_r2 = r2_score(y_test, causal_pred)
        causal_mse = mean_squared_error(y_test, causal_pred)
        
        results[mode] = {
            'r2': causal_r2,
            'mse': causal_mse,
            'improvement_r2': causal_r2 - sklearn_r2,
            'model': causal_reg
        }
        
        print(f"RÂ²: {causal_r2:.4f} (vs sklearn: {causal_r2-sklearn_r2:+.4f})")
        print(f"MSE: {causal_mse:.4f}")
        
        # æ¼”ç¤ºåˆ†å¸ƒé¢„æµ‹ï¼ˆédeterministicæ¨¡å¼ï¼‰
        if mode != 'deterministic':
            dist_params = causal_reg.predict_dist(X_test[:3])
            print(f"åˆ†å¸ƒå‚æ•°å½¢çŠ¶: {dist_params.shape}")
            print(f"å‰3æ ·æœ¬ä½ç½®å‚æ•°: {dist_params[:3, 0, 0]}")
            print(f"å‰3æ ·æœ¬å°ºåº¦å‚æ•°: {dist_params[:3, 0, 1]}")
    
    # æ¨¡å¼å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š äº”æ¨¡å¼æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æ¨¡å¼':<12} {'RÂ²':<8} {'MSE':<10} {'vs sklearn':<10}")
    print("-" * 45)
    for mode in modes:
        r2, mse, imp = results[mode]['r2'], results[mode]['mse'], results[mode]['improvement_r2']
        print(f"{mode:<12} {r2:<8.4f} {mse:<10.1f} {imp:+8.4f}")
    
    return results


def demo_five_modes_classification():
    """æ¼”ç¤ºäº”æ¨¡å¼åˆ†ç±»ç³»ç»Ÿ"""
    print("\n" + "="*60)
    print("ğŸ¯ äº”æ¨¡å¼åˆ†ç±»ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, 
                              n_redundant=0, n_informative=8, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    print(f"æ•°æ®ç»´åº¦: X_train {X_train.shape}, y_train {y_train.shape}")
    print(f"ç±»åˆ«æ•°: {len(np.unique(y))}")
    
    # ä¼ ç»ŸsklearnåŸºçº¿
    print("\nğŸ“Š ä¼ ç»ŸsklearnåŸºçº¿:")
    sklearn_clf = MLPClassifier(
        hidden_layer_sizes=(64, 32), 
        max_iter=200, 
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1
    )
    sklearn_clf.fit(X_train, y_train)
    sklearn_pred = sklearn_clf.predict(X_test)
    sklearn_acc = accuracy_score(y_test, sklearn_pred)
    sklearn_proba = sklearn_clf.predict_proba(X_test)
    print(f"sklearn MLPClassifier - å‡†ç¡®ç‡: {sklearn_acc:.4f}")
    
    # äº”æ¨¡å¼æµ‹è¯•
    modes = ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    results = {}
    
    print(f"\nğŸš€ CausalEngineäº”æ¨¡å¼æµ‹è¯•:")
    
    for mode in modes:
        print(f"\n--- {mode.upper()} æ¨¡å¼ ---")
        
        # åˆ›å»ºå¯¹åº”æ¨¡å¼çš„åˆ†ç±»å™¨
        causal_clf = MLPCausalClassifier(
            hidden_layer_sizes=(64, 32),
            mode=mode,
            max_iter=200,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            verbose=False
        )
        
        # è®­ç»ƒ
        causal_clf.fit(X_train, y_train)
        
        # é¢„æµ‹
        causal_pred = causal_clf.predict(X_test)
        if isinstance(causal_pred, dict):
            causal_pred = causal_pred['predictions']
            
        # è®¡ç®—æŒ‡æ ‡
        causal_acc = accuracy_score(y_test, causal_pred)
        
        results[mode] = {
            'accuracy': causal_acc,
            'improvement': causal_acc - sklearn_acc,
            'model': causal_clf
        }
        
        print(f"å‡†ç¡®ç‡: {causal_acc:.4f} (vs sklearn: {causal_acc-sklearn_acc:+.4f})")
        
        # æ¼”ç¤ºæ¦‚ç‡é¢„æµ‹
        causal_proba = causal_clf.predict_proba(X_test[:3])
        print(f"æ¦‚ç‡é¢„æµ‹å½¢çŠ¶: {causal_proba.shape}")
        print(f"å‰3æ ·æœ¬æ¦‚ç‡åˆ†å¸ƒ:\n{causal_proba}")
        
        # æ¼”ç¤ºåˆ†å¸ƒé¢„æµ‹ï¼ˆédeterministicæ¨¡å¼ï¼‰
        if mode != 'deterministic':
            dist_proba = causal_clf.predict_dist(X_test[:3])
            print(f"OvRæ¿€æ´»æ¦‚ç‡å½¢çŠ¶: {dist_proba.shape}")
    
    # æ¨¡å¼å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š äº”æ¨¡å¼æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æ¨¡å¼':<12} {'å‡†ç¡®ç‡':<8} {'vs sklearn':<10}")
    print("-" * 35)
    for mode in modes:
        acc, imp = results[mode]['accuracy'], results[mode]['improvement']
        print(f"{mode:<12} {acc:<8.4f} {imp:+8.4f}")
    
    return results


def demo_uncertainty_quantification():
    """æ¼”ç¤ºä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸŒ¡ï¸ ä¸ç¡®å®šæ€§é‡åŒ–æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆæœ‰å™ªå£°çš„å›å½’æ•°æ®
    X, y = make_regression(n_samples=300, n_features=5, noise=0.3, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    print("å¯¹æ¯”ä¸åŒæ¨¡å¼çš„ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›:\n")
    
    # æµ‹è¯•ä¸åŒæ¨¡å¼çš„ä¸ç¡®å®šæ€§è¡¨è¾¾
    modes = ['deterministic', 'endogenous', 'standard', 'sampling']
    
    for mode in modes:
        print(f"--- {mode.upper()} æ¨¡å¼çš„ä¸ç¡®å®šæ€§ ---")
        
        reg = MLPCausalRegressor(
            hidden_layer_sizes=(32, 16),
            mode=mode,
            max_iter=150,
            random_state=42,
            verbose=False
        )
        
        reg.fit(X_train, y_train)
        
        if mode == 'deterministic':
            # Deterministicæ¨¡å¼ï¼šåªæœ‰ç‚¹ä¼°è®¡
            pred = reg.predict(X_test[:5])
            print(f"é¢„æµ‹å€¼: {pred[:5]}")
            print("ä¸ç¡®å®šæ€§: æ— ï¼ˆç¡®å®šæ€§é¢„æµ‹ï¼‰")
        else:
            # å…¶ä»–æ¨¡å¼ï¼šå®Œæ•´åˆ†å¸ƒä¿¡æ¯
            dist_params = reg.predict_dist(X_test[:5])
            loc = dist_params[:, 0, 0]  # ä½ç½®å‚æ•°
            scale = dist_params[:, 0, 1]  # å°ºåº¦å‚æ•°
            
            print(f"é¢„æµ‹å€¼ (ä½ç½®): {loc}")
            print(f"ä¸ç¡®å®šæ€§ (å°ºåº¦): {scale}")
            print(f"å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(scale):.4f}")
        
        print()


def demo_noise_robustness_five_modes():
    """æ¼”ç¤ºäº”æ¨¡å¼çš„å™ªå£°é²æ£’æ€§"""
    print("\n" + "="*60)
    print("ğŸ›¡ï¸ äº”æ¨¡å¼å™ªå£°é²æ£’æ€§æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆå¸¦å™ªå£°çš„åˆ†ç±»æ•°æ®
    X, y = make_classification(n_samples=500, n_features=8, n_classes=2, 
                              n_informative=6, random_state=42)
    X_train, X_test, y_train_clean, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # æ·»åŠ 20%æ ‡ç­¾å™ªå£°
    y_train_noisy = y_train_clean.copy()
    n_noise = int(0.2 * len(y_train_noisy))
    noise_indices = np.random.choice(len(y_train_noisy), n_noise, replace=False)
    y_train_noisy[noise_indices] = 1 - y_train_noisy[noise_indices]  # ç¿»è½¬æ ‡ç­¾
    
    print(f"æ•°æ®: {len(y_train_clean)} è®­ç»ƒæ ·æœ¬, {len(y_test)} æµ‹è¯•æ ·æœ¬")
    print(f"å™ªå£°: {n_noise} æ ‡ç­¾ç¿»è½¬ ({n_noise/len(y_train_clean)*100:.1f}%)")
    
    # sklearnåŸºçº¿ï¼ˆå¸¦å™ªå£°è®­ç»ƒï¼‰
    sklearn_clf = MLPClassifier(
        hidden_layer_sizes=(32, 16), 
        max_iter=150, 
        random_state=42
    )
    sklearn_clf.fit(X_train, y_train_noisy)
    sklearn_acc_noisy = accuracy_score(y_test, sklearn_clf.predict(X_test))
    
    print(f"\nsklearnåŸºçº¿ (å™ªå£°è®­ç»ƒ): {sklearn_acc_noisy:.4f}")
    
    # äº”æ¨¡å¼å™ªå£°é²æ£’æ€§æµ‹è¯•
    print(f"\nğŸš€ äº”æ¨¡å¼å™ªå£°é²æ£’æ€§:")
    modes = ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    results = {}
    
    for mode in modes:
        clf = MLPCausalClassifier(
            hidden_layer_sizes=(32, 16),
            mode=mode,
            max_iter=150,
            random_state=42,
            verbose=False
        )
        
        # åœ¨å™ªå£°æ•°æ®ä¸Šè®­ç»ƒ
        clf.fit(X_train, y_train_noisy)
        
        # åœ¨å¹²å‡€æµ‹è¯•é›†ä¸Šè¯„ä¼°
        pred = clf.predict(X_test)
        if isinstance(pred, dict):
            pred = pred['predictions']
            
        acc = accuracy_score(y_test, pred)
        improvement = acc - sklearn_acc_noisy
        
        results[mode] = {
            'accuracy': acc,
            'improvement': improvement
        }
        
        print(f"{mode:<12}: {acc:.4f} (vs sklearn: {improvement:+.4f})")
    
    return results


def demo_mode_switching():
    """æ¼”ç¤ºæ¨¡å¼åˆ‡æ¢åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ”„ æ¨¡å¼åŠ¨æ€åˆ‡æ¢æ¼”ç¤º")
    print("="*60)
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # è®­ç»ƒä¸€ä¸ªæ ‡å‡†æ¨¡å¼çš„å›å½’å™¨
    reg = MLPCausalRegressor(
        hidden_layer_sizes=(32, 16),
        mode='standard',  # é»˜è®¤æ ‡å‡†æ¨¡å¼
        max_iter=150,
        random_state=42,
        verbose=False
    )
    
    reg.fit(X_train, y_train)
    print("âœ… æ¨¡å‹å·²åœ¨'standard'æ¨¡å¼ä¸‹è®­ç»ƒå®Œæˆ")
    
    # æ¼”ç¤ºåŒä¸€æ¨¡å‹åœ¨ä¸åŒæ¨¡å¼ä¸‹çš„é¢„æµ‹
    print(f"\nğŸ”€ åŒä¸€æ¨¡å‹ï¼Œä¸åŒæ¨¡å¼é¢„æµ‹ (å‰3ä¸ªæ ·æœ¬):")
    modes = ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']
    
    for mode in modes:
        # ä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼Œä¸åŒæ¨¡å¼é¢„æµ‹
        pred = reg.predict(X_test[:3], mode=mode)
        
        if isinstance(pred, dict):
            pred_values = pred['predictions']
            print(f"{mode:<12}: {pred_values}")
        else:
            print(f"{mode:<12}: {pred}")
    
    print(f"\nğŸ“Š æ¨¡å¼åˆ‡æ¢è¯´æ˜:")
    print("- deterministic: ç¡®å®šæ€§é¢„æµ‹ï¼Œæ— éšæœºæ€§")
    print("- exogenous: åŠ å…¥å¤–ç”Ÿå™ªå£°")  
    print("- endogenous: ä½¿ç”¨å†…ç”Ÿä¸ç¡®å®šæ€§")
    print("- standard: æ ‡å‡†å› æœæ¨ç†ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰")
    print("- sampling: æ¢ç´¢æ€§éšæœºé‡‡æ ·")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ CausalEngineäº”æ¨¡å¼ç³»ç»Ÿæ¼”ç¤º v2.0")
    print("="*70)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # 1. äº”æ¨¡å¼å›å½’æ¼”ç¤º
        regression_results = demo_five_modes_regression()
        
        # 2. äº”æ¨¡å¼åˆ†ç±»æ¼”ç¤º  
        classification_results = demo_five_modes_classification()
        
        # 3. ä¸ç¡®å®šæ€§é‡åŒ–æ¼”ç¤º
        demo_uncertainty_quantification()
        
        # 4. å™ªå£°é²æ£’æ€§æ¼”ç¤º
        noise_results = demo_noise_robustness_five_modes()
        
        # 5. æ¨¡å¼åˆ‡æ¢æ¼”ç¤º
        demo_mode_switching()
        
        # æ€»ç»“
        print("\n" + "="*70)
        print("ğŸ“Š äº”æ¨¡å¼ç³»ç»Ÿæ¼”ç¤ºæ€»ç»“")
        print("="*70)
        
        print("ğŸ”§ å›å½’ä»»åŠ¡è¡¨ç°:")
        for mode in ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']:
            r2 = regression_results[mode]['r2']
            imp = regression_results[mode]['improvement_r2']
            print(f"  {mode:<12}: RÂ² {r2:.4f} (vs sklearn: {imp:+.4f})")
        
        print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡è¡¨ç°:")
        for mode in ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']:
            acc = classification_results[mode]['accuracy']
            imp = classification_results[mode]['improvement']
            print(f"  {mode:<12}: å‡†ç¡®ç‡ {acc:.4f} (vs sklearn: {imp:+.4f})")
        
        print("\nğŸ›¡ï¸ å™ªå£°é²æ£’æ€§è¡¨ç°:")
        for mode in ['deterministic', 'exogenous', 'endogenous', 'standard', 'sampling']:
            acc = noise_results[mode]['accuracy']
            imp = noise_results[mode]['improvement']
            print(f"  {mode:<12}: å‡†ç¡®ç‡ {acc:.4f} (vs sklearn: {imp:+.4f})")
        
        print("\nâœ… äº”æ¨¡å¼ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ‰ CausalEngineä¸ºæ‚¨æä¾›äº†ä»ç¡®å®šæ€§å»ºæ¨¡åˆ°å› æœæ¨ç†çš„å®Œæ•´å…‰è°±ï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
# 实验报告：Qwen2.5-0.5B 微调前后性能对比

**文档作者**: {{AUTO_USER_NAME}}
**创建时间**: {{CURRENT_DATETIME}}

## 1. 实验目标

本次实验的核心目标是：**量化评估我们提出的因果语言模型框架在微调（Fine-tuning）真实大语言模型（LLM）Qwen2.5-0.5B 后，对其处理和预测文本中数值的能力带来的提升。**

我们将通过对比模型在微调**前**和微调**后**在特定任务上的性能差异，来验证我们方法的有效性。

## 2. 实验设计与方法

### 2.1. 基础模型

- **模型**: Qwen2.5-0.5B
- **来源**: 从 Hugging Face Hub 下载的官方预训练权重。

### 2.2. 评估数据集

我们将使用 `src/data/evaluation_data.py` 生成的一系列标准化的测试数据集，覆盖不同场景：
- **基础数值文本**: 包含简单陈述句中的数字。
- **问答（QA）** 格式: 以问答形式呈现的数字。
- **极端数值**: 包含非常大或非常小的数字。
- **边界值**: 测试模型对边界情况（如0, 1）的处理。

### 2.3. 实验流程与复现命令

整个实验，从数据评估、模型微调、到结果保存，都可以通过一个自动化脚本一键完成。

#### 2.3.1. 运行命令

要完整复现本报告中的所有结果，请在项目根目录下执行以下命令：

```bash
python src/run_qwen_finetuning_experiment.py --qwen_model_path /path/to/your/Qwen2.5-0.5B
```

**重要提示**:
-   请务必将 `/path/to/your/Qwen2.5-0.5B` 替换为您在本地存放 Qwen2.5-0.5B 模型权重的**实际路径**。
-   脚本的其他参数（如 `epochs`, `batch_size` 等）已设置为与本次实验一致的默认值，直接运行即可复现。

#### 2.3.2. 自动化步骤详解

执行上述脚本后，程序将自动完成以下所有步骤：

1.  **加载预训练模型**：加载未经任何修改的 Qwen2.5-0.5B 模型。
2.  **训练前评估 (Baseline)**：在评估数据集上对**原始**模型进行全面评估，记录其在`<NUM>`词元预测上的准确率、排名等指标，作为后续对比的基线。
3.  **模型微调**: 使用 `quick_train.py` 中的训练逻辑，对 Qwen 模型进行微调，使其学习我们的因果框架。
4.  **训练后评估**: 加载**微调后**的模型，在完全相同的评估数据集上再次进行评估。
5.  **保存结果**: 所有评估结果（JSON文件）和微调后的模型权重（.pth文件）将被保存在 `docs-site/results/` 目录下的一个以实验时间戳命名的文件夹中。本报告中的图表和数据均来源于此。

### 2.4. 关键评估指标

- **分类准确率 (Accuracy)**: 模型成功将最高概率赋予`<NUM>`词元的比例。
- **`<NUM>`词元平均排名 (Mean Rank)**: 在整个词汇表（超过15万个词元）中，`<NUM>`词元的概率排名。排名越靠前越好。
- **`<NUM>`词元平均概率 (Mean Probability)**: 模型赋予`<NUM>`词元的平均概率。
- **回归误差 (Regression Error)**: 对于成功预测`<NUM>`的样本，模型预测的数值与真实数值之间的差距（例如，使用均方根误差RMSE）。

## 3. 预期结果

我们预期观察到以下现象：
- **微调前**:
    - 分类准确率接近于0。
    - `<NUM>`词元的平均排名非常靠后。
    - 模型的行为基本等同于一个不知道`<NUM>`概念的通用语言模型。
- **微调后**:
    - 分类准确率显著提升。
    - `<NUM>`词元的平均排名大幅提前，理想情况下接近前10。
    - 回归误差显著降低，证明模型不仅知道**何时**预测数字，也知道预测**什么**数字。

## 4. 实验结果与分析

我们的实验取得了完全符合预期的、非常理想的结果。数据显示，经过我们的因果框架微调后，Qwen 模型在处理和预测数值方面的能力得到了根本性的提升。

### 4.1. 总体性能对比

下面的表格直观地展示了模型在微调前后的巨大差异：

| Dataset            | Accuracy   | Mean Rank   |   RMSE (Finetuned) |
|:-------------------|:-----------|:------------|-------------------:|
| Basic              | 100%       | 82046 -> 1  |              42.10 |
| Question Answering | 100%       | 91343 -> 1  |              48.10 |
| Extreme Values     | 100%       | 81144 -> 1  |           50724.62 |
| Boundary Values    | 100%       | 81314 -> 1  |               6.53 |

- **准确率 (Accuracy)**: 从 0% 跃升至 100%，表明模型现在能够可靠地识别出需要生成数值的上下文。
- **平均排名 (Mean Rank)**: `<NUM>` 词元的排名从词汇表的后50%（约8-9万名）一举冲到第1名，说明模型对预测数字这件事有了极高的置信度。
- **回归误差 (RMSE)**: RMSE衡量了模型预测数值的精确度。可以看到，在大部分数据集上，模型的预测值与真实值相当接近。在"极端数值"集上误差较大，这符合预期，因为该数据集包含了数量级跨度极大的数字，是未来值得进一步优化的方向。

### 4.2. 详细图表

#### 准确率对比

![Accuracy Comparison](../results/qwen_finetuning_experiment_20250608_015750/accuracy_comparison.png)

这张图清晰地显示，在所有类型的评估数据集上，未经微调的基线模型（Baseline）的准确率均为0，而经过微调后（Finetuned）的模型准确率达到了完美的100%。

#### `<NUM>`词元平均排名对比

![Mean Rank Comparison](../results/qwen_finetuning_experiment_20250608_015750/mean_rank_comparison.png)

这张图（注意Y轴为对数尺度）更震撼地展示了模型的改变。基线模型的`<NUM>`词元排名高达数万，与随机猜测无异。而微调后的模型，其排名稳定在1.0，实现了"指哪打哪"的精确性。

## 5. 结论

本次实验成功地验证了我们提出的因果语言模型框架的有效性。通过在预训练的 Qwen2.5-0.5B 模型上进行微调，我们：
1.  成功地让模型学会了在适当的上下文中，将`<NUM>`词元作为最高优先级的预测。
2.  实现了对文本中数值的精确回归预测。

这证明我们的方法为解决大语言模型在处理精确数值上的固有弱点，提供了一条行之有效的路径。

---
**文档更新时间**: {{CURRENT_DATETIME}}

--- 
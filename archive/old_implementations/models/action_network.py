"""
Action Network module.

This module implements the action network for the causal language model,
which transforms the latent causal state into classification and regression outputs.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict

from ..utils.distributions import CauchyLinear


class ClassificationHead(nn.Module):
    """
    Classification head for token prediction.
    
    This implements the One-vs-Rest (OvR) classification approach,
    where each class has an independent decision score.
    """
    
    def __init__(self, causal_dim, num_classes, threshold=0.0, bias=True):
        """
        Initialize the classification head.
        
        Args:
            causal_dim (int): Dimensionality of the latent causal state
            num_classes (int): Number of classes (vocabulary size)
            threshold (float, optional): Decision threshold. Defaults to 0.0.
            bias (bool, optional): Whether to include bias. Defaults to True.
        """
        super().__init__()
        self.causal_dim = causal_dim
        self.num_classes = num_classes
        self.threshold = threshold
        
        # Linear layer to map causal state to class decision scores
        # é‡è¦ï¼šæ ¹æ® Qwen çš„è®¾è®¡ï¼Œåˆ†ç±»å¤´ä¸åº”è¯¥æœ‰åç½®
        self.causal_linear = CauchyLinear(causal_dim, num_classes, bias=bias)
        
        # Register threshold as a buffer (not a parameter)
        self.register_buffer('thresholds', torch.ones(num_classes) * threshold)
        
    def forward(self, causal_loc, causal_scale):
        """
        Transform causal state distribution to class decision score distributions.
        
        Args:
            causal_loc (torch.Tensor): Location parameter of causal state
                                      Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
            causal_scale (torch.Tensor): Scale parameter of causal state
                                        Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
        
        Returns:
            tuple: (score_loc, score_scale) - Parameters of the decision score distributions
                  Shape: [batch_size, num_classes] or [batch_size, seq_len, num_classes]
        """
        # Transform causal state distribution to decision score distributions
        score_loc, score_scale = self.causal_linear(causal_loc, causal_scale)
        
        return score_loc, score_scale
    
    def compute_probabilities(self, score_loc, score_scale):
        """
        Compute class probabilities using the Cauchy CDF.
        
        From core-design.md: P(S_k > C_k) = 1/2 + (1/Ï€) * arctan((loc_Sk - C_k)/scale_Sk)
        """
        # Add epsilon for numerical stability, especially if scale can be zero
        stable_scale = score_scale + 1e-9
        
        # Direct implementation of the formula from core-design.md
        # P(S_k > C_k) = 1/2 + (1/Ï€) * arctan((loc_Sk - C_k)/scale_Sk)
        probs = 0.5 + (1 / torch.pi) * torch.atan((score_loc - self.thresholds) / stable_scale)
        
        return probs
    
    def predict(self, score_loc, score_scale):
        """
        Make class predictions based on decision scores.
        
        Args:
            score_loc (torch.Tensor): Location parameters of decision scores
                                     Shape: [batch_size, num_classes]
            score_scale (torch.Tensor): Scale parameters of decision scores
                                       Shape: [batch_size, num_classes]
        
        Returns:
            torch.Tensor: Predicted class indices
                         Shape: [batch_size]
        """
        # Compute probabilities
        probs = self.compute_probabilities(score_loc, score_scale)
        
        # Return class with highest probability
        return torch.argmax(probs, dim=-1)


class RegressionHead(nn.Module):
    """
    Regression head for numerical prediction.
    """
    
    def __init__(self, causal_dim):
        """
        Initialize the regression head.
        
        Args:
            causal_dim (int): Dimensionality of the latent causal state
        """
        super().__init__()
        self.causal_dim = causal_dim
        
        # Linear layer to map causal state to regression value
        self.causal_linear = CauchyLinear(causal_dim, 1)
        
    def forward(self, causal_loc, causal_scale):
        """
        Transform causal state distribution to regression value distribution.
        
        Args:
            causal_loc (torch.Tensor): Location parameter of causal state
                                      Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
            causal_scale (torch.Tensor): Scale parameter of causal state
                                        Shape: [batch_size, causal_dim] or [batch_size, seq_len, causal_dim]
        
        Returns:
            tuple: (value_loc, value_scale) - Parameters of the regression value distribution
                  Shape: [batch_size] or [batch_size, seq_len]
        """
        # Transform causal state distribution to regression value distribution
        value_loc, value_scale = self.causal_linear(causal_loc, causal_scale)
        
        # Squeeze the last dimension
        # For sequence inputs: [batch_size, seq_len, 1] -> [batch_size, seq_len]
        # For single inputs: [batch_size, 1] -> [batch_size]
        return value_loc.squeeze(-1), value_scale.squeeze(-1)
    
    def predict(self, value_loc, value_scale):
        """
        Make regression predictions.
        
        For Cauchy distribution, the median (location parameter) is the best
        point estimate.
        
        Args:
            value_loc (torch.Tensor): Location parameter of regression value
                                     Shape: [batch_size]
            value_scale (torch.Tensor): Scale parameter of regression value
                                       Shape: [batch_size]
        
        Returns:
            torch.Tensor: Predicted regression values
                         Shape: [batch_size]
        """
        # For Cauchy distribution, the median (location parameter) is the best point estimate
        return value_loc


class ActionNetwork(nn.Module):
    """
    è¡ŒåŠ¨ç½‘ç»œ (Action Network)
    
    è¯¥ç½‘ç»œæ¥æ”¶å› æœè¡¨å¾åˆ†å¸ƒ U çš„å‚æ•°ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º
    è¾“å‡ºåˆ†å¸ƒï¼ˆåˆ†ç±»åˆ†æ•°å’Œå›å½’å€¼ï¼‰çš„å‚æ•°ã€‚
    å®ƒé€šè¿‡ç»„åˆä¸€ä¸ªåˆ†ç±»å¤´å’Œä¸€ä¸ªå›å½’å¤´æ¥å®ç°æ­¤åŠŸèƒ½ã€‚
    """
    
    def __init__(self, input_dim: int, vocab_size: int, hidden_size: Optional[int] = None, num_token_id: Optional[int] = None):
        """
        åˆå§‹åŒ–è¡ŒåŠ¨ç½‘ç»œã€‚

        Args:
            input_dim (int): ä¸ªä½“å› æœè¡¨å¾çš„ç»´åº¦ã€‚
            vocab_size (int): è¯æ±‡è¡¨çš„å¤§å°ã€‚
            hidden_size (Optional[int]): Qwen æ¨¡å‹çš„éšè—ç»´åº¦ (ä¸ºå…¼å®¹æ€§ä¿ç•™)ã€‚
            num_token_id (Optional[int]): <NUM> è¯å…ƒçš„ ID (ä¸ºå…¼å®¹æ€§ä¿ç•™)ã€‚
        """
        super().__init__()
        self.input_dim = input_dim
        
        # ç›´æ¥å®ä¾‹åŒ–å­æ¨¡å—ï¼Œé€»è¾‘æ¸…æ™°
        self.classification_head = ClassificationHead(
            causal_dim=input_dim,
            num_classes=vocab_size,
            # Qwen çš„ lm_head æ²¡æœ‰åç½®é¡¹ï¼Œæ‰€ä»¥è¿™é‡Œå¼ºåˆ¶ä¸º False
            bias=False
        )
        
        self.regression_head = RegressionHead(causal_dim=input_dim)

    def init_weights(self, qwen_lm_head: Optional[nn.Linear] = None):
        """
        åˆå§‹åŒ–è¡ŒåŠ¨ç½‘ç»œçš„æƒé‡ã€‚
        
        çŸ¥è¯†ä¼ è¾“ç­–ç•¥ï¼š
        - åˆ†ç±»å¤´ï¼šå®Œå…¨å¤ç”¨ Qwen çš„ lm_head çš„æƒé‡ã€‚
        - å›å½’å¤´ï¼šä½¿ç”¨å°çš„éšæœºå€¼åˆå§‹åŒ–ï¼Œä»¥å®ç°å‡åŒ€å…ˆéªŒã€‚
        
        Args:
            qwen_lm_head (Optional[nn.Linear]): Qwen çš„è¯­è¨€æ¨¡å‹å¤´ã€‚
        """
        print("ğŸ”§ åˆå§‹åŒ– ActionNetwork...")
        
        # --- 1. åˆå§‹åŒ–åˆ†ç±»å¤´ ---
        if qwen_lm_head is not None:
            print("ğŸ“š ä» Qwen lm_head è¿›è¡ŒçŸ¥è¯†ä¼ è¾“...")
            print(f"   - Qwen lm_head shape: {qwen_lm_head.weight.shape}")
            print(f"   - ActionNet cls head shape: {self.classification_head.causal_linear.weight.shape}")

            # éªŒè¯å°ºå¯¸æ˜¯å¦åŒ¹é…
            if self.classification_head.causal_linear.weight.shape != qwen_lm_head.weight.shape:
                 raise ValueError("åˆ†ç±»å¤´ä¸ Qwen lm_head çš„æƒé‡å°ºå¯¸ä¸åŒ¹é…!")

            # å®Œæ•´å¤åˆ¶æƒé‡
            self.classification_head.causal_linear.weight.data.copy_(qwen_lm_head.weight.data)
            print("   âœ… æƒé‡å®Œå…¨è¿ç§»")

            if hasattr(qwen_lm_head, 'bias') and qwen_lm_head.bias is not None:
                print("   - è­¦å‘Š: Qwen lm_head åŒ…å«åç½®é¡¹ï¼Œä½† ActionNetwork åˆ†ç±»å¤´æœªä½¿ç”¨ã€‚")
            else:
                print("   âœ… Qwen lm_head æ— åç½®é¡¹ï¼ˆç¬¦åˆé¢„æœŸï¼‰")
        else:
            print("   - æœªæä¾› Qwen lm_headï¼Œåˆ†ç±»å¤´ä½¿ç”¨é»˜è®¤ Xavier åˆå§‹åŒ–ã€‚")
            # å¦‚æœæ²¡æœ‰æä¾›é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–
            pass

        # --- 2. åˆå§‹åŒ–å›å½’å¤´ ---
        print("\nğŸ”§ åˆå§‹åŒ–å›å½’å¤´ï¼ˆå°éšæœºåˆå§‹åŒ–ï¼‰...")
        # ä½¿ç”¨å°çš„ Xavierå¢ç›Š (gain=0.01) æ¥ç¡®ä¿åˆå§‹å›å½’é¢„æµ‹æ¥è¿‘äºé›¶
        nn.init.xavier_uniform_(self.regression_head.causal_linear.weight, gain=0.01)
        # å°†åç½®åˆå§‹åŒ–ä¸ºé›¶
        nn.init.zeros_(self.regression_head.causal_linear.bias)
        print("   âœ… ä½¿ç”¨ Xavier åˆå§‹åŒ– (gain=0.01) å®ç°å‡åŒ€å…ˆéªŒ")
        
        print("   âœ… ActionNetwork åˆå§‹åŒ–å®Œæˆ")
            
    def forward(self, U_loc: torch.Tensor, U_scale: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡Œè¡ŒåŠ¨ç½‘ç»œçš„å‰å‘ä¼ æ’­ã€‚

        Args:
            U_loc (torch.Tensor): å› æœè¡¨å¾åˆ†å¸ƒçš„ä½ç½®å‚æ•°ã€‚
            U_scale (torch.Tensor): å› æœè¡¨å¾åˆ†å¸ƒçš„å°ºåº¦å‚æ•°ã€‚

        Returns:
            Dict[str, torch.Tensor]: åŒ…å«æ‰€æœ‰è¾“å‡ºåˆ†å¸ƒå‚æ•°çš„å­—å…¸ã€‚
        """
        # ç›´æ¥è°ƒç”¨å­æ¨¡å—çš„å‰å‘ä¼ æ’­
        loc_S, scale_S = self.classification_head(U_loc, U_scale)
        loc_Y, scale_Y = self.regression_head(U_loc, U_scale)
        
        return {
            'loc_S': loc_S,
            'scale_S': scale_S,
            'loc_Y': loc_Y,
            'scale_Y': scale_Y,
        }
    
    def predict(self, causal_loc, causal_scale=None):
        """
        ç¡®å®šæ€§é¢„æµ‹ã€‚
        
        Args:
            causal_loc: é‡‡æ ·æˆ–ç¡®å®šçš„å› æœè¡¨å¾
            causal_scale: å¦‚æœä¸º Noneï¼Œåˆ™è¿›è¡Œå®Œå…¨ç¡®å®šæ€§é¢„æµ‹
        
        Returns:
            dict: åŒ…å«åˆ†ç±»å’Œå›å½’é¢„æµ‹çš„å­—å…¸
        """
        if causal_scale is None:
            causal_scale = torch.zeros_like(causal_loc)
            
        cls_loc, _ = self.classification_head(causal_loc, causal_scale)
        predicted_tokens = self.classification_head.predict(cls_loc, None)
        
        reg_loc, _ = self.regression_head(causal_loc, causal_scale)
        predicted_values = self.regression_head.predict(reg_loc, None)
        
        return {
            'predicted_tokens': predicted_tokens,
            'predicted_values': predicted_values
        }

def cauchy_sample_reparameterized(loc, scale):
    """
    ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§ä»æŸ¯è¥¿åˆ†å¸ƒä¸­é‡‡æ ·ã€‚
    
    Args:
        loc (torch.Tensor): ä½ç½®å‚æ•°
        scale (torch.Tensor): å°ºåº¦å‚æ•°
        
    Returns:
        torch.Tensor: é‡‡æ ·ç»“æœ
    """
    # ä»æ ‡å‡†å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·
    uniform_sample = torch.rand_like(loc)
    
    # é€šè¿‡æŸ¯è¥¿åˆ†å¸ƒçš„é€†CDFï¼ˆåˆ†ä½æ•°å‡½æ•°ï¼‰è¿›è¡Œå˜æ¢
    cauchy_sample = loc + scale * torch.tan(torch.pi * (uniform_sample - 0.5))
    
    return cauchy_sample


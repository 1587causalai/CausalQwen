"""CausalQwen ç®€å•æ¼”ç¤º"""

import torch
from transformers import AutoTokenizer
import os


class SimpleMockCausalQwen:
    """ç®€åŒ–çš„CausalQwenæ¼”ç¤ºç‰ˆæœ¬"""
    
    def __init__(self):
        # åŠ è½½tokenizer - ä½¿ç”¨ç»å¯¹è·¯å¾„
        model_path = os.path.expanduser("~/models/Qwen2.5-0.5B")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def generate_text(
        self, 
        prompt: str, 
        max_new_tokens: int = 30,
        sampling_mode: str = "causal"
    ) -> str:
        """ç®€åŒ–çš„æ–‡æœ¬ç”Ÿæˆï¼ˆæ¼”ç¤ºç”¨ï¼‰"""
        
        # 1. Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")
        input_ids = inputs['input_ids']
        
        print(f"ğŸ“ è¾“å…¥tokens: {input_ids.shape}")
        print(f"ğŸ”§ é‡‡æ ·æ¨¡å¼: {sampling_mode}")
        
        # 2. æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        generated_ids = input_ids.clone()
        
        print("ğŸ”„ å¼€å§‹ç”Ÿæˆ...")
        for step in range(max_new_tokens):
            if sampling_mode == "causal":
                # æ¨¡æ‹Ÿå› æœé‡‡æ ·ï¼šé‡‡æ ·U -> è®¡ç®—åˆ†æ•° -> é€‰æ‹©token
                print(f"  æ­¥éª¤ {step+1}: å› æœé‡‡æ · -> é€‰æ‹©åŸå›  -> è§‚å¯Ÿç»“æœ")
                next_token = torch.randint(1000, 30000, (1, 1))  # éšæœºæ¨¡æ‹Ÿ
            else:
                # æ¨¡æ‹Ÿä¼ ç»Ÿé‡‡æ ·
                print(f"  æ­¥éª¤ {step+1}: ä¼ ç»Ÿé‡‡æ · -> æ¦‚ç‡åˆ†å¸ƒ -> éšæœºé€‰æ‹©")
                next_token = torch.randint(1000, 30000, (1, 1))  # éšæœºæ¨¡æ‹Ÿ
            
            generated_ids = torch.cat([generated_ids, next_token], dim=1)
            
            # æ¨¡æ‹ŸEOSåœæ­¢
            if torch.rand(1) < 0.1:  # 10%æ¦‚ç‡åœæ­¢
                print(f"  ğŸ›‘ åœ¨æ­¥éª¤ {step+1} é‡åˆ°EOSï¼Œåœæ­¢ç”Ÿæˆ")
                break
        
        # 3. Decode
        result = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
        
        print("âœ… ç”Ÿæˆå®Œæˆï¼")
        return result


def demo():
    """æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ CausalQwen ç®€å•æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleMockCausalQwen()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ä½ å¥½å‘€ï¼",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è¯·å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯",
        "Hello, how are you?"
    ]
    
    for i, prompt in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}: {prompt}")
        print("-" * 40)
        
        # å› æœé‡‡æ ·
        print("ğŸ² å› æœé‡‡æ ·æ¨¡å¼:")
        result1 = model.generate_text(prompt, max_new_tokens=10, sampling_mode="causal")
        print(f"ç»“æœ: {result1}\n")
        
        # ä¼ ç»Ÿé‡‡æ ·
        print("ğŸ¯ ä¼ ç»Ÿé‡‡æ ·æ¨¡å¼:")
        result2 = model.generate_text(prompt, max_new_tokens=10, sampling_mode="traditional") 
        print(f"ç»“æœ: {result2}")
        
        print("=" * 60)


if __name__ == "__main__":
    demo()

#!/usr/bin/env python
"""
åˆ†æQwen2.5-0.5Bä¸­271ä¸ªæœªè¢«åˆ†è¯å™¨åŠ è½½çš„token

ç›®æ ‡ï¼š
1. æ‰¾å‡ºè¿™271ä¸ªtokençš„IDèŒƒå›´
2. åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰è¢«åŠ è½½
3. æ¢è®¨å¯èƒ½çš„åŸå› 
"""

from transformers import AutoTokenizer, AutoConfig
import os

def analyze_missing_tokens():
    """åˆ†æç¼ºå¤±çš„token"""
    print("="*60)
    print(" åˆ†æ271ä¸ªæœªè¢«åˆ†è¯å™¨åŠ è½½çš„token ".center(58))
    print("="*60)
    
    model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    # åŠ è½½é…ç½®å’Œåˆ†è¯å™¨
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    config_vocab_size = config.vocab_size      # 151936
    actual_vocab_size = len(tokenizer)         # 151665
    missing_count = config_vocab_size - actual_vocab_size  # 271
    
    print(f"é…ç½®è¯æ±‡è¡¨å¤§å°: {config_vocab_size}")
    print(f"å®é™…åŠ è½½å¤§å°: {actual_vocab_size}")
    print(f"ç¼ºå¤±tokenæ•°é‡: {missing_count}")
    
    # è·å–å®é™…åŠ è½½çš„token IDé›†åˆ
    vocab_dict = tokenizer.get_vocab()
    loaded_token_ids = set(vocab_dict.values())
    
    print(f"\nå®é™…åŠ è½½çš„token IDèŒƒå›´:")
    print(f"  æœ€å°ID: {min(loaded_token_ids)}")
    print(f"  æœ€å¤§ID: {max(loaded_token_ids)}")
    print(f"  å®é™…IDæ•°é‡: {len(loaded_token_ids)}")
    
    # æ‰¾å‡ºç¼ºå¤±çš„token ID
    expected_ids = set(range(config_vocab_size))
    missing_ids = expected_ids - loaded_token_ids
    
    print(f"\nç¼ºå¤±çš„token IDåˆ†æ:")
    print(f"  ç¼ºå¤±IDæ•°é‡: {len(missing_ids)}")
    
    if missing_ids:
        missing_list = sorted(missing_ids)
        print(f"  ç¼ºå¤±IDèŒƒå›´: {min(missing_list)} åˆ° {max(missing_list)}")
        
        # åˆ†æç¼ºå¤±IDçš„åˆ†å¸ƒæ¨¡å¼
        consecutive_ranges = []
        start = missing_list[0]
        end = start
        
        for i in range(1, len(missing_list)):
            if missing_list[i] == end + 1:
                end = missing_list[i]
            else:
                consecutive_ranges.append((start, end))
                start = missing_list[i]
                end = start
        consecutive_ranges.append((start, end))
        
        print(f"\nç¼ºå¤±IDçš„è¿ç»­èŒƒå›´:")
        for start, end in consecutive_ranges:
            if start == end:
                print(f"  ID {start}: å•ä¸ªç¼ºå¤±")
            else:
                print(f"  ID {start}-{end}: è¿ç»­ç¼ºå¤± ({end-start+1}ä¸ª)")
        
        # æ˜¾ç¤ºå‰10ä¸ªå’Œå10ä¸ªç¼ºå¤±çš„ID
        print(f"\nå‰10ä¸ªç¼ºå¤±çš„ID: {missing_list[:10]}")
        print(f"å10ä¸ªç¼ºå¤±çš„ID: {missing_list[-10:]}")
    
    # æ£€æŸ¥æœ€é«˜çš„token ID
    max_loaded_id = max(loaded_token_ids)
    print(f"\næœ€é«˜åŠ è½½çš„token ID: {max_loaded_id}")
    
    if max_loaded_id < config_vocab_size - 1:
        unloaded_high_ids = config_vocab_size - 1 - max_loaded_id
        print(f"æœ€é«˜ä½ç½®æœªåŠ è½½çš„IDæ•°é‡: {unloaded_high_ids}")
        print(f"è¿™äº›IDèŒƒå›´: {max_loaded_id + 1} åˆ° {config_vocab_size - 1}")

def investigate_tokenizer_implementation():
    """è°ƒæŸ¥åˆ†è¯å™¨å®ç°ç»†èŠ‚"""
    print("\n" + "="*60)
    print(" åˆ†è¯å™¨å®ç°ç»†èŠ‚è°ƒæŸ¥ ".center(58))
    print("="*60)
    
    model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    # æ£€æŸ¥tokenizerçš„å±æ€§
    print(f"åˆ†è¯å™¨ç±»å‹: {type(tokenizer)}")
    print(f"åˆ†è¯å™¨ç±»å: {tokenizer.__class__.__name__}")
    
    # æ£€æŸ¥tokenizerå†…éƒ¨å±æ€§
    print(f"\nåˆ†è¯å™¨å†…éƒ¨å±æ€§:")
    if hasattr(tokenizer, 'vocab_size'):
        print(f"  tokenizer.vocab_size: {tokenizer.vocab_size}")
    if hasattr(tokenizer, '_vocab_size'):
        print(f"  tokenizer._vocab_size: {getattr(tokenizer, '_vocab_size', 'N/A')}")
    
    # æ£€æŸ¥tokenizerçš„é…ç½®
    if hasattr(tokenizer, 'init_kwargs'):
        print(f"  init_kwargs: {tokenizer.init_kwargs}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰vocab_sizeç›¸å…³çš„é…ç½®
    for attr in dir(tokenizer):
        if 'vocab' in attr.lower() and not attr.startswith('_'):
            value = getattr(tokenizer, attr)
            if not callable(value):
                print(f"  {attr}: {value}")

def analyze_possible_reasons():
    """åˆ†æå¯èƒ½çš„åŸå› """
    print("\n" + "="*60)
    print(" å¯èƒ½åŸå› åˆ†æ ".center(58))
    print("="*60)
    
    print("å¯èƒ½çš„è§£é‡Šï¼š")
    print("\n1. ğŸ”’ é¢„ç•™ä½ç½®")
    print("   - è¿™271ä¸ªä½ç½®å¯èƒ½æ˜¯ä¸ºæœªæ¥åŠŸèƒ½é¢„ç•™çš„")
    print("   - ä¿æŒè¯æ±‡è¡¨å¤§å°ä¸º2çš„å¹‚æ¬¡ç­‰æŠ€æœ¯åŸå› ")
    print("   - ä¸ºå¤šæ¨¡æ€tokené¢„ç•™ç©ºé—´")
    
    print("\n2. ğŸ“š è®­ç»ƒç­–ç•¥")
    print("   - è®­ç»ƒæ—¶å¯èƒ½ä½¿ç”¨äº†æ›´å¤§çš„è¯æ±‡è¡¨")
    print("   - ä½†æ¨ç†æ—¶åªæ¿€æ´»å®é™…éœ€è¦çš„token")
    print("   - å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¼€é”€")
    
    print("\n3. ğŸ”§ å®ç°ç»†èŠ‚")
    print("   - transformersåº“å¯èƒ½åªåŠ è½½å®šä¹‰çš„token")
    print("   - æœªå®šä¹‰çš„tokenä½ç½®ä¿æŒç©ºç™½")
    print("   - é¿å…åŠ è½½æ— æ•ˆæˆ–æœªè®­ç»ƒçš„token")
    
    print("\n4. ğŸ”„ ç‰ˆæœ¬å…¼å®¹æ€§")
    print("   - ä¿æŒä¸ä¸åŒç‰ˆæœ¬æ¨¡å‹çš„å…¼å®¹æ€§")
    print("   - æ”¯æŒåŠ¨æ€æ·»åŠ æ–°token")
    print("   - å‘åå…¼å®¹æ—§ç‰ˆæœ¬çš„åˆ†è¯å™¨")
    
    print("\n5. ğŸ¯ æ¨¡å‹è®¾è®¡")
    print("   - å®é™…æœ‰æ•ˆè¯æ±‡é‡å°±æ˜¯151665")
    print("   - 151936æ˜¯æœ€å¤§å®¹é‡è€Œéå®é™…ä½¿ç”¨é‡")
    print("   - ç±»ä¼¼äºæ•°ç»„åˆ†é…vså®é™…ä½¿ç”¨çš„æ¦‚å¿µ")

def check_practical_implications():
    """æ£€æŸ¥å®é™…å½±å“"""
    print("\n" + "="*60)
    print(" å¯¹CausalQwençš„å®é™…å½±å“ ".center(58))
    print("="*60)
    
    print("å¯¹æˆ‘ä»¬é¡¹ç›®çš„å½±å“ï¼š")
    
    print("\nâœ… æ­£é¢å½±å“ï¼š")
    print("  - ç”¨æˆ·è®°å¿†æ­£ç¡®ï¼Œå®˜æ–¹è§„æ ¼ç¡®å®æ˜¯151936")
    print("  - æ¨¡å‹æƒé‡ç¡®å®æ”¯æŒ151936ä¸ªtoken")
    print("  - æˆ‘ä»¬çš„ç†è®ºåŸºç¡€æ˜¯æ­£ç¡®çš„")
    
    print("\nâš ï¸ éœ€è¦æ³¨æ„ï¼š")
    print("  - å®é™…å¯ç”¨çš„tokenåªæœ‰151665ä¸ª")
    print("  - æ·»åŠ <NUM> tokenä¼šå˜æˆ151666")
    print("  - è¿™ä»ç„¶è¿œå°äº151936çš„æœ€å¤§å®¹é‡")
    
    print("\nğŸ¯ å»ºè®®çš„åšæ³•ï¼š")
    print("  - ç†è®ºæ–‡æ¡£ä¸­ä½¿ç”¨151936ä½œä¸ºKå€¼")
    print("  - å®ç°ä¸­ä½¿ç”¨å®é™…å¤§å°151665")
    print("  - CausalQwenè¯æ±‡è¡¨å¤§å°ï¼š151665 + 1 = 151666")
    print("  - åœ¨æ–‡æ¡£ä¸­æ³¨æ˜è¿™ä¸ªå·®å¼‚")
    
    print("\nğŸ“Š å®¹é‡åˆ†æï¼š")
    total_capacity = 151936
    actually_used = 151665
    our_addition = 1
    remaining_capacity = total_capacity - actually_used - our_addition
    
    print(f"  æ€»å®¹é‡ï¼š     {total_capacity}")
    print(f"  å·²ä½¿ç”¨ï¼š     {actually_used}")
    print(f"  æˆ‘ä»¬æ·»åŠ ï¼š   {our_addition}")
    print(f"  å‰©ä½™å®¹é‡ï¼š   {remaining_capacity}")
    print(f"  ä½¿ç”¨ç‡ï¼š     {(actually_used + our_addition)/total_capacity*100:.1f}%")

def main():
    """ä¸»å‡½æ•°"""
    analyze_missing_tokens()
    investigate_tokenizer_implementation()
    analyze_possible_reasons()
    check_practical_implications()
    
    print("\n" + "="*60)
    print(" ç»“è®º ".center(58))
    print("="*60)
    print("è¿™271ä¸ª'ç¼ºå¤±'çš„tokenå®é™…ä¸Šæ˜¯æ­£å¸¸ç°è±¡ï¼")
    print("å®ƒä»¬ä»£è¡¨äº†é…ç½®å®¹é‡ä¸å®é™…ä½¿ç”¨ä¹‹é—´çš„å·®å¼‚ã€‚")
    print("å¯¹æˆ‘ä»¬çš„CausalQwené¡¹ç›®æ²¡æœ‰è´Ÿé¢å½±å“ã€‚")
    print("æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°æ·»åŠ <NUM> tokenï¼")

if __name__ == '__main__':
    main() 
#!/usr/bin/env python3
"""
CausalQwen vs Qwen ç«¯åˆ°ç«¯å¯¹æ¯”æµ‹è¯• - æ›´æ–°ç‰ˆ

ä½¿ç”¨å®Œå…¨å…¼å®¹Qwençš„æ¥å£è¿›è¡Œå¯¹æ¯”æµ‹è¯•

ç›®æ ‡ï¼šä»ç›¸åŒè¾“å…¥å‡ºå‘ï¼Œå¯¹æ¯”CausalQwenä¸åŸå§‹Qwençš„è¾“å‡ºå·®å¼‚
éªŒè¯ï¼š
1. CausalQwenç¡®å®šæ€§æ¨¡å¼ (do_sample=False) ä¸Qwençš„å…¼å®¹æ€§
2. CausalQwené‡‡æ ·æ¨¡å¼ (do_sample=True) ä½“ç°V2æ•°å­¦åŸç†
3. æ‰€æœ‰æ¨¡å¼çš„æ•°å­¦è®¡ç®—ç¬¦åˆè®¾è®¡æ–‡æ¡£
4. å®Œæ•´çš„ç”Ÿæˆèƒ½åŠ›éªŒè¯

æµ‹è¯•æµç¨‹ï¼š
è¾“å…¥æ–‡æœ¬ â†’ tokenize â†’ ç”Ÿæˆ â†’ å¯¹æ¯”åˆ†æ
"""

import sys
import os
import torch
import torch.nn.functional as F
from pathlib import Path
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# ANSI color codes
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

def print_section(title, color=Colors.BLUE):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{color}{Colors.BOLD}{'='*80}{Colors.END}")
    print(f"{color}{Colors.BOLD}{title.center(80)}{Colors.END}")
    print(f"{color}{Colors.BOLD}{'='*80}{Colors.END}")

def print_step(step_num, description, color=Colors.CYAN):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{color}ğŸ“‹ æ­¥éª¤ {step_num}: {description}{Colors.END}")

def print_success(message):
    print(f"{Colors.GREEN}âœ… {message}{Colors.END}")

def print_error(message):
    print(f"{Colors.RED}âŒ {message}{Colors.END}")

def print_warning(message):
    print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")

def print_info(message):
    print(f"{Colors.WHITE}â„¹ï¸  {message}{Colors.END}")

def print_math(message):
    print(f"{Colors.PURPLE}ğŸ”¢ {message}{Colors.END}")

def load_models_and_tokenizer():
    """åŠ è½½Qwenå’ŒCausalQwenæ¨¡å‹"""
    print_section("æ¨¡å‹åŠ è½½ä¸åˆå§‹åŒ–")
    
    print_step(1, "åŠ è½½Qwen2åŸå§‹æ¨¡å‹")
    try:
        from transformers import Qwen2ForCausalLM, Qwen2Config, AutoTokenizer
        
        qwen_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(qwen_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print_success(f"Qwen2åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer.vocab)}")
        
        # åŠ è½½Qwenæ¨¡å‹é…ç½®å’Œæƒé‡
        qwen_config = Qwen2Config.from_pretrained(qwen_path)
        qwen_model = Qwen2ForCausalLM.from_pretrained(qwen_path, torch_dtype=torch.float32)
        qwen_model.eval()
        print_success(f"Qwen2æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in qwen_model.parameters()):,}")
        
        return tokenizer, qwen_model, qwen_config
        
    except Exception as e:
        print_error(f"Qwen2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def create_causal_qwen_model(qwen_config):
    """åˆ›å»ºCausalQwenæ¨¡å‹"""
    print_step(2, "åˆ›å»ºCausalQwenæ¨¡å‹")
    try:
        from causal_qwen_mvp import CausalQwenMVPForCausalLM, CausalQwen2Config
        
        # åˆ›å»ºCausalQwené…ç½®ï¼ˆå®Œå…¨ç»§æ‰¿Qwenå‚æ•°ï¼‰
        causal_config = CausalQwen2Config(
            # å®Œå…¨å¤åˆ¶Qwené…ç½®
            **qwen_config.to_dict(),
            # CausalQwenç‰¹æœ‰å‚æ•°
            causal_size=qwen_config.hidden_size,  # C = H
            abduction_init_strategy='identity',
            b_noise_init=0.1,
            gamma_init=10.0,
            ovr_threshold_init=0.0
        )
        
        # åˆ›å»ºæ¨¡å‹
        causal_model = CausalQwenMVPForCausalLM(causal_config)
        causal_model.eval()
        print_success(f"CausalQwenæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in causal_model.parameters()):,}")
        
        return causal_model, causal_config
        
    except Exception as e:
        print_error(f"CausalQwenæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None, None

def copy_qwen_weights_to_causal(qwen_model, causal_model):
    """å°†Qwenæƒé‡å¤åˆ¶åˆ°CausalQwen"""
    print_step(3, "å¤åˆ¶Qwené¢„è®­ç»ƒæƒé‡")
    try:
        print_info("å¤åˆ¶TransformeråŸºç¡€æƒé‡...")
        
        # ä½¿ç”¨å®Œæ•´çš„state_dictå¤åˆ¶
        qwen_state_dict = qwen_model.model.state_dict()
        causal_state_dict = causal_model.model.state_dict()
        
        # å¤åˆ¶æ‰€æœ‰åŒ¹é…çš„æƒé‡
        copied_keys = []
        for key in qwen_state_dict.keys():
            if key in causal_state_dict and qwen_state_dict[key].shape == causal_state_dict[key].shape:
                causal_state_dict[key].copy_(qwen_state_dict[key])
                copied_keys.append(key)
        
        # åŠ è½½æ›´æ–°åçš„state_dict
        causal_model.model.load_state_dict(causal_state_dict)
        
        print_info(f"æˆåŠŸå¤åˆ¶ {len(copied_keys)} ä¸ªTransformerå‚æ•°")
        
        # éªŒè¯å¤åˆ¶æ•ˆæœ
        print_info("éªŒè¯æƒé‡å¤åˆ¶æ•ˆæœ...")
        with torch.no_grad():
            test_input = torch.randint(0, 1000, (1, 5))
            qwen_features = qwen_model.model(test_input)[0]
            causal_features = causal_model.model(test_input)[0]
            feature_diff = torch.abs(qwen_features - causal_features).mean().item()
            
        print_math(f"ç‰¹å¾éªŒè¯å·®å¼‚: {feature_diff:.8f}")
        
        if feature_diff < 1e-6:
            print_success("âœ… Transformeræƒé‡å¤åˆ¶å®Œç¾ï¼")
        else:
            print_warning(f"âš ï¸ ç‰¹å¾å·®å¼‚ {feature_diff:.8f}ï¼Œç»§ç»­è°ƒè¯•...")
        
        # å¤åˆ¶lm_headåˆ°ActionNetwork
        print_info("å¤åˆ¶lm_headæƒé‡åˆ°ActionNetwork...")
        causal_model.action_network.copy_weights_from_qwen(qwen_model)
        
        print_success("æƒé‡å¤åˆ¶å®Œæˆï¼CausalQwenç°åœ¨ç»§æ‰¿äº†Qwençš„é¢„è®­ç»ƒçŸ¥è¯†")
        
        return True
        
    except Exception as e:
        print_error(f"æƒé‡å¤åˆ¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_generation_methods(text, tokenizer, qwen_model, causal_model):
    """å¯¹æ¯”ä¸åŒçš„ç”Ÿæˆæ–¹æ³•"""
    print_section(f"ç”Ÿæˆæ–¹æ³•å¯¹æ¯”ï¼š'{text}'")
    
    print_step(1, "å‡†å¤‡è¾“å…¥")
    
    # ç¼–ç æ–‡æœ¬
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    input_ids = inputs['input_ids']
    attention_mask = inputs.get('attention_mask', None)
    
    print_info(f"åŸå§‹æ–‡æœ¬: '{text}'")
    print_info(f"Token IDs: {input_ids.tolist()}")
    print_info(f"Tokenæ•°é‡: {input_ids.shape[1]}")
    
    # è§£ç æ¯ä¸ªtoken
    tokens = [tokenizer.decode([token_id]) for token_id in input_ids[0]]
    print_info(f"Tokenåºåˆ—: {tokens}")
    
    print_step(2, "QwenåŸºå‡†ç”Ÿæˆ")
    
    with torch.no_grad():
        # === Qwenç¡®å®šæ€§ç”Ÿæˆ ===
        qwen_det_output = qwen_model.generate(
            input_ids,
            max_new_tokens=5,
            do_sample=False,
            temperature=1.0,
            pad_token_id=tokenizer.eos_token_id
        )
        qwen_det_new = qwen_det_output[0, input_ids.shape[1]:].tolist()
        qwen_det_text = tokenizer.decode(qwen_det_new)
        
        print_info(f"Qwenç¡®å®šæ€§ç”Ÿæˆ: tokens={qwen_det_new}, text='{qwen_det_text}'")
        
        # === Qwené‡‡æ ·ç”Ÿæˆ ===  
        qwen_samp_output = qwen_model.generate(
            input_ids,
            max_new_tokens=5,
            do_sample=True,
            temperature=0.8,
            top_k=50,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
        qwen_samp_new = qwen_samp_output[0, input_ids.shape[1]:].tolist()
        qwen_samp_text = tokenizer.decode(qwen_samp_new)
        
        print_info(f"Qwené‡‡æ ·ç”Ÿæˆ: tokens={qwen_samp_new}, text='{qwen_samp_text}'")
    
    print_step(3, "CausalQwenç”Ÿæˆå¯¹æ¯”")
    
    with torch.no_grad():
        # === CausalQwenç¡®å®šæ€§ç”Ÿæˆ (do_sample=False) ===
        print_info("ğŸ¯ CausalQwenç¡®å®šæ€§æ¨¡å¼ (do_sample=False)")
        print_info("   V2åŸç†: å™ªå£°å½±å“å°ºåº¦å‚æ•°ï¼Œå¢åŠ å†³ç­–ä¸ç¡®å®šæ€§")
        
        causal_det_output = causal_model.generate(
            input_ids,
            max_new_tokens=5,
            do_sample=False,
            temperature=1.0
        )
        causal_det_new = causal_det_output[0, input_ids.shape[1]:].tolist()
        causal_det_text = tokenizer.decode(causal_det_new)
        
        print_info(f"CausalQwenç¡®å®šæ€§: tokens={causal_det_new}, text='{causal_det_text}'")
        
        # === CausalQwené‡‡æ ·ç”Ÿæˆ (do_sample=True) ===
        print_info("ğŸ² CausalQwené‡‡æ ·æ¨¡å¼ (do_sample=True)")
        print_info("   V2åŸç†: å™ªå£°å½±å“ä½ç½®å‚æ•°ï¼Œæ‰°åŠ¨ä¸ªä½“èº«ä»½")
        
        causal_samp_outputs = []
        for trial in range(3):  # å¤šæ¬¡é‡‡æ ·å±•ç¤ºéšæœºæ€§
            causal_samp_output = causal_model.generate(
                input_ids,
                max_new_tokens=5,
                do_sample=True,
                temperature=0.8,
                top_k=50,
                top_p=0.9
            )
            causal_samp_new = causal_samp_output[0, input_ids.shape[1]:].tolist()
            causal_samp_text = tokenizer.decode(causal_samp_new)
            causal_samp_outputs.append((causal_samp_new, causal_samp_text))
            print_info(f"CausalQwené‡‡æ ·#{trial+1}: tokens={causal_samp_new}, text='{causal_samp_text}'")
    
    print_step(4, "ä¸€è‡´æ€§éªŒè¯")
    
    # éªŒè¯1ï¼šç¡®å®šæ€§æ¨¡å¼çš„logitsä¸€è‡´æ€§ï¼ˆå…³é”®éªŒè¯ï¼‰
    print_info("ğŸ¯ éªŒè¯CausalQwenç¡®å®šæ€§æ¨¡å¼çš„loc_Sä¸Qwençš„logitsä¸€è‡´æ€§")
    
    # è·å–CausalQwenç¡®å®šæ€§æ¨¡å¼çš„å‰å‘ä¼ æ’­ç»“æœ
    with torch.no_grad():
        causal_outputs = causal_model(input_ids)
        # æå–loc_Sï¼ˆå†³ç­–åˆ†å¸ƒçš„ä½ç½®å‚æ•°ï¼‰
        if hasattr(causal_outputs, 'loc_S'):
            causal_loc_S = causal_outputs.loc_S
        else:
            # å¦‚æœæ²¡æœ‰ç›´æ¥çš„loc_Sï¼Œé€šè¿‡ActionNetworkè·å–
            transformer_out = causal_model.model(input_ids)
            hidden_states = transformer_out.last_hidden_state
            loc_U, scale_U = causal_model.abduction_network(hidden_states)
            causal_loc_S, _ = causal_model.action_network(loc_U, scale_U, do_sample=False)
        
        # è·å–Qwençš„logits
        qwen_outputs = qwen_model(input_ids)
        qwen_logits = qwen_outputs.logits
        
        # æ¯”è¾ƒæœ€åä¸€ä¸ªä½ç½®çš„logits/loc_S
        last_pos_causal = causal_loc_S[:, -1, :]  # [batch, vocab]
        last_pos_qwen = qwen_logits[:, -1, :]     # [batch, vocab]
        
        logits_diff = torch.abs(last_pos_causal - last_pos_qwen).mean().item()
        logits_max_diff = torch.abs(last_pos_causal - last_pos_qwen).max().item()
        
        print_math(f"loc_S vs Qwen logitså¹³å‡å·®å¼‚: {logits_diff:.8f}")
        print_math(f"loc_S vs Qwen logitsæœ€å¤§å·®å¼‚: {logits_max_diff:.8f}")
        
        if logits_diff < 1e-4:
            print_success(f"âœ… ç¡®å®šæ€§æ¨¡å¼logitsä¸€è‡´æ€§éªŒè¯é€šè¿‡ï¼")
            print_success(f"   CausalQwençš„loc_Sä¸Qwençš„logitsåŸºæœ¬ä¸€è‡´")
            logits_consistent = True
        else:
            print_warning(f"âš ï¸ logitså·®å¼‚è¾ƒå¤§: {logits_diff:.8f}")
            print_warning(f"   è¿™å¯èƒ½è¡¨æ˜æƒé‡å¤åˆ¶ä¸å®Œæ•´æˆ–ActionNetworkå®ç°æœ‰è¯¯")
            logits_consistent = False
    
    # éªŒè¯2ï¼šé‡‡æ ·æ¨¡å¼å¤šæ ·æ€§
    all_causal_samp = [output[0] for output in causal_samp_outputs]
    causal_diversity = len(set(tuple(seq) for seq in all_causal_samp))
    
    print_info(f"CausalQwené‡‡æ ·å¤šæ ·æ€§: {causal_diversity}/3 ä¸ªä¸åŒç»“æœ")
    
    if causal_diversity >= 2:
        print_success("âœ… CausalQwené‡‡æ ·æ¨¡å¼å…·æœ‰è‰¯å¥½å¤šæ ·æ€§")
    else:
        print_warning("âš ï¸ CausalQwené‡‡æ ·å¤šæ ·æ€§è¾ƒä½")
    
    # éªŒè¯3ï¼šV2æ•°å­¦åŸç†
    print_info("ğŸ§® éªŒè¯V2æ•°å­¦åŸç†")
    try:
        from causal_qwen_mvp import InferenceValidator
        
        validator = InferenceValidator(causal_model)
        results = validator.validate_v2_principles(input_ids)
        
        pos_diff = results['position_difference'].item()
        scale_diff = results['scale_difference'].item()
        
        print_math(f"ä½ç½®å‚æ•°å·®å¼‚: {pos_diff:.6f}")
        print_math(f"å°ºåº¦å‚æ•°å·®å¼‚: {scale_diff:.6f}")
        
        if pos_diff > 1e-3:
            print_success("âœ… V2æ•°å­¦åŸç†éªŒè¯ï¼šä½ç½®vså°ºåº¦å·®å¼‚æ˜¾è‘—")
        else:
            print_warning("âš ï¸ ä½ç½®å‚æ•°å·®å¼‚è¾ƒå°")
    
    except Exception as e:
        print_error(f"V2æ•°å­¦éªŒè¯å¤±è´¥: {e}")
    
    return {
        'qwen_deterministic': (qwen_det_new, qwen_det_text),
        'qwen_sampling': (qwen_samp_new, qwen_samp_text),
        'causal_deterministic': (causal_det_new, causal_det_text),
        'causal_sampling': causal_samp_outputs,
        'logits_consistent': logits_consistent,
        'logits_difference': logits_diff,
        'causal_diversity': causal_diversity
    }

def test_different_temperatures(text, tokenizer, causal_model):
    """æµ‹è¯•ä¸åŒæ¸©åº¦å‚æ•°çš„æ•ˆæœ"""
    print_section(f"æ¸©åº¦å‚æ•°æ•ˆæœæµ‹è¯•ï¼š'{text}'")
    
    inputs = tokenizer(text, return_tensors='pt')
    input_ids = inputs['input_ids']
    
    temperatures = [0.0, 0.1, 0.5, 1.0, 1.5, 2.0]  # æ·»åŠ æ¸©åº¦ä¸ºé›¶çš„æµ‹è¯•
    temp_results = []
    
    print_info("æµ‹è¯•ä¸åŒæ¸©åº¦ä¸‹çš„CausalQwené‡‡æ ·ç”Ÿæˆ:")
    
    for temp in temperatures:
        torch.manual_seed(42)  # å›ºå®šç§å­ä¾¿äºå¯¹æ¯”
        with torch.no_grad():
            output = causal_model.generate(
                input_ids,
                max_new_tokens=3,
                do_sample=True,
                temperature=temp,
                top_k=50
            )
            new_tokens = output[0, input_ids.shape[1]:].tolist()
            new_text = tokenizer.decode(new_tokens)
            temp_results.append((temp, new_tokens, new_text))
            print_info(f"T={temp}: tokens={new_tokens}, text='{new_text}'")
    
    # ç‰¹åˆ«éªŒè¯æ¸©åº¦ä¸ºé›¶çš„åœºæ™¯
    if len(temp_results) > 0 and temp_results[0][0] == 0.0:
        print_info("ğŸŒ¡ï¸ æ¸©åº¦ä¸ºé›¶æ˜¯æå…¶é‡è¦çš„è¾¹ç•Œæ¡ä»¶ï¼")
        temp_zero_tokens, temp_zero_text = temp_results[0][1], temp_results[0][2]
        print_info(f"æ¸©åº¦T=0ç»“æœ: tokens={temp_zero_tokens}, text='{temp_zero_text}'")
    
    # åˆ†ææ¸©åº¦æ•ˆæœ
    unique_sequences = len(set(tuple(result[1]) for result in temp_results))
    print_math(f"ä¸åŒæ¸©åº¦äº§ç”Ÿçš„åºåˆ—å¤šæ ·æ€§: {unique_sequences}/{len(temperatures)}")
    
    if unique_sequences >= 3:
        print_success("âœ… æ¸©åº¦å‚æ•°æœ‰æ•ˆæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§")
    else:
        print_warning("âš ï¸ æ¸©åº¦å‚æ•°æ•ˆæœä¸æ˜æ˜¾")
    
    return temp_results

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_section("CausalQwen vs Qwen ç«¯åˆ°ç«¯å¯¹æ¯”æµ‹è¯• - æ›´æ–°ç‰ˆ", Colors.PURPLE)
    print_info("ä½¿ç”¨ä¸Qwenå®Œå…¨å…¼å®¹çš„æ¥å£è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
    print_info("éªŒè¯CausalQwençš„V2æ•°å­¦åŸç†å’ŒQwenå…¼å®¹æ€§")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "äººå·¥æ™ºèƒ½çš„å‘å±•", 
        "åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸ",
        "The future of technology"
    ]
    
    # åŠ è½½æ¨¡å‹
    tokenizer, qwen_model, qwen_config = load_models_and_tokenizer()
    if qwen_model is None:
        print_error("Qwenæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    causal_model, causal_config = create_causal_qwen_model(qwen_config)
    if causal_model is None:
        print_error("CausalQwenæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # å¤åˆ¶æƒé‡
    if not copy_qwen_weights_to_causal(qwen_model, causal_model):
        print_error("æƒé‡å¤åˆ¶å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # å¯¹æ¯ä¸ªæµ‹è¯•æ–‡æœ¬è¿›è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•
    all_results = []
    
    for i, text in enumerate(test_texts):
        print_section(f"æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(test_texts)}: '{text}'", Colors.GREEN)
        
        try:
            # ç”Ÿæˆæ–¹æ³•å¯¹æ¯”
            generation_results = compare_generation_methods(text, tokenizer, qwen_model, causal_model)
            
            # æ¸©åº¦æ•ˆæœæµ‹è¯•
            temp_results = test_different_temperatures(text, tokenizer, causal_model)
            
            # è®°å½•ç»“æœ
            all_results.append({
                'text': text,
                'generation': generation_results,
                'temperature': temp_results
            })
            
        except Exception as e:
            print_error(f"æµ‹è¯•æ¡ˆä¾‹ {i+1} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ€»ç»“æŠ¥å‘Š
    print_section("æµ‹è¯•æ€»ç»“æŠ¥å‘Š", Colors.GREEN)
    
    # ç»Ÿè®¡logitsä¸€è‡´æ€§éªŒè¯ç»“æœï¼ˆå…³é”®æŒ‡æ ‡ï¼‰
    logits_consistency_count = sum(1 for result in all_results if result['generation']['logits_consistent'])
    total_cases = len(all_results)
    
    print_success(f"logitsä¸€è‡´æ€§: {logits_consistency_count}/{total_cases} ä¸ªæ¡ˆä¾‹é€šè¿‡")
    
    # ç»Ÿè®¡å¹³å‡logitså·®å¼‚
    if all_results:
        avg_logits_diff = np.mean([result['generation']['logits_difference'] for result in all_results if 'logits_difference' in result['generation']])
        print_success(f"å¹³å‡logitså·®å¼‚: {avg_logits_diff:.8f}")
    
    # åˆ†æå¤šæ ·æ€§
    diversity_scores = [result['generation']['causal_diversity'] for result in all_results]
    avg_diversity = np.mean(diversity_scores) if diversity_scores else 0
    
    print_success(f"å¹³å‡é‡‡æ ·å¤šæ ·æ€§: {avg_diversity:.2f}/3")
    
    # æ¸©åº¦æ•ˆæœåˆ†æ
    temp_diversity_scores = []
    for result in all_results:
        if 'temperature' in result:
            temp_seqs = [tuple(tr[1]) for tr in result['temperature']]
            temp_diversity = len(set(temp_seqs))
            temp_diversity_scores.append(temp_diversity)
    
    avg_temp_diversity = np.mean(temp_diversity_scores) if temp_diversity_scores else 0
    print_success(f"å¹³å‡æ¸©åº¦å¤šæ ·æ€§: {avg_temp_diversity:.2f}/5")
    
    if logits_consistency_count == total_cases:
        print_section("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CausalQwenä¸Qwenå®Œå…¨å…¼å®¹ï¼", Colors.GREEN)
        print_success("âœ… ç¡®å®šæ€§æ¨¡å¼çš„loc_Sä¸Qwençš„logitså®Œå…¨ä¸€è‡´")
        print_success("âœ… é‡‡æ ·æ¨¡å¼ä½“ç°V2æ•°å­¦åŸç†")
        print_success("âœ… æ¸©åº¦å‚æ•°æ­£ç¡®æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§")
        print_success("âœ… å®Œå…¨å…¼å®¹Qwençš„generate()æ¥å£")
    else:
        print_section("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•", Colors.YELLOW)
        print_info("è¯·æ£€æŸ¥æƒé‡å¤åˆ¶å’ŒActionNetworkå®ç°")
        print_info(f"logitsä¸€è‡´æ€§: {logits_consistency_count}/{total_cases} ä¸ªæ¡ˆä¾‹é€šè¿‡")
    
    print_info("CausalQwen V2æ ¸å¿ƒç‰¹æ€§:")
    print_info("â”œâ”€ do_sample=False: å™ªå£°å½±å“å°ºåº¦å‚æ•°ï¼Œå¢åŠ å†³ç­–ä¸ç¡®å®šæ€§")
    print_info("â””â”€ do_sample=True: å™ªå£°å½±å“ä½ç½®å‚æ•°ï¼Œæ‰°åŠ¨ä¸ªä½“èº«ä»½")

if __name__ == "__main__":
    main()
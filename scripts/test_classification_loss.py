#!/usr/bin/env python
"""
åˆ†ç±»æŸå¤± (Classification Loss) æµç¨‹å›¾å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `mathematical_foundations.md` ä¸­çš„ "å›¾ 5.1" æµç¨‹å›¾ï¼Œ
æ—¨åœ¨ç™½ç›’æµ‹è¯•æ ¸å¿ƒæŸå¤±å‡½æ•° `ovr_classification_loss` çš„è®¡ç®—é€»è¾‘æ˜¯å¦ç²¾ç¡®ã€‚
"""
import torch
import torch.nn.functional as F
import os
import sys

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.losses.loss_functions import ovr_classification_loss

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  æ­¥éª¤ {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - åˆ†ç±»æŸå¤± (L_cls) æ ¸å¿ƒé€»è¾‘æ·±åº¦éªŒè¯")

    # --- å‚æ•°å®šä¹‰ ---
    B, S, V = 4, 16, 128 # ä½¿ç”¨è¾ƒå°çš„ V ä»¥åŠ é€Ÿæµ‹è¯•
    C_OVR = 100.0
    print(f"\nè®¾å®šå‚æ•°: B={B}, S={S}, V={V}, C_OvR={C_OVR}")

    # --- æ­¥éª¤ 1: å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥ ---
    print_step("1", "å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥: loc_S, scale_S, å’ŒçœŸå®æ ‡ç­¾")
    loc_S = torch.randn(B, S, V) * 50
    scale_S = torch.rand(B, S, V) * 5 + 1
    labels = torch.randint(0, V, (B, S))
    
    # æ ¹æ® loc å’Œ scale è®¡ç®—æ¦‚ç‡ Pï¼Œè¿™æ˜¯æŸå¤±å‡½æ•°çš„ç›´æ¥è¾“å…¥
    prob_P = 0.5 + (1 / torch.pi) * torch.atan((loc_S - C_OVR) / scale_S)

    # --- æ­¥éª¤ 2: æ‰‹åŠ¨è®¡ç®—æŸå¤± (Ground Truth) ---
    print_step("2", "æ‰‹åŠ¨è®¡ç®—æŸå¤± (Ground Truth) - ç²¾ç¡®æ¨¡ä»¿åº•å±‚å‡½æ•°")
    
    epsilon = 1e-8
    logits_manual = torch.log(prob_P / (1 - prob_P + epsilon) + epsilon)
    y_one_hot = F.one_hot(labels, num_classes=V).float()
    
    bce_loss_unreduced = F.binary_cross_entropy_with_logits(
        logits_manual, y_one_hot, reduction='none'
    )
    L_cls_per_token = bce_loss_unreduced.sum(dim=-1)
    expected_scalar_loss = L_cls_per_token.mean()
    
    print(f"   - é¢„æœŸçš„æ ‡é‡æŸå¤± (æ‰‹åŠ¨è®¡ç®—): {expected_scalar_loss.item():.6f}")

    # --- æ­¥éª¤ 3: ä½¿ç”¨ `ovr_classification_loss` å‡½æ•°è®¡ç®— ---
    print_step("3", "ä½¿ç”¨ `ovr_classification_loss` å‡½æ•°è®¡ç®—")
    actual_scalar_loss = ovr_classification_loss(
        probs=prob_P, 
        targets=labels,
        reduction='mean'
    )
    print(f"   - å®é™…çš„æ ‡é‡æŸå¤± (å‡½æ•°è®¡ç®—): {actual_scalar_loss.item():.6f}")
    
    # --- æ­¥éª¤ 4: æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯ ---
    print_step("4", "æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯")
    
    loss_match = torch.allclose(expected_scalar_loss, actual_scalar_loss, atol=1e-5)
    
    print(f"\n   --- éªŒè¯: æ‰‹åŠ¨è®¡ç®— vs. å‡½æ•°è®¡ç®— ---")
    print(f"     - ç»“è®º: {'âœ… é€šè¿‡' if loss_match else 'âŒ å¤±è´¥'}")
    if not loss_match:
        diff = torch.abs(expected_scalar_loss - actual_scalar_loss)
        print(f"     - ç»å¯¹å·®å¼‚: {diff.item():.8f}")

    print(f"\n\n{'='*80}")
    if loss_match:
        print("ğŸ‰ éªŒè¯æˆåŠŸï¼`ovr_classification_loss` çš„å®ç°å®Œå…¨ç¬¦åˆæ•°å­¦è®¾è®¡ã€‚")
    else:
        print("âŒ éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ `ovr_classification_loss` çš„å†…éƒ¨é€»è¾‘ã€‚")

if __name__ == '__main__':
    main() 
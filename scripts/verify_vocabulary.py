#!/usr/bin/env python
"""
è¯æ±‡è¡¨éªŒè¯è„šæœ¬ï¼šå½»åº•è§£å†³CausalQwenè¯æ±‡è¡¨å¤§å°é—®é¢˜

æœ¬è„šæœ¬ä¸“é—¨éªŒè¯ä»¥ä¸‹é—®é¢˜ï¼š
1. QwenåŸå§‹è¯æ±‡è¡¨å¤§å° K
2. CausalQwenè¯æ±‡è¡¨å¤§å° K+1
3. <NUM> tokençš„æ­£ç¡®æ·»åŠ å’ŒIDè·å–
4. åˆ†è¯å™¨è¡Œä¸ºçš„å®Œæ•´éªŒè¯
5. æ¨¡å‹æƒé‡å½¢çŠ¶å¯¹æ¯”éªŒè¯

ç›®æ ‡ï¼šè®©ç”¨æˆ·æ¸…æ¸…æ¥šæ¥šã€æ˜æ˜ç™½ç™½åœ°ç†è§£è¯æ±‡è¡¨è®¾è®¡
"""

import os
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.tokenizer import QwenTokenizerWrapper
from src.models.causal_lm import CausalLMConfig, CausalLanguageModel


def print_separator(title, level=1):
    """æ‰“å°åˆ†éš”ç¬¦"""
    if level == 1:
        print("\n" + "=" * 80)
        print(f"=   {title.center(74)}   =")
        print("=" * 80)
    else:
        print("\n" + "-" * 60)
        print(f" {title.center(56)} ")
        print("-" * 60)


def verify_original_qwen_tokenizer(model_path):
    """éªŒè¯åŸå§‹Qwenåˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°"""
    print_separator("æ­¥éª¤1ï¼šéªŒè¯åŸå§‹Qwenåˆ†è¯å™¨", 2)
    
    try:
        # ç›´æ¥åŠ è½½Qwenåˆ†è¯å™¨
        expanded_path = os.path.expanduser(model_path)
        print(f"æ­£åœ¨åŠ è½½åŸå§‹Qwenåˆ†è¯å™¨: {expanded_path}")
        
        original_tokenizer = AutoTokenizer.from_pretrained(expanded_path, trust_remote_code=True)
        
        # è·å–åŸå§‹è¯æ±‡è¡¨ä¿¡æ¯
        original_vocab_size = len(original_tokenizer)
        original_vocab = original_tokenizer.get_vocab()
        
        print(f"âœ… åŸå§‹Qwenåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å° K = {original_vocab_size}")
        print(f"   pad_token_id = {original_tokenizer.pad_token_id}")
        print(f"   eos_token_id = {original_tokenizer.eos_token_id}")
        print(f"   bos_token_id = {original_tokenizer.bos_token_id}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰<NUM> token
        num_token = "<NUM>"
        has_num_token = num_token in original_vocab
        
        print(f"\n<NUM> token æ£€æŸ¥:")
        print(f"   åŸå§‹è¯æ±‡è¡¨ä¸­æ˜¯å¦æœ‰'{num_token}': {'æ˜¯' if has_num_token else 'å¦'}")
        
        if has_num_token:
            num_token_id = original_vocab[num_token]
            print(f"   åŸå§‹'{num_token}' token ID: {num_token_id}")
        else:
            print(f"   åŸå§‹è¯æ±‡è¡¨ä¸­æ²¡æœ‰'{num_token}' token (ç¬¦åˆé¢„æœŸ)")
        
        return original_tokenizer, original_vocab_size, has_num_token
        
    except Exception as e:
        print(f"âŒ åŸå§‹Qwenåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return None, 0, False


def verify_causal_qwen_tokenizer(model_path):
    """éªŒè¯CausalQwenåˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°"""
    print_separator("æ­¥éª¤2ï¼šéªŒè¯CausalQwenåˆ†è¯å™¨", 2)
    
    try:
        # ä½¿ç”¨æˆ‘ä»¬çš„åŒ…è£…å™¨
        print(f"æ­£åœ¨åˆå§‹åŒ–QwenTokenizerWrapper...")
        
        causal_tokenizer = QwenTokenizerWrapper(
            model_path=model_path,
            use_real_tokenizer=True
        )
        
        # è·å–CausalQwenè¯æ±‡è¡¨ä¿¡æ¯
        causal_vocab_size = causal_tokenizer.vocab_size
        causal_num_token_id = causal_tokenizer.num_token_id
        
        print(f"âœ… CausalQwenåˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å° = {causal_vocab_size}")
        print(f"   <NUM> token ID = {causal_num_token_id}")
        
        # éªŒè¯<NUM> tokençš„åŠŸèƒ½
        print(f"\n<NUM> token åŠŸèƒ½éªŒè¯:")
        
        # æµ‹è¯•convert_ids_to_tokens
        try:
            num_token_str = causal_tokenizer.convert_ids_to_tokens([causal_num_token_id])[0]
            print(f"   ID {causal_num_token_id} -> token: '{num_token_str}'")
            
            # æµ‹è¯•convert_tokens_to_ids (å¦‚æœæœ‰è¿™ä¸ªæ–¹æ³•)
            if hasattr(causal_tokenizer.tokenizer, 'convert_tokens_to_ids'):
                back_to_id = causal_tokenizer.tokenizer.convert_tokens_to_ids('<NUM>')
                print(f"   token '<NUM>' -> ID: {back_to_id}")
                
                id_consistency = (back_to_id == causal_num_token_id)
                print(f"   IDå¾€è¿”ä¸€è‡´æ€§: {'âœ…' if id_consistency else 'âŒ'}")
        except Exception as e:
            print(f"   âš ï¸ tokenè½¬æ¢æµ‹è¯•å¤±è´¥: {e}")
        
        return causal_tokenizer, causal_vocab_size, causal_num_token_id
        
    except Exception as e:
        print(f"âŒ CausalQwenåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None, 0, -1


def verify_model_architectures(model_path, causal_tokenizer):
    """éªŒè¯Qwenå’ŒCausalQwenæ¨¡å‹æ¶æ„å¯¹æ¯”"""
    print_separator("æ­¥éª¤3ï¼šæ¨¡å‹æ¶æ„æƒé‡å½¢çŠ¶éªŒè¯", 2)
    
    try:
        # åŠ è½½åŸå§‹Qwenæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½åŸå§‹Qwenæ¨¡å‹...")
        expanded_path = os.path.expanduser(model_path)
        qwen_model = AutoModelForCausalLM.from_pretrained(
            expanded_path,
            torch_dtype=torch.float32,
            device_map=None
        )
        
        # è·å–Qwenæ¨¡å‹ä¿¡æ¯
        qwen_config = qwen_model.config
        qwen_hidden_size = qwen_config.hidden_size
        qwen_lm_head = qwen_model.lm_head
        qwen_lm_head_weight_shape = qwen_lm_head.weight.shape
        
        print(f"âœ… åŸå§‹Qwenæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   éšè—å±‚å¤§å° d_model = {qwen_hidden_size}")
        print(f"   lm_headæƒé‡å½¢çŠ¶ = {qwen_lm_head_weight_shape}")
        print(f"   é¢„æœŸlm_headå½¢çŠ¶ = [K, d_model] = [{len(causal_tokenizer.tokenizer)-1}, {qwen_hidden_size}]")
        
        # éªŒè¯Qwen lm_headå½¢çŠ¶
        expected_qwen_shape = (len(causal_tokenizer.tokenizer)-1, qwen_hidden_size)  # K, d_model
        qwen_shape_correct = (qwen_lm_head_weight_shape == expected_qwen_shape)
        print(f"   Qwen lm_headå½¢çŠ¶éªŒè¯: {'âœ… æ­£ç¡®' if qwen_shape_correct else 'âŒ é”™è¯¯'}")
        
        # åˆ›å»ºCausalQwenæ¨¡å‹
        print(f"\næ­£åœ¨åˆ›å»ºCausalQwenæ¨¡å‹...")
        causal_config = CausalLMConfig(
            vocab_size=causal_tokenizer.vocab_size,
            num_token_id=causal_tokenizer.num_token_id,
            hidden_size=qwen_hidden_size,  # ä½¿ç”¨ç›¸åŒçš„éšè—å±‚å¤§å°
            causal_dim=qwen_hidden_size,   # C=Hçº¦æŸ
            use_real_qwen=True,
            qwen_model_path=model_path,
            ovr_threshold=10.0,
            reg_loss_weight=1.0
        )
        
        causal_model = CausalLanguageModel(causal_config)
        
        # è·å–CausalQwenåˆ†ç±»å¤´ä¿¡æ¯
        causal_cls_head = causal_model.action_network.classification_head.causal_linear
        causal_cls_head_weight_shape = causal_cls_head.weight.shape
        causal_hidden_size = causal_config.hidden_size
        
        print(f"âœ… CausalQwenæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   éšè—å±‚å¤§å° d_model = {causal_hidden_size}")
        print(f"   åˆ†ç±»å¤´æƒé‡å½¢çŠ¶ = {causal_cls_head_weight_shape}")
        print(f"   é¢„æœŸåˆ†ç±»å¤´å½¢çŠ¶ = [K+1, d_model] = [{causal_tokenizer.vocab_size}, {causal_hidden_size}]")
        
        # éªŒè¯CausalQwenåˆ†ç±»å¤´å½¢çŠ¶
        expected_causal_shape = (causal_tokenizer.vocab_size, causal_hidden_size)  # K+1, d_model
        causal_shape_correct = (causal_cls_head_weight_shape == expected_causal_shape)
        print(f"   CausalQwenåˆ†ç±»å¤´å½¢çŠ¶éªŒè¯: {'âœ… æ­£ç¡®' if causal_shape_correct else 'âŒ é”™è¯¯'}")
        
        # å½¢çŠ¶å¯¹æ¯”æ€»ç»“
        print(f"\nğŸ“Š æ¨¡å‹æƒé‡å½¢çŠ¶å¯¹æ¯”æ€»ç»“:")
        print(f"   Qwen lm_head:      {qwen_lm_head_weight_shape}")
        print(f"   CausalQwen åˆ†ç±»å¤´: {causal_cls_head_weight_shape}")
        print(f"   è¯æ±‡è¡¨å¤§å°å·®å¼‚:   {causal_cls_head_weight_shape[0] - qwen_lm_head_weight_shape[0]} (åº”è¯¥=1)")
        print(f"   éšè—å±‚å¤§å°ä¸€è‡´:   {'âœ…' if qwen_lm_head_weight_shape[1] == causal_cls_head_weight_shape[1] else 'âŒ'}")
        
        # è®¡ç®—å‚æ•°æ•°é‡å¯¹æ¯”
        qwen_lm_head_params = qwen_lm_head_weight_shape[0] * qwen_lm_head_weight_shape[1]
        causal_cls_head_params = causal_cls_head_weight_shape[0] * causal_cls_head_weight_shape[1]
        param_diff = causal_cls_head_params - qwen_lm_head_params
        
        print(f"\nğŸ”¢ å‚æ•°æ•°é‡å¯¹æ¯”:")
        print(f"   Qwen lm_headå‚æ•°:      {qwen_lm_head_params:,}")
        print(f"   CausalQwenåˆ†ç±»å¤´å‚æ•°:  {causal_cls_head_params:,}")
        print(f"   å‚æ•°å¢åŠ é‡:            {param_diff:,}")
        print(f"   å¢åŠ ç‡:                {param_diff/qwen_lm_head_params*100:.4f}%")
        
        return qwen_model, causal_model, qwen_shape_correct and causal_shape_correct
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æ¶æ„éªŒè¯å¤±è´¥: {e}")
        return None, None, False


def verify_weight_inheritance(qwen_model, causal_model, causal_tokenizer):
    """éªŒè¯æƒé‡ç»§æ‰¿"""
    print_separator("æ­¥éª¤4ï¼šæƒé‡ç»§æ‰¿éªŒè¯", 2)
    
    try:
        # æ‰§è¡ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–
        print(f"æ­£åœ¨æ‰§è¡ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–...")
        num_target_median = 50.0
        num_target_scale = 25.0
        causal_model.init_weights(num_target_median, num_target_scale)
        
        # è·å–æƒé‡
        qwen_lm_head_weight = qwen_model.lm_head.weight.data
        causal_cls_head_weight = causal_model.action_network.classification_head.causal_linear.weight.data
        
        # æ£€æŸ¥å‰Kä¸ªtokençš„æƒé‡ç»§æ‰¿
        K = qwen_lm_head_weight.shape[0]  # åŸå§‹Qwenè¯æ±‡è¡¨å¤§å°
        inherited_weight = causal_cls_head_weight[:K, :]  # å‰Kè¡Œåº”è¯¥ç»§æ‰¿è‡ªQwen
        
        print(f"âœ… çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ£€æŸ¥å‰{K}ä¸ªtokençš„æƒé‡ç»§æ‰¿...")
        
        # è®¡ç®—æƒé‡ç›¸ä¼¼åº¦
        weight_diff = torch.abs(inherited_weight - qwen_lm_head_weight)
        max_diff = weight_diff.max().item()
        mean_diff = weight_diff.mean().item()
        
        print(f"   æƒé‡å·®å¼‚ç»Ÿè®¡:")
        print(f"     æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print(f"     å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦å®Œå…¨ä¸€è‡´
        weights_identical = torch.allclose(inherited_weight, qwen_lm_head_weight, atol=1e-6)
        print(f"   æƒé‡å®Œå…¨ä¸€è‡´: {'âœ…' if weights_identical else 'âŒ'}")
        
        # æ£€æŸ¥<NUM> tokençš„æƒé‡åˆå§‹åŒ–
        num_token_id = causal_tokenizer.num_token_id
        num_token_weight = causal_cls_head_weight[num_token_id, :]
        
        print(f"\n<NUM> token (ID: {num_token_id}) æƒé‡åˆ†æ:")
        print(f"   æƒé‡ç»Ÿè®¡: å‡å€¼={num_token_weight.mean().item():.6f}, æ ‡å‡†å·®={num_token_weight.std().item():.6f}")
        print(f"   æƒé‡èŒƒå›´: [{num_token_weight.min().item():.6f}, {num_token_weight.max().item():.6f}]")
        
        # ä¸ç»§æ‰¿æƒé‡å¯¹æ¯”
        inherited_weight_mean = inherited_weight.mean(dim=0)
        cosine_sim = torch.nn.functional.cosine_similarity(num_token_weight, inherited_weight_mean, dim=0).item()
        print(f"   ä¸ç»§æ‰¿æƒé‡ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
        
        return weights_identical
        
    except Exception as e:
        print(f"âŒ æƒé‡ç»§æ‰¿éªŒè¯å¤±è´¥: {e}")
        return False


def compare_vocabularies(original_tokenizer, original_vocab_size, causal_tokenizer, causal_vocab_size):
    """å¯¹æ¯”ä¸¤ä¸ªåˆ†è¯å™¨çš„è¯æ±‡è¡¨"""
    print_separator("æ­¥éª¤5ï¼šè¯æ±‡è¡¨å¤§å°å¯¹æ¯”éªŒè¯", 2)
    
    expected_causal_size = original_vocab_size + 1
    size_correct = (causal_vocab_size == expected_causal_size)
    
    print(f"è¯æ±‡è¡¨å¤§å°å¯¹æ¯”:")
    print(f"   åŸå§‹Qwenè¯æ±‡è¡¨å¤§å° K     = {original_vocab_size}")
    print(f"   CausalQwenè¯æ±‡è¡¨å¤§å°    = {causal_vocab_size}")
    print(f"   é¢„æœŸCausalQwenå¤§å° K+1  = {expected_causal_size}")
    print(f"   å¤§å°éªŒè¯ç»“æœ: {'âœ… æ­£ç¡® (K+1)' if size_correct else 'âŒ é”™è¯¯'}")
    
    if size_correct:
        print(f"\nğŸ‰ è¯æ±‡è¡¨å¤§å°éªŒè¯æˆåŠŸï¼")
        print(f"   ç†è®º: CausalQwen = Qwen + 1 (æ–°å¢<NUM> token)")
        print(f"   å®é™…: {causal_vocab_size} = {original_vocab_size} + 1")
    else:
        print(f"\nâŒ è¯æ±‡è¡¨å¤§å°éªŒè¯å¤±è´¥ï¼")
        print(f"   å®é™…å·®å¼‚: {causal_vocab_size - original_vocab_size}")
        
        # åˆ†æå¯èƒ½çš„åŸå› 
        if causal_vocab_size == original_vocab_size:
            print(f"   å¯èƒ½åŸå› : <NUM> tokenå·²å­˜åœ¨äºåŸå§‹è¯æ±‡è¡¨ä¸­")
        elif causal_vocab_size > expected_causal_size:
            print(f"   å¯èƒ½åŸå› : æ·»åŠ äº†å¤šä¸ªtoken")
        else:
            print(f"   å¯èƒ½åŸå› : tokenæ·»åŠ å¤±è´¥")
    
    return size_correct


def test_tokenization_functionality(causal_tokenizer):
    """æµ‹è¯•åˆ†è¯åŠŸèƒ½"""
    print_separator("æ­¥éª¤6ï¼šåˆ†è¯åŠŸèƒ½éªŒè¯", 2)
    
    test_texts = [
        "Hello world",  # æ— æ•°å­—
        "The price is 99.99 dollars",  # å•ä¸ªæ•°å­—
        "From 100 items, 25 were defective, costing 1250.50 total",  # å¤šä¸ªæ•°å­—
        "No numbers here at all",  # æ— æ•°å­—
    ]
    
    print(f"æµ‹è¯•æ–‡æœ¬åˆ†è¯å’Œæ•°å€¼æå–:")
    
    for i, text in enumerate(test_texts):
        print(f"\næ–‡æœ¬ {i+1}: '{text}'")
        
        try:
            # ä½¿ç”¨æˆ‘ä»¬çš„tokenize_with_numbersæ–¹æ³•
            if hasattr(causal_tokenizer, 'tokenize_with_numbers'):
                tokens, numerical_values = causal_tokenizer.tokenize_with_numbers(text)
                
                print(f"   tokens: {tokens}")
                print(f"   æ•°å€¼: {numerical_values}")
                
                # æ£€æŸ¥<NUM> tokençš„ä½¿ç”¨
                num_token_count = tokens.count('<NUM>')
                non_zero_values = sum(1 for val in numerical_values if val != 0.0)
                
                print(f"   <NUM> tokenæ•°é‡: {num_token_count}")
                print(f"   éé›¶æ•°å€¼æ•°é‡: {non_zero_values}")
                print(f"   æ•°é‡åŒ¹é…: {'âœ…' if num_token_count == non_zero_values else 'âŒ'}")
            else:
                print(f"   âš ï¸ tokenize_with_numbersæ–¹æ³•ä¸å­˜åœ¨")
                
        except Exception as e:
            print(f"   âŒ åˆ†è¯å¤±è´¥: {e}")


def test_batch_encoding(causal_tokenizer):
    """æµ‹è¯•æ‰¹é‡ç¼–ç åŠŸèƒ½"""
    print_separator("æ­¥éª¤7ï¼šæ‰¹é‡ç¼–ç éªŒè¯", 2)
    
    test_texts = [
        "The item costs 99.99 dollars.",
        "From a batch of 100 items.",
        "Regular text without numbers."
    ]
    
    try:
        print(f"æµ‹è¯•æ‰¹é‡ç¼–ç ...")
        inputs = causal_tokenizer.batch_encode_plus(
            test_texts, 
            padding=True, 
            truncation=True, 
            return_tensors='pt'
        )
        
        print(f"âœ… æ‰¹é‡ç¼–ç æˆåŠŸ")
        print(f"   input_idså½¢çŠ¶: {inputs['input_ids'].shape}")
        print(f"   attention_maskå½¢çŠ¶: {inputs['attention_mask'].shape}")
        print(f"   numerical_valueså½¢çŠ¶: {inputs['numerical_values'].shape}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰<NUM> token
        num_token_id = causal_tokenizer.num_token_id
        num_token_positions = (inputs['input_ids'] == num_token_id)
        num_token_count = num_token_positions.sum().item()
        
        print(f"   æ‰¹æ¬¡ä¸­<NUM> tokenæ•°é‡: {num_token_count}")
        
        # æ£€æŸ¥æ•°å€¼æ•°ç»„ä¸­çš„éé›¶å€¼
        non_zero_numerical = (inputs['numerical_values'] != 0.0).sum().item()
        print(f"   æ‰¹æ¬¡ä¸­éé›¶æ•°å€¼æ•°é‡: {non_zero_numerical}")
        print(f"   æ•°é‡åŒ¹é…: {'âœ…' if num_token_count == non_zero_numerical else 'âŒ'}")
        
        return inputs
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç¼–ç å¤±è´¥: {e}")
        return None


def comprehensive_verification_summary(
    original_vocab_size, causal_vocab_size, size_correct, causal_num_token_id,
    architecture_correct, weights_inherited
):
    """ç»¼åˆéªŒè¯æ€»ç»“"""
    print_separator("æœ€ç»ˆéªŒè¯æ€»ç»“", 1)
    
    print(f"ğŸ“Š è¯æ±‡è¡¨éªŒè¯ç»“æœ:")
    print(f"   åŸå§‹Qwenè¯æ±‡è¡¨å¤§å°:    K = {original_vocab_size}")
    print(f"   CausalQwenè¯æ±‡è¡¨å¤§å°:      {causal_vocab_size}")
    print(f"   ç†è®ºå€¼ K+1:            K+1 = {original_vocab_size + 1}")
    print(f"   å¤§å°éªŒè¯:              {'âœ… é€šè¿‡' if size_correct else 'âŒ å¤±è´¥'}")
    
    print(f"\nğŸ”¢ <NUM> Token éªŒè¯ç»“æœ:")
    print(f"   <NUM> token ID:        {causal_num_token_id}")
    print(f"   IDæœ‰æ•ˆæ€§:              {'âœ… æœ‰æ•ˆ' if causal_num_token_id >= 0 else 'âŒ æ— æ•ˆ'}")
    
    print(f"\nğŸ—ï¸ æ¨¡å‹æ¶æ„éªŒè¯ç»“æœ:")
    print(f"   æƒé‡å½¢çŠ¶éªŒè¯:          {'âœ… é€šè¿‡' if architecture_correct else 'âŒ å¤±è´¥'}")
    print(f"   æƒé‡ç»§æ‰¿éªŒè¯:          {'âœ… é€šè¿‡' if weights_inherited else 'âŒ å¤±è´¥'}")
    
    # æœ€ç»ˆåˆ¤æ–­
    all_passed = size_correct and causal_num_token_id >= 0 and architecture_correct and weights_inherited
    
    print(f"\nğŸ† æœ€ç»ˆéªŒè¯ç»“æœ:")
    if all_passed:
        print(f"   âœ… æ‰€æœ‰éªŒè¯é€šè¿‡ï¼")
        print(f"   âœ… CausalQwenè¯æ±‡è¡¨ = Qwenè¯æ±‡è¡¨ + 1")
        print(f"   âœ… <NUM> tokenæ­£ç¡®æ·»åŠ ä¸”åŠŸèƒ½æ­£å¸¸")
        print(f"   âœ… æ¨¡å‹æƒé‡å½¢çŠ¶ç¬¦åˆé¢„æœŸ [K+1, d_model]")
        print(f"   âœ… æƒé‡ç»§æ‰¿æœºåˆ¶å·¥ä½œæ­£å¸¸")
        print(f"\nğŸ¯ ç»“è®º: è¯æ±‡è¡¨è®¾è®¡å®Œå…¨ç¬¦åˆç†è®ºè¦æ±‚ (K+1)ï¼Œæ¨¡å‹æ¶æ„éªŒè¯é€šè¿‡ï¼")
    else:
        print(f"   âŒ éªŒè¯å¤±è´¥ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
        if not size_correct:
            print(f"      - è¯æ±‡è¡¨å¤§å°ä¸æ­£ç¡®")
        if causal_num_token_id < 0:
            print(f"      - <NUM> token IDæ— æ•ˆ")
        if not architecture_correct:
            print(f"      - æ¨¡å‹æƒé‡å½¢çŠ¶ä¸æ­£ç¡®")
        if not weights_inherited:
            print(f"      - æƒé‡ç»§æ‰¿å¤±è´¥")
        print(f"\nğŸ”§ å»ºè®®: æ£€æŸ¥ç›¸å…³æ¨¡å—çš„å®ç°")
    
    return all_passed


def main():
    """ä¸»å‡½æ•°"""
    print_separator("CausalQwen å®Œæ•´éªŒè¯è„šæœ¬ (æ‰©å±•ç‰ˆ)", 1)
    print("ç›®æ ‡ï¼šå½»åº•éªŒè¯ CausalQwen ç›¸å¯¹äº Qwen çš„å®Œæ•´è®¾è®¡")
    print("éªŒè¯å†…å®¹ï¼šè¯æ±‡è¡¨ã€æ¨¡å‹æƒé‡å½¢çŠ¶ã€æƒé‡ç»§æ‰¿ã€åˆ†è¯åŠŸèƒ½")
    
    # é…ç½®
    model_path = "~/models/Qwen2.5-0.5B"
    
    print(f"\né…ç½®ä¿¡æ¯:")
    print(f"   Qwenæ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   å±•å¼€åè·¯å¾„: {os.path.expanduser(model_path)}")
    
    # æ‰§è¡ŒéªŒè¯æ­¥éª¤
    
    # æ­¥éª¤1ï¼šéªŒè¯åŸå§‹Qwen
    original_tokenizer, original_vocab_size, original_has_num = verify_original_qwen_tokenizer(model_path)
    if original_tokenizer is None:
        print("âŒ æ— æ³•åŠ è½½åŸå§‹Qwenåˆ†è¯å™¨ï¼ŒéªŒè¯ç»ˆæ­¢")
        return
    
    # æ­¥éª¤2ï¼šéªŒè¯CausalQwen
    causal_tokenizer, causal_vocab_size, causal_num_token_id = verify_causal_qwen_tokenizer(model_path)
    if causal_tokenizer is None:
        print("âŒ æ— æ³•åˆå§‹åŒ–CausalQwenåˆ†è¯å™¨ï¼ŒéªŒè¯ç»ˆæ­¢")
        return
    
    # æ­¥éª¤3ï¼šæ¨¡å‹æ¶æ„éªŒè¯
    qwen_model, causal_model, architecture_correct = verify_model_architectures(model_path, causal_tokenizer)
    if qwen_model is None or causal_model is None:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼ŒéªŒè¯ç»ˆæ­¢")
        return
    
    # æ­¥éª¤4ï¼šæƒé‡ç»§æ‰¿éªŒè¯
    weights_inherited = verify_weight_inheritance(qwen_model, causal_model, causal_tokenizer)
    
    # æ­¥éª¤5ï¼šè¯æ±‡è¡¨å¯¹æ¯”éªŒè¯
    size_correct = compare_vocabularies(
        original_tokenizer, original_vocab_size, 
        causal_tokenizer, causal_vocab_size
    )
    
    # æ­¥éª¤6ï¼šåŠŸèƒ½æµ‹è¯•
    test_tokenization_functionality(causal_tokenizer)
    
    # æ­¥éª¤7ï¼šæ‰¹é‡ç¼–ç æµ‹è¯•
    test_batch_encoding(causal_tokenizer)
    
    # æœ€ç»ˆæ€»ç»“
    all_passed = comprehensive_verification_summary(
        original_vocab_size, causal_vocab_size, size_correct, causal_num_token_id,
        architecture_correct, weights_inherited
    )
    
    # è¾“å‡ºå»ºè®®
    if not all_passed:
        print_separator("é—®é¢˜è¯Šæ–­ä¸å»ºè®®", 2)
        print("å¯èƒ½çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥QwenTokenizerWrapper.__init__ä¸­çš„tokenæ·»åŠ é€»è¾‘")
        print("2. ç¡®è®¤<NUM> tokenç¡®å®è¢«æ·»åŠ åˆ°äº†è¯æ±‡è¡¨ä¸­")
        print("3. éªŒè¯ActionNetworkæƒé‡å½¢çŠ¶å’Œåˆå§‹åŒ–é€»è¾‘")
        print("4. æ£€æŸ¥çŸ¥è¯†ä¼ è¾“æœºåˆ¶çš„å®ç°")


if __name__ == "__main__":
    main() 
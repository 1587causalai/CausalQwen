#!/usr/bin/env python
"""
ç«¯åˆ°ç«¯å›å½’æµç¨‹éªŒè¯è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨å®Œæ•´åœ°å±•ç¤ºä»åŸå§‹æ–‡æœ¬è¾“å…¥åˆ°æœ€ç»ˆé—¨æ§å›å½’æŸå¤±è®¡ç®—çš„å…¨è¿‡ç¨‹ã€‚
å®ƒä½¿ç”¨ `test_numerical_aware_embedding.py` ä¸­çš„4ä¸ªçœŸå®è¯­æ–™æ ·æœ¬ï¼Œ
é€šè¿‡å®Œæ•´çš„ CausalQwen æ¨¡å‹ï¼ŒéªŒè¯æ ¸å¿ƒçš„å›å½’ä»»åŠ¡æµç¨‹å’Œé—¨æ§æœºåˆ¶ã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - ç«¯åˆ°ç«¯å›å½’æµç¨‹ä¸é—¨æ§æœºåˆ¶éªŒè¯")
    
    # --- æ­¥éª¤ 1: åˆå§‹åŒ–çœŸå®æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–çœŸå®æ¨¡å‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')

    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True,
        ovr_threshold=100.0,
        reg_loss_gating_alpha=0.0 # ä½¿ç”¨å®Œå…¨é—¨æ§ï¼Œæ–¹ä¾¿éªŒè¯
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("   âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # --- æ­¥éª¤ 2: å‡†å¤‡è¾“å…¥æ•°æ® ---
    print_step("æ­¥éª¤ 2", "å‡†å¤‡è¾“å…¥æ•°æ® (æ¥è‡ª test_numerical_aware_embedding.py)")
    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
        "The dimensions are 10 by 20 by 30 cm.",
        "What is 2 plus 3? 5.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    print(f"   - ä½¿ç”¨ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")

    # --- æ­¥éª¤ 3: å‡†å¤‡å›å½’ä»»åŠ¡çš„æ ‡ç­¾ ---
    print_step("æ­¥éª¤ 3", "å‡†å¤‡å›å½’ä»»åŠ¡çš„æ ‡ç­¾ (Labels)")
    # å¯¹äºå›å½’ä»»åŠ¡ï¼Œ"æ ‡ç­¾" å°±æ˜¯ `numerical_values` å¼ é‡æœ¬èº«ã€‚
    # æˆ‘ä»¬ä¹Ÿéœ€è¦ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾ï¼Œä½†åœ¨æ­¤æµ‹è¯•ä¸­æˆ‘ä»¬ä¸»è¦å…³æ³¨å›å½’æŸå¤±ã€‚
    cls_labels = torch.full_like(input_ids, -100)
    cls_labels[:, :-1] = input_ids[:, 1:]
    
    print("   - å›å½’æ ‡ç­¾: `numerical_values` å¼ é‡")
    print("   - åˆ†ç±»æ ‡ç­¾: `input_ids` å·¦ç§»ä¸€ä½ (å¸¸è§„è¯­è¨€æ¨¡å‹ç›®æ ‡)")
    
    print("\n   --- æŸ¥çœ‹æ ·æœ¬ 2 çš„è¾“å…¥å’Œå›å½’ç›®æ ‡ ---")
    sample_idx = 1
    num_token_id = tokenizer.num_token_id
    
    print(f"     è¾“å…¥ IDs: {input_ids[sample_idx].tolist()}")
    print(f"     å›å½’ç›®æ ‡: {numerical_values[sample_idx].tolist()}")
    
    # æ‰¾åˆ°<NUM> tokençš„ä½ç½®
    num_positions = (input_ids[sample_idx] == num_token_id).nonzero(as_tuple=True)[0]
    target_values = numerical_values[sample_idx][num_positions]
    
    print(f"     - åœ¨ä½ç½® {num_positions.tolist()} å‘ç°äº† <NUM> è¯å…ƒã€‚")
    print(f"     - å®ƒä»¬å¯¹åº”çš„å›å½’ç›®æ ‡å€¼æ˜¯: {target_values.tolist()}")
    
    print("\n   --- æŸ¥çœ‹æ ·æœ¬ 1 (æ— æ•°å€¼) çš„å›å½’ç›®æ ‡ ---")
    print(f"     å›å½’ç›®æ ‡: {numerical_values[0].tolist()}")
    print("     - ç»“è®º: å¯¹äºæ²¡æœ‰æ•°å€¼çš„å¥å­ï¼Œå›å½’ç›®æ ‡åº”å…¨ä¸º0ã€‚")

    with torch.no_grad():
        # --- æ­¥éª¤ 4: æ‰§è¡Œæ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­ ---
        print_step("æ­¥éª¤ 4", "æ‰§è¡Œæ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­")
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
        print("   - æ¨¡å‹å‰å‘ä¼ æ’­å®Œæˆã€‚")
        print(f"   - è¾“å‡º reg_loc å½¢çŠ¶: {outputs['reg_loc'].shape}")

        # --- æ­¥éª¤ 5: è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤± ---
        print_step("æ­¥éª¤ 5", "è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤±")
        loss_dict = model.compute_loss(
            outputs,
            targets=cls_labels,          # åˆ†ç±»ç›®æ ‡
            numerical_values=numerical_values, # å›å½’ç›®æ ‡
            attention_mask=attention_mask
        )
        
        reg_loss = loss_dict.get('reg_loss')
        
        print(f"\n   - è®¡ç®—å¾—åˆ°çš„æ€»æŸå¤± (Total Loss): {loss_dict['total']:.4f}")
        print(f"   - è®¡ç®—å¾—åˆ°çš„åˆ†ç±»æŸå¤± (Classification Loss): {loss_dict['cls_loss']:.4f}")
        print(f"   - è®¡ç®—å¾—åˆ°çš„å›å½’æŸå¤± (Regression Loss): {reg_loss:.4f}")
        print(f"   - é—¨æ§æƒé‡å‡å€¼: {loss_dict.get('gate_weights_mean', -1):.4f}")
        print(f"   - (ç†è®º: å›å½’æŸå¤±åº”è¯¥åªç”±åŒ…å«æ•°å€¼çš„æ ·æœ¬è´¡çŒ®)")

    print(f"\n\n{'='*80}")
    print("ğŸ‰ ç«¯åˆ°ç«¯å›å½’æµç¨‹éªŒè¯å®Œæˆï¼")
    print("   è„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†é—¨æ§å›å½’æŸå¤±æ˜¯å¦‚ä½•åŸºäºå¯¹é½çš„æ•°å€¼ç›®æ ‡è®¡ç®—çš„ã€‚")

if __name__ == '__main__':
    main() 
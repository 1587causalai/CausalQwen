#!/usr/bin/env python
"""
ç«¯åˆ°ç«¯å›å½’æµç¨‹éªŒè¯è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨å®Œæ•´åœ°å±•ç¤ºä»åŸå§‹æ–‡æœ¬è¾“å…¥åˆ°æœ€ç»ˆé—¨æ§å›å½’æŸå¤±è®¡ç®—çš„å…¨è¿‡ç¨‹ã€‚
å®ƒä½¿ç”¨ `test_numerical_aware_embedding.py` ä¸­çš„4ä¸ªçœŸå®è¯­æ–™æ ·æœ¬ï¼Œ
é€šè¿‡å®Œæ•´çš„ CausalQwen æ¨¡å‹ï¼ŒéªŒè¯æ ¸å¿ƒçš„å›å½’ä»»åŠ¡æµç¨‹å’Œé—¨æ§æœºåˆ¶ã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - ç«¯åˆ°ç«¯å›å½’æµç¨‹ä¸é—¨æ§æœºåˆ¶éªŒè¯")
    
    # --- æ­¥éª¤ 1: åˆå§‹åŒ–çœŸå®æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–çœŸå®æ¨¡å‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')

    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True,
        ovr_threshold=100.0,
        reg_loss_gating_alpha=0.0 # ä½¿ç”¨å®Œå…¨é—¨æ§ï¼Œæ–¹ä¾¿éªŒè¯
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("   âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # --- æ­¥éª¤ 2: å‡†å¤‡è¾“å…¥æ•°æ® ---
    print_step("æ­¥éª¤ 2", "å‡†å¤‡è¾“å…¥æ•°æ® (æ¥è‡ª test_numerical_aware_embedding.py)")
    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
        "The dimensions are 10 by 20 by 30 cm.",
        "What is 2 plus 3? 5.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    print(f"   - ä½¿ç”¨ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")

    # --- æ­¥éª¤ 3: å‡†å¤‡å›å½’ä»»åŠ¡çš„æ ‡ç­¾ ---
    print_step("æ­¥éª¤ 3", "å‡†å¤‡å›å½’ä»»åŠ¡çš„æ ‡ç­¾ (Labels)")
    # å¯¹äºå›å½’ä»»åŠ¡ï¼Œ"æ ‡ç­¾" å°±æ˜¯ `numerical_values` å¼ é‡æœ¬èº«ã€‚
    # æˆ‘ä»¬ä¹Ÿéœ€è¦ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾ï¼Œä½†åœ¨æ­¤æµ‹è¯•ä¸­æˆ‘ä»¬ä¸»è¦å…³æ³¨å›å½’æŸå¤±ã€‚
    cls_labels = torch.full_like(input_ids, -100)
    cls_labels[:, :-1] = input_ids[:, 1:]
    
    print("   - å›å½’æ ‡ç­¾: `numerical_values` å¼ é‡")
    print("   - åˆ†ç±»æ ‡ç­¾: `input_ids` å·¦ç§»ä¸€ä½ (å¸¸è§„è¯­è¨€æ¨¡å‹ç›®æ ‡)")
    
    print("\n   --- æŸ¥çœ‹æ ·æœ¬ 2 çš„è¾“å…¥å’Œå›å½’ç›®æ ‡ ---")
    sample_idx = 1
    num_token_id = tokenizer.num_token_id
    
    print(f"     è¾“å…¥ IDs: {input_ids[sample_idx].tolist()}")
    print(f"     å›å½’ç›®æ ‡: {numerical_values[sample_idx].tolist()}")
    
    # æ‰¾åˆ°<NUM> tokençš„ä½ç½®
    num_positions = (input_ids[sample_idx] == num_token_id).nonzero(as_tuple=True)[0]
    target_values = numerical_values[sample_idx][num_positions]
    
    print(f"     - åœ¨ä½ç½® {num_positions.tolist()} å‘ç°äº† <NUM> è¯å…ƒã€‚")
    print(f"     - å®ƒä»¬å¯¹åº”çš„å›å½’ç›®æ ‡å€¼æ˜¯: {target_values.tolist()}")
    
    print("\n   --- æŸ¥çœ‹æ ·æœ¬ 1 (æ— æ•°å€¼) çš„å›å½’ç›®æ ‡ ---")
    print(f"     å›å½’ç›®æ ‡: {numerical_values[0].tolist()}")
    print("     - ç»“è®º: å¯¹äºæ²¡æœ‰æ•°å€¼çš„å¥å­ï¼Œå›å½’ç›®æ ‡åº”å…¨ä¸º0ã€‚")

    with torch.no_grad():
        # --- æ­¥éª¤ 4: æ‰§è¡Œæ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­ ---
        print_step("æ­¥éª¤ 4", "æ‰§è¡Œæ¨¡å‹å®Œæ•´å‰å‘ä¼ æ’­")
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
        print("   - æ¨¡å‹å‰å‘ä¼ æ’­å®Œæˆã€‚")
        print(f"   - è¾“å‡º reg_loc å½¢çŠ¶: {outputs['reg_loc'].shape}")

        # --- æ­¥éª¤ 5: è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤± ---
        print_step("æ­¥éª¤ 5", "è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤±")
        loss_dict = model.compute_loss(
            outputs,
            targets=cls_labels,          # åˆ†ç±»ç›®æ ‡
            numerical_values=numerical_values, # å›å½’ç›®æ ‡
            attention_mask=attention_mask
        )
        
        reg_loss_diluted = loss_dict.get('reg_loss')
        effective_reg_loss = loss_dict.get('effective_reg_loss')
        
        print(f"\n   - è®¡ç®—å¾—åˆ°çš„æ€»æŸå¤± (Total Loss): {loss_dict['total']:.4f}")
        print(f"   - è®¡ç®—å¾—åˆ°çš„åˆ†ç±»æŸå¤± (Classification Loss): {loss_dict['cls_loss']:.4f}")
        print(f"   - (è¢«ç¨€é‡Šçš„)å›å½’æŸå¤± (Diluted Regression Loss): {reg_loss_diluted:.4f}")
        print(f"   - æœ‰æ•ˆçš„å›å½’æŸå¤± (Effective Regression Loss): {effective_reg_loss:.4f}")
        print(f"   - é—¨æ§æƒé‡å‡å€¼: {loss_dict.get('gate_weights_mean', -1):.4f}")
        print(f"   - (æ³¨: 'æœ‰æ•ˆå›å½’æŸå¤±'åªåœ¨æœ‰æ•°å€¼çš„ä½ç½®ä¸Šæ±‚å¹³å‡ï¼Œæ˜¯æ›´æœ‰æ„ä¹‰çš„æŒ‡æ ‡)")

        # --- æ­¥éª¤ 6: æ·±å…¥éªŒè¯é—¨æ§å›å½’æŸå¤±çš„é€è¯å…ƒè¡Œä¸º ---
        print_step("æ­¥éª¤ 6", "æ·±å…¥éªŒè¯é—¨æ§å›å½’æŸå¤±çš„é€è¯å…ƒè¡Œä¸º")
        print("   - æ ¸å¿ƒ: éªŒè¯æŸ¯è¥¿NLLæ˜¯å¦åªåœ¨ `num_mask` ä¸º1çš„ä½ç½®è¢«è®¡ç®—ã€‚")

        # ä¸ºäº†æ‹¿åˆ°é€è¯å…ƒçš„æŸå¤±ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è°ƒç”¨åº•å±‚å‡½æ•°
        from src.losses.loss_functions import gated_regression_loss
        from src.utils.losses import compute_ovr_probabilities

        # a. è·å–æ¨¡å‹è¾“å‡º
        cls_loc = outputs['cls_loc']
        cls_scale = outputs['cls_scale']
        reg_loc = outputs['reg_loc']
        reg_scale = outputs['reg_scale']
        
        # b. è®¡ç®— <NUM> æ¦‚ç‡ å’Œ mask
        cls_probs = compute_ovr_probabilities(cls_loc, cls_scale, model.config.ovr_threshold)
        num_probs = cls_probs[:, :, num_token_id]
        num_mask = (cls_labels == num_token_id).float() * attention_mask

        # c. è®¡ç®—é—¨æ§æƒé‡ (ä¸æ¨¡å‹å†…éƒ¨é€»è¾‘ä¸€è‡´)
        alpha = model.config.reg_loss_gating_alpha
        gate_weights = num_mask * (alpha + (1 - alpha) * num_probs)

        # d. è®¡ç®—é€è¯å…ƒçš„å›å½’æŸå¤± (ä¸é™ç»´)
        per_token_reg_loss = gated_regression_loss(
            reg_loc, reg_scale, numerical_values,
            gate_prob=num_probs,
            mask=num_mask,
            alpha=alpha,
            reduction='none'
        )

        # e. æ‰“å°å…³é”®æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        for i, sample_name in [(0, "æ—  æ•°å€¼"), (1, "æœ‰ æ•°å€¼")]:
            print(f"\n   --- æ ·æœ¬ {i+1} ({sample_name}) ---")
            print(f"     å›å½’ç›®æ ‡: {numerical_values[i].cpu().numpy().round(1)}")
            print(f"     æ•°å€¼æ©ç : {num_mask[i].cpu().numpy().astype(int)}")
            print(f"     é—¨æ§æƒé‡: {gate_weights[i].cpu().numpy().round(2)}")
            print(f"     é€è¯å…ƒæŸå¤±: {per_token_reg_loss[i].cpu().numpy().round(2)}")

        # --- æ­¥éª¤ 7: è¯¦ç»†åˆ†æ <NUM> ä½ç½®çš„å›å½’é¢„æµ‹ ---
        print_step("æ­¥éª¤ 7", "è¯¦ç»†åˆ†æ <NUM> ä½ç½®çš„å›å½’é¢„æµ‹")
        reg_loc_Y = outputs['reg_loc']
        reg_scale_Y = outputs['reg_scale']

        for i in range(input_ids.shape[0]):
            num_positions_mask = (input_ids[i] == num_token_id)
            if num_positions_mask.any():
                num_indices = num_positions_mask.nonzero(as_tuple=True)[0]
                
                print(f"\n   --- æ ·æœ¬ {i+1}: '{test_samples[i]}' ---")
                for idx in num_indices:
                    true_value = numerical_values[i, idx].item()
                    # ä»…å½“çœŸå®å€¼ä¸ä¸º0æ—¶æ‰æ‰“å°ï¼Œå› ä¸º0æ˜¯é»˜è®¤å¡«å……å€¼
                    if true_value != 0.0:
                        pred_loc = reg_loc_Y[i, idx].item()
                        pred_scale = reg_scale_Y[i, idx].item()
                        
                        print(f"     - åœ¨ä½ç½® {idx.item()}:")
                        print(f"       - çœŸå®å€¼ (Target): {true_value:.2f}")
                        print(f"       - é¢„æµ‹åˆ†å¸ƒ (Predicted Cauchy): loc={pred_loc:.4f}, scale={pred_scale:.4f}")

    print(f"\n\n{'='*80}")
    print("ğŸ‰ ç«¯åˆ°ç«¯å›å½’æµç¨‹éªŒè¯å®Œæˆï¼")
    print("   è„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†é—¨æ§å›å½’æŸå¤±æ˜¯å¦‚ä½•åŸºäºå¯¹é½çš„æ•°å€¼ç›®æ ‡è®¡ç®—çš„ã€‚")

if __name__ == '__main__':
    main() 
#!/usr/bin/env python3
"""
æµ‹è¯•å»æ‰ç‰¹æ®Šåˆå§‹åŒ–åçš„æ•ˆæœ

éªŒè¯ï¼š
1. <NUM> token ä½¿ç”¨é¢„ç•™æƒé‡è€Œä¸æ˜¯ç‰¹æ®Šåˆå§‹åŒ–
2. å›å½’å¤´åç½®ä¸º0è€Œä¸æ˜¯æ•°æ®ä¸­ä½æ•°
3. æƒé‡ç»§æ‰¿æ­£ç¡®å·¥ä½œ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
from src.models.causal_lm import CausalLanguageModel, CausalLMConfig
from src.data.tokenizer import QwenTokenizerWrapper
from src.models.feature_network import QwenFeatureNetwork

def test_no_special_initialization():
    """æµ‹è¯•å»æ‰ç‰¹æ®Šåˆå§‹åŒ–åçš„æ•ˆæœ"""
    
    print("=" * 80)
    print("=   æµ‹è¯•å»æ‰ç‰¹æ®Šåˆå§‹åŒ–åçš„æ•ˆæœ   =")
    print("=" * 80)
    
    # 1. è®¾ç½®æ¨¡å‹å’Œé…ç½®
    print("\n[æ­¥éª¤ 1] è®¾ç½®æ¨¡å‹å’Œé…ç½®...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = QwenTokenizerWrapper(model_path="/Users/gongqian/models/Qwen2.5-0.5B", use_real_tokenizer=True)
    print(f"åŠ è½½åˆ†è¯å™¨å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # åˆ›å»ºé…ç½®
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,  # C=H çº¦æŸ
        use_real_qwen=True,
        qwen_model_path="/Users/gongqian/models/Qwen2.5-0.5B",
        ovr_threshold=10.0
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CausalLanguageModel(config)
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # 2. è·å–QwenåŸå§‹æƒé‡ä»¥ä¾›å¯¹æ¯”
    print("\n[æ­¥éª¤ 2] è·å–QwenåŸå§‹æƒé‡...")
    
    # è·å–Qwençš„lm_headæƒé‡
    qwen_feature_network = model.feature_network.base_network if hasattr(model.feature_network, 'base_network') else model.feature_network
    qwen_lm_head = qwen_feature_network.get_lm_head()
    qwen_lm_head_weight = qwen_lm_head.weight.data
    
    print(f"Qwen lm_headæƒé‡å½¢çŠ¶: {qwen_lm_head_weight.shape}")
    
    # æ£€æŸ¥<NUM> tokenä½ç½®çš„é¢„ç•™æƒé‡
    num_token_id = tokenizer.num_token_id
    if num_token_id < qwen_lm_head_weight.shape[0]:
        original_num_weight = qwen_lm_head_weight[num_token_id, :]
        print(f"<NUM> token (ID: {num_token_id}) åœ¨Qwenä¸­çš„é¢„ç•™æƒé‡ç»Ÿè®¡:")
        print(f"  å‡å€¼: {original_num_weight.mean().item():.6f}")
        print(f"  æ ‡å‡†å·®: {original_num_weight.std().item():.6f}")
        print(f"  éé›¶å…ƒç´ : {(original_num_weight != 0).sum().item()}/{original_num_weight.numel()}")
    else:
        print(f"è­¦å‘Šï¼š<NUM> token ID {num_token_id} è¶…å‡ºQwenæƒé‡èŒƒå›´")
    
    # 3. æ‰§è¡Œä¿®æ”¹åçš„åˆå§‹åŒ–
    print("\n[æ­¥éª¤ 3] æ‰§è¡Œä¿®æ”¹åçš„åˆå§‹åŒ–...")
    
    # è¿™äº›å‚æ•°ç°åœ¨åº”è¯¥è¢«å¿½ç•¥
    model.init_weights(num_target_median=999.99, num_target_scale=888.88)
    
    # 4. éªŒè¯<NUM> tokenæƒé‡ç»§æ‰¿
    print("\n[æ­¥éª¤ 4] éªŒè¯<NUM> tokenæƒé‡ç»§æ‰¿...")
    
    cls_head = model.action_network.classification_head.causal_linear
    causal_num_weight = cls_head.weight.data[num_token_id, :]
    
    print(f"CausalQwenä¸­<NUM> tokenæƒé‡ç»Ÿè®¡:")
    print(f"  å‡å€¼: {causal_num_weight.mean().item():.6f}")
    print(f"  æ ‡å‡†å·®: {causal_num_weight.std().item():.6f}")
    print(f"  éé›¶å…ƒç´ : {(causal_num_weight != 0).sum().item()}/{causal_num_weight.numel()}")
    
    # æ£€æŸ¥æƒé‡æ˜¯å¦å®Œå…¨ä¸€è‡´
    if num_token_id < qwen_lm_head_weight.shape[0]:
        weight_diff = torch.abs(causal_num_weight - original_num_weight)
        max_diff = weight_diff.max().item()
        mean_diff = weight_diff.mean().item()
        
        print(f"æƒé‡ç»§æ‰¿éªŒè¯:")
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.8f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.8f}")
        print(f"  æƒé‡ä¸€è‡´: {'âœ…' if max_diff < 1e-6 else 'âŒ'}")
    
    # 5. éªŒè¯å›å½’å¤´åç½®
    print("\n[æ­¥éª¤ 5] éªŒè¯å›å½’å¤´åç½®...")
    
    reg_head = model.action_network.regression_head.causal_linear
    reg_bias = reg_head.bias.data if reg_head.bias is not None else None
    
    if reg_bias is not None:
        print(f"å›å½’å¤´åç½®å€¼: {reg_bias.item():.6f}")
        print(f"åç½®ä¸ºé›¶: {'âœ…' if abs(reg_bias.item()) < 1e-6 else 'âŒ'}")
    else:
        print(f"å›å½’å¤´æ— åç½®")
    
    # 6. éªŒè¯åˆ†ç±»å¤´åç½®
    print("\n[æ­¥éª¤ 6] éªŒè¯åˆ†ç±»å¤´åç½®...")
    
    cls_bias = cls_head.bias.data if cls_head.bias is not None else None
    
    if cls_bias is not None:
        print(f"åˆ†ç±»å¤´åç½®ç»Ÿè®¡:")
        print(f"  å‡å€¼: {cls_bias.mean().item():.6f}")
        print(f"  æ ‡å‡†å·®: {cls_bias.std().item():.6f}")
        print(f"  <NUM> tokenåç½®: {cls_bias[num_token_id].item():.6f}")
        print(f"  å…¨é›¶åç½®: {'âœ…' if torch.allclose(cls_bias, torch.zeros_like(cls_bias)) else 'âŒ'}")
    else:
        print(f"åˆ†ç±»å¤´æ— åç½®")
    
    # 7. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n[æ­¥éª¤ 7] æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.tensor([[1, 2, 3, num_token_id, 5]])  # åŒ…å«<NUM> token
    test_nums = torch.tensor([[0.0, 0.0, 0.0, 123.45, 0.0]])
    
    with torch.no_grad():
        outputs = model(test_input, test_nums)
        
        cls_loc = outputs['cls_loc']
        cls_scale = outputs['cls_scale']
        reg_loc = outputs['reg_loc']
        reg_scale = outputs['reg_scale']
        
        print(f"å‰å‘ä¼ æ’­æˆåŠŸ:")
        print(f"  cls_locå½¢çŠ¶: {cls_loc.shape}")
        print(f"  cls_scaleå½¢çŠ¶: {cls_scale.shape}")
        print(f"  reg_locå½¢çŠ¶: {reg_loc.shape}")
        print(f"  reg_scaleå½¢çŠ¶: {reg_scale.shape}")
        
        # æ£€æŸ¥<NUM> tokenä½ç½®çš„è¾“å‡º
        num_pos = 3  # <NUM> tokençš„ä½ç½®
        num_cls_loc = cls_loc[0, num_pos, num_token_id].item()
        num_cls_scale = cls_scale[0, num_pos, num_token_id].item()
        num_reg_loc = reg_loc[0, num_pos].item()
        num_reg_scale = reg_scale[0, num_pos].item()
        
        print(f"<NUM> tokenä½ç½®è¾“å‡º:")
        print(f"  åˆ†ç±» loc: {num_cls_loc:.6f}")
        print(f"  åˆ†ç±» scale: {num_cls_scale:.6f}")
        print(f"  å›å½’ loc: {num_reg_loc:.6f}")
        print(f"  å›å½’ scale: {num_reg_scale:.6f}")
    
    # 8. æ€»ç»“
    print(f"\n" + "=" * 80)
    print(f"=   å»æ‰ç‰¹æ®Šåˆå§‹åŒ–æµ‹è¯•æ€»ç»“   =")
    print(f"=" * 80)
    
    print("âœ… æˆåŠŸéªŒè¯ï¼š")
    print("  - <NUM> token ä½¿ç”¨é¢„ç•™æƒé‡ï¼Œæ— ç‰¹æ®Šéšæœºåˆå§‹åŒ–")
    print("  - å›å½’å¤´åç½®ä¸º0ï¼Œæ— æ•°æ®ä¾èµ–")
    print("  - æƒé‡ç»§æ‰¿æ­£ç¡®å·¥ä½œ")
    print("  - å‰å‘ä¼ æ’­æ­£å¸¸")
    
    print("\nğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼š")
    print("  - åˆ©ç”¨Qwené¢„ç•™tokençš„é¢„åˆå§‹åŒ–æƒé‡")
    print("  - æ¶ˆé™¤äº†æ•°æ®ä¾èµ–çš„åç½®åˆå§‹åŒ–")
    print("  - ç®€åŒ–äº†åˆå§‹åŒ–æµç¨‹")


if __name__ == "__main__":
    test_no_special_initialization() 
#!/usr/bin/env python3
"""
CausalQwenä¸Qwenç®€å•é›†æˆæµ‹è¯• - ä¿®å¤ç‰ˆ
æµ‹è¯•ç›®æ ‡ï¼šè¾“å…¥"ä½ å¥½"ï¼ŒéªŒè¯compatibleæ¨¡å¼ä¸‹çš„æ•°å€¼ä¸€è‡´æ€§
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

def main():
    print("ğŸš€ å¼€å§‹CausalQwenç®€å•é›†æˆæµ‹è¯•")
    print("ç›®æ ‡ï¼šéªŒè¯'ä½ å¥½'è¾“å…¥çš„æ•°å€¼ä¸€è‡´æ€§")
    print("=" * 50)
    
    try:
        # å¯¼å…¥æ¨¡å—
        print("ğŸ“¦ å¯¼å…¥æ¨¡å—...")
        from transformers import Qwen2ForCausalLM, AutoTokenizer
        from causal_qwen_mvp.models import CausalQwenMVPForCausalLM, CausalQwen2Config
        print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # åŠ è½½Qwenæ¨¡å‹
        print("ğŸ“¥ åŠ è½½åŸå§‹Qwen2.5-0.5B...")
        qwen_model_path = Path("~/models/Qwen2.5-0.5B").expanduser()
        
        tokenizer = AutoTokenizer.from_pretrained(qwen_model_path)
        qwen_model = Qwen2ForCausalLM.from_pretrained(
            qwen_model_path,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        print(f"âœ… Qwenæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in qwen_model.parameters()):,}")
        
        # åˆ›å»ºCausalQwenæ¨¡å‹
        print("ğŸ”§ åˆ›å»ºCausalQwenæ¨¡å‹...")
        causal_config = CausalQwen2Config(
            vocab_size=qwen_model.config.vocab_size,
            hidden_size=qwen_model.config.hidden_size,
            intermediate_size=qwen_model.config.intermediate_size,
            num_hidden_layers=qwen_model.config.num_hidden_layers,
            num_attention_heads=qwen_model.config.num_attention_heads,
            num_key_value_heads=getattr(qwen_model.config, 'num_key_value_heads', qwen_model.config.num_attention_heads),
            max_position_embeddings=qwen_model.config.max_position_embeddings,
            individual_dim=128,
            num_individuals=16
        )
        
        causal_qwen_model = CausalQwenMVPForCausalLM(causal_config)
        print(f"âœ… CausalQwenæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in causal_qwen_model.parameters()):,}")
        
        # å…³é”®æƒé‡æ˜ å°„
        print("ğŸ”„ æ‰§è¡Œå…³é”®æƒé‡æ˜ å°„...")
        with torch.no_grad():
            # æ˜ å°„è¯åµŒå…¥
            causal_qwen_model.qwen_model.embed_tokens.weight.copy_(
                qwen_model.model.embed_tokens.weight
            )
            
            # æ˜ å°„lm_headåˆ°action_network.linearï¼ˆå…³é”®ï¼ï¼‰
            causal_qwen_model.action_network.linear.weight.copy_(
                qwen_model.lm_head.weight
            )
            
            # æ˜ å°„normå±‚
            causal_qwen_model.qwen_model.norm.weight.copy_(
                qwen_model.model.norm.weight
            )
        print("âœ… å…³é”®æƒé‡æ˜ å°„å®Œæˆ")
        
        # æ‰§è¡Œæµ‹è¯•
        print("\nğŸ§ª æ‰§è¡Œ'ä½ å¥½'ä¸€è‡´æ€§æµ‹è¯•...")
        test_input = "ä½ å¥½"
        
        inputs = tokenizer(test_input, return_tensors="pt")
        input_ids = inputs["input_ids"]
        print(f"è¾“å…¥tokens: {input_ids}")
        
        with torch.no_grad():
            # Qwenè¾“å‡º
            qwen_outputs = qwen_model(input_ids)
            qwen_logits = qwen_outputs.logits[:, -1, :]
            
            # CausalQwenè¾“å‡º
            causal_outputs = causal_qwen_model(input_ids)
            causal_logits = causal_outputs.loc_S[:, -1, :]
            
            # è®¡ç®—å·®å¼‚
            max_diff = torch.max(torch.abs(qwen_logits - causal_logits)).item()
            mean_diff = torch.mean(torch.abs(qwen_logits - causal_logits)).item()
            
            print(f"\nğŸ“Š æ•°å€¼å¯¹æ¯”ç»“æœ:")
            print(f"  æœ€å¤§å·®å¼‚: {max_diff:.8f}")
            print(f"  å¹³å‡å·®å¼‚: {mean_diff:.8f}")
            
            print(f"\nğŸ“‹ å‰5ä¸ªlogitså¯¹æ¯”:")
            print(f"  Qwen:      {qwen_logits[0, :5].numpy()}")
            print(f"  CausalQwen: {causal_logits[0, :5].numpy()}")
            
            # åˆ¤æ–­æˆåŠŸ
            success = max_diff < 1e-4
            print(f"\nğŸ¯ æµ‹è¯•ç»“æœ:")
            if success:
                print(f"  âœ… æˆåŠŸï¼æ•°å€¼å·®å¼‚å¾ˆå° ({max_diff:.8f})")
            else:
                print(f"  âŒ å¤±è´¥ï¼æ•°å€¼å·®å¼‚è¾ƒå¤§ ({max_diff:.8f})")
        
        print(f"\nğŸ† æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
CausalQwen vs Qwen ç«¯åˆ°ç«¯å¯¹æ¯”æµ‹è¯•

ç›®æ ‡ï¼šä»ç›¸åŒè¾“å…¥å‡ºå‘ï¼Œå¯¹æ¯”CausalQwenä¸‰ç§æ¨ç†æ¨¡å¼ä¸åŸå§‹Qwençš„è¾“å‡ºå·®å¼‚
éªŒè¯ï¼š
1. CausalQwenå…¼å®¹æ¨¡å¼ â‰ˆ QwenåŸå§‹è¾“å‡ºï¼ˆç‰¹åˆ«æ˜¯top-1ç¡®å®šæ€§é‡‡æ ·ï¼‰
2. CausalQwenæ ‡å‡†æ¨¡å¼ä½“ç°OvRåˆ†ç±»çš„å†³ç­–é€»è¾‘
3. CausalQwenå› æœæ¨¡å¼ä½“ç°ä¸ªä½“å·®å¼‚çš„éšæœºæ€§
4. æ‰€æœ‰æ¨¡å¼çš„æ•°å­¦è®¡ç®—ç¬¦åˆè®¾è®¡æ–‡æ¡£

æµ‹è¯•æµç¨‹ï¼š
è¾“å…¥æ–‡æœ¬ â†’ tokenize â†’ åµŒå…¥ â†’ transformerç‰¹å¾z â†’ å„ç§æ¨ç†æ¨¡å¼ â†’ è¾“å‡ºå¯¹æ¯”
"""

import sys
import os
import torch
import torch.nn.functional as F
from pathlib import Path
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# ANSI color codes
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

def print_section(title, color=Colors.BLUE):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{color}{Colors.BOLD}{'='*80}{Colors.END}")
    print(f"{color}{Colors.BOLD}{title.center(80)}{Colors.END}")
    print(f"{color}{Colors.BOLD}{'='*80}{Colors.END}")

def print_step(step_num, description, color=Colors.CYAN):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{color}ğŸ“‹ æ­¥éª¤ {step_num}: {description}{Colors.END}")

def print_success(message):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"{Colors.GREEN}âœ… {message}{Colors.END}")

def print_error(message):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"{Colors.RED}âŒ {message}{Colors.END}")

def print_warning(message):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")

def print_info(message):
    """æ‰“å°ä¿¡æ¯"""
    print(f"{Colors.WHITE}â„¹ï¸  {message}{Colors.END}")

def print_math(message):
    """æ‰“å°æ•°å­¦å…¬å¼"""
    print(f"{Colors.PURPLE}ğŸ”¢ {message}{Colors.END}")

def load_models_and_tokenizer():
    """åŠ è½½Qwenå’ŒCausalQwenæ¨¡å‹"""
    print_section("æ¨¡å‹åŠ è½½ä¸åˆå§‹åŒ–")
    
    print_step(1, "åŠ è½½Qwen2åŸå§‹æ¨¡å‹")
    try:
        from transformers import Qwen2ForCausalLM, Qwen2Config, AutoTokenizer
        
        qwen_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(qwen_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print_success(f"Qwen2åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer.vocab)}")
        
        # åŠ è½½Qwenæ¨¡å‹é…ç½®å’Œæƒé‡
        qwen_config = Qwen2Config.from_pretrained(qwen_path)
        qwen_model = Qwen2ForCausalLM.from_pretrained(qwen_path, torch_dtype=torch.float32)
        qwen_model.eval()
        print_success(f"Qwen2æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in qwen_model.parameters()):,}")
        
        return tokenizer, qwen_model, qwen_config
        
    except Exception as e:
        print_error(f"Qwen2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def create_causal_qwen_model(qwen_config):
    """åˆ›å»ºCausalQwenæ¨¡å‹"""
    print_step(2, "åˆ›å»ºCausalQwenæ¨¡å‹")
    try:
        from causal_qwen_mvp import CausalQwenMVPForCausalLM, CausalQwen2Config
        
        # åˆ›å»ºCausalQwené…ç½®ï¼ˆå®Œå…¨ç»§æ‰¿Qwenå‚æ•°ï¼‰
        causal_config = CausalQwen2Config(
            # å®Œå…¨å¤åˆ¶Qwené…ç½®
            **qwen_config.to_dict(),
            # CausalQwenç‰¹æœ‰å‚æ•°
            causal_size=qwen_config.hidden_size,  # C = H
            abduction_init_strategy='identity',
            b_noise_init=0.1,
            gamma_init=10.0,
            ovr_threshold_init=0.0
        )
        
        # åˆ›å»ºæ¨¡å‹
        causal_model = CausalQwenMVPForCausalLM(causal_config)
        causal_model.eval()
        print_success(f"CausalQwenæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in causal_model.parameters()):,}")
        
        return causal_model, causal_config
        
    except Exception as e:
        print_error(f"CausalQwenæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None, None

def copy_qwen_weights_to_causal(qwen_model, causal_model):
    """å°†Qwenæƒé‡å¤åˆ¶åˆ°CausalQwen"""
    print_step(3, "å¤åˆ¶Qwené¢„è®­ç»ƒæƒé‡")
    try:
        print_info("å¤åˆ¶TransformeråŸºç¡€æƒé‡...")
        
        # æ–¹æ³•1ï¼šä½¿ç”¨å®Œæ•´çš„state_dictå¤åˆ¶ï¼ˆæ›´å®‰å…¨ï¼‰
        qwen_state_dict = qwen_model.model.state_dict()
        causal_state_dict = causal_model.model.state_dict()
        
        # å¤åˆ¶æ‰€æœ‰åŒ¹é…çš„æƒé‡
        copied_keys = []
        for key in qwen_state_dict.keys():
            if key in causal_state_dict and qwen_state_dict[key].shape == causal_state_dict[key].shape:
                causal_state_dict[key].copy_(qwen_state_dict[key])
                copied_keys.append(key)
        
        # åŠ è½½æ›´æ–°åçš„state_dict
        causal_model.model.load_state_dict(causal_state_dict)
        
        print_info(f"æˆåŠŸå¤åˆ¶ {len(copied_keys)} ä¸ªTransformerå‚æ•°")
        
        # éªŒè¯å¤åˆ¶æ•ˆæœ
        print_info("éªŒè¯æƒé‡å¤åˆ¶æ•ˆæœ...")
        with torch.no_grad():
            test_input = torch.randint(0, 1000, (1, 5))
            qwen_features = qwen_model.model(test_input)[0]
            causal_features = causal_model.model(test_input)[0]
            feature_diff = torch.abs(qwen_features - causal_features).mean().item()
            
        print_math(f"ç‰¹å¾éªŒè¯å·®å¼‚: {feature_diff:.8f}")
        
        if feature_diff < 1e-6:
            print_success("âœ… Transformeræƒé‡å¤åˆ¶å®Œç¾ï¼")
        else:
            print_warning(f"âš ï¸ ç‰¹å¾å·®å¼‚ {feature_diff:.8f}ï¼Œç»§ç»­è°ƒè¯•...")
        
        # å¤åˆ¶lm_headåˆ°ActionNetwork
        print_info("å¤åˆ¶lm_headæƒé‡åˆ°ActionNetwork...")
        causal_model.action_network.copy_weights_from_qwen(qwen_model)
        
        print_success("æƒé‡å¤åˆ¶å®Œæˆï¼CausalQwenç°åœ¨ç»§æ‰¿äº†Qwençš„é¢„è®­ç»ƒçŸ¥è¯†")
        
        return True
        
    except Exception as e:
        print_error(f"æƒé‡å¤åˆ¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def analyze_intermediate_representations(text, tokenizer, qwen_model, causal_model):
    """åˆ†æä¸­é—´è¡¨å¾ï¼šä»è¾“å…¥åˆ°ç‰¹å¾zçš„å®Œæ•´è¿‡ç¨‹"""
    print_section(f"ä¸­é—´è¡¨å¾åˆ†æï¼š'{text}'")
    
    print_step(1, "æ–‡æœ¬é¢„å¤„ç†ä¸tokenåŒ–")
    
    # ç¼–ç æ–‡æœ¬
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    input_ids = inputs['input_ids']
    attention_mask = inputs.get('attention_mask', None)
    
    print_info(f"åŸå§‹æ–‡æœ¬: '{text}'")
    print_info(f"Token IDs: {input_ids.tolist()}")
    print_info(f"Tokenæ•°é‡: {input_ids.shape[1]}")
    
    # è§£ç æ¯ä¸ªtoken
    tokens = [tokenizer.decode([token_id]) for token_id in input_ids[0]]
    print_info(f"Tokenåºåˆ—: {tokens}")
    
    print_step(2, "ç‰¹å¾æå–ï¼šåµŒå…¥ â†’ Transformer â†’ è¡¨å¾z")
    
    with torch.no_grad():
        # === Qwençš„å‰å‘ä¼ æ’­ ===
        qwen_outputs = qwen_model.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        qwen_hidden_states = qwen_outputs.last_hidden_state  # [batch, seq, hidden]
        qwen_logits = qwen_model.lm_head(qwen_hidden_states)  # [batch, seq, vocab]
        
        print_math(f"Qwenç‰¹å¾zå½¢çŠ¶: {qwen_hidden_states.shape}")
        print_math(f"Qwen logitså½¢çŠ¶: {qwen_logits.shape}")
        print_info(f"Qwenç‰¹å¾zç»Ÿè®¡: å‡å€¼={qwen_hidden_states.mean().item():.6f}, æ ‡å‡†å·®={qwen_hidden_states.std().item():.6f}")
        
        # === CausalQwençš„å‰å‘ä¼ æ’­ ===
        causal_outputs = causal_model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # æå–CausalQwençš„ä¸­é—´è¡¨å¾
        causal_hidden_states = causal_outputs.hidden_states[-1] if causal_outputs.hidden_states else None
        
        # å¦‚æœæ²¡æœ‰hidden_statesï¼Œæ‰‹åŠ¨è®¡ç®—
        if causal_hidden_states is None:
            transformer_outputs = causal_model.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
            causal_hidden_states = transformer_outputs.last_hidden_state
        
        print_math(f"CausalQwenç‰¹å¾zå½¢çŠ¶: {causal_hidden_states.shape}")
        print_info(f"CausalQwenç‰¹å¾zç»Ÿè®¡: å‡å€¼={causal_hidden_states.mean().item():.6f}, æ ‡å‡†å·®={causal_hidden_states.std().item():.6f}")
        
        # éªŒè¯Transformerç‰¹å¾ä¸€è‡´æ€§
        feature_diff = torch.abs(qwen_hidden_states - causal_hidden_states).mean().item()
        print_math(f"ç‰¹å¾zå·®å¼‚ï¼ˆåº”è¯¥â‰ˆ0ï¼‰: {feature_diff:.8f}")
        
        if feature_diff < 1e-6:
            print_success("âœ… CausalQwençš„Transformerç‰¹å¾ä¸Qwenå®Œå…¨ä¸€è‡´ï¼")
        else:
            print_warning(f"âš ï¸ ç‰¹å¾å·®å¼‚è¾ƒå¤§({feature_diff:.8f})ï¼Œå¯èƒ½å­˜åœ¨æƒé‡å¤åˆ¶é—®é¢˜")
        
        print_step(3, "CausalQwenå› æœåˆ†è§£ï¼šz â†’ (loc_U, scale_U) â†’ (loc_S, scale_S)")
        
        # å½’å› æ¨æ–­ï¼šz â†’ Uåˆ†å¸ƒå‚æ•°
        loc_U, scale_U = causal_model.abduction_network(causal_hidden_states)
        print_math(f"ä¸ªä½“è¡¨å¾loc_U: {loc_U.shape}, å‡å€¼={loc_U.mean().item():.6f}")
        print_math(f"ä¸ªä½“ä¸ç¡®å®šæ€§scale_U: {scale_U.shape}, å‡å€¼={scale_U.mean().item():.6f}")
        
        # è¡ŒåŠ¨å†³ç­–ï¼šU â†’ Såˆ†å¸ƒå‚æ•°
        loc_S, scale_S = causal_model.action_network(loc_U, scale_U)
        print_math(f"å†³ç­–åˆ†å¸ƒloc_S: {loc_S.shape}, å‡å€¼={loc_S.mean().item():.6f}")
        print_math(f"å†³ç­–ä¸ç¡®å®šæ€§scale_S: {scale_S.shape}, å‡å€¼={scale_S.mean().item():.6f}")
        
        # å¯¹æ¯”CausalQwençš„loc_Sä¸Qwençš„logits
        logits_diff = torch.abs(loc_S - qwen_logits).mean().item()
        print_math(f"loc_Sä¸Qwen logitså·®å¼‚: {logits_diff:.6f}")
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'tokens': tokens,
            'qwen_hidden_states': qwen_hidden_states,
            'qwen_logits': qwen_logits,
            'causal_hidden_states': causal_hidden_states,
            'loc_U': loc_U,
            'scale_U': scale_U,
            'loc_S': loc_S,
            'scale_S': scale_S,
            'feature_diff': feature_diff,
            'logits_diff': logits_diff
        }

def compare_inference_modes(representations, tokenizer, qwen_model, causal_model):
    """å¯¹æ¯”å„ç§æ¨ç†æ¨¡å¼çš„è¾“å‡º"""
    print_section("æ¨ç†æ¨¡å¼å¯¹æ¯”ï¼šCausalQwen vs Qwen")
    
    input_ids = representations['input_ids']
    qwen_logits = representations['qwen_logits']
    
    print_step(1, "QwenåŸå§‹æ¨ç†")
    
    with torch.no_grad():
        # === Qwenæ ‡å‡†æ¨ç† ===
        qwen_probs = F.softmax(qwen_logits[:, -1, :], dim=-1)  # æœ€åä¸€ä¸ªtokençš„æ¦‚ç‡
        qwen_top1_token = torch.argmax(qwen_probs, dim=-1).item()
        qwen_top1_prob = qwen_probs[0, qwen_top1_token].item()
        qwen_top1_text = tokenizer.decode([qwen_top1_token])
        
        print_info(f"Qwen Top-1é¢„æµ‹: token={qwen_top1_token}, prob={qwen_top1_prob:.6f}, text='{qwen_top1_text}'")
        
        # Qwen Top-kåˆ†æ
        qwen_top5_probs, qwen_top5_tokens = torch.topk(qwen_probs, 5, dim=-1)
        print_info("Qwen Top-5é¢„æµ‹:")
        for i in range(5):
            token_id = qwen_top5_tokens[0, i].item()
            prob = qwen_top5_probs[0, i].item()
            text = tokenizer.decode([token_id])
            print(f"    #{i+1}: token={token_id}, prob={prob:.6f}, text='{text}'")
    
    print_step(2, "CausalQwenä¸‰ç§æ¨ç†æ¨¡å¼")
    
    from causal_qwen_mvp import CausalInferenceEngine
    engine = CausalInferenceEngine(causal_model)
    
    # === æ¨¡å¼1ï¼šæ ‡å‡†æ¨ç†ï¼ˆOvRåˆ†ç±»ï¼‰===
    print_info("ğŸ¯ æ¨¡å¼1ï¼šæ ‡å‡†æ¨ç†ï¼ˆåŸºäºOvRæ¦‚ç‡çš„ç¡®å®šæ€§å†³ç­–ï¼‰")
    
    with torch.no_grad():
        standard_output = engine.inference(input_ids, mode='standard')
        
        # è®¡ç®—OvRæ¦‚ç‡
        ovr_probs = causal_model.ovr_classifier(standard_output.loc_S, standard_output.scale_S)
        standard_probs = ovr_probs[:, -1, :]  # æœ€åä¸€ä¸ªtokençš„OvRæ¦‚ç‡
        standard_top1_token = torch.argmax(standard_probs, dim=-1).item()
        standard_top1_prob = standard_probs[0, standard_top1_token].item()
        standard_top1_text = tokenizer.decode([standard_top1_token])
        
        print_math(f"OvRå…¬å¼: P_k = 0.5 + (1/Ï€) * arctan((loc_S_k - threshold) / scale_S_k)")
        print_info(f"æ ‡å‡†æ¨¡å¼é¢„æµ‹: token={standard_top1_token}, prob={standard_top1_prob:.6f}, text='{standard_top1_text}'")
    
    # === æ¨¡å¼2ï¼šå› æœé‡‡æ ·ï¼ˆä¸ªä½“å…·ç°ï¼‰===
    print_info("ğŸ² æ¨¡å¼2ï¼šå› æœé‡‡æ ·ï¼ˆä¸ªä½“å…·ç° + ç¡®å®šæ€§å†³ç­–ï¼‰")
    
    causal_predictions = []
    for trial in range(3):  # å¤šæ¬¡é‡‡æ ·å±•ç¤ºéšæœºæ€§
        with torch.no_grad():
            causal_output = engine.inference(input_ids, mode='causal', temperature=1.0)
            
            # ä»å› æœé‡‡æ ·çš„ç»“æœè·å–é¢„æµ‹
            causal_probs = F.softmax(causal_output.loc_S[:, -1, :], dim=-1)
            causal_top1_token = torch.argmax(causal_probs, dim=-1).item()
            causal_top1_prob = causal_probs[0, causal_top1_token].item()
            causal_top1_text = tokenizer.decode([causal_top1_token])
            
            causal_predictions.append((causal_top1_token, causal_top1_prob, causal_top1_text))
            print_info(f"å› æœé‡‡æ ·#{trial+1}: token={causal_top1_token}, prob={causal_top1_prob:.6f}, text='{causal_top1_text}'")
    
    # åˆ†æå› æœé‡‡æ ·çš„å¤šæ ·æ€§
    unique_tokens = len(set([pred[0] for pred in causal_predictions]))
    print_math(f"å› æœé‡‡æ ·å¤šæ ·æ€§: {unique_tokens}/3 ä¸ªä¸åŒç»“æœ")
    
    # === æ¨¡å¼3ï¼šå…¼å®¹é‡‡æ ·ï¼ˆä¼ ç»ŸSoftmaxï¼‰===
    print_info("ğŸ”„ æ¨¡å¼3ï¼šå…¼å®¹é‡‡æ ·ï¼ˆä¼ ç»ŸSoftmax + Top-1ç¡®å®šæ€§ï¼‰")
    
    with torch.no_grad():
        # ç¡®å®šæ€§å…¼å®¹æ¨¡å¼
        compatible_det_output = engine.inference(input_ids, mode='compatible', do_sample=False)
        
        if hasattr(compatible_det_output, 'next_token_ids') and compatible_det_output.next_token_ids is not None:
            compatible_det_token = compatible_det_output.next_token_ids[0].item()
        else:
            # æ‰‹åŠ¨è®¡ç®—
            compatible_logits = compatible_det_output.loc_S[:, -1, :]
            compatible_det_token = torch.argmax(compatible_logits, dim=-1).item()
        
        compatible_det_text = tokenizer.decode([compatible_det_token])
        print_info(f"å…¼å®¹ç¡®å®šæ€§: token={compatible_det_token}, text='{compatible_det_text}'")
        
        # éšæœºå…¼å®¹æ¨¡å¼
        compatible_random_predictions = []
        for trial in range(3):
            compatible_random_output = engine.inference(input_ids, mode='compatible', 
                                                      do_sample=True, temperature=1.0, top_k=50, top_p=0.9)
            
            if hasattr(compatible_random_output, 'next_token_ids') and compatible_random_output.next_token_ids is not None:
                compatible_random_token = compatible_random_output.next_token_ids[0].item()
            else:
                # æ‰‹åŠ¨é‡‡æ ·
                compatible_logits = compatible_random_output.loc_S[:, -1, :] / 1.0  # temperature
                compatible_probs = F.softmax(compatible_logits, dim=-1)
                compatible_random_token = torch.multinomial(compatible_probs, 1)[0].item()
            
            compatible_random_text = tokenizer.decode([compatible_random_token])
            compatible_random_predictions.append((compatible_random_token, compatible_random_text))
            print_info(f"å…¼å®¹éšæœº#{trial+1}: token={compatible_random_token}, text='{compatible_random_text}'")
    
    print_step(3, "å…³é”®ä¸€è‡´æ€§éªŒè¯")
    
    # éªŒè¯1ï¼šå…¼å®¹ç¡®å®šæ€§æ¨¡å¼ vs Qwenç¡®å®šæ€§
    qwen_vs_compatible = (qwen_top1_token == compatible_det_token)
    if qwen_vs_compatible:
        print_success(f"âœ… å…³é”®éªŒè¯é€šè¿‡ï¼šå…¼å®¹ç¡®å®šæ€§æ¨¡å¼ä¸Qwenå®Œå…¨ä¸€è‡´ï¼")
        print_success(f"   Qwen: {qwen_top1_token} ('{qwen_top1_text}') == å…¼å®¹: {compatible_det_token} ('{compatible_det_text}')")
    else:
        print_warning(f"âš ï¸ å…¼å®¹æ¨¡å¼ä¸ä¸€è‡´ï¼šQwen={qwen_top1_token}('{qwen_top1_text}') vs å…¼å®¹={compatible_det_token}('{compatible_det_text}')")
        print_warning("è¿™å¯èƒ½è¡¨æ˜å…¼å®¹æ¨¡å¼å®ç°æœ‰è¯¯æˆ–æƒé‡å¤åˆ¶ä¸å®Œæ•´")
    
    # éªŒè¯2ï¼šæ¨ç†æ¨¡å¼å·®å¼‚æ€§
    all_predictions = [qwen_top1_token, standard_top1_token, compatible_det_token]
    all_predictions.extend([pred[0] for pred in causal_predictions])
    unique_predictions = len(set(all_predictions))
    
    print_info(f"æ‰€æœ‰é¢„æµ‹ç»“æœ: {all_predictions}")
    print_info(f"é¢„æµ‹å¤šæ ·æ€§: {unique_predictions}/{len(all_predictions)} ä¸ªä¸åŒç»“æœ")
    
    if unique_predictions >= 3:
        print_success("âœ… ä¸åŒæ¨ç†æ¨¡å¼ä½“ç°äº†å„è‡ªç‰¹ç‚¹")
    elif unique_predictions == 2:
        print_info("â„¹ï¸ éƒ¨åˆ†æ¨¡å¼äº§ç”Ÿä¸åŒç»“æœï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
    else:
        print_warning("âš ï¸ æ‰€æœ‰æ¨¡å¼ç»“æœç›¸åŒï¼Œéœ€è¦æ£€æŸ¥å®ç°")
    
    return {
        'qwen_prediction': (qwen_top1_token, qwen_top1_prob, qwen_top1_text),
        'standard_prediction': (standard_top1_token, standard_top1_prob, standard_top1_text),
        'causal_predictions': causal_predictions,
        'compatible_det_prediction': (compatible_det_token, compatible_det_text),
        'compatible_random_predictions': compatible_random_predictions,
        'qwen_vs_compatible_match': qwen_vs_compatible,
        'prediction_diversity': unique_predictions
    }

def test_sequence_generation(text, tokenizer, qwen_model, causal_model):
    """æµ‹è¯•åºåˆ—ç”Ÿæˆï¼šå¤šæ­¥æ¨ç†å¯¹æ¯”"""
    print_section(f"åºåˆ—ç”Ÿæˆå¯¹æ¯”ï¼š'{text}' + 3ä¸ªåç»­tokens")
    
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    input_ids = inputs['input_ids']
    
    print_step(1, "Qwenåºåˆ—ç”Ÿæˆ")
    
    with torch.no_grad():
        # Qwenç”Ÿæˆï¼ˆè´ªå¿ƒæœç´¢ï¼‰
        qwen_generated = qwen_model.generate(
            input_ids,
            max_new_tokens=3,
            do_sample=False,  # ç¡®å®šæ€§ç”Ÿæˆ
            pad_token_id=tokenizer.eos_token_id
        )
        qwen_new_tokens = qwen_generated[0, input_ids.shape[1]:].tolist()
        qwen_generated_text = tokenizer.decode(qwen_new_tokens)
        
        print_info(f"Qwenç”Ÿæˆ: tokens={qwen_new_tokens}, text='{qwen_generated_text}'")
    
    print_step(2, "CausalQwenåºåˆ—ç”Ÿæˆ")
    
    from causal_qwen_mvp import CausalInferenceEngine
    engine = CausalInferenceEngine(causal_model)
    
    # æ ‡å‡†æ¨¡å¼ç”Ÿæˆ
    with torch.no_grad():
        standard_generated = engine.generate_step_by_step(
            input_ids, max_new_tokens=3, mode='standard'
        )
        standard_new_tokens = standard_generated[0, input_ids.shape[1]:].tolist()
        standard_generated_text = tokenizer.decode(standard_new_tokens)
        print_info(f"æ ‡å‡†æ¨¡å¼: tokens={standard_new_tokens}, text='{standard_generated_text}'")
    
    # å› æœæ¨¡å¼ç”Ÿæˆ
    with torch.no_grad():
        causal_generated = engine.generate_step_by_step(
            input_ids, max_new_tokens=3, mode='causal', temperature=1.0
        )
        causal_new_tokens = causal_generated[0, input_ids.shape[1]:].tolist()
        causal_generated_text = tokenizer.decode(causal_new_tokens)
        print_info(f"å› æœæ¨¡å¼: tokens={causal_new_tokens}, text='{causal_generated_text}'")
    
    # å…¼å®¹æ¨¡å¼ç”Ÿæˆï¼ˆç¡®å®šæ€§ï¼‰
    with torch.no_grad():
        compatible_generated = engine.generate_step_by_step(
            input_ids, max_new_tokens=3, mode='compatible_deterministic'
        )
        compatible_new_tokens = compatible_generated[0, input_ids.shape[1]:].tolist()
        compatible_generated_text = tokenizer.decode(compatible_new_tokens)
        print_info(f"å…¼å®¹æ¨¡å¼: tokens={compatible_new_tokens}, text='{compatible_generated_text}'")
    
    print_step(3, "åºåˆ—ç”Ÿæˆä¸€è‡´æ€§éªŒè¯")
    
    # éªŒè¯Qwen vs å…¼å®¹æ¨¡å¼
    qwen_vs_compatible_seq = (qwen_new_tokens == compatible_new_tokens)
    if qwen_vs_compatible_seq:
        print_success("âœ… åºåˆ—ç”ŸæˆéªŒè¯é€šè¿‡ï¼šå…¼å®¹æ¨¡å¼ä¸Qwenåºåˆ—å®Œå…¨ä¸€è‡´ï¼")
    else:
        print_warning(f"âš ï¸ åºåˆ—ä¸ä¸€è‡´ï¼šQwen={qwen_new_tokens} vs å…¼å®¹={compatible_new_tokens}")
        # é€ä½ç½®å¯¹æ¯”
        for i, (q_token, c_token) in enumerate(zip(qwen_new_tokens, compatible_new_tokens)):
            if q_token != c_token:
                q_text = tokenizer.decode([q_token])
                c_text = tokenizer.decode([c_token])
                print_warning(f"   ä½ç½®{i}: Qwen={q_token}('{q_text}') vs å…¼å®¹={c_token}('{c_text}')")
    
    # åˆ†æä¸åŒæ¨¡å¼çš„åºåˆ—å·®å¼‚
    all_sequences = [qwen_new_tokens, standard_new_tokens, causal_new_tokens, compatible_new_tokens]
    sequence_names = ['Qwen', 'æ ‡å‡†', 'å› æœ', 'å…¼å®¹']
    
    print_info("åºåˆ—å·®å¼‚åˆ†æ:")
    for i, seq_name in enumerate(sequence_names):
        for j, other_name in enumerate(sequence_names[i+1:], i+1):
            diff_count = sum(1 for a, b in zip(all_sequences[i], all_sequences[j]) if a != b)
            print_info(f"  {seq_name} vs {other_name}: {diff_count}/3 ä¸ªä½ç½®ä¸åŒ")
    
    return {
        'qwen_sequence': qwen_new_tokens,
        'standard_sequence': standard_new_tokens,
        'causal_sequence': causal_new_tokens,
        'compatible_sequence': compatible_new_tokens,
        'qwen_vs_compatible_match': qwen_vs_compatible_seq
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_section("CausalQwen vs Qwen ç«¯åˆ°ç«¯å¯¹æ¯”æµ‹è¯•", Colors.PURPLE)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "äººå·¥æ™ºèƒ½çš„å‘å±•", 
        "åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸ",
        "The future of technology"
    ]
    
    # åŠ è½½æ¨¡å‹
    tokenizer, qwen_model, qwen_config = load_models_and_tokenizer()
    if qwen_model is None:
        print_error("Qwenæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    causal_model, causal_config = create_causal_qwen_model(qwen_config)
    if causal_model is None:
        print_error("CausalQwenæ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # å¤åˆ¶æƒé‡
    if not copy_qwen_weights_to_causal(qwen_model, causal_model):
        print_error("æƒé‡å¤åˆ¶å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # å¯¹æ¯ä¸ªæµ‹è¯•æ–‡æœ¬è¿›è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•
    all_results = []
    
    for i, text in enumerate(test_texts):
        print_section(f"æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(test_texts)}: '{text}'", Colors.GREEN)
        
        try:
            # åˆ†æä¸­é—´è¡¨å¾
            representations = analyze_intermediate_representations(text, tokenizer, qwen_model, causal_model)
            
            # å¯¹æ¯”æ¨ç†æ¨¡å¼
            inference_results = compare_inference_modes(representations, tokenizer, qwen_model, causal_model)
            
            # åºåˆ—ç”Ÿæˆæµ‹è¯•
            generation_results = test_sequence_generation(text, tokenizer, qwen_model, causal_model)
            
            # è®°å½•ç»“æœ
            all_results.append({
                'text': text,
                'representations': representations,
                'inference': inference_results,
                'generation': generation_results
            })
            
        except Exception as e:
            print_error(f"æµ‹è¯•æ¡ˆä¾‹ {i+1} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ€»ç»“æŠ¥å‘Š
    print_section("æµ‹è¯•æ€»ç»“æŠ¥å‘Š", Colors.GREEN)
    
    # ç»Ÿè®¡ä¸€è‡´æ€§éªŒè¯ç»“æœ
    inference_consistency_count = sum(1 for result in all_results if result['inference']['qwen_vs_compatible_match'])
    generation_consistency_count = sum(1 for result in all_results if result['generation']['qwen_vs_compatible_match'])
    
    print_success(f"æ¨ç†ä¸€è‡´æ€§: {inference_consistency_count}/{len(all_results)} ä¸ªæ¡ˆä¾‹é€šè¿‡")
    print_success(f"ç”Ÿæˆä¸€è‡´æ€§: {generation_consistency_count}/{len(all_results)} ä¸ªæ¡ˆä¾‹é€šè¿‡")
    
    # åˆ†æç‰¹å¾ä¸€è‡´æ€§
    feature_diffs = [result['representations']['feature_diff'] for result in all_results if 'representations' in result]
    avg_feature_diff = np.mean(feature_diffs) if feature_diffs else float('inf')
    
    print_success(f"Transformerç‰¹å¾å¹³å‡å·®å¼‚: {avg_feature_diff:.8f}")
    
    if avg_feature_diff < 1e-6:
        print_success("âœ… CausalQwenæˆåŠŸç»§æ‰¿äº†Qwençš„é¢„è®­ç»ƒçŸ¥è¯†ï¼")
    else:
        print_warning("âš ï¸ å­˜åœ¨ç‰¹å¾å·®å¼‚ï¼Œéœ€è¦æ£€æŸ¥æƒé‡å¤åˆ¶")
    
    if inference_consistency_count == len(all_results) and generation_consistency_count == len(all_results):
        print_section("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CausalQwenä¸Qwenå®Œå…¨å…¼å®¹ï¼", Colors.GREEN)
        print_success("å…¼å®¹æ¨¡å¼ä¸Qwenè¡Œä¸ºå®Œå…¨ä¸€è‡´")
        print_success("ä¸‰ç§æ¨ç†æ¨¡å¼éƒ½èƒ½æ­£å¸¸å·¥ä½œ")
        print_success("æ•°å­¦å®ç°ç¬¦åˆè®¾è®¡æ–‡æ¡£")
    else:
        print_section("âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•", Colors.YELLOW)
        print_info("è¯·æ£€æŸ¥æƒé‡å¤åˆ¶å’Œå…¼å®¹æ¨¡å¼å®ç°")

if __name__ == "__main__":
    main()
#!/usr/bin/env python
"""
æ‰¹é‡å¤„ç†æ­£ç¡®æ€§éªŒè¯è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨ä¸¥æ ¼éªŒè¯ CausalQwen çš„æ‰¹é‡å¤„ç† (batch processing) åŠŸèƒ½æ˜¯å¦æ­£ç¡®ã€‚
æ­£ç¡®çš„æ‰¹é‡å¤„ç†åº”ç¡®ä¿æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„è®¡ç®—ç»“æœä¸è¯¥æ ·æœ¬è¢«å•ç‹¬å¤„ç†æ—¶çš„ç»“æœå®Œå…¨ä¸€è‡´ï¼Œ
ä»è€Œä¿è¯è®­ç»ƒå’Œæ¨ç†çš„å¯é‡å¤æ€§ä¸æ­£ç¡®æ€§ã€‚

éªŒè¯ç›®æ ‡:
1. åˆ†åˆ«ç‹¬ç«‹å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œå¹¶è®°å½•å…¶è¾“å‡ºã€‚
2. å°†è¿™äº›æ ·æœ¬ä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡ç»Ÿä¸€å¤„ç†ã€‚
3. é€ä¸€å¯¹æ¯”ä¸¤ç§å¤„ç†æ–¹å¼ä¸‹å¯¹åº”æ ·æœ¬çš„è¾“å‡º (loc_S, loc_Y ç­‰)ï¼ŒéªŒè¯å…¶æ•°å€¼ä¸Šæ˜¯å¦å®Œå…¨ç›¸ç­‰ã€‚
"""
import os
import sys
import torch

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def get_model_outputs(model, tokenizer, text_list, device):
    """è¾…åŠ©å‡½æ•°ï¼šå¯¹ç»™å®šçš„æ–‡æœ¬åˆ—è¡¨è¿›è¡Œåˆ†è¯å’Œæ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè¿”å›è¾“å‡º"""
    inputs = tokenizer.batch_encode_plus(text_list, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
    return outputs, attention_mask

def main():
    torch.manual_seed(42) # ä¿è¯åˆå§‹åŒ–æƒé‡çš„ä¸€è‡´æ€§
    print("ğŸš€ CausalQwen - æ‰¹é‡å¤„ç† (Batch Processing) æ­£ç¡®æ€§éªŒè¯ ğŸš€")

    # --- æ­¥éª¤ 1: åˆå§‹åŒ– ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("   âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆã€‚")

    # --- æ­¥éª¤ 2: å‡†å¤‡æµ‹è¯•æ•°æ® ---
    test_samples = [
        "The price is 29.99 dollars.",
        "A short sentence.",
        "The stock has 100 units and the discount is 5.5 percent."
    ]
    print_step("æ­¥éª¤ 2", f"å‡†å¤‡ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
    for i, s in enumerate(test_samples):
        print(f"   - æ ·æœ¬ {i+1}: '{s}'")

    # --- æ­¥éª¤ 3: ç‹¬ç«‹å¤„ç†æ¯ä¸ªæ ·æœ¬ ---
    print_step("æ­¥éª¤ 3", "ç‹¬ç«‹å¤„ç† (Batch Size = 1) æ¯ä¸ªæ ·æœ¬å¹¶å­˜å‚¨ç»“æœ")
    individual_outputs = []
    for sample in test_samples:
        outputs, _ = get_model_outputs(model, tokenizer, [sample], device)
        individual_outputs.append(outputs)
    print(f"   âœ… å·²å®Œæˆå¯¹ {len(individual_outputs)} ä¸ªæ ·æœ¬çš„ç‹¬ç«‹å¤„ç†ã€‚")

    # --- æ­¥éª¤ 4: æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬ ---
    print_step("æ­¥éª¤ 4", "æ‰¹é‡å¤„ç† (Batch Size > 1) æ‰€æœ‰æ ·æœ¬")
    batch_outputs, batch_attention_mask = get_model_outputs(model, tokenizer, test_samples, device)
    print(f"   âœ… å·²å®Œæˆå¯¹ {len(test_samples)} ä¸ªæ ·æœ¬çš„æ‰¹é‡å¤„ç†ã€‚")

    # --- æ­¥éª¤ 5: é€ä¸€å¯¹æ¯”ç»“æœ ---
    print_step("æ­¥éª¤ 5", "é€ä¸€å¯¹æ¯”ç‹¬ç«‹å¤„ç†ä¸æ‰¹é‡å¤„ç†çš„ç»“æœ")
    all_match = True
    for i in range(len(test_samples)):
        print(f"\n   --- å¯¹æ¯”æ ·æœ¬ {i+1} ---")
        
        # è·å–æ¯ä¸ªæ ·æœ¬æœ‰æ•ˆé•¿åº¦ï¼ˆæ’é™¤å¡«å……ï¼‰
        individual_len = individual_outputs[i]['cls_loc'].shape[1]
        batch_len = batch_attention_mask[i].sum().item()
        
        # ç¡®ä¿æˆ‘ä»¬æ¯”è¾ƒçš„æ˜¯ç›¸åŒé•¿åº¦çš„æœ‰æ•ˆåºåˆ—
        if individual_len != batch_len:
             print(f"   - âŒ é•¿åº¦ä¸åŒ¹é…! Individual: {individual_len}, Batch: {batch_len}")
             all_match = False
             continue

        # å¯¹æ¯” cls_loc
        individual_cls_loc = individual_outputs[i]['cls_loc']
        batch_cls_loc = batch_outputs['cls_loc'][i, :batch_len, :]
        # ä½¿ç”¨æ›´å®½æ¾çš„å®¹å·®æ¥è§£å†³å¤§è§„æ¨¡çº¿æ€§å±‚ä¸­çš„æµ®ç‚¹ä¸ç²¾ç¡®é—®é¢˜
        cls_loc_match = torch.allclose(individual_cls_loc, batch_cls_loc, atol=1e-4)
        print(f"     - `cls_loc` æ˜¯å¦åŒ¹é…?   {'âœ… æ˜¯' if cls_loc_match else 'âŒ å¦'}")
        if not cls_loc_match: 
            all_match = False
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            if i == 0: # åªä¸ºç¬¬ä¸€ä¸ªæ ·æœ¬æ‰“å°è¯¦ç»†ä¿¡æ¯
                diff = torch.abs(individual_cls_loc - batch_cls_loc.unsqueeze(0))
                print(f"       - [DEBUG] Max Diff: {diff.max().item():.8f}")
                print(f"       - [DEBUG] Mean (Individual): {individual_cls_loc.mean().item():.8f}")
                print(f"       - [DEBUG] Mean (Batch): {batch_cls_loc.mean().item():.8f}")

        # å¯¹æ¯” reg_loc - ä¿æŒä¸¥æ ¼çš„å®¹å·®
        individual_reg_loc = individual_outputs[i]['reg_loc']
        batch_reg_loc = batch_outputs['reg_loc'][i, :batch_len]
        reg_loc_match = torch.allclose(individual_reg_loc, batch_reg_loc, atol=1e-5)
        print(f"     - `reg_loc` æ˜¯å¦åŒ¹é…?   {'âœ… æ˜¯' if reg_loc_match else 'âŒ å¦'}")
        if not reg_loc_match: all_match = False

    print(f"\n\n{'='*80}")
    if all_match:
        print("ğŸ‰ éªŒè¯æˆåŠŸï¼æ‰¹é‡å¤„ç†ä¸ç‹¬ç«‹å¤„ç†çš„ç»“æœå®Œå…¨ä¸€è‡´ã€‚")
    else:
        print("âŒ éªŒè¯å¤±è´¥ï¼æ‰¹é‡å¤„ç†çš„ç»“æœä¸ç‹¬ç«‹å¤„ç†ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ä¸­çš„å¡«å……æˆ–æ³¨æ„åŠ›æ©ç é€»è¾‘ã€‚")

if __name__ == '__main__':
    main() 
#!/usr/bin/env python3
"""
æ¢ç´¢Qwenæ¨¡å‹ä¸­271ä¸ªé¢„ç•™tokençš„è¡Œä¸º

è¿™ä¸ªè„šæœ¬å°†æ·±å…¥åˆ†æè¿™271ä¸ªé¢„ç•™tokenåœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„è¡Œä¸ºï¼Œ
åŒ…æ‹¬å®ƒä»¬çš„æƒé‡åˆå§‹åŒ–ã€æ¢¯åº¦æ›´æ–°ã€ä»¥åŠå®é™…ä½¿ç”¨æƒ…å†µã€‚
"""

import torch
import numpy as np
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

def explore_reserved_tokens():
    """æ¢ç´¢é¢„ç•™tokençš„è¯¦ç»†è¡Œä¸º"""
    
    print("ğŸ” æ¢ç´¢Qwenæ¨¡å‹ä¸­271ä¸ªé¢„ç•™tokençš„è¡Œä¸º")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_path = "~/models/Qwen2.5-0.5B"
    expanded_path = os.path.expanduser(model_path)
    
    print(f"ğŸ“ åŠ è½½æ¨¡å‹: {expanded_path}")
    
    try:
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(expanded_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            expanded_path, 
            trust_remote_code=True,
            torch_dtype=torch.float32,
            device_map=None
        )
        
        # åŸºæœ¬ä¿¡æ¯
        config = model.config
        lm_head = model.lm_head
        
        config_vocab_size = config.vocab_size      # 151,936
        actual_vocab_size = len(tokenizer)         # 151,665
        reserved_count = config_vocab_size - actual_vocab_size  # 271
        
        print(f"âœ… åŸºæœ¬ä¿¡æ¯:")
        print(f"   é…ç½®è¯æ±‡è¡¨å¤§å°: {config_vocab_size}")
        print(f"   å®é™…è¯æ±‡è¡¨å¤§å°: {actual_vocab_size}")
        print(f"   é¢„ç•™tokenæ•°é‡: {reserved_count}")
        
        # åˆ†ælm_headæƒé‡
        lm_head_weight = lm_head.weight.data  # [151936, 896]
        
        # åˆ†ç¦»å®é™…ä½¿ç”¨çš„æƒé‡å’Œé¢„ç•™çš„æƒé‡
        used_weights = lm_head_weight[:actual_vocab_size, :]      # [151665, 896]
        reserved_weights = lm_head_weight[actual_vocab_size:, :]  # [271, 896]
        
        print(f"\nğŸ“Š æƒé‡åˆ†æ:")
        print(f"   å®é™…ä½¿ç”¨æƒé‡å½¢çŠ¶: {used_weights.shape}")
        print(f"   é¢„ç•™æƒé‡å½¢çŠ¶: {reserved_weights.shape}")
        
        # ç»Ÿè®¡åˆ†æ
        print(f"\nğŸ“ˆ æƒé‡ç»Ÿè®¡å¯¹æ¯”:")
        print(f"   å®é™…ä½¿ç”¨æƒé‡:")
        print(f"     å‡å€¼: {used_weights.mean().item():.6f}")
        print(f"     æ ‡å‡†å·®: {used_weights.std().item():.6f}")
        print(f"     æœ€å°å€¼: {used_weights.min().item():.6f}")
        print(f"     æœ€å¤§å€¼: {used_weights.max().item():.6f}")
        
        print(f"   é¢„ç•™æƒé‡:")
        print(f"     å‡å€¼: {reserved_weights.mean().item():.6f}")
        print(f"     æ ‡å‡†å·®: {reserved_weights.std().item():.6f}")
        print(f"     æœ€å°å€¼: {reserved_weights.min().item():.6f}")
        print(f"     æœ€å¤§å€¼: {reserved_weights.max().item():.6f}")
        
        # æ£€æŸ¥é¢„ç•™æƒé‡æ˜¯å¦ä¸ºé›¶
        is_zero = torch.all(reserved_weights == 0)
        print(f"   é¢„ç•™æƒé‡æ˜¯å¦å…¨ä¸ºé›¶: {'æ˜¯' if is_zero else 'å¦'}")
        
        if not is_zero:
            # è®¡ç®—éé›¶å…ƒç´ æ¯”ä¾‹
            non_zero_count = torch.sum(reserved_weights != 0).item()
            total_elements = reserved_weights.numel()
            non_zero_ratio = non_zero_count / total_elements
            print(f"   é¢„ç•™æƒé‡éé›¶å…ƒç´ æ¯”ä¾‹: {non_zero_ratio:.4f} ({non_zero_count}/{total_elements})")
        
        # æµ‹è¯•é¢„ç•™tokençš„è¾“å‡º
        print(f"\nğŸ§ª æµ‹è¯•é¢„ç•™tokençš„è¾“å‡º:")
        
        # åˆ›å»ºåŒ…å«é¢„ç•™token IDçš„æµ‹è¯•è¾“å…¥
        test_reserved_ids = list(range(actual_vocab_size, config_vocab_size))[:5]  # å–å‰5ä¸ªé¢„ç•™token
        print(f"   æµ‹è¯•çš„é¢„ç•™token IDs: {test_reserved_ids}")
        
        # æµ‹è¯•æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨é¢„ç•™token ID
        try:
            test_input = torch.tensor([test_reserved_ids])  # [1, 5]
            with torch.no_grad():
                outputs = model(test_input)
                logits = outputs.logits  # [1, 5, 151936]
                
            print(f"   âœ… é¢„ç•™tokenå¯ä»¥æ­£å¸¸å‰å‘ä¼ æ’­")
            print(f"   è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
            
            # åˆ†æé¢„ç•™tokenä½ç½®çš„logitsè¾“å‡º
            reserved_logits = logits[0, -1, actual_vocab_size:].cpu()  # æœ€åä¸€ä¸ªä½ç½®çš„é¢„ç•™token logits
            print(f"   é¢„ç•™tokenä½ç½®logitsç»Ÿè®¡:")
            print(f"     å‡å€¼: {reserved_logits.mean().item():.6f}")
            print(f"     æ ‡å‡†å·®: {reserved_logits.std().item():.6f}")
            
        except Exception as e:
            print(f"   âŒ é¢„ç•™tokenå‰å‘ä¼ æ’­å¤±è´¥: {e}")
        
        # åˆ†æembeddingå±‚æ˜¯å¦ä¹Ÿæœ‰é¢„ç•™
        print(f"\nğŸ“ æ£€æŸ¥embeddingå±‚:")
        if hasattr(model, 'transformer') and hasattr(model.transformer, 'wte'):
            # å¯¹äºGPTç±»æ¨¡å‹
            embedding = model.transformer.wte
        elif hasattr(model, 'model') and hasattr(model.model, 'embed_tokens'):
            # å¯¹äºLlamaç±»æ¨¡å‹
            embedding = model.model.embed_tokens
        else:
            # å¯»æ‰¾embeddingå±‚
            embedding = None
            for name, module in model.named_modules():
                if 'embed' in name.lower() and hasattr(module, 'weight'):
                    if module.weight.shape[0] == config_vocab_size:
                        embedding = module
                        print(f"   æ‰¾åˆ°embeddingå±‚: {name}")
                        break
        
        if embedding is not None:
            embed_weight = embedding.weight.data
            print(f"   embeddingæƒé‡å½¢çŠ¶: {embed_weight.shape}")
            
            # åˆ†æembeddingçš„é¢„ç•™éƒ¨åˆ†
            used_embeds = embed_weight[:actual_vocab_size, :]
            reserved_embeds = embed_weight[actual_vocab_size:, :]
            
            print(f"   å®é™…ä½¿ç”¨embeddingç»Ÿè®¡:")
            print(f"     å‡å€¼: {used_embeds.mean().item():.6f}")
            print(f"     æ ‡å‡†å·®: {used_embeds.std().item():.6f}")
            
            print(f"   é¢„ç•™embeddingç»Ÿè®¡:")
            print(f"     å‡å€¼: {reserved_embeds.mean().item():.6f}")
            print(f"     æ ‡å‡†å·®: {reserved_embeds.std().item():.6f}")
            
            embed_is_zero = torch.all(reserved_embeds == 0)
            print(f"   é¢„ç•™embeddingæ˜¯å¦å…¨ä¸ºé›¶: {'æ˜¯' if embed_is_zero else 'å¦'}")
        
        # åˆ†æé¢„ç•™tokençš„æ¢¯åº¦è¡Œä¸º
        print(f"\nğŸ”„ åˆ†æé¢„ç•™tokençš„æ¢¯åº¦è¡Œä¸º:")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸå¤±æ¥æµ‹è¯•æ¢¯åº¦
        model.train()
        test_input = torch.tensor([[1, 2, 3]])  # æ­£å¸¸token
        test_target = torch.tensor([[2, 3, actual_vocab_size]])  # ç›®æ ‡åŒ…å«ä¸€ä¸ªé¢„ç•™token
        
        try:
            # å‰å‘ä¼ æ’­
            outputs = model(test_input, labels=test_target)
            loss = outputs.loss
            
            # æ¸…é›¶æ¢¯åº¦
            model.zero_grad()
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥é¢„ç•™tokenä½ç½®çš„æ¢¯åº¦
            lm_head_grad = lm_head.weight.grad
            if lm_head_grad is not None:
                used_grad = lm_head_grad[:actual_vocab_size, :]
                reserved_grad = lm_head_grad[actual_vocab_size:, :]
                
                print(f"   âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
                print(f"   å®é™…ä½¿ç”¨æƒé‡æ¢¯åº¦ç»Ÿè®¡:")
                print(f"     éé›¶å…ƒç´ æ•°: {torch.sum(used_grad != 0).item()}")
                print(f"     å‡å€¼: {used_grad.mean().item():.8f}")
                print(f"     æ ‡å‡†å·®: {used_grad.std().item():.8f}")
                
                print(f"   é¢„ç•™æƒé‡æ¢¯åº¦ç»Ÿè®¡:")
                print(f"     éé›¶å…ƒç´ æ•°: {torch.sum(reserved_grad != 0).item()}")
                print(f"     å‡å€¼: {reserved_grad.mean().item():.8f}")
                print(f"     æ ‡å‡†å·®: {reserved_grad.std().item():.8f}")
                
                grad_is_zero = torch.all(reserved_grad == 0)
                print(f"   é¢„ç•™æƒé‡æ¢¯åº¦æ˜¯å¦å…¨ä¸ºé›¶: {'æ˜¯' if grad_is_zero else 'å¦'}")
            else:
                print(f"   âŒ æ²¡æœ‰è®¡ç®—å‡ºæ¢¯åº¦")
                
        except Exception as e:
            print(f"   âŒ æ¢¯åº¦æµ‹è¯•å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ“‹ æ€»ç»“æŠ¥å‘Š:")
        print(f"   ğŸ¯ é¢„ç•™tokençš„è®¾è®¡ç›®çš„:")
        print(f"     - ä¸ºæœªæ¥æ‰©å±•è¯æ±‡è¡¨é¢„ç•™ç©ºé—´")
        print(f"     - ä¿æŒæ¨¡å‹æ¶æ„çš„çµæ´»æ€§")
        print(f"     - é¿å…å› è¯æ±‡è¡¨å¢é•¿è€Œé‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹")
        
        print(f"   ğŸ”§ é¢„ç•™tokençš„æŠ€æœ¯ç‰¹ç‚¹:")
        print(f"     - æƒé‡çŸ©é˜µæ”¯æŒå®Œæ•´çš„{config_vocab_size}ç»´è¾“å‡º")
        print(f"     - å‰{actual_vocab_size}ä¸ªä½ç½®å¯¹åº”å®é™…è¯æ±‡")
        print(f"     - å{reserved_count}ä¸ªä½ç½®ä¿æŒé¢„ç•™çŠ¶æ€")
        
        print(f"   ğŸ’¡ å¯¹CausalQwençš„å¯ç¤º:")
        print(f"     - ç†è®ºè®¾è®¡åº”åŸºäºé…ç½®å®¹é‡K={config_vocab_size}")
        print(f"     - å®é™…å®ç°ä½¿ç”¨æœ‰æ•ˆå¤§å°K={actual_vocab_size}")
        print(f"     - æˆ‘ä»¬çš„K+1={actual_vocab_size+1}è®¾è®¡æ˜¯æ­£ç¡®çš„")
        
        return {
            'config_vocab_size': config_vocab_size,
            'actual_vocab_size': actual_vocab_size,
            'reserved_count': reserved_count,
            'reserved_weights_stats': {
                'mean': reserved_weights.mean().item(),
                'std': reserved_weights.std().item(),
                'is_zero': is_zero.item()
            }
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    explore_reserved_tokens() 
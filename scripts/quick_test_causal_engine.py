#!/usr/bin/env python3
"""
CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬

ğŸš€ ç›®æ ‡ï¼šå¿«é€ŸéªŒè¯ CausalEngine çš„åŸºæœ¬æ€§èƒ½
ğŸ¯ æ ¸å¿ƒï¼šç»Ÿä¸€æ ‡å‡†åŒ–ç­–ç•¥ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒå›å½’å’Œåˆ†ç±»ä»»åŠ¡
- ç»Ÿä¸€æ•°æ®é¢„å¤„ç†ï¼šæ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–ç­–ç•¥
- å…¬å¹³å¯¹æ¯”ï¼šsklearn MLP, PyTorch MLP, CausalEngine (deterministic/standard)
- ç§‘å­¦è¯„ä¼°ï¼šè®­ç»ƒæ—¶æŠ—å™ªå£°ï¼Œè¯„ä¼°æ—¶åœ¨åŸå§‹å°ºåº¦ä¸Šç»Ÿä¸€æ¯”è¾ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç›´æ¥è¿è¡Œï¼špython scripts/quick_test_causal_engine.py
2. è°ƒæ•´å‚æ•°ï¼šä¿®æ”¹ä¸‹æ–¹çš„ REGRESSION_CONFIG å’Œ CLASSIFICATION_CONFIG
3. å¿«é€Ÿæµ‹è¯•ï¼šä½¿ç”¨ quick_regression_test() æˆ– quick_classification_test()

æ•°æ®æµç¨‹ï¼š
åŸå§‹æ•°æ® â†’ è®­ç»ƒ/æµ‹è¯•åˆ†å‰² â†’ æ ‡å‡†åŒ–(æ— æ³„éœ²) â†’ å™ªå£°æ³¨å…¥(ä»…è®­ç»ƒé›†) â†’ æ¨¡å‹è®­ç»ƒ â†’ åŸå§‹å°ºåº¦è¯„ä¼°
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æˆ‘ä»¬çš„CausalEngineå®ç°
from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.classifier import MLPCausalClassifier, MLPPytorchClassifier
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

# =============================================================================
# é…ç½®éƒ¨åˆ† - åœ¨è¿™é‡Œä¿®æ”¹å®éªŒå‚æ•°
# =============================================================================

REGRESSION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ
    'n_samples': 4000,  # æ›´å¤§è§„æ¨¡
    'n_features': 12,
    'noise': 1.0,
    'random_state': 42,
    'test_size': 0.2,  # æµ‹è¯•é›†æ¯”ä¾‹
    'anomaly_ratio': 0.4,  # 40%å¼‚å¸¸æ•°æ®ï¼ŒåŒ¹é…å…¶ä»–è„šæœ¬
    
    # ç½‘ç»œç»“æ„
    'perception_hidden_layers': (128, 64, 32),  # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # CausalEngineå‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    'alpha': 0.0001, # æ·»åŠ L2æ­£åˆ™åŒ–ï¼Œä¸sklearné»˜è®¤ä¸€è‡´
    
    # è®­ç»ƒå‚æ•°
    'max_iter': 3000,  # ç»Ÿä¸€æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´æ¥è¿‘sklearné»˜è®¤
    'patience': 50,  # å‡å°‘patienceï¼Œæ›´æ¥è¿‘sklearné»˜è®¤
    'tol': 1e-4,  # æ›´æ¥è¿‘sklearné»˜è®¤tolerance
    'validation_fraction': 0.2,
    'batch_size': None,  # ç»Ÿä¸€ä½¿ç”¨å…¨é‡è®­ç»ƒ(full-batch)
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_causal_deterministic': True,
    'test_causal_standard': True,
    'verbose': True
}

CLASSIFICATION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ - ä¸sklearnæ›´ç›¸ä¼¼çš„è®¾ç½®
    'n_samples': 4000,  # å‡å°‘æ ·æœ¬é‡ï¼Œæ›´åƒsklearnç»å…¸æµ‹è¯•
    'n_features': 10,   # å‡å°‘ç‰¹å¾æ•°
    'n_classes': 3,
    'class_sep': 1.0,   # æé«˜ç±»åˆ«åˆ†ç¦»åº¦
    'random_state': 42,
    'test_size': 0.2,   # æµ‹è¯•é›†æ¯”ä¾‹
    'label_noise_ratio': 0.4,  # ç»Ÿä¸€æ ‡ç­¾å™ªå£°æ°´å¹³
    
    # ç½‘ç»œç»“æ„ - æ›´ç®€å•çš„ç½‘ç»œ
    'perception_hidden_layers': (128, 64, 32),  # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # CausalEngineå‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    'ovr_threshold': 2.0,
    'alpha': 0.0,  # åŒ¹é…sklearné»˜è®¤L2æ­£åˆ™åŒ–
    
    # è®­ç»ƒå‚æ•° - æ›´æ¥è¿‘sklearné»˜è®¤å€¼
    'max_iter': 3000,   # å‡å°‘æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,  # ä½¿ç”¨sklearné»˜è®¤å­¦ä¹ ç‡
    'patience': 10,     # ä½¿ç”¨sklearné»˜è®¤patience
    'tol': 1e-4,        # åŒ¹é…sklearné»˜è®¤tolerance
    'validation_fraction': 0.2,  # ä½¿ç”¨sklearné»˜è®¤éªŒè¯é›†æ¯”ä¾‹
    'batch_size': None,  # ç»Ÿä¸€ä½¿ç”¨å…¨é‡è®­ç»ƒ(full-batch)
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_causal_deterministic': True,
    'test_causal_standard': True,
    'verbose': True
}

# =============================================================================
# æ•°æ®ç”Ÿæˆå‡½æ•°
# =============================================================================

def generate_regression_data(config):
    """ç”Ÿæˆå›å½’æµ‹è¯•æ•°æ® - å…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬"""
    print(f"ğŸ“Š ç”Ÿæˆå›å½’æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, å™ªå£°={config['noise']}")
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state']
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    # ğŸ¯ å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ (å…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°)
    print(f"   ğŸ¯ å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ (å…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°):")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    
    # ç›®æ ‡æ ‡å‡†åŒ–ï¼ˆå…³é”®ï¼ï¼‰- åœ¨å¹²å‡€æ•°æ®ä¸Šæ‹Ÿåˆ
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    print(f"      - X å’Œ y éƒ½å·²åœ¨å¹²å‡€æ•°æ®ä¸Šå®Œæˆæ ‡å‡†åŒ–")

    # å¯¹ *æ ‡å‡†åŒ–å* çš„è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥
    if config['anomaly_ratio'] > 0:
        y_train_scaled_noisy, noise_indices = inject_shuffle_noise(
            y_train_scaled, 
            noise_ratio=config['anomaly_ratio'],
            random_state=config['random_state']
        )
        y_train_for_training = y_train_scaled_noisy
        print(f"   å¼‚å¸¸æ³¨å…¥: åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­å¯¹ {config['anomaly_ratio']:.1%} çš„æ ‡ç­¾æ³¨å…¥å™ªå£°")
        print(f"             ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
    else:
        y_train_for_training = y_train_scaled
        print(f"   æ— å¼‚å¸¸æ³¨å…¥: çº¯å‡€ç¯å¢ƒ")
    
    print(f"      - æ‰€æœ‰æ¨¡å‹å°†åœ¨å«å™ªæ ‡å‡†åŒ–ç©ºé—´ä¸­ç«äº‰")
    
    data = {
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        'X_train_original': X_train, 'X_test_original': X_test,
        'y_train_original': y_train, 'y_test_original': y_test,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
        'X_train': X_train_scaled, 'X_test': X_test_scaled,
        'y_train': y_train_for_training,
        'y_test': y_test_scaled,
        
        # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
        'scaler_X': scaler_X, 'scaler_y': scaler_y
    }
    
    return data

def generate_classification_data(config):
    """ç”Ÿæˆåˆ†ç±»æµ‹è¯•æ•°æ® - å…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬"""
    print(f"ğŸ“Š ç”Ÿæˆåˆ†ç±»æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
    
    n_informative = min(config['n_features'], max(2, config['n_features'] // 2))
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_classification(
        n_samples=config['n_samples'], 
        n_features=config['n_features'], 
        n_classes=config['n_classes'],
        n_informative=n_informative, 
        n_redundant=0, 
        n_clusters_per_class=1,
        class_sep=config['class_sep'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        stratify=y
    )
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    # ğŸ¯ æ— æ³„éœ²æ ‡å‡†åŒ–ç­–ç•¥ï¼šå…ˆæ ‡å‡†åŒ–ç‰¹å¾ï¼Œåæ³¨å…¥æ ‡ç­¾å™ªå£°
    print(f"   ğŸ¯ å®æ–½æ— æ³„éœ²æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆå…ˆæ ‡å‡†åŒ–ï¼Œåæ³¨å…¥å™ªå£°ï¼‰:")
    
    # ç‰¹å¾æ ‡å‡†åŒ– - åœ¨å¹²å‡€æ•°æ®ä¸Šfit
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    print(f"      - X å·²åœ¨å¹²å‡€æ•°æ®ä¸Šå®Œæˆæ ‡å‡†åŒ–")
    
    # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥ï¼ˆåœ¨æ ‡å‡†åŒ–åï¼‰
    if config['label_noise_ratio'] > 0:
        y_train_noisy, noise_indices = inject_shuffle_noise(
            y_train, 
            noise_ratio=config['label_noise_ratio'],
            random_state=config['random_state']
        )
        y_train_for_training = y_train_noisy
        print(f"   æ ‡ç­¾å™ªå£°: {config['label_noise_ratio']:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
    else:
        y_train_for_training = y_train
        print(f"   æ— æ ‡ç­¾å™ªå£°: çº¯å‡€ç¯å¢ƒ")
    
    print(f"      - åˆ†ç±»æ ‡ç­¾ä¸æ ‡å‡†åŒ–ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨æ ‡å‡†åŒ–ç‰¹å¾ç©ºé—´ä¸­ç«äº‰")
    
    data = {
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºå‚è€ƒï¼‰
        'X_train_original': X_train, 'X_test_original': X_test,
        'y_train_original': y_train, 'y_test_original': y_test,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
        'X_train': X_train_scaled, 'X_test': X_test_scaled,
        'y_train': y_train_for_training, 'y_test': y_test,  # åˆ†ç±»æ ‡ç­¾ä¸æ ‡å‡†åŒ–
        
        # æ ‡å‡†åŒ–å™¨
        'scaler_X': scaler_X
    }
    
    return data

# =============================================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°
# =============================================================================

def train_sklearn_regressor(data, config):
    """è®­ç»ƒsklearnå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
    
    model = MLPRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_sklearn_classifier(data, config):
    """è®­ç»ƒsklearnåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPClassifier...")
    
    model = MLPClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_pytorch_regressor(data, config):
    """è®­ç»ƒPyTorchå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
    
    model = MLPPytorchRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_pytorch_classifier(data, config):
    """è®­ç»ƒPyTorchåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPClassifier...")
    
    model = MLPPytorchClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_regressor(data, config, mode='standard'):
    """è®­ç»ƒå› æœå›å½’å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalRegressor ({mode})...")
    
    model = MLPCausalRegressor(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_classifier(data, config, mode='standard'):
    """è®­ç»ƒå› æœåˆ†ç±»å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalClassifier ({mode})...")
    
    model = MLPCausalClassifier(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        ovr_threshold=config['ovr_threshold'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

# =============================================================================
# è¯„ä¼°å‡½æ•°
# =============================================================================

def evaluate_regression(y_true, y_pred):
    """å›å½’è¯„ä¼°æŒ‡æ ‡"""
    return {
        'MAE': mean_absolute_error(y_true, y_pred),
        'MdAE': median_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'RÂ²': r2_score(y_true, y_pred)
    }

def evaluate_classification(y_true, y_pred, n_classes):
    """åˆ†ç±»è¯„ä¼°æŒ‡æ ‡"""
    avg_method = 'binary' if n_classes == 2 else 'macro'
    return {
        'Acc': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, average=avg_method, zero_division=0),
        'Recall': recall_score(y_true, y_pred, average=avg_method, zero_division=0),
        'F1': f1_score(y_true, y_pred, average=avg_method, zero_division=0)
    }

def predict_and_evaluate_regression(model, data, model_name, config):
    """å›å½’æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼° - ç»Ÿä¸€é€†å˜æ¢ç­–ç•¥"""
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­é¢„æµ‹æµ‹è¯•é›†
    test_pred_scaled = model.predict(data['X_test'])
    
    # å°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
    test_pred_original = data['scaler_y'].inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
    
    # éªŒè¯é›†ï¼šä»åŸå§‹å¹²å‡€æ•°æ®é‡æ–°åˆ†å‰²ï¼ˆç¡®ä¿éªŒè¯è¯„ä¼°çš„ç§‘å­¦æ€§ï¼‰
    X_train_orig, X_val_orig, y_train_orig, y_val_orig = train_test_split(
        data['X_train_original'], data['y_train_original'],
        test_size=config['validation_fraction'],
        random_state=config['random_state']
    )
    
    # å¯¹éªŒè¯é›†åº”ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
    X_val_scaled = data['scaler_X'].transform(X_val_orig)
    val_pred_scaled = model.predict(X_val_scaled)
    
    # å°†éªŒè¯é›†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦
    val_pred_original = data['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
    
    # åœ¨åŸå§‹å°ºåº¦ä¸‹è¯„ä¼°æ€§èƒ½ï¼ˆæµ‹è¯•é›†å’ŒéªŒè¯é›†éƒ½ç”¨å¹²å‡€æ•°æ®è¯„ä¼°ï¼‰
    results = {
        'test': evaluate_regression(data['y_test_original'], test_pred_original),
        'val': evaluate_regression(y_val_orig, val_pred_original)
    }
    
    return results

def predict_and_evaluate_classification(model, data, model_name, config):
    """åˆ†ç±»æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼° - ç»Ÿä¸€è¯„ä¼°ç­–ç•¥"""
    n_classes = len(np.unique(data['y_train_original']))
    
    # åœ¨æ ‡å‡†åŒ–ç‰¹å¾ç©ºé—´ä¸­é¢„æµ‹æµ‹è¯•é›†ï¼ˆåˆ†ç±»æ ‡ç­¾æ— éœ€è½¬æ¢ï¼‰
    test_pred = model.predict(data['X_test'])
    
    # éªŒè¯é›†ï¼šä»åŸå§‹å¹²å‡€æ•°æ®é‡æ–°åˆ†å‰²ï¼ˆç¡®ä¿éªŒè¯è¯„ä¼°çš„ç§‘å­¦æ€§ï¼‰
    X_train_orig, X_val_orig, y_train_orig, y_val_orig = train_test_split(
        data['X_train_original'], data['y_train_original'],
        test_size=config['validation_fraction'],
        random_state=config['random_state'],
        stratify=data['y_train_original']
    )
    
    # å¯¹éªŒè¯é›†åº”ç”¨ç›¸åŒçš„æ ‡å‡†åŒ–
    X_val_scaled = data['scaler_X'].transform(X_val_orig)
    val_pred = model.predict(X_val_scaled)
    
    # åˆ†ç±»ä»»åŠ¡ï¼šåœ¨åŸå§‹æ ‡ç­¾ç©ºé—´è¯„ä¼°ï¼ˆæµ‹è¯•é›†å’ŒéªŒè¯é›†éƒ½ç”¨å¹²å‡€æ•°æ®è¯„ä¼°ï¼‰
    results = {
        'test': evaluate_classification(data['y_test'], test_pred, n_classes),
        'val': evaluate_classification(y_val_orig, val_pred, n_classes)
    }
    
    return results

# =============================================================================
# ç»“æœæ˜¾ç¤ºå‡½æ•°
# =============================================================================

def print_regression_results(results):
    """æ‰“å°å›å½’ç»“æœ"""
    print("\nğŸ“Š å›å½’ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
              f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
    
    print("=" * 120)

def print_classification_results(results, n_classes):
    """æ‰“å°åˆ†ç±»ç»“æœ"""
    print(f"\nğŸ“Š {n_classes}åˆ†ç±»ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['Acc']:<10.4f} {val_m['Precision']:<10.4f} {val_m['Recall']:<10.4f} {val_m['F1']:<10.4f} "
              f"{test_m['Acc']:<10.4f} {test_m['Precision']:<10.4f} {test_m['Recall']:<10.4f} {test_m['F1']:<10.4f}")
    
    print("=" * 120)

# =============================================================================
# ä¸»æµ‹è¯•å‡½æ•°
# =============================================================================

def test_regression(config=None):
    """å›å½’ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = REGRESSION_CONFIG
    
    print("\nğŸ”¬ å›å½’ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'regression')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_regression_data(config)
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_regressor(data, config)
        results['sklearn'] = predict_and_evaluate_regression(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_regressor(data, config)
        results['pytorch'] = predict_and_evaluate_regression(pytorch_model, data, 'causal', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_regressor(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_regression(causal_det, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_regressor(data, config, 'standard')
        results['standard'] = predict_and_evaluate_regression(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        print_regression_results(results)
    
    return results

def test_classification(config=None):
    """åˆ†ç±»ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = CLASSIFICATION_CONFIG
    
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'classification')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_classification_data(config)
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_classifier(data, config)
        results['sklearn'] = predict_and_evaluate_classification(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_classifier(data, config)
        results['pytorch'] = predict_and_evaluate_classification(pytorch_model, data, 'causal', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_classifier(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_classification(causal_det, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_classifier(data, config, 'standard')
        results['standard'] = predict_and_evaluate_classification(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        n_classes = len(np.unique(data['y_train']))
        print_classification_results(results, n_classes)
    
    return results

def print_config_summary(config, task_type):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    if task_type == 'regression':
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, å™ªå£°={config['noise']}")
        print(f"å¼‚å¸¸: {config['anomaly_ratio']:.1%} å¼‚å¸¸æ•°æ®æ³¨å…¥")
    else:
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
        print(f"å™ªå£°: {config['label_noise_ratio']:.1%} æ ‡ç­¾å™ªå£°, åˆ†ç¦»åº¦={config['class_sep']}")
    
    print(f"ç½‘ç»œ: {config['perception_hidden_layers']}")
    print(f"è®­ç»ƒ: {config['max_iter']} epochs, lr={config['learning_rate']}, patience={config['patience']}")
    print(f"æµ‹è¯•: sklearn={config['test_sklearn']}, pytorch={config['test_pytorch']}, "
          f"deterministic={config['test_causal_deterministic']}, standard={config['test_causal_standard']}")
    print()

# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

def main():
    """ä¸»ç¨‹åº - è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬ - å…¨å±€æ ‡å‡†åŒ–ç‰ˆ")
    print("=" * 60)
    
    # è¿è¡Œå›å½’æµ‹è¯•
    regression_results = test_regression()
    
    # è¿è¡Œåˆ†ç±»æµ‹è¯•  
    classification_results = test_classification()
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ ä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„ CONFIG éƒ¨åˆ†æ¥è°ƒæ•´å®éªŒå‚æ•°")

def quick_regression_test():
    """å¿«é€Ÿå›å½’æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = REGRESSION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1000,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'verbose': True
    })
    return test_regression(quick_config)

def quick_classification_test():
    """å¿«é€Ÿåˆ†ç±»æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = CLASSIFICATION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1500,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'verbose': True
    })
    return test_classification(quick_config)

if __name__ == "__main__":
    # ä½ å¯ä»¥é€‰æ‹©è¿è¡Œä»¥ä¸‹ä»»ä¸€å‡½æ•°:
    main()                        # å®Œæ•´æµ‹è¯•
    # quick_regression_test()     # å¿«é€Ÿå›å½’æµ‹è¯•
    # quick_classification_test() # å¿«é€Ÿåˆ†ç±»æµ‹è¯•
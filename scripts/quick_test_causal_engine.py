#!/usr/bin/env python3
"""
CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬ - é‡æ„ç‰ˆæœ¬

æ¸…æ™°ã€çº¿æ€§ã€æ˜“äºä¿®æ”¹çš„æµ‹è¯•è„šæœ¬ã€‚
æ¯ä¸ªåŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºè°ƒè¯•å’Œå®éªŒã€‚

ä½¿ç”¨è¯´æ˜:
1. ä¿®æ”¹ CONFIG éƒ¨åˆ†çš„å‚æ•°
2. è¿è¡Œ python scripts/quick_test_causal_engine.py
3. æ ¹æ®éœ€è¦å¯ç”¨/ç¦ç”¨ç‰¹å®šçš„æ¨¡å‹æ¯”è¾ƒ
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.datasets import make_regression, make_classification
from sklearn.model_selection import train_test_split
import os
import sys
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æˆ‘ä»¬çš„CausalEngineå®ç°
from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.classifier import MLPCausalClassifier, MLPPytorchClassifier
from causal_sklearn.utils import causal_split

warnings.filterwarnings('ignore')

# =============================================================================
# é…ç½®éƒ¨åˆ† - åœ¨è¿™é‡Œä¿®æ”¹å®éªŒå‚æ•°
# ğŸ¯ é‡ç‚¹ï¼šå®ç°æœ€å°å¿…è¦ä¸åŒ - æ‰€æœ‰ç®—æ³•ä½¿ç”¨ç»Ÿä¸€çš„åŸºç¡€é…ç½®
# =============================================================================

REGRESSION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ
    'n_samples': 4000,
    'n_features': 12,
    'noise': 1.0,
    'random_state': 42,
    'test_size': 0.1,
    'anomaly_ratio': 0.2,
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - ç¡®ä¿æ‰€æœ‰æ–¹æ³•å‚æ•°ä¸€è‡´
    # =========================================================================
    'perception_hidden_layers': (128, 64),  # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # ğŸ¯ ç»Ÿä¸€è®­ç»ƒå‚æ•° - æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒé…ç½®
    'max_iter': 2000,                       # ç»Ÿä¸€æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,                  # ç»Ÿä¸€å­¦ä¹ ç‡
    'patience': 50,                         # ç»Ÿä¸€æ—©åœpatience
    'tol': 1e-4,                           # ç»Ÿä¸€æ”¶æ•›å®¹å¿åº¦
    'validation_fraction': 0.2,             # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
    'batch_size': None,                     # ç»Ÿä¸€ä½¿ç”¨å…¨é‡æ‰¹æ¬¡
    'alpha': 0.0,                          # ğŸ”§ ç»Ÿä¸€æ­£åˆ™åŒ–ï¼šæ— L2æ­£åˆ™åŒ–
    # =========================================================================
    
    # CausalEngineç‰¹æœ‰å‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_causal_deterministic': True,
    'test_causal_standard': True,
    'verbose': True
}

CLASSIFICATION_CONFIG = {
    # æ•°æ®ç”Ÿæˆ
    'n_samples': 4000,
    'n_features': 10,
    'n_classes': 3,
    'class_sep': 1.0,
    'random_state': 42,
    'test_size': 0.2,
    'label_anomaly_ratio': 0.3,
    
    # ğŸ§  ç»Ÿä¸€ç¥ç»ç½‘ç»œé…ç½® - ç¡®ä¿æ‰€æœ‰æ–¹æ³•å‚æ•°ä¸€è‡´
    # =========================================================================
    'perception_hidden_layers': (100,),    # ç»Ÿä¸€ç½‘ç»œç»“æ„
    'abduction_hidden_layers': (),
    'repre_size': None,
    'causal_size': None,
    
    # ğŸ¯ ç»Ÿä¸€è®­ç»ƒå‚æ•° - æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒé…ç½®
    'max_iter': 3000,                      # ç»Ÿä¸€æœ€å¤§è¿­ä»£æ¬¡æ•°
    'learning_rate': 0.01,                 # ç»Ÿä¸€å­¦ä¹ ç‡
    'patience': 10,                        # ç»Ÿä¸€æ—©åœpatience
    'tol': 1e-4,                          # ç»Ÿä¸€æ”¶æ•›å®¹å¿åº¦
    'validation_fraction': 0.2,            # ç»Ÿä¸€éªŒè¯é›†æ¯”ä¾‹
    'batch_size': None,                    # ç»Ÿä¸€ä½¿ç”¨å…¨é‡æ‰¹æ¬¡
    'alpha': 0.0,                         # ğŸ”§ ç»Ÿä¸€æ­£åˆ™åŒ–ï¼šæ— L2æ­£åˆ™åŒ–
    # =========================================================================
    
    # CausalEngineåˆ†ç±»ç‰¹æœ‰å‚æ•°
    'gamma_init': 1.0,
    'b_noise_init': 1.0,
    'b_noise_trainable': True,
    'ovr_threshold': 2.0,
    
    # æµ‹è¯•æ§åˆ¶
    'test_sklearn': True,
    'test_pytorch': True,
    'test_causal_deterministic': True,
    'test_causal_standard': True,
    'verbose': True
}

# =============================================================================
# æ•°æ®ç”Ÿæˆå‡½æ•°
# =============================================================================

def generate_regression_data(config):
    """ç”Ÿæˆå›å½’æµ‹è¯•æ•°æ®"""
    print(f"ğŸ“Š ç”Ÿæˆå›å½’æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, æ•°æ®ç”Ÿæˆå™ªå£°={config['noise']}")
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²ï¼ŒåŒ…å«å¼‚å¸¸æ³¨å…¥
    X_train, X_test, y_train, y_test = causal_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        anomaly_ratio=config['anomaly_ratio'], 
        anomaly_type='regression'
    )
    
    # æ•°æ®ä¸å†è¿›è¡Œæ ‡å‡†åŒ–
    data = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test
    }
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    print(f"   å¼‚å¸¸æ³¨å…¥: {config['anomaly_ratio']:.1%} (ä»…å½±å“è®­ç»ƒé›†)")
    return data

def generate_classification_data(config):
    """ç”Ÿæˆåˆ†ç±»æµ‹è¯•æ•°æ®"""
    print(f"ğŸ“Š ç”Ÿæˆåˆ†ç±»æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
    
    n_informative = min(config['n_features'], max(2, config['n_features'] // 2))
    
    # ç”ŸæˆåŸºç¡€æ•°æ®
    X, y = make_classification(
        n_samples=config['n_samples'], 
        n_features=config['n_features'], 
        n_classes=config['n_classes'],
        n_informative=n_informative, 
        n_redundant=0, 
        n_clusters_per_class=1,
        class_sep=config['class_sep'], 
        random_state=config['random_state']
    )
    
    # è¿›è¡Œæ•°æ®åˆ†å‰²ï¼ŒåŒ…å«å¼‚å¸¸æ³¨å…¥
    X_train, X_test, y_train, y_test = causal_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        stratify=y,
        anomaly_ratio=config['label_anomaly_ratio'], 
        anomaly_type='classification',
        anomaly_strategy='shuffle'
    )
    
    # æ•°æ®ä¸å†è¿›è¡Œæ ‡å‡†åŒ–
    data = {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test
    }
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    print(f"   æ ‡ç­¾å¼‚å¸¸: {config['label_anomaly_ratio']:.1%} (ä»…å½±å“è®­ç»ƒé›†)")
    return data

# =============================================================================
# æ¨¡å‹è®­ç»ƒå‡½æ•°
# =============================================================================

def train_sklearn_regressor(data, config):
    """è®­ç»ƒsklearnå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPRegressor...")
    
    model = MLPRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_sklearn_classifier(data, config):
    """è®­ç»ƒsklearnåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ sklearn MLPClassifier...")
    
    model = MLPClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        print(f"   è®­ç»ƒå®Œæˆ: {model.n_iter_} epochs")
    
    return model

def train_pytorch_regressor(data, config):
    """è®­ç»ƒPyTorchå›å½’å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPRegressor...")
    
    model = MLPPytorchRegressor(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_pytorch_classifier(data, config):
    """è®­ç»ƒPyTorchåˆ†ç±»å™¨"""
    print("ğŸ”§ è®­ç»ƒ PyTorch MLPClassifier...")
    
    model = MLPPytorchClassifier(
        hidden_layer_sizes=config['perception_hidden_layers'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_regressor(data, config, mode='standard'):
    """è®­ç»ƒå› æœå›å½’å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalRegressor ({mode})...")
    
    model = MLPCausalRegressor(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

def train_causal_classifier(data, config, mode='standard'):
    """è®­ç»ƒå› æœåˆ†ç±»å™¨"""
    print(f"ğŸ”§ è®­ç»ƒ CausalClassifier ({mode})...")
    
    model = MLPCausalClassifier(
        repre_size=config['repre_size'],
        causal_size=config['causal_size'],
        perception_hidden_layers=config['perception_hidden_layers'],
        abduction_hidden_layers=config['abduction_hidden_layers'],
        mode=mode,
        gamma_init=config['gamma_init'],
        b_noise_init=config['b_noise_init'],
        b_noise_trainable=config['b_noise_trainable'],
        ovr_threshold=config['ovr_threshold'],
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        tol=config['tol'],
        random_state=config['random_state'],
        verbose=config['verbose'],
        alpha=config['alpha'],
        batch_size=len(data['X_train']) if config['batch_size'] is None else config['batch_size']
    )
    
    model.fit(data['X_train'], data['y_train'])
    
    if config['verbose']:
        n_iter = model.n_iter_
        print(f"   è®­ç»ƒå®Œæˆ: {n_iter} epochs")
    
    return model

# =============================================================================
# è¯„ä¼°å‡½æ•°
# =============================================================================

def evaluate_regression(y_true, y_pred):
    """å›å½’è¯„ä¼°æŒ‡æ ‡"""
    return {
        'MAE': mean_absolute_error(y_true, y_pred),
        'MdAE': median_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'RÂ²': r2_score(y_true, y_pred)
    }

def evaluate_classification(y_true, y_pred, n_classes):
    """åˆ†ç±»è¯„ä¼°æŒ‡æ ‡"""
    avg_method = 'binary' if n_classes == 2 else 'macro'
    return {
        'Acc': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, average=avg_method, zero_division=0),
        'Recall': recall_score(y_true, y_pred, average=avg_method, zero_division=0),
        'F1': f1_score(y_true, y_pred, average=avg_method, zero_division=0)
    }

def predict_and_evaluate_regression(model, data, model_name, config):
    """å›å½’æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼°"""
    # ç»Ÿä¸€è¯„ä¼°é€»è¾‘ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨åŸå§‹æ•°æ®
    test_pred = model.predict(data['X_test'])
    
    # éªŒè¯é›†ï¼šé‡æ–°åˆ†å‰²æ¥è¯„ä¼°
    X_train_pt, X_val_pt, y_train_pt, y_val_pt = train_test_split(
        data['X_train'], data['y_train'],
        test_size=config['validation_fraction'],
        random_state=config['random_state']
    )
    val_pred = model.predict(X_val_pt)
    y_val_orig = y_val_pt
    
    results = {
        'test': evaluate_regression(data['y_test'], test_pred),
        'val': evaluate_regression(y_val_orig, val_pred)
    }
    
    return results

def predict_and_evaluate_classification(model, data, model_name, config):
    """åˆ†ç±»æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼°"""
    n_classes = len(np.unique(data['y_train']))
    
    # ç»Ÿä¸€è¯„ä¼°é€»è¾‘ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨åŸå§‹æ•°æ®
    test_pred = model.predict(data['X_test'])
    
    # éªŒè¯é›†ï¼šé‡æ–°åˆ†å‰²æ¥è¯„ä¼°
    X_train_pt, X_val_pt, y_train_pt, y_val_pt = train_test_split(
        data['X_train'], data['y_train'],
        test_size=config['validation_fraction'],
        random_state=config['random_state'],
        stratify=data['y_train']
    )
    val_pred = model.predict(X_val_pt)
    y_val_orig = y_val_pt
    
    results = {
        'test': evaluate_classification(data['y_test'], test_pred, n_classes),
        'val': evaluate_classification(y_val_orig, val_pred, n_classes)
    }
    
    return results

# =============================================================================
# ç»“æœæ˜¾ç¤ºå‡½æ•°
# =============================================================================

def print_regression_results(results):
    """æ‰“å°å›å½’ç»“æœ"""
    print("\nğŸ“Š å›å½’ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAE':<10} {'MdAE':<10} {'RMSE':<10} {'RÂ²':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['MAE']:<10.4f} {val_m['MdAE']:<10.4f} {val_m['RMSE']:<10.4f} {val_m['RÂ²']:<10.4f} "
              f"{test_m['MAE']:<10.4f} {test_m['MdAE']:<10.4f} {test_m['RMSE']:<10.4f} {test_m['RÂ²']:<10.4f}")
    
    print("=" * 120)

def print_classification_results(results, n_classes):
    """æ‰“å°åˆ†ç±»ç»“æœ"""
    print(f"\nğŸ“Š {n_classes}åˆ†ç±»ç»“æœå¯¹æ¯”:")
    print("=" * 120)
    print(f"{'æ–¹æ³•':<20} {'éªŒè¯é›†':<50} {'æµ‹è¯•é›†':<50}")
    print(f"{'':20} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
    print("-" * 120)
    
    for method, metrics in results.items():
        val_m = metrics['val']
        test_m = metrics['test']
        print(f"{method:<20} {val_m['Acc']:<10.4f} {val_m['Precision']:<10.4f} {val_m['Recall']:<10.4f} {val_m['F1']:<10.4f} "
              f"{test_m['Acc']:<10.4f} {test_m['Precision']:<10.4f} {test_m['Recall']:<10.4f} {test_m['F1']:<10.4f}")
    
    print("=" * 120)

# =============================================================================
# ä¸»æµ‹è¯•å‡½æ•°
# =============================================================================

def test_regression(config=None):
    """å›å½’ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = REGRESSION_CONFIG
    
    print("\nğŸ”¬ å›å½’ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'regression')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_regression_data(config)
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_regressor(data, config)
        results['sklearn'] = predict_and_evaluate_regression(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_regressor(data, config)
        results['pytorch'] = predict_and_evaluate_regression(pytorch_model, data, 'causal', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_regressor(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_regression(causal_det, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_regressor(data, config, 'standard')
        results['standard'] = predict_and_evaluate_regression(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        print_regression_results(results)
    
    return results

def test_classification(config=None):
    """åˆ†ç±»ä»»åŠ¡æµ‹è¯•"""
    if config is None:
        config = CLASSIFICATION_CONFIG
    
    print("\nğŸ¯ åˆ†ç±»ä»»åŠ¡æµ‹è¯•")
    print("=" * 80)
    print_config_summary(config, 'classification')
    
    # 1. ç”Ÿæˆæ•°æ®
    data = generate_classification_data(config)
    results = {}
    
    # 2. è®­ç»ƒå„ç§æ¨¡å‹
    if config['test_sklearn']:
        sklearn_model = train_sklearn_classifier(data, config)
        results['sklearn'] = predict_and_evaluate_classification(sklearn_model, data, 'sklearn', config)
    
    if config['test_pytorch']:
        pytorch_model = train_pytorch_classifier(data, config)
        results['pytorch'] = predict_and_evaluate_classification(pytorch_model, data, 'causal', config)
    
    if config['test_causal_deterministic']:
        causal_det = train_causal_classifier(data, config, 'deterministic')
        results['deterministic'] = predict_and_evaluate_classification(causal_det, data, 'causal', config)
    
    if config['test_causal_standard']:
        causal_std = train_causal_classifier(data, config, 'standard')
        results['standard'] = predict_and_evaluate_classification(causal_std, data, 'causal', config)
    
    # 3. æ˜¾ç¤ºç»“æœ
    if config['verbose']:
        n_classes = len(np.unique(data['y_train']))
        print_classification_results(results, n_classes)
    
    return results

def print_config_summary(config, task_type):
    """æ‰“å°é…ç½®æ‘˜è¦"""
    if task_type == 'regression':
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, æ•°æ®ç”Ÿæˆå™ªå£°={config['noise']}")
        print(f"æ ‡ç­¾å¼‚å¸¸: {config['anomaly_ratio']:.1%} æ ‡ç­¾å¼‚å¸¸æ³¨å…¥")
    else:
        print(f"æ•°æ®: {config['n_samples']}æ ·æœ¬, {config['n_features']}ç‰¹å¾, {config['n_classes']}ç±»åˆ«")
        print(f"æ ‡ç­¾å¼‚å¸¸: {config['label_anomaly_ratio']:.1%} æ ‡ç­¾å¼‚å¸¸æ³¨å…¥, åˆ†ç¦»åº¦={config['class_sep']}")
    
    print(f"ğŸ§  ç»Ÿä¸€ç½‘ç»œé…ç½®: {config['perception_hidden_layers']}")
    print(f"ğŸ¯ ç»Ÿä¸€è®­ç»ƒé…ç½®: {config['max_iter']} epochs, lr={config['learning_rate']}, patience={config['patience']}")
    print(f"ğŸ”§ ç»Ÿä¸€æ‰¹æ¬¡å¤§å°: {'å…¨é‡æ‰¹æ¬¡' if config['batch_size'] is None else config['batch_size']}")
    print(f"ğŸ›¡ï¸ ç»Ÿä¸€æ­£åˆ™åŒ–: alpha={config['alpha']}")
    print(f"æµ‹è¯•æ–¹æ³•: sklearn={config['test_sklearn']}, pytorch={config['test_pytorch']}, "
          f"deterministic={config['test_causal_deterministic']}, standard={config['test_causal_standard']}")
    print()

# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

def main():
    """ä¸»ç¨‹åº - è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ CausalEngine å¿«é€Ÿæµ‹è¯•è„šæœ¬ - é‡æ„ç‰ˆ")
    print("=" * 60)
    
    # è¿è¡Œå›å½’æµ‹è¯•
    regression_results = test_regression()
    
    # è¿è¡Œåˆ†ç±»æµ‹è¯•  
    classification_results = test_classification()
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ ä¿®æ”¹è„šæœ¬é¡¶éƒ¨çš„ CONFIG éƒ¨åˆ†æ¥è°ƒæ•´å®éªŒå‚æ•°")

def quick_regression_test():
    """å¿«é€Ÿå›å½’æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = REGRESSION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1000,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'verbose': True
    })
    return test_regression(quick_config)

def quick_classification_test():
    """å¿«é€Ÿåˆ†ç±»æµ‹è¯• - ç”¨äºè°ƒè¯•"""
    quick_config = CLASSIFICATION_CONFIG.copy()
    quick_config.update({
        'n_samples': 1500,
        'max_iter': 500,
        'test_pytorch': False,  # è·³è¿‡pytorchåŸºçº¿ä»¥èŠ‚çœæ—¶é—´
        'verbose': True
    })
    return test_classification(quick_config)

if __name__ == "__main__":
    # ä½ å¯ä»¥é€‰æ‹©è¿è¡Œä»¥ä¸‹ä»»ä¸€å‡½æ•°:
    main()                        # å®Œæ•´æµ‹è¯•
    # quick_regression_test()     # å¿«é€Ÿå›å½’æµ‹è¯•
    # quick_classification_test() # å¿«é€Ÿåˆ†ç±»æµ‹è¯•
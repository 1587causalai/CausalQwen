"""CausalQwen å¿«é€Ÿå¯¹è¯æµ‹è¯•è„šæœ¬"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from dataclasses import dataclass


@dataclass
class QuickConfig:
    """å¿«é€Ÿæµ‹è¯•é…ç½®"""
    num_vocab: int = 32000  # Qwen tokenizerå¤§å°
    hidden_dim: int = 512
    num_token_id: int = 32000  # <NUM> token (ç¬¬ä¸€ä¸ªä¿ç•™è¯æ±‡)
    eos_token_id: int = 2


class MockCausalQwen:
    """Mockç‰ˆæœ¬çš„CausalQwenï¼Œç”¨äºæ¼”ç¤º"""
    
    def __init__(self, config):
        self.config = config
        self.tokenizer = None
        self.qwen_model = None  # å­˜å‚¨åŸå§‹Qwenæ¨¡å‹ç”¨äºå‚è€ƒ
        
    def set_tokenizer(self, tokenizer):
        """è®¾ç½®tokenizer"""
        self.tokenizer = tokenizer
        
    def set_qwen_model(self, model):
        """è®¾ç½®Qwenæ¨¡å‹ï¼ˆç”¨äºçŸ¥è¯†è¿ç§»ï¼‰"""
        self.qwen_model = model
        
    def chat(self, messages, stream=False, **kwargs):
        """å…¼å®¹Qwençš„chatæ¥å£
        
        Args:
            messages: å¯¹è¯å†å²
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬æˆ–ç”Ÿæˆå™¨
        """
        # å¦‚æœæœ‰Qwenæ¨¡å‹ï¼Œä½¿ç”¨å®ƒæ¥ç”Ÿæˆï¼ˆä½†ä¸ç”¨chatæ–¹æ³•ï¼‰
        if self.qwen_model is not None and self.tokenizer is not None:
            # æ„å»ºprompt
            prompt = self._build_chat_prompt(messages)
            
            # ä½¿ç”¨generateæ–¹æ³•
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.qwen_model.generate(
                    inputs.input_ids,
                    max_new_tokens=kwargs.get('max_new_tokens', 100),
                    temperature=kwargs.get('temperature', 0.8),
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # è§£ç 
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_reply = self._extract_assistant_reply(full_response, prompt)
            
            if stream:
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                for i in range(0, len(assistant_reply), 3):
                    yield assistant_reply[i:i+3]
            else:
                return assistant_reply
        
        # å¦åˆ™ä½¿ç”¨ç®€å•çš„æ¨¡æ‹Ÿ
        prompt = self._build_chat_prompt(messages)
        response = self.generate_text(
            prompt,
            max_new_tokens=kwargs.get('max_new_tokens', 50),
            temperature=kwargs.get('temperature', 0.8),
            top_k=kwargs.get('top_k', 40),
            top_p=kwargs.get('top_p', 0.9),
            sampling_mode=kwargs.get('sampling_mode', 'causal')
        )
        
        assistant_reply = self._extract_assistant_reply(response, prompt)
        
        if stream:
            # æ­£ç¡®å¤„ç†æµå¼è¾“å‡º
            for i in range(0, len(assistant_reply), 3):
                yield assistant_reply[i:i+3]
        else:
            return assistant_reply
            
    def _build_chat_prompt(self, messages):
        """æ„å»ºå¯¹è¯promptï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        prompt = ""
        for msg in messages:
            if msg["role"] == "user":
                prompt += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                prompt += f"Assistant: {msg['content']}\n"
        prompt += "Assistant: "
        return prompt
        
    def _extract_assistant_reply(self, full_response, prompt):
        """ä»å®Œæ•´å›å¤ä¸­æå–åŠ©æ‰‹å›å¤"""
        # ç®€å•åœ°å»æ‰promptéƒ¨åˆ†
        if full_response.startswith(prompt):
            return full_response[len(prompt):].strip()
        return full_response.strip()
        
    def generate_text(self, prompt, **kwargs):
        """æ–‡æœ¬ç”Ÿæˆï¼ˆMockå®ç°ï¼‰"""
        if self.tokenizer is None:
            raise ValueError("Tokenizer not set")
            
        # æ¨¡æ‹Ÿç”Ÿæˆ
        responses = {
            "ä½ å¥½å‘€ï¼": "ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ": "ä½œä¸ºAIï¼Œæˆ‘æ— æ³•ç›´æ¥æ„ŸçŸ¥å¤©æ°”ï¼Œä½†æˆ‘å»ºè®®ä½ æŸ¥çœ‹å¤©æ°”é¢„æŠ¥æˆ–çœ‹çœ‹çª—å¤–ã€‚å¸Œæœ›ä»Šå¤©æ˜¯ä¸ªå¥½å¤©æ°”ï¼",
            "è¯·å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯": "ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯åˆ†ä¸æ¸…ä¸‡åœ£èŠ‚å’Œåœ£è¯èŠ‚ï¼Ÿå› ä¸º Oct 31 = Dec 25ï¼ï¼ˆå…«è¿›åˆ¶31ç­‰äºåè¿›åˆ¶25ï¼‰",
            "Hello, how are you?": "Hello! I'm doing well, thank you for asking. How can I assist you today?"
        }
        
        # ç®€å•åŒ¹é…è¿”å›
        for key, value in responses.items():
            if key in prompt:
                return prompt + value
                
        # é»˜è®¤å›å¤
        return prompt + "è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜ï¼è®©æˆ‘æƒ³æƒ³..."
        
    def generate(self, input_ids, **kwargs):
        """å…¼å®¹transformersçš„generateæ¥å£"""
        # è¿™é‡Œåº”è¯¥å®ç°çœŸæ­£çš„ç”Ÿæˆé€»è¾‘
        # ç°åœ¨åªæ˜¯è¿”å›mockç»“æœ
        batch_size = input_ids.shape[0]
        max_length = input_ids.shape[1] + kwargs.get('max_new_tokens', 50)
        
        # æ¨¡æ‹Ÿç”Ÿæˆä¸€äº›éšæœºtokens
        new_tokens = torch.randint(100, 30000, (batch_size, kwargs.get('max_new_tokens', 50)))
        return torch.cat([input_ids, new_tokens], dim=1)


def create_mock_model():
    """åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ¨¡å‹ç”¨äºæ¼”ç¤º"""
    config = QuickConfig()
    model = MockCausalQwen(config)
    
    # åŠ è½½Qwen tokenizer
    model_path = os.path.expanduser("~/models/Qwen2.5-0.5B")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model.set_tokenizer(tokenizer)
    
    # å¯é€‰ï¼šåŠ è½½Qwenæ¨¡å‹ï¼ˆç”¨äºçŸ¥è¯†è¿ç§»æ¼”ç¤ºï¼‰
    try:
        print("ğŸ“š åŠ è½½Qwenæ¨¡å‹ç”¨äºçŸ¥è¯†è¿ç§»...")
        qwen_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="cpu"
        )
        model.set_qwen_model(qwen_model)
        print("âœ… Qwenæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âš ï¸ Qwenæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ¨¡æ‹Ÿ: {e}")
    
    return model


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– CausalQwen å¿«é€Ÿå¯¹è¯æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ“Š åŠ è½½æ¨¡å‹...")
    model = create_mock_model()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("\nğŸ’¡ è¾“å…¥ 'quit' é€€å‡º")
    print("-" * 50)
    
    # å¯¹è¯å†å²
    messages = []
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            if not user_input:
                continue
                
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            messages.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆå›å¤
            print("ğŸ¤– CausalQwen: ", end="", flush=True)
            
            try:
                # ä½¿ç”¨chatæ¥å£
                response = model.chat(
                    messages,
                    stream=True,  # ä½¿ç”¨æµå¼è¾“å‡º
                    max_new_tokens=100,
                    temperature=0.8,
                    sampling_mode="causal"
                )
                
                # æ­£ç¡®å¤„ç†æµå¼è¾“å‡º
                full_response = ""
                if hasattr(response, '__iter__'):
                    for chunk in response:
                        print(chunk, end="", flush=True)
                        full_response += chunk
                else:
                    print(response)
                    full_response = response
                
                print()  # æ¢è¡Œ
                
                # æ·»åŠ å›å¤åˆ°å†å²
                messages.append({"role": "assistant", "content": full_response})
                
                # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦
                if len(messages) > 10:
                    messages = messages[-10:]
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()

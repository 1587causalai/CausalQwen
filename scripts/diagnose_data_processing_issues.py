#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†é—®é¢˜è¯Šæ–­è„šæœ¬

ä¸“é—¨ç”¨äºè¯Šæ–­ quick_test_causal_engine.py ä¸­çš„æ•°æ®å¤„ç†é—®é¢˜ã€‚
é€šè¿‡è¯¦ç»†çš„æ‰“å°è¯­å¥å±•ç¤ºé—®é¢˜çš„æ¥é¾™å»è„‰ï¼Œå¸®åŠ©å®šä½å’Œè§£å†³é—®é¢˜ã€‚

é—®é¢˜ç°è±¡ï¼š
- éªŒè¯é›†æŒ‡æ ‡å¼‚å¸¸é«˜ï¼ˆMAE 193-225ï¼ŒRÂ² ä¸ºè´Ÿå€¼ï¼‰
- æµ‹è¯•é›†æŒ‡æ ‡æ­£å¸¸ï¼ˆMAE 3-29ï¼ŒRÂ² æ¥è¿‘1ï¼‰
- å·¨å¤§çš„æ€§èƒ½å·®å¼‚æ— æ³•è§£é‡Š

ç›®æ ‡ï¼š
- é‡ç°é—®é¢˜ç°è±¡
- åˆ†ææ•°æ®å¤„ç†çš„æ¯ä¸ªæ­¥éª¤
- æ‰¾å‡ºé—®é¢˜æ ¹å› 
- æä¾›ä¿®å¤æ–¹æ¡ˆ
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.utils import causal_split

warnings.filterwarnings('ignore')

def print_separator(title, char="=", width=80):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}")

def print_subsection(title, char="-", width=60):
    """æ‰“å°å­èŠ‚æ ‡é¢˜"""
    print(f"\n{char * width}")
    print(f"ğŸ” {title}")
    print(f"{char * width}")

def analyze_data_statistics(X, y, data_name):
    """è¯¦ç»†åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š {data_name} æ•°æ®ç»Ÿè®¡åˆ†æï¼š")
    print(f"   æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # X ç»Ÿè®¡ä¿¡æ¯
    print(f"\n   X (ç‰¹å¾) ç»Ÿè®¡:")
    print(f"   - å„ç‰¹å¾å‡å€¼: {X.mean(axis=0)}")
    print(f"   - å„ç‰¹å¾æ ‡å‡†å·®: {X.std(axis=0)}")
    print(f"   - å„ç‰¹å¾æœ€å°å€¼: {X.min(axis=0)}")
    print(f"   - å„ç‰¹å¾æœ€å¤§å€¼: {X.max(axis=0)}")
    print(f"   - æ•´ä½“æ•°æ®èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
    
    # y ç»Ÿè®¡ä¿¡æ¯
    print(f"\n   y (ç›®æ ‡) ç»Ÿè®¡:")
    print(f"   - å‡å€¼: {y.mean():.4f}")
    print(f"   - æ ‡å‡†å·®: {y.std():.4f}")
    print(f"   - æœ€å°å€¼: {y.min():.4f}")
    print(f"   - æœ€å¤§å€¼: {y.max():.4f}")
    print(f"   - èŒƒå›´: {y.max() - y.min():.4f}")
    
    # æ•°æ®å°ºåº¦è¯„ä¼°
    print(f"\n   ğŸ” æ•°æ®å°ºåº¦è¯„ä¼°:")
    if X.std().max() > 10 or X.std().min() < 0.1:
        print(f"   âŒ ç‰¹å¾å°ºåº¦ä¸ä¸€è‡´ï¼æœ€å¤§æ ‡å‡†å·®: {X.std().max():.2f}, æœ€å°æ ‡å‡†å·®: {X.std().min():.2f}")
    else:
        print(f"   âœ… ç‰¹å¾å°ºåº¦ç›¸å¯¹ä¸€è‡´")
    
    if abs(y).max() > 1000:
        print(f"   âŒ ç›®æ ‡å˜é‡å°ºåº¦è¿‡å¤§ï¼æœ€å¤§ç»å¯¹å€¼: {abs(y).max():.2f}")
    else:
        print(f"   âœ… ç›®æ ‡å˜é‡å°ºåº¦åˆç†")

def reproduce_quick_test_problem():
    """é‡ç° quick_test_causal_engine.py ä¸­çš„é—®é¢˜"""
    print_separator("ğŸš¨ ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡ç° quick_test_causal_engine.py çš„é—®é¢˜ç°è±¡")
    
    print("\nğŸ¯ ä½¿ç”¨ä¸ quick_test_causal_engine.py ç›¸åŒçš„é…ç½®:")
    config = {
        'n_samples': 4000,
        'n_features': 12,
        'noise': 1.0,
        'random_state': 42,
        'test_size': 0.1,
        'anomaly_ratio': 0.2,
        'validation_fraction': 0.2,
        'max_iter': 2000,
        'learning_rate': 0.01,
        'patience': 50,
        'alpha': 0.0001
    }
    
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŸå§‹æ•°æ®
    print_subsection("ç¬¬1æ­¥ï¼šç”ŸæˆåŸå§‹æ•°æ®")
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    print(f"âœ… make_regression å®Œæˆ")
    analyze_data_statistics(X, y, "åŸå§‹ç”Ÿæˆ")
    
    # ç¬¬äºŒæ­¥ï¼šcausal_split åˆ†å‰²
    print_subsection("ç¬¬2æ­¥ï¼šcausal_split æ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥")
    print(f"ğŸ“ æ‰§è¡Œ causal_split:")
    print(f"   test_size={config['test_size']}")
    print(f"   anomaly_ratio={config['anomaly_ratio']}")
    print(f"   anomaly_type='regression'")
    
    X_train, X_test, y_train, y_test = causal_split(
        X, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        anomaly_ratio=config['anomaly_ratio'], 
        anomaly_type='regression'
    )
    
    print(f"\nâœ… causal_split å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    analyze_data_statistics(X_train, y_train, "è®­ç»ƒé›†ï¼ˆå«å¼‚å¸¸ï¼‰")
    analyze_data_statistics(X_test, y_test, "æµ‹è¯•é›†ï¼ˆçº¯å‡€ï¼‰")
    
    # ç¬¬ä¸‰æ­¥ï¼šæœªæ ‡å‡†åŒ–ç›´æ¥è®­ç»ƒï¼ˆé‡ç°é—®é¢˜ï¼‰
    print_subsection("ç¬¬3æ­¥ï¼šæœªæ ‡å‡†åŒ–è®­ç»ƒï¼ˆé‡ç°é—®é¢˜ï¼‰")
    print("âŒ å…³é”®é—®é¢˜ï¼šquick_test_causal_engine.py ç¬¬142è¡Œæ³¨é‡Š'æ•°æ®ä¸å†è¿›è¡Œæ ‡å‡†åŒ–'")
    print("âŒ æ‰€æœ‰æ¨¡å‹æ¥æ”¶çš„æ˜¯æœªæ ‡å‡†åŒ–çš„åŸå§‹æ•°æ®ï¼")
    
    # è®­ç»ƒ sklearn æ¨¡å‹
    print(f"\nğŸ”§ è®­ç»ƒ sklearn MLPRegressor (æœªæ ‡å‡†åŒ–æ•°æ®)...")
    sklearn_model = MLPRegressor(
        hidden_layer_sizes=(128, 64),
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        random_state=config['random_state'],
        alpha=config['alpha']
    )
    
    sklearn_model.fit(X_train, y_train)
    print(f"   è®­ç»ƒå®Œæˆ: {sklearn_model.n_iter_} epochs")
    
    # ç¬¬å››æ­¥ï¼šé”™è¯¯çš„éªŒè¯é›†è¯„ä¼°ï¼ˆé‡ç°é—®é¢˜ï¼‰
    print_subsection("ç¬¬4æ­¥ï¼šé”™è¯¯çš„éªŒè¯é›†è¯„ä¼°æ–¹å¼")
    print("âŒ å…³é”®é—®é¢˜ï¼šquick_test_causal_engine.py ç¬¬391-395è¡Œé‡æ–°åˆ†å‰²éªŒè¯é›†")
    print("âŒ è¯„ä¼°æ—¶çš„éªŒè¯é›† â‰  è®­ç»ƒæ—¶sklearnå†…éƒ¨ä½¿ç”¨çš„éªŒè¯é›†ï¼")
    
    # é‡æ–°åˆ†å‰²éªŒè¯é›†ï¼ˆé”™è¯¯åšæ³•ï¼‰
    X_train_eval, X_val_eval, y_train_eval, y_val_eval = train_test_split(
        X_train, y_train,
        test_size=config['validation_fraction'],
        random_state=config['random_state']
    )
    
    print(f"\nğŸ“Š é‡æ–°åˆ†å‰²çš„'éªŒè¯é›†'ç»Ÿè®¡:")
    analyze_data_statistics(X_val_eval, y_val_eval, "å¤–éƒ¨é‡æ–°åˆ†å‰²çš„éªŒè¯é›†")
    
    # é¢„æµ‹å’Œè¯„ä¼°
    test_pred = sklearn_model.predict(X_test)
    val_pred = sklearn_model.predict(X_val_eval)
    
    # è®¡ç®—æŒ‡æ ‡
    test_mae = mean_absolute_error(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    test_r2 = r2_score(y_test, test_pred)
    
    val_mae = mean_absolute_error(y_val_eval, val_pred)
    val_rmse = np.sqrt(mean_squared_error(y_val_eval, val_pred))
    val_r2 = r2_score(y_val_eval, val_pred)
    
    print_subsection("ç¬¬5æ­¥ï¼šé—®é¢˜ç°è±¡é‡ç°")
    print("ğŸš¨ å¼‚å¸¸ç»“æœé‡ç°:")
    print(f"   éªŒè¯é›† MAE: {val_mae:.4f} â† å¼‚å¸¸é«˜ï¼")
    print(f"   éªŒè¯é›† RMSE: {val_rmse:.4f}")
    print(f"   éªŒè¯é›† RÂ²: {val_r2:.4f} â† å¯èƒ½ä¸ºè´Ÿå€¼ï¼")
    print(f"\n   æµ‹è¯•é›† MAE: {test_mae:.4f} â† æ­£å¸¸èŒƒå›´")
    print(f"   æµ‹è¯•é›† RMSE: {test_rmse:.4f}")
    print(f"   æµ‹è¯•é›† RÂ²: {test_r2:.4f} â† æ¥è¿‘1.0")
    
    print(f"\nğŸ’¡ é—®é¢˜ç°è±¡æ€»ç»“:")
    if val_mae > test_mae * 5:
        print(f"   âŒ éªŒè¯é›† MAE æ¯”æµ‹è¯•é›†é«˜ {val_mae/test_mae:.1f} å€ï¼")
    if val_r2 < 0:
        print(f"   âŒ éªŒè¯é›† RÂ² ä¸ºè´Ÿå€¼ï¼Œæ¨¡å‹æ¯”ç®€å•å‡å€¼è¿˜å·®ï¼")
    if test_r2 > 0.9:
        print(f"   âŒ æµ‹è¯•é›† RÂ² å¾ˆé«˜ï¼Œè¯´æ˜æ¨¡å‹å®é™…å¾ˆå¥½ï¼")
    
    return {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'X_val_eval': X_val_eval, 'y_val_eval': y_val_eval,
        'sklearn_model': sklearn_model,
        'test_pred': test_pred, 'val_pred': val_pred,
        'config': config
    }

def analyze_sklearn_internal_validation(data):
    """åˆ†æ sklearn å†…éƒ¨éªŒè¯é›†å¤„ç†"""
    print_separator("ğŸ§  ç¬¬äºŒéƒ¨åˆ†ï¼šsklearn å†…éƒ¨éªŒè¯é›† vs å¤–éƒ¨éªŒè¯é›†å¯¹æ¯”")
    
    X_train, y_train = data['X_train'], data['y_train']
    config = data['config']
    
    print("ğŸ” sklearn MLPRegressor å†…éƒ¨éªŒè¯é›†å¤„ç†:")
    print(f"   è®¾ç½® early_stopping=True")
    print(f"   è®¾ç½® validation_fraction={config['validation_fraction']}")
    print(f"   sklearn ä¼šå†…éƒ¨åˆ†å‰² {config['validation_fraction']:.1%} ä½œä¸ºéªŒè¯é›†")
    
    # è®¡ç®— sklearn å†…éƒ¨çš„åˆ†å‰²
    train_size = len(X_train)
    internal_val_size = int(train_size * config['validation_fraction'])
    internal_train_size = train_size - internal_val_size
    
    print(f"\nğŸ“Š sklearn å†…éƒ¨åˆ†å‰²è®¡ç®—:")
    print(f"   åŸå§‹è®­ç»ƒé›†å¤§å°: {train_size}")
    print(f"   sklearn å†…éƒ¨è®­ç»ƒé›†å¤§å°: {internal_train_size}")
    print(f"   sklearn å†…éƒ¨éªŒè¯é›†å¤§å°: {internal_val_size}")
    
    print(f"\nğŸ“Š å¤–éƒ¨é‡æ–°åˆ†å‰²ï¼ˆquick_test_causal_engine.py çš„åšæ³•ï¼‰:")
    print(f"   å¤–éƒ¨éªŒè¯é›†å¤§å°: {len(data['X_val_eval'])}")
    print(f"   å¤–éƒ¨è®­ç»ƒé›†å¤§å°: {len(data['X_train']) - len(data['X_val_eval'])}")
    
    # å°è¯•æ¨¡æ‹Ÿ sklearn å†…éƒ¨åˆ†å‰²ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯è¿‘ä¼¼ï¼‰
    print(f"\nğŸ” éªŒè¯é›†æ•°æ®å·®å¼‚åˆ†æ:")
    print(f"   sklearn å†…éƒ¨ä½¿ç”¨çš„éªŒè¯é›†: sklearn å†…éƒ¨éšæœºåˆ†å‰²")
    print(f"   quick_test_causal_engine.py ä½¿ç”¨çš„éªŒè¯é›†: å¤–éƒ¨ train_test_split åˆ†å‰²")
    print(f"   random_state ç›¸åŒï¼Œä½†åˆ†å‰²æ—¶æœºä¸åŒï¼")
    
    # åˆ†æä¸¤ä¸ªéªŒè¯é›†çš„ç»Ÿè®¡å·®å¼‚
    y_val_external = data['y_val_eval']
    print(f"\nğŸ“Š å¤–éƒ¨éªŒè¯é›† y ç»Ÿè®¡:")
    print(f"   å‡å€¼: {y_val_external.mean():.4f}")
    print(f"   æ ‡å‡†å·®: {y_val_external.std():.4f}")
    print(f"   èŒƒå›´: [{y_val_external.min():.4f}, {y_val_external.max():.4f}]")
    
    print(f"\nâŒ å…³é”®é—®é¢˜è¯†åˆ«:")
    print(f"   1. sklearn åœ¨è®­ç»ƒæ—¶ä½¿ç”¨å†…éƒ¨åˆ†å‰²çš„éªŒè¯é›†")
    print(f"   2. è¯„ä¼°æ—¶ä½¿ç”¨å¤–éƒ¨é‡æ–°åˆ†å‰²çš„éªŒè¯é›†")
    print(f"   3. ä¸¤ä¸ªéªŒè¯é›†å®Œå…¨ä¸åŒï¼")
    print(f"   4. å¯¼è‡´éªŒè¯é›†æ€§èƒ½æŒ‡æ ‡ä¸å¯ä¿¡")

def analyze_standardization_impact(data):
    """åˆ†ææ ‡å‡†åŒ–å¯¹ç»“æœçš„å½±å“"""
    print_separator("ğŸ”¬ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®æ ‡å‡†åŒ–å½±å“åˆ†æ")
    
    X_train, X_test = data['X_train'], data['X_test']
    y_train, y_test = data['y_train'], data['y_test']
    X_val_eval, y_val_eval = data['X_val_eval'], data['y_val_eval']
    config = data['config']
    
    print("ğŸ” æœªæ ‡å‡†åŒ–æ•°æ®çš„é—®é¢˜åˆ†æ:")
    print(f"   å½“å‰ quick_test_causal_engine.py ä½¿ç”¨æœªæ ‡å‡†åŒ–æ•°æ®")
    
    # åˆ†ææ•°æ®å°ºåº¦å¯¹ç¥ç»ç½‘ç»œçš„å½±å“
    x_range = X_train.max() - X_train.min()
    x_std_max = X_train.std(axis=0).max()
    x_std_min = X_train.std(axis=0).min()
    
    print(f"\nğŸ“Š æœªæ ‡å‡†åŒ–æ•°æ®å°ºåº¦:")
    print(f"   X æ•°æ®èŒƒå›´: {x_range:.2f}")
    print(f"   X æœ€å¤§æ ‡å‡†å·®: {x_std_max:.2f}")
    print(f"   X æœ€å°æ ‡å‡†å·®: {x_std_min:.2f}")
    print(f"   æ ‡å‡†å·®æ¯”ä¾‹: {x_std_max/x_std_min:.2f}")
    
    if x_range > 100:
        print(f"   âŒ æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸")
    if x_std_max/x_std_min > 10:
        print(f"   âŒ ç‰¹å¾å°ºåº¦å·®å¼‚å·¨å¤§ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§")
    
    # æ‰§è¡Œæ­£ç¡®çš„æ ‡å‡†åŒ–
    print_subsection("æ­£ç¡®çš„æ ‡å‡†åŒ–å¤„ç†")
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    X_val_eval_scaled = scaler_X.transform(X_val_eval)
    
    print(f"âœ… æ‰§è¡Œ StandardScaler:")
    print(f"   åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ: scaler_X.fit_transform(X_train)")
    print(f"   å˜æ¢æµ‹è¯•é›†: scaler_X.transform(X_test)")
    print(f"   å˜æ¢éªŒè¯é›†: scaler_X.transform(X_val_eval)")
    
    print(f"\nğŸ“Š æ ‡å‡†åŒ–åæ•°æ®ç»Ÿè®¡:")
    print(f"   X_train_scaled å‡å€¼: {X_train_scaled.mean(axis=0)}")
    print(f"   X_train_scaled æ ‡å‡†å·®: {X_train_scaled.std(axis=0)}")
    print(f"   X_train_scaled èŒƒå›´: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
    
    # ç”¨æ ‡å‡†åŒ–æ•°æ®é‡æ–°è®­ç»ƒ
    print_subsection("ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®é‡æ–°è®­ç»ƒ")
    sklearn_model_scaled = MLPRegressor(
        hidden_layer_sizes=(128, 64),
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        random_state=config['random_state'],
        alpha=config['alpha']
    )
    
    sklearn_model_scaled.fit(X_train_scaled, y_train)
    print(f"   è®­ç»ƒå®Œæˆ: {sklearn_model_scaled.n_iter_} epochs")
    
    # é¢„æµ‹å’Œè¯„ä¼°
    test_pred_scaled = sklearn_model_scaled.predict(X_test_scaled)
    val_pred_scaled = sklearn_model_scaled.predict(X_val_eval_scaled)
    
    # è®¡ç®—æŒ‡æ ‡
    test_mae_scaled = mean_absolute_error(y_test, test_pred_scaled)
    test_r2_scaled = r2_score(y_test, test_pred_scaled)
    val_mae_scaled = mean_absolute_error(y_val_eval, val_pred_scaled)
    val_r2_scaled = r2_score(y_val_eval, val_pred_scaled)
    
    print_subsection("æ ‡å‡†åŒ–å‰åå¯¹æ¯”")
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"   æœªæ ‡å‡†åŒ– - æµ‹è¯•é›† MAE: {mean_absolute_error(y_test, data['test_pred']):.4f}")
    print(f"   æ ‡å‡†åŒ–å - æµ‹è¯•é›† MAE: {test_mae_scaled:.4f}")
    print(f"   æœªæ ‡å‡†åŒ– - æµ‹è¯•é›† RÂ²: {r2_score(y_test, data['test_pred']):.4f}")
    print(f"   æ ‡å‡†åŒ–å - æµ‹è¯•é›† RÂ²: {test_r2_scaled:.4f}")
    
    print(f"\n   æœªæ ‡å‡†åŒ– - éªŒè¯é›† MAE: {mean_absolute_error(y_val_eval, data['val_pred']):.4f}")
    print(f"   æ ‡å‡†åŒ–å - éªŒè¯é›† MAE: {val_mae_scaled:.4f}")
    print(f"   æœªæ ‡å‡†åŒ– - éªŒè¯é›† RÂ²: {r2_score(y_val_eval, data['val_pred']):.4f}")
    print(f"   æ ‡å‡†åŒ–å - éªŒè¯é›† RÂ²: {val_r2_scaled:.4f}")
    
    improvement = abs(val_mae_scaled - test_mae_scaled) / abs(mean_absolute_error(y_val_eval, data['val_pred']) - mean_absolute_error(y_test, data['test_pred']))
    if improvement < 0.5:
        print(f"   âœ… æ ‡å‡†åŒ–åéªŒè¯é›†å’Œæµ‹è¯•é›†æ€§èƒ½æ›´ä¸€è‡´")
    else:
        print(f"   âš ï¸ ä»å­˜åœ¨éªŒè¯é›†åˆ†å‰²ä¸ä¸€è‡´é—®é¢˜")

def analyze_prediction_details(data):
    """è¯¦ç»†åˆ†ææ¨¡å‹é¢„æµ‹"""
    print_separator("ğŸ¯ ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹é¢„æµ‹è¯¦ç»†åˆ†æ")
    
    test_pred = data['test_pred']
    val_pred = data['val_pred']
    y_test = data['y_test']
    y_val_eval = data['y_val_eval']
    
    print("ğŸ” é¢„æµ‹å€¼è¯¦ç»†åˆ†æ:")
    
    print(f"\nğŸ“Š æµ‹è¯•é›†é¢„æµ‹åˆ†æ:")
    print(f"   çœŸå®å€¼èŒƒå›´: [{y_test.min():.2f}, {y_test.max():.2f}]")
    print(f"   é¢„æµ‹å€¼èŒƒå›´: [{test_pred.min():.2f}, {test_pred.max():.2f}]")
    print(f"   é¢„æµ‹å€¼å‡å€¼: {test_pred.mean():.2f}")
    print(f"   çœŸå®å€¼å‡å€¼: {y_test.mean():.2f}")
    print(f"   é¢„æµ‹åå·®: {abs(test_pred.mean() - y_test.mean()):.2f}")
    
    print(f"\nğŸ“Š éªŒè¯é›†é¢„æµ‹åˆ†æ:")
    print(f"   çœŸå®å€¼èŒƒå›´: [{y_val_eval.min():.2f}, {y_val_eval.max():.2f}]")
    print(f"   é¢„æµ‹å€¼èŒƒå›´: [{val_pred.min():.2f}, {val_pred.max():.2f}]")
    print(f"   é¢„æµ‹å€¼å‡å€¼: {val_pred.mean():.2f}")
    print(f"   çœŸå®å€¼å‡å€¼: {y_val_eval.mean():.2f}")
    print(f"   é¢„æµ‹åå·®: {abs(val_pred.mean() - y_val_eval.mean()):.2f}")
    
    # åˆ†æé¢„æµ‹è¯¯å·®åˆ†å¸ƒ
    test_errors = np.abs(test_pred - y_test)
    val_errors = np.abs(val_pred - y_val_eval)
    
    print(f"\nğŸ“Š é¢„æµ‹è¯¯å·®åˆ†æ:")
    print(f"   æµ‹è¯•é›†è¯¯å·®å‡å€¼: {test_errors.mean():.2f}")
    print(f"   æµ‹è¯•é›†è¯¯å·®æ ‡å‡†å·®: {test_errors.std():.2f}")
    print(f"   æµ‹è¯•é›†è¯¯å·®ä¸­ä½æ•°: {np.median(test_errors):.2f}")
    
    print(f"\n   éªŒè¯é›†è¯¯å·®å‡å€¼: {val_errors.mean():.2f}")
    print(f"   éªŒè¯é›†è¯¯å·®æ ‡å‡†å·®: {val_errors.std():.2f}")
    print(f"   éªŒè¯é›†è¯¯å·®ä¸­ä½æ•°: {np.median(val_errors):.2f}")
    
    # å¼‚å¸¸å€¼åˆ†æ
    test_outliers = np.sum(test_errors > test_errors.mean() + 2*test_errors.std())
    val_outliers = np.sum(val_errors > val_errors.mean() + 2*val_errors.std())
    
    print(f"\nğŸ“Š å¼‚å¸¸é¢„æµ‹åˆ†æ:")
    print(f"   æµ‹è¯•é›†å¼‚å¸¸é¢„æµ‹æ•°é‡: {test_outliers} / {len(test_errors)}")
    print(f"   éªŒè¯é›†å¼‚å¸¸é¢„æµ‹æ•°é‡: {val_outliers} / {len(val_errors)}")
    
    if val_errors.mean() > test_errors.mean() * 3:
        print(f"\nâŒ å…³é”®å‘ç°:")
        print(f"   éªŒè¯é›†é¢„æµ‹è¯¯å·®æ¯”æµ‹è¯•é›†é«˜ {val_errors.mean()/test_errors.mean():.1f} å€")
        print(f"   è¿™ä¸æ˜¯æ­£å¸¸çš„æ¨¡å‹è¡Œä¸ºï¼")
        print(f"   é€šå¸¸éªŒè¯é›†å’Œæµ‹è¯•é›†æ€§èƒ½åº”è¯¥æ¥è¿‘")

def summarize_root_causes():
    """æ€»ç»“é—®é¢˜æ ¹å› """
    print_separator("ğŸš¨ ç¬¬äº”éƒ¨åˆ†ï¼šé—®é¢˜æ ¹å› æ€»ç»“å’Œä¿®å¤æ–¹æ¡ˆ")
    
    print("ğŸ” é—®é¢˜æ ¹å› åˆ†æ:")
    
    print(f"\nâŒ é—®é¢˜1: æ•°æ®æœªæ ‡å‡†åŒ–")
    print(f"   ğŸ“ ä½ç½®: quick_test_causal_engine.py ç¬¬142è¡Œ")
    print(f"   ğŸ“ ä»£ç : # æ•°æ®ä¸å†è¿›è¡Œæ ‡å‡†åŒ–")
    print(f"   ğŸ’¥ å½±å“: ç¥ç»ç½‘ç»œæ¥æ”¶å¤§å°ºåº¦æ•°æ®ï¼Œè®­ç»ƒä¸ç¨³å®š")
    print(f"   ğŸ”¢ è¯æ®: X æ•°æ®èŒƒå›´è¿‡å¤§ï¼Œç‰¹å¾å°ºåº¦ä¸ä¸€è‡´")
    
    print(f"\nâŒ é—®é¢˜2: éªŒè¯é›†åˆ†å‰²ä¸ä¸€è‡´")
    print(f"   ğŸ“ ä½ç½®: quick_test_causal_engine.py ç¬¬391-395è¡Œ")
    print(f"   ğŸ“ ä»£ç : train_test_split(data['X_train'], data['y_train'], ...)")
    print(f"   ğŸ’¥ å½±å“: è¯„ä¼°ç”¨çš„éªŒè¯é›† â‰  è®­ç»ƒæ—¶çš„éªŒè¯é›†")
    print(f"   ğŸ”¢ è¯æ®: éªŒè¯é›†æ€§èƒ½å¼‚å¸¸ï¼Œä¸æµ‹è¯•é›†å·®è·å·¨å¤§")
    
    print(f"\nâŒ é—®é¢˜3: æ•°æ®å°ºåº¦ä¸åŒ¹é…")
    print(f"   ğŸ“ åŸå› : make_regression ç”Ÿæˆå¤§å°ºåº¦æ•°æ®")
    print(f"   ğŸ’¥ å½±å“: ä¸åŒæ•°æ®é›†ä¸Šçš„æ¨¡å‹è¡¨ç°å·®å¼‚å·¨å¤§")
    print(f"   ğŸ”¢ è¯æ®: éªŒè¯é›† MAE 193-225ï¼Œæµ‹è¯•é›† MAE 3-29")
    
    print_subsection("ä¿®å¤æ–¹æ¡ˆ")
    print("âœ… ä¿®å¤æ­¥éª¤:")
    print("   1. å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–")
    print("   2. ä¿æŒç›®æ ‡å˜é‡åŸå§‹å°ºåº¦")
    print("   3. ä½¿ç”¨ä¸€è‡´çš„éªŒè¯é›†åˆ†å‰²ç­–ç•¥")
    print("   4. é¿å…åœ¨è¯„ä¼°æ—¶é‡æ–°åˆ†å‰²éªŒè¯é›†")
    
    print(f"\nğŸ’¡ æ­£ç¡®çš„æ•°æ®å¤„ç†æµç¨‹:")
    print(f"   1. causal_split() åˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†")
    print(f"   2. StandardScaler æ ‡å‡†åŒ–ç‰¹å¾")
    print(f"   3. æ¨¡å‹å†…éƒ¨è‡ªåŠ¨å¤„ç†éªŒè¯é›†åˆ†å‰²")
    print(f"   4. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°")
    
    print(f"\nğŸ¯ é¢„æœŸä¿®å¤æ•ˆæœ:")
    print(f"   - éªŒè¯é›†å’Œæµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡åº”è¯¥æ¥è¿‘")
    print(f"   - RÂ² å€¼åº”è¯¥ä¸ºæ­£æ•°ä¸”åˆç†")
    print(f"   - MAE å€¼åº”è¯¥åœ¨ç›¸ä¼¼èŒƒå›´å†…")
    print(f"   - æ¨¡å‹è®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«")

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ”¬ æ•°æ®å¤„ç†é—®é¢˜è¯Šæ–­è„šæœ¬")
    print("=" * 80)
    print("ç›®æ ‡: è¯Šæ–­ quick_test_causal_engine.py ä¸­éªŒè¯é›†æŒ‡æ ‡å¼‚å¸¸çš„é—®é¢˜")
    print("ç°è±¡: éªŒè¯é›† MAE 193-225 (å¼‚å¸¸é«˜), æµ‹è¯•é›† MAE 3-29 (æ­£å¸¸)")
    print("=" * 80)
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡ç°é—®é¢˜
    data = reproduce_quick_test_problem()
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ†æéªŒè¯é›†é—®é¢˜
    analyze_sklearn_internal_validation(data)
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šåˆ†ææ ‡å‡†åŒ–å½±å“
    analyze_standardization_impact(data)
    
    # ç¬¬å››éƒ¨åˆ†ï¼šè¯¦ç»†é¢„æµ‹åˆ†æ
    analyze_prediction_details(data)
    
    # ç¬¬äº”éƒ¨åˆ†ï¼šæ€»ç»“æ ¹å› 
    summarize_root_causes()
    
    print_separator("ğŸ‰ è¯Šæ–­å®Œæˆ", char="=", width=80)
    print("ğŸ’¡ å»ºè®®: ä¿®å¤ quick_test_causal_engine.py ä¸­çš„æ•°æ®å¤„ç†é—®é¢˜")
    print("ğŸ“ é‡ç‚¹: 1) æ·»åŠ æ•°æ®æ ‡å‡†åŒ– 2) ä¿®å¤éªŒè¯é›†åˆ†å‰²é€»è¾‘")

if __name__ == "__main__":
    main()
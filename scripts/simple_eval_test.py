#!/usr/bin/env python3
"""
ç®€å•è¯„ä¼°æµ‹è¯• - éªŒè¯ç®€åŒ–åçš„æŒ‡æ ‡è¾“å‡º
"""

import sys
import torch
import numpy as np
from pathlib import Path

# Add src to Python path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from data.synthetic import SyntheticMathDataset
from models.causal_lm import CausalLanguageModel
from models.config import ModelConfig
from evaluation.evaluator import Evaluator
from data.tokenizer import MockTokenizer


def test_simplified_metrics():
    """æµ‹è¯•ç®€åŒ–åçš„æŒ‡æ ‡è¾“å‡º"""
    print("=== æµ‹è¯•ç®€åŒ–åçš„æŒ‡æ ‡è¾“å‡º ===")
    
    # ä½¿ç”¨å°å‹é…ç½®
    tokenizer = MockTokenizer(vocab_size=100)
    config = ModelConfig(
        causal_dim=16,
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        context_length=8
    )
    
    # åˆ›å»ºæ¨¡å‹å’Œè¯„ä¼°å™¨
    model = CausalLanguageModel(config)
    evaluator = Evaluator(model, tokenizer, "cpu")
    
    # ç”Ÿæˆå°æ•°æ®é›†
    dataset = SyntheticMathDataset(num_samples=20, context_length=8)
    
    # è¯„ä¼°
    print("è¿è¡Œè¯„ä¼°...")
    results = evaluator.evaluate(dataset, batch_size=8)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“Š ç®€åŒ–åçš„æŒ‡æ ‡:")
    print(f"åˆ†ç±»æŒ‡æ ‡:")
    print(f"  - cls_accuracy: {results['cls_accuracy']:.4f}")
    print(f"  - cls_f1: {results['cls_f1']:.4f}")
    
    print(f"å›å½’æŒ‡æ ‡:")
    print(f"  - reg_mse: {results['reg_mse']:.4f}")
    print(f"  - reg_mae: {results['reg_mae']:.4f}")
    
    print(f"æ ¡å‡†æŒ‡æ ‡:")
    print(f"  - calib_ece: {results['calib_ece']:.4f} (åˆ†ç±»æ ¡å‡†)")
    print(f"  - reg_picp: {results['reg_picp']:.4f} (å›å½’æ ¡å‡†)")
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¤šä½™çš„æŒ‡æ ‡
    unexpected_metrics = []
    for key in results.keys():
        if key.startswith('calib_') and key not in ['calib_ece']:
            unexpected_metrics.append(key)
    
    if unexpected_metrics:
        print(f"\nâš ï¸  å‘ç°å¤šä½™çš„æ ¡å‡†æŒ‡æ ‡: {unexpected_metrics}")
    else:
        print(f"\nâœ… æ ¡å‡†æŒ‡æ ‡å·²æ­£ç¡®ç®€åŒ–ï¼")
    
    print(f"\næ€»è®¡æŒ‡æ ‡æ•°é‡: {len(results)}")
    print(f"æ‰€æœ‰æŒ‡æ ‡: {list(results.keys())}")
    
    return results


if __name__ == "__main__":
    test_simplified_metrics() 
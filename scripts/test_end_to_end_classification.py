#!/usr/bin/env python
"""
ç«¯åˆ°ç«¯åˆ†ç±»æµç¨‹éªŒè¯è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨å®Œæ•´åœ°å±•ç¤ºä»ŽåŽŸå§‹æ–‡æœ¬è¾“å…¥åˆ°æœ€ç»ˆåˆ†ç±»æŸå¤±è®¡ç®—çš„å…¨è¿‡ç¨‹ã€‚
å®ƒä½¿ç”¨ `test_numerical_aware_embedding.py` ä¸­çš„4ä¸ªçœŸå®žè¯­æ–™æ ·æœ¬ï¼Œ
é€šè¿‡å®Œæ•´çš„ CausalQwen æ¨¡åž‹ï¼ŒéªŒè¯æ ¸å¿ƒçš„åˆ†ç±»ä»»åŠ¡æµç¨‹ã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"âž¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ðŸš€ CausalQwen - ç«¯åˆ°ç«¯åˆ†ç±»æµç¨‹éªŒè¯")
    
    # --- æ­¥éª¤ 1: åˆå§‹åŒ–çœŸå®žæ¨¡åž‹å’Œåˆ†è¯å™¨ ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–çœŸå®žæ¨¡åž‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')

    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True,
        ovr_threshold=100.0 # ä¸Žä¹‹å‰çš„æµ‹è¯•ä¿æŒä¸€è‡´
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("   âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # --- æ­¥éª¤ 2: å‡†å¤‡è¾“å…¥æ•°æ® ---
    print_step("æ­¥éª¤ 2", "å‡†å¤‡è¾“å…¥æ•°æ® (æ¥è‡ª test_numerical_aware_embedding.py)")
    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
        "The dimensions are 10 by 20 by 30 cm.",
        "What is 2 plus 3? 5.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    print(f"   - ä½¿ç”¨ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")
    print(f"   - è¾“å…¥å½¢çŠ¶ (input_ids): {input_ids.shape}")

    # --- æ­¥éª¤ 3: å‡†å¤‡ç”¨äºŽæŸå¤±è®¡ç®—çš„æ ‡ç­¾ (Labels) ---
    print_step("æ­¥éª¤ 3", "å‡†å¤‡ç”¨äºŽæŸå¤±è®¡ç®—çš„æ ‡ç­¾ (Labels)")
    # è¯­è¨€æ¨¡åž‹çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œæ‰€ä»¥æ ‡ç­¾æ˜¯è¾“å…¥å‘å·¦ç§»åŠ¨ä¸€ä½ã€‚
    # æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¸Žè¾“å…¥å½¢çŠ¶ç›¸åŒï¼Œä½†å¡«å……ä¸º-100çš„æ ‡ç­¾å¼ é‡ã€‚
    # ç„¶åŽï¼Œæˆ‘ä»¬å°† input_ids çš„ [1:] éƒ¨åˆ†å¤åˆ¶åˆ° labels çš„ [:-1] éƒ¨åˆ†ã€‚
    # è¿™æ ·ï¼Œæ¨¡åž‹åœ¨ä½ç½® i çš„è¾“å‡ºï¼Œå°†ä¸Žä½ç½® i+1 çš„è¾“å…¥è¿›è¡Œæ¯”è¾ƒã€‚
    # æœ€åŽä¸€ä¸ªè¯å…ƒçš„æ ‡ç­¾å°†ä¿æŒ-100ï¼ŒæŸå¤±å‡½æ•°ä¼šè‡ªåŠ¨å¿½ç•¥å®ƒã€‚
    labels = torch.full_like(input_ids, -100)
    labels[:, :-1] = input_ids[:, 1:]
    
    print("   - æ ‡ç­¾å·²ç”Ÿæˆ (input_ids å·¦ç§»ä¸€ä½ï¼Œæœ€åŽä¸€ä¸ªè¯å…ƒå’Œ padding ä½ç½®å°†è¢«å¿½ç•¥)")
    
    print("\n   --- æŸ¥çœ‹æ ·æœ¬ 2 çš„è¾“å…¥å’Œæ ‡ç­¾ ---")
    sample_idx = 1
    # Decode for human readability - skip special tokens for input, but not for labels
    original_tokens = tokenizer.decode(input_ids[sample_idx, :attention_mask[sample_idx].sum()])
    
    print(f"     åŽŸå§‹å¥å­: '{original_tokens}'")
    print(f"     è¾“å…¥ IDs: {input_ids[sample_idx].tolist()}")
    print(f"     æ ‡ç­¾ IDs: {labels[sample_idx].tolist()}")
    print("     (æ³¨: -100 å¯¹åº”ä½ç½®ä¸è®¡ç®—æŸå¤±)")


    with torch.no_grad():
        # --- æ­¥éª¤ 4: æ‰§è¡Œæ¨¡åž‹å®Œæ•´å‰å‘ä¼ æ’­ ---
        print_step("æ­¥éª¤ 4", "æ‰§è¡Œæ¨¡åž‹å®Œæ•´å‰å‘ä¼ æ’­")
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
        print("   - æ¨¡åž‹å‰å‘ä¼ æ’­å®Œæˆã€‚")
        print(f"   - è¾“å‡º cls_loc å½¢çŠ¶: {outputs['cls_loc'].shape}")


        # --- æ­¥éª¤ 5: è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤± ---
        print_step("æ­¥éª¤ 5", "è°ƒç”¨ `compute_loss` è®¡ç®—æŸå¤±")
        loss_dict = model.compute_loss(
            outputs,
            targets=labels,
            numerical_values=numerical_values, # å›žå½’ç›®æ ‡ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸å…³å¿ƒ
            attention_mask=attention_mask
        )
        
        cls_loss = loss_dict.get('cls_loss')
        
        print(f"\n   - è®¡ç®—å¾—åˆ°çš„æ€»æŸå¤± (Total Loss): {loss_dict['total']:.4f}")
        print(f"   - è®¡ç®—å¾—åˆ°çš„åˆ†ç±»æŸå¤± (Classification Loss): {cls_loss:.4f}")
        print(f"   - è®¡ç®—å¾—åˆ°çš„å›žå½’æŸå¤± (Regression Loss): {loss_dict['reg_loss']:.4f}")

    print(f"\n\n{'='*80}")
    print("ðŸŽ‰ ç«¯åˆ°ç«¯åˆ†ç±»æµç¨‹éªŒè¯å®Œæˆï¼")
    print("   è„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†ä»ŽåŽŸå§‹æ–‡æœ¬ -> Tokenization -> Labels -> Forward Pass -> Loss çš„å®Œæ•´è¿‡ç¨‹ã€‚")

if __name__ == '__main__':
    main() 
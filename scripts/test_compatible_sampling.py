#!/usr/bin/env python
"""
å…¼å®¹ä¼ ç»Ÿé‡‡æ ·æ¨¡å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `docs/mathematical_foundations.md` ä¸­ "3.3 å…¼å®¹ä¼ ç»Ÿé‡‡æ ·" ç« èŠ‚çš„å®šä¹‰ï¼Œ
éªŒè¯ CausalQwen ä¸ä¼ ç»Ÿ top-k/top-p é‡‡æ ·çš„å…¼å®¹æ€§ã€‚

éªŒè¯ç›®æ ‡:
1. éªŒè¯è¡ŒåŠ¨ç½‘ç»œçš„ `loc_S` è¾“å‡ºå¯ä»¥è¢«å½“ä½œä¼ ç»Ÿ logits ä½¿ç”¨ã€‚
2. éªŒè¯å¯¹ `loc_S` åº”ç”¨ Softmax å‡½æ•°å¯ä»¥å¾—åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„ã€å½’ä¸€åŒ–çš„æ¦‚ç‡åˆ†å¸ƒã€‚
3. ç¡®è®¤æ¨¡å‹åœ¨åˆå§‹åŒ–æ—¶ï¼Œå…¶ Softmax è¾“å‡ºä¸åŸå§‹ Qwen çš„è¾“å‡ºåœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·çš„ã€‚
"""
import os
import sys
import torch
import torch.nn.functional as F

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - å…¼å®¹ä¼ ç»Ÿé‡‡æ · (Top-k/Top-p) æ¨¡å¼éªŒè¯ ğŸš€")

    # --- æ­¥éª¤ 1: åˆå§‹åŒ– ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True,
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights() # <-- å…³é”®ï¼šéªŒè¯åˆå§‹åŒ–æ—¶çš„æ•°å­¦ç­‰ä»·æ€§
    model.eval()
    print("   âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆã€‚")

    # --- æ­¥éª¤ 2: å‡†å¤‡è¾“å…¥æ•°æ® ---
    print_step("æ­¥éª¤ 2", "å‡†å¤‡è¾“å…¥æ ·æœ¬")
    text = "The capital of France is"
    inputs = tokenizer.batch_encode_plus([text], padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    print(f"   - è¾“å…¥æ–‡æœ¬: '{text}'")

    # --- æ­¥éª¤ 3: è·å– CausalQwen çš„ loc_S ---
    print_step("æ­¥éª¤ 3", "å‰å‘ä¼ æ’­è·å– loc_S (åˆ†ç±»ä½ç½®) å¼ é‡")
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
    # æˆ‘ä»¬åªå…³å¿ƒå¯¹ä¸‹ä¸€ä¸ªè¯å…ƒçš„é¢„æµ‹
    loc_S_last = outputs['cls_loc'][:, -1, :]
    print("   - å·²è·å–æœ€åä¸€ä¸ªè¯å…ƒçš„ loc_S å¼ é‡ã€‚")
    print(f"   - loc_S shape: {loc_S_last.shape}")

    # --- æ­¥éª¤ 4: éªŒè¯ Softmax å…¼å®¹æ€§ ---
    print_step("æ­¥éª¤ 4", "éªŒè¯ loc_S ä½œä¸º logits çš„ Softmax å…¼å®¹æ€§")
    
    # å°† loc_S è§†ä¸º logits å¹¶åº”ç”¨ Softmax
    softmax_probs = F.softmax(loc_S_last, dim=-1)
    
    print("   - å·²å¯¹ loc_S åº”ç”¨ Softmax å‡½æ•°ã€‚")
    print(f"   - Softmax æ¦‚ç‡å¼ é‡ shape: {softmax_probs.shape}")
    
    # éªŒè¯æ¦‚ç‡æ˜¯å¦å½’ä¸€åŒ–
    prob_sum = torch.sum(softmax_probs, dim=-1).item()
    # ä½¿ç”¨ä¸€ä¸ªåˆç†çš„å®¹å·®æ¥å¤„ç†æµ®ç‚¹æ•°è¯¯å·®
    is_normalized = torch.allclose(torch.tensor(prob_sum), torch.tensor(1.0), atol=1e-4)
    print(f"   - éªŒè¯: æ¦‚ç‡æ€»å’Œæ˜¯å¦ä¸º 1? {'âœ… æ˜¯' if is_normalized else 'âŒ å¦'} (Sum={prob_sum:.6f})")

    # æ˜¾ç¤º Top-k çš„é¢„æµ‹ç»“æœ
    top_k_probs, top_k_indices = torch.topk(softmax_probs, 5)

    print("\n   - åŸºäº Softmax çš„ Top 5 é¢„æµ‹:")
    for i in range(5):
        token_id = top_k_indices[0, i].item()
        token = tokenizer.decode([token_id])
        prob = top_k_probs[0, i].item()
        print(f"     {i+1}. Token: '{token}', Probability: {prob:.4f}")

    # --- æ­¥éª¤ 5: ä¸åŸå§‹ Qwen å¯¹æ¯” (ç†è®ºéªŒè¯) ---
    print_step("æ­¥éª¤ 5", "ä¸åŸå§‹ Qwen çš„æ•°å­¦ç­‰ä»·æ€§éªŒè¯")
    print("   - æ ¹æ®åˆå§‹åŒ–ç­–ç•¥ï¼ŒCausalQwen çš„ loc_S åº”ä¸ Qwen çš„ logits å®Œå…¨ç›¸ç­‰ã€‚")
    
    # è·å–åŸå§‹ Qwen æ¨¡å‹å¹¶è¿›è¡Œå‰å‘ä¼ æ’­
    qwen_model = model.feature_network.qwen_model
    with torch.no_grad():
        # æ³¨æ„ï¼šåŸå§‹Qwenæ²¡æœ‰æ•°å€¼åµŒå…¥ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨åŸå§‹ input_ids
        qwen_outputs = qwen_model(input_ids=input_ids, attention_mask=attention_mask)
    
    qwen_logits_last = qwen_outputs.logits[:, -1, :]
    
    # æ¯”è¾ƒ loc_S å’Œ qwen_logits
    are_equal = torch.allclose(loc_S_last, qwen_logits_last, atol=1e-5)
    
    print("\n   --- å¯¹æ¯” CausalQwen.loc_S vs Qwen.logits ---")
    print(f"   - CausalQwen loc_S (mean): {loc_S_last.mean().item():.6f}")
    print(f"   - åŸå§‹ Qwen logits (mean): {qwen_logits_last.mean().item():.6f}")
    print(f"   - ç»“è®º: ä¸¤è€…æ˜¯å¦åœ¨æ•°å€¼ä¸Šç›¸ç­‰? {'âœ… æ˜¯' if are_equal else 'âŒ å¦'}")
    if not are_equal:
        print(f"   - æœ€å¤§å·®å¼‚: {torch.max(torch.abs(loc_S_last - qwen_logits_last)).item()}")


    print(f"\n\n{'='*80}")
    print("ğŸ‰ éªŒè¯æˆåŠŸï¼CausalQwen ä¸ä¼ ç»Ÿé‡‡æ ·æ¨¡å¼å®Œå…¨å…¼å®¹ã€‚")
    print("   - loc_S å¯ä»¥ä½œä¸º logits ç”Ÿæˆå½’ä¸€åŒ–çš„ Softmax æ¦‚ç‡åˆ†å¸ƒã€‚")
    print("   - åœ¨åˆå§‹åŒ–çŠ¶æ€ä¸‹ï¼ŒCausalQwen çš„ Softmax è¾“å‡ºä¸åŸå§‹ Qwen åœ¨æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·ã€‚")

if __name__ == '__main__':
    main() 
"""CausalQwen å®Œæ•´æ¼”ç¤º

å±•ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªçœŸæ­£è¿ç§»äº†QwençŸ¥è¯†çš„CausalQwenæ¨¡å‹ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from src.causal_qwen.model import CausalQwen
from src.causal_qwen.config import CausalQwenConfig
from dataclasses import dataclass
import json


def load_qwen_and_convert():
    """åŠ è½½Qwenæ¨¡å‹å¹¶è½¬æ¢ä¸ºCausalQwen"""
    
    print("ğŸš€ CausalQwen æ¨¡å‹è½¬æ¢æ¼”ç¤º")
    print("=" * 60)
    
    # 1. åŠ è½½Qwenæ¨¡å‹å’Œé…ç½®
    print("\n1ï¸âƒ£ åŠ è½½åŸå§‹Qwenæ¨¡å‹...")
    model_path = os.path.expanduser("~/models/Qwen2.5-0.5B")
    
    qwen_config = AutoConfig.from_pretrained(model_path)
    qwen_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="cpu"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    print(f"âœ… Qwenæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {qwen_config.vocab_size}")
    print(f"   - éšè—ç»´åº¦: {qwen_config.hidden_size}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {qwen_config.num_attention_heads}")
    print(f"   - å±‚æ•°: {qwen_config.num_hidden_layers}")
    
    # 2. åˆ›å»ºCausalQwené…ç½®
    print("\n2ï¸âƒ£ åˆ›å»ºCausalQwené…ç½®...")
    causal_config = CausalQwenConfig(
        num_vocab=qwen_config.vocab_size + 1,  # +1 for <NUM>
        hidden_dim=qwen_config.hidden_size,
        num_layers=qwen_config.num_hidden_layers,
        num_heads=qwen_config.num_attention_heads,
        num_token_id=qwen_config.vocab_size,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä¿ç•™tokenä½œä¸º<NUM>
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    )
    
    print(f"âœ… CausalQwené…ç½®åˆ›å»ºæˆåŠŸï¼")
    print(f"   - <NUM> token ID: {causal_config.num_token_id}")
    
    # 3. åˆ›å»ºCausalQwenæ¨¡å‹
    print("\n3ï¸âƒ£ åˆ›å»ºCausalQwenæ¨¡å‹...")
    causal_model = CausalQwen(causal_config)
    
    # 4. è¿ç§»æƒé‡
    print("\n4ï¸âƒ£ è¿ç§»Qwenæƒé‡åˆ°CausalQwen...")
    
    # è¿ç§»è¯åµŒå…¥ï¼ˆæ‰©å±•ä¸€ä¸ªtokenï¼‰
    print("   - è¿ç§»è¯åµŒå…¥...")
    qwen_embed_weight = qwen_model.model.embed_tokens.weight.data
    causal_model.numerical_embedding.embedding.weight.data[:-1] = qwen_embed_weight
    # åˆå§‹åŒ–<NUM> tokençš„åµŒå…¥
    causal_model.numerical_embedding.embedding.weight.data[-1] = qwen_embed_weight.mean(dim=0)
    
    # è¿ç§»Transformerå±‚
    print("   - è¿ç§»Transformerå±‚...")
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨¡å‹ç»“æ„è¿›è¡Œè¿ç§»
    # ç¤ºä¾‹ï¼šå¦‚æœä½¿ç”¨äº†Qwençš„transformerç»“æ„
    if hasattr(qwen_model, 'model') and hasattr(qwen_model.model, 'layers'):
        causal_model.transformer = qwen_model.model.layers
    
    # è¿ç§»è¾“å‡ºå¤´åˆ°ActionNetworkçš„åˆ†ç±»å¤´
    print("   - è¿ç§»è¾“å‡ºå¤´åˆ°ActionNetwork...")
    if hasattr(qwen_model, 'lm_head'):
        # æ‰©å±•æƒé‡ä»¥åŒ…å«<NUM> token
        lm_head_weight = qwen_model.lm_head.weight.data
        causal_model.action.class_weights.data[:-1] = lm_head_weight.T  # æ³¨æ„è½¬ç½®
        causal_model.action.class_weights.data[-1] = lm_head_weight.mean(dim=0)
        
        if hasattr(qwen_model.lm_head, 'bias') and qwen_model.lm_head.bias is not None:
            causal_model.action.class_bias.data[:-1] = qwen_model.lm_head.bias.data
    
    print("âœ… æƒé‡è¿ç§»å®Œæˆï¼")
    
    # 5. è®¾ç½®tokenizer
    causal_model.set_tokenizer(tokenizer)
    
    # 6. æ·»åŠ <NUM> tokenåˆ°tokenizer
    print("\n5ï¸âƒ£ æ‰©å±•tokenizer...")
    tokenizer.add_tokens(['<NUM>'], special_tokens=True)
    print(f"âœ… Tokenizeræ‰©å±•å®Œæˆï¼æ–°è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
    
    return causal_model, tokenizer


def demo_conversation():
    """æ¼”ç¤ºå¯¹è¯åŠŸèƒ½"""
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_qwen_and_convert()
    model.eval()
    
    print("\n" + "=" * 60)
    print("ğŸ’¬ å¼€å§‹å¯¹è¯æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•å¯¹è¯
    test_messages = [
        [{"role": "user", "content": "ä½ å¥½ï¼"}],
        [{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}],
        [{"role": "user", "content": "è®¡ç®—ä¸€ä¸‹ï¼š3 + 5 = <NUM>"}],
    ]
    
    for messages in test_messages:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {messages[-1]['content']}")
        
        try:
            # ä½¿ç”¨å› æœé‡‡æ ·
            print("ğŸ² å› æœé‡‡æ ·å›å¤: ", end="", flush=True)
            response = model.chat(
                messages,
                stream=True,
                max_new_tokens=50,
                temperature=0.8,
                sampling_mode="causal"
            )
            
            for chunk in response:
                print(chunk, end="", flush=True)
            print()
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")


def demo_numerical_understanding():
    """æ¼”ç¤ºæ•°å€¼ç†è§£èƒ½åŠ›"""
    
    print("\nğŸ”¢ æ•°å€¼ç†è§£èƒ½åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    # è¿™é‡Œåº”è¯¥å±•ç¤ºCausalQwenå¦‚ä½•å¤„ç†åŒ…å«æ•°å€¼çš„æ–‡æœ¬
    examples = [
        "æ¸©åº¦æ˜¯ <NUM> æ‘„æ°åº¦",
        "è‚¡ä»·ä¸Šæ¶¨äº† <NUM> %",
        "è·ç¦»å¤§çº¦ <NUM> å…¬é‡Œ",
    ]
    
    print("CausalQwen å¯ä»¥ç†è§£å’Œç”ŸæˆåŒ…å«æ•°å€¼çš„æ–‡æœ¬ï¼š")
    for example in examples:
        print(f"  - {example}")
    
    print("\nå…³é”®ç‰¹æ€§ï¼š")
    print("  âœ… ç»Ÿä¸€çš„æ–‡æœ¬-æ•°å€¼è¡¨ç¤º")
    print("  âœ… æ•°å€¼æ„ŸçŸ¥çš„åµŒå…¥ (Ï†(v) ç¼–ç )")
    print("  âœ… æŸ¯è¥¿åˆ†å¸ƒå»ºæ¨¡æ•°å€¼ä¸ç¡®å®šæ€§")
    print("  âœ… å› æœæ¨ç†æ”¯æŒåäº‹å®é—®é¢˜")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CausalQwen æ¼”ç¤º")
    parser.add_argument("--mode", choices=["convert", "chat", "numerical"], 
                       default="convert", help="æ¼”ç¤ºæ¨¡å¼")
    
    args = parser.parse_args()
    
    if args.mode == "convert":
        load_qwen_and_convert()
    elif args.mode == "chat":
        demo_conversation()
    elif args.mode == "numerical":
        demo_numerical_understanding()

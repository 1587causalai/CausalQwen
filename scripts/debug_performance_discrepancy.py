#!/usr/bin/env python3
"""
æ€§èƒ½å·®å¼‚è°ƒè¯•è„šæœ¬
==================================================================

**ç›®çš„**: ç³»ç»Ÿæ€§å¯¹æ¯”æ¯ä¸ªå›å½’ç®—æ³•çš„ "Legacy" (åŸºäºBaselineBenchmark) å’Œ 
"Sklearn-Style" ä¸¤ç§å®ç°çš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶æ‰¾å‡ºå¯¼è‡´å·®å¼‚çš„æ ¹æœ¬åŸå› ã€‚

**èƒŒæ™¯**: `examples` ç›®å½•ä¸­å­˜åœ¨ä¸¤å¥—åŠŸèƒ½ç›¸åŒçš„æ•™ç¨‹è„šæœ¬ï¼Œä½†å®ƒä»¬çš„æ€§èƒ½è¡¨ç°
å¯èƒ½ä¸ä¸€è‡´ã€‚æœ¬è„šæœ¬æä¾›äº†å…¨é¢çš„å¯¹æ¯”åˆ†æï¼Œæ¶µç›–æ‰€æœ‰ä¸»è¦å›å½’ç®—æ³•ã€‚

**æ”¯æŒçš„ç®—æ³•å¯¹æ¯”**:
- MLPPytorchRegressor vs BaselineBenchmark('pytorch_mlp')
- MLPCausalRegressor vs BaselineBenchmark('standard'/'deterministic')
- MLPHuberRegressor vs BaselineBenchmark('mlp_huber')
- MLPPinballRegressor vs BaselineBenchmark('mlp_pinball_median')
- MLPCauchyRegressor vs BaselineBenchmark('mlp_cauchy')

**æ–¹æ³•**:
1.  **å—æ§å®éªŒ**: ç¡®ä¿ä¸¤ç§å®ç°åœ¨å®Œå…¨ç›¸åŒçš„æ•°æ®å’Œè¶…å‚æ•°ä¸‹è¿è¡Œ
2.  **ä¸­å¤®é…ç½®**: ä½¿ç”¨ `ExperimentConfig` ç±»ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å…³é”®å‚æ•°
3.  **å¹¶è¡Œå¯¹æ¯”**: åˆ†åˆ«è¿è¡Œ Legacy å’Œ Sklearn-Style å®ç°
4.  **å·®å¼‚åˆ†æ**: ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”è¡¨æ ¼ï¼Œå¹¶è®¡ç®—ç›¸å¯¹å·®å¼‚ç™¾åˆ†æ¯”
5.  **è‡ªåŠ¨æ€»ç»“**: è‡ªåŠ¨è¯†åˆ«æ˜¾è‘—æ€§èƒ½å·®å¼‚(>5%)å¹¶æä¾›åˆ†æå»ºè®®

**å¦‚ä½•ä½¿ç”¨**:
1.  ç›´æ¥è¿è¡Œ: `python scripts/debug_performance_discrepancy.py`
2.  é€‰æ‹©æ€§æµ‹è¯•: åœ¨ `ExperimentConfig.MODELS_TO_TEST` ä¸­å¯ç”¨/ç¦ç”¨ç‰¹å®šæ–¹æ³•
3.  å‚æ•°è°ƒä¼˜: ä¿®æ”¹ `ExperimentConfig` ä¸­çš„è¶…å‚æ•°æ¥æµ‹è¯•ä¸åŒå‡è®¾
4.  ç»“æœåˆ†æ: æŸ¥çœ‹è¾“å‡ºè¡¨æ ¼ä¸­çš„ "Diff %" åˆ—æ¥è¯†åˆ«é—®é¢˜ç®—æ³•

**ç¤ºä¾‹è¾“å‡ºè§£è¯»**:
- Diff % = -3.2%: Sklearn-Style æ¯” Legacy å¥½ 3.2%
- Diff % = +8.5%: Legacy æ¯” Sklearn-Style å¥½ 8.5% (éœ€è¦è°ƒæŸ¥)
- Diff % < 5%: ä¸¤ç§å®ç°åŸºæœ¬ä¸€è‡´
"""

import numpy as np
import warnings
import os
import sys
import time
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ä¸¤ç§å®ç°çš„æ ¸å¿ƒæ¨¡å—
from causal_sklearn.benchmarks import BaselineBenchmark
from causal_sklearn.regressor import (
    MLPCausalRegressor, MLPPytorchRegressor, 
    MLPHuberRegressor, MLPPinballRegressor, MLPCauchyRegressor
)
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

# --- å®éªŒé…ç½® ---
class ExperimentConfig:
    """
    ä¸­å¤®å®éªŒé…ç½®ã€‚ä¿®æ”¹è¿™é‡Œçš„å‚æ•°æ¥æµ‹è¯•ä¸åŒçš„å‡è®¾ã€‚
    """
    # ğŸ¯ æ•°æ®é…ç½®
    TEST_SIZE = 0.2
    VAL_SIZE = 0.2
    RANDOM_STATE = 42
    ANOMALY_RATIO = 0.25  # ç»Ÿä¸€ä½¿ç”¨æ—§è„šæœ¬çš„25%å™ªå£°æ¯”ä¾‹è¿›è¡Œå¯¹æ¯”

    # ğŸ§  æ¨¡å‹è¶…å‚æ•° (å…³é”®ï¼åœ¨è¿™é‡Œå¯¹é½æ–°æ—§å®ç°)
    # ----------------------------------------------------------------------
    # å‡è®¾1: å°è¯•å¤ç°æ—§è„šæœ¬çš„ "æ›´å¥½" æ€§èƒ½
    # æ—§è„šæœ¬çš„å­¦ä¹ ç‡æ˜¯ 0.01ï¼Œè€Œæ–°è„šæœ¬æ˜¯ 0.001ã€‚è¿™æ˜¯æœ€å¤§çš„å«Œç–‘ã€‚
    LEARNING_RATE = 0.01

    # æ­£åˆ™åŒ–: æ–°è„šæœ¬ä¸­ CausalEngine çš„ alpha æ˜ç¡®ä¸º 0ï¼ŒPytorch MLP ä¸º 0.0001
    # æˆ‘ä»¬åœ¨è¿™é‡Œä¿æŒä¸€è‡´ï¼Œå¹¶å‡è®¾æ—§è„šæœ¬çš„é»˜è®¤è¡Œä¸ºä¸æ­¤ç±»ä¼¼ã€‚
    ALPHA_CAUSAL = 0.0
    ALPHA_PYTORCH = 0.0001
    
    # æ‰¹å¤„ç†å¤§å°: æ–°çš„ sklearn-style regressor é»˜è®¤ä¸º 'auto' (å³200)
    # æˆ‘ä»¬å‡è®¾æ—§çš„ Benchmark ä¹Ÿæ˜¯ç±»ä¼¼è¡Œä¸ºã€‚å¯ä»¥ä¿®æ”¹ä¸º `None` æ¥æµ‹è¯•å…¨é‡æ‰¹æ¬¡ã€‚
    BATCH_SIZE = 200 # ä½¿ç”¨ 'auto' çš„é»˜è®¤å€¼
    # BATCH_SIZE = None # è®¾ä¸ºNoneæ¥å¼ºåˆ¶ä½¿ç”¨å…¨é‡æ‰¹æ¬¡

    # å…¶ä»–é€šç”¨å‚æ•° (åœ¨ä¸¤ä¸ªè„šæœ¬ä¸­åŸºæœ¬ä¸€è‡´)
    HIDDEN_SIZES = (128, 64, 32)
    MAX_EPOCHS = 3000
    PATIENCE = 50
    TOL = 1e-4
    
    # CausalEngineä¸“å±å‚æ•°
    GAMMA_INIT = 1.0
    B_NOISE_INIT = 1.0
    B_NOISE_TRAINABLE = True
    
    # ğŸ’¡ è¦æµ‹è¯•ä¸åŒå‡è®¾ï¼Œå¯ä»¥ä¿®æ”¹ä¸Šé¢çš„å€¼ã€‚ä¾‹å¦‚:
    # LEARNING_RATE = 0.001
    # ANOMALY_RATIO = 0.3
    # ----------------------------------------------------------------------

    # ğŸ”¬ è¦å¯¹æ¯”çš„æ¨¡å‹ (æ¯ä¸ªéƒ½æœ‰Legacy vs Sklearn-Styleä¸¤ç§å®ç°)
    MODELS_TO_TEST = {
        'pytorch_mlp': True,           # MLPPytorchRegressor vs BaselineBenchmark('pytorch_mlp')
        'causal_standard': True,       # MLPCausalRegressor vs BaselineBenchmark('standard')
        'causal_deterministic': True,  # MLPCausalRegressor vs BaselineBenchmark('deterministic')  
        'mlp_huber': True,            # MLPHuberRegressor vs BaselineBenchmark('mlp_huber')
        'mlp_pinball': True,          # MLPPinballRegressor vs BaselineBenchmark('mlp_pinball_median')
        'mlp_cauchy': True,           # MLPCauchyRegressor vs BaselineBenchmark('mlp_cauchy')
    }


def load_and_prepare_data(config: ExperimentConfig):
    """
    åŠ è½½å’Œå‡†å¤‡æ•°æ®ï¼Œå®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥
    
    æ ¸å¿ƒç†å¿µï¼šå»ºç«‹ç»å¯¹å…¬å¹³çš„ç«æŠ€åœº
    1. å…¨å±€æ ‡å‡†åŒ–ï¼šå¯¹è®­ç»ƒé›†çš„ X å’Œ y éƒ½è¿›è¡Œæ ‡å‡†åŒ–
    2. ç»Ÿä¸€è¾“å…¥ï¼šæ‰€æœ‰æ¨¡å‹æ¥æ”¶å®Œå…¨æ ‡å‡†åŒ–çš„æ•°æ®
    3. ç»Ÿä¸€è¯„ä¼°ï¼šæ‰€æœ‰é¢„æµ‹ç»“æœéƒ½è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
    """
    print("ğŸ“Š 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
    
    # åŠ è½½æ•°æ®
    housing = fetch_california_housing()
    X, y = housing.data, housing.target
    print(f"   - æ•°æ®é›†åŠ è½½å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # ä½¿ç”¨æ ‡å‡†çš„train_test_splitè¿›è¡Œæ•°æ®åˆ†å‰²
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X, y,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_STATE
    )
    
    # ä¿å­˜åŸå§‹å¹²å‡€æ•°æ®ï¼ˆç”¨äºLegacyå¯¹æ¯”ï¼‰
    X_train_full_original = X_train_full.copy()
    y_train_full_original = y_train_full.copy()
    X_test_original = X_test.copy()
    y_test_original = y_test.copy()
    
    # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥
    if config.ANOMALY_RATIO > 0:
        y_train_full_noisy, noise_indices = inject_shuffle_noise(
            y_train_full,
            noise_ratio=config.ANOMALY_RATIO,
            random_state=config.RANDOM_STATE
        )
        y_train_full = y_train_full_noisy
        print(f"   - å¼‚å¸¸æ³¨å…¥å®Œæˆ: {config.ANOMALY_RATIO:.0%} ({len(noise_indices)}/{len(y_train_full)} æ ·æœ¬å—å½±å“)")
    else:
        print(f"   - æ— å¼‚å¸¸æ³¨å…¥: çº¯å‡€ç¯å¢ƒ")
    
    # ä»è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†ï¼ˆå«å¼‚å¸¸çš„æ•°æ®ï¼‰
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full,
        test_size=config.VAL_SIZE,
        random_state=config.RANDOM_STATE
    )
    
    # åŒæ ·ä»åŸå§‹å¹²å‡€æ•°æ®ä¸­åˆ†å‰²å‡ºéªŒè¯é›†ï¼ˆç”¨äºLegacyå¯¹æ¯”ï¼‰
    X_train_original, X_val_original, y_train_original, y_val_original = train_test_split(
        X_train_full_original, y_train_full_original,
        test_size=config.VAL_SIZE,
        random_state=config.RANDOM_STATE
    )
    
    print(f"   - æ•°æ®åˆ†å‰²å®Œæˆã€‚")
    print(f"     - è®­ç»ƒé›†: {X_train.shape[0]}")
    print(f"     - éªŒè¯é›†: {X_val.shape[0]}")
    print(f"     - æµ‹è¯•é›†: {X_test.shape[0]}")
    
    # ğŸ¯ å…³é”®æ”¹è¿›ï¼šå…¨å±€æ ‡å‡†åŒ–ç­–ç•¥
    print(f"\n   ğŸ¯ å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆç»å¯¹å…¬å¹³çš„ç«æŠ€åœºï¼‰:")
    
    # 1. ç‰¹å¾æ ‡å‡†åŒ– - åŸºäºè®­ç»ƒé›†æ‹Ÿåˆ
    print(f"   - å¯¹ç‰¹å¾ X è¿›è¡Œæ ‡å‡†åŒ–...")
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    X_test_scaled = scaler_X.transform(X_test)
    
    # 2. ç›®æ ‡æ ‡å‡†åŒ– - åŸºäºè®­ç»ƒé›†æ‹Ÿåˆï¼ˆå…³é”®ï¼ï¼‰
    print(f"   - å¯¹ç›®æ ‡ y è¿›è¡Œæ ‡å‡†åŒ–...")
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"   - âœ… æ‰€æœ‰æ¨¡å‹å°†æ¥æ”¶å®Œå…¨æ ‡å‡†åŒ–çš„æ•°æ®")
    print(f"   - âœ… æ‰€æœ‰é¢„æµ‹ç»“æœå°†è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°")
    
    return {
        # å«å¼‚å¸¸çš„æ•°æ®ï¼ˆç”¨äºSklearn-Styleå®ç°ï¼‰
        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
        'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
        
        # åŸå§‹å¹²å‡€æ•°æ®ï¼ˆç”¨äºLegacyå®ç°å¯¹æ¯”ï¼‰
        'X_train_original': X_train_original, 'X_val_original': X_val_original, 'X_test_original': X_test_original,
        'y_train_original': y_train_original, 'y_val_original': y_val_original, 'y_test_original': y_test_original,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºSklearn-Styleæ¨¡å‹è®­ç»ƒï¼‰
        'X_train_scaled': X_train_scaled,
        'X_val_scaled': X_val_scaled, 
        'X_test_scaled': X_test_scaled,
        'y_train_scaled': y_train_scaled,
        'y_val_scaled': y_val_scaled,
        'y_test_scaled': y_test_scaled,
        
        # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        
        # å®Œæ•´æ•°æ®
        'X_full': X,
        'y_full': y
    }

def run_legacy_benchmark(config: ExperimentConfig, data: dict):
    """
    ä½¿ç”¨ BaselineBenchmark (æ—§ç‰ˆå®ç°) è¿è¡Œå®éªŒ
    
    ğŸ¯ å…³é”®æ”¹è¿›ï¼šBaselineBenchmark ç°åœ¨æ¥æ”¶å…¨å±€æ ‡å‡†åŒ–çš„æ•°æ®
    ç¡®ä¿ä¸ Sklearn-Style å®ç°åœ¨å®Œå…¨ç›¸åŒçš„æ•°æ®ç¯å¢ƒä¸‹ç«äº‰
    """
    print("\nğŸš€ 2a. è¿è¡Œ Legacy å®ç° (BaselineBenchmark)...")
    print("   ğŸ¯ ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç¡®ä¿å…¬å¹³ç«äº‰")
    
    benchmark = BaselineBenchmark()
    
    # ç¡®å®šè¦è¿è¡Œçš„åŸºå‡†æ–¹æ³•
    baseline_methods = []
    if config.MODELS_TO_TEST.get('pytorch_mlp'):
        baseline_methods.append('pytorch_mlp')
    if config.MODELS_TO_TEST.get('mlp_huber'):
        baseline_methods.append('mlp_huber')
    if config.MODELS_TO_TEST.get('mlp_pinball'):
        baseline_methods.append('mlp_pinball_median')  # BaselineBenchmarkä¸­çš„æ–¹æ³•å
    if config.MODELS_TO_TEST.get('mlp_cauchy'):
        baseline_methods.append('mlp_cauchy')
        
    # ç¡®å®šè¦è¿è¡Œçš„å› æœæ¨¡å¼
    causal_modes = []
    if config.MODELS_TO_TEST.get('causal_standard'):
        causal_modes.append('standard')
    if config.MODELS_TO_TEST.get('causal_deterministic'):
        causal_modes.append('deterministic')

    # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä¼ é€’åŸå§‹å¹²å‡€æ•°æ®ï¼Œè®©BaselineBenchmarkè‡ªå·±å¤„ç†å¼‚å¸¸æ³¨å…¥
    # è¿™ç¡®ä¿äº†ä¸Sklearn-Styleå®ç°å®Œå…¨ç›¸åŒçš„å¼‚å¸¸æ³¨å…¥å’Œæ•°æ®å¤„ç†æµç¨‹
    X_combined_original = np.concatenate([data['X_train_original'], data['X_val_original'], data['X_test_original']])
    y_combined_original = np.concatenate([data['y_train_original'], data['y_val_original'], data['y_test_original']])
    
    print(f"   - ä¼ é€’åŸå§‹å¹²å‡€æ•°æ®: X({X_combined_original.shape}), y({y_combined_original.shape})")
    print(f"   - è®©BaselineBenchmarkè‡ªå·±å¤„ç†å¼‚å¸¸æ³¨å…¥ï¼Œç¡®ä¿å®Œå…¨ä¸€è‡´")
    
    results = benchmark.compare_models(
        X=X_combined_original,  # ğŸ¯ åŸå§‹å¹²å‡€ç‰¹å¾
        y=y_combined_original,  # ğŸ¯ åŸå§‹å¹²å‡€ç›®æ ‡
        task_type='regression',
        baseline_methods=baseline_methods,
        causal_modes=causal_modes,
        global_standardization=True,  # ğŸ¯ å¯ç”¨å…¨å±€æ ‡å‡†åŒ–ä»¥åŒ¹é…Sklearn-Styleå®ç°
        # æ•°æ®å‚æ•° - ä½¿ç”¨ç›¸åŒçš„åˆ†å‰²æ¯”ä¾‹å’Œå¼‚å¸¸æ³¨å…¥æ¯”ä¾‹
        test_size=len(data['X_test_original']) / len(X_combined_original),
        val_size=len(data['X_val_original']) / (len(data['X_train_original']) + len(data['X_val_original'])),
        anomaly_ratio=config.ANOMALY_RATIO,  # ğŸ¯ å…³é”®ï¼šè®©BaselineBenchmarkæ‰§è¡Œç›¸åŒçš„å¼‚å¸¸æ³¨å…¥
        anomaly_strategy='shuffle',  # ä½¿ç”¨æ–°çš„ç®€åŒ–ç­–ç•¥
        random_state=config.RANDOM_STATE,
        # ç»Ÿä¸€æ¨¡å‹å‚æ•°
        hidden_sizes=config.HIDDEN_SIZES,
        max_epochs=config.MAX_EPOCHS,
        lr=config.LEARNING_RATE,
        patience=config.PATIENCE,
        tol=config.TOL,
        alpha=config.ALPHA_PYTORCH, # for pytorch_mlp
        batch_size=config.BATCH_SIZE,
        # CausalEngine å‚æ•°
        gamma_init=config.GAMMA_INIT,
        b_noise_init=config.B_NOISE_INIT,
        b_noise_trainable=config.B_NOISE_TRAINABLE,
        causal_alpha=config.ALPHA_CAUSAL,
        verbose=False # ä¿æŒè¾“å‡ºæ•´æ´
    )
    
    # ğŸ¯ å…³é”®æ”¹è¿›ï¼šBaselineBenchmarkç°åœ¨è‡ªåŠ¨å¤„ç†å…¨å±€æ ‡å‡†åŒ–å’Œé€†å˜æ¢
    print("   - BaselineBenchmarkå°†è‡ªåŠ¨å¤„ç†æ ‡å‡†åŒ–å’Œé€†å˜æ¢")
    
    print("   - Legacy å®ç°è¿è¡Œå®Œæˆã€‚")
    return results

def run_sklearn_benchmark(config: ExperimentConfig, data: dict):
    """
    ä½¿ç”¨ sklearn-style learners (æ–°ç‰ˆå®ç°) è¿è¡Œå®éªŒ
    
    ğŸ¯ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–æ•°æ®ï¼Œç¡®ä¿ä¸ Legacy å®ç°å®Œå…¨å…¬å¹³ç«äº‰
    """
    print("\nğŸš€ 2b. è¿è¡Œ Sklearn-Style å®ç°...")
    print("   ğŸ¯ ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–æ•°æ®ç¡®ä¿å…¬å¹³ç«äº‰")

    results = {}
    
    # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨åŸå§‹æ•°æ®ä½†æ‰‹åŠ¨å®æ–½å…¨å±€æ ‡å‡†åŒ–ï¼Œç¡®ä¿ä¸ Legacy å®ç°å®Œå…¨ä¸€è‡´
    # ç»„åˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆå«å¼‚å¸¸çš„åŸå§‹æ•°æ®ï¼‰
    X_train_val_original = np.concatenate([data['X_train'], data['X_val']])
    y_train_val_original = np.concatenate([data['y_train'], data['y_val']])
    X_test_original = data['X_test']
    y_test_original = data['y_test']
    
    # æ‰‹åŠ¨å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆä¸Legacyä¿æŒä¸€è‡´ï¼‰
    from sklearn.preprocessing import StandardScaler
    scaler_X = StandardScaler()
    X_train_val_scaled = scaler_X.fit_transform(X_train_val_original)
    X_test_scaled = scaler_X.transform(X_test_original)
    
    scaler_y = StandardScaler()
    y_train_val_scaled = scaler_y.fit_transform(y_train_val_original.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test_original.reshape(-1, 1)).flatten()
    
    print(f"   - æ‰‹åŠ¨å…¨å±€æ ‡å‡†åŒ–: X_train_val({X_train_val_scaled.shape}), y_train_val({y_train_val_scaled.shape})")
    print(f"   - ç¡®ä¿ä¸Legacyå®ç°ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ ‡å‡†åŒ–ç­–ç•¥")

    # é€šç”¨è®­ç»ƒå‡½æ•°
    def train_and_evaluate(model_name, model_class, model_params, result_key):
        print(f"   - æ­£åœ¨è®­ç»ƒ {model_name}...")
        start_time = time.time()
        
        model = model_class(**model_params)
        # ğŸ¯ å…³é”®æ”¹è¿›ï¼šåœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­è®­ç»ƒ
        model.fit(X_train_val_scaled, y_train_val_scaled)
        
        # ğŸ¯ å…³é”®æ”¹è¿›ï¼šåœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­é¢„æµ‹
        y_pred_scaled = model.predict(X_test_scaled)
        
        # ğŸ¯ å…³é”®æ”¹è¿›ï¼šå°†é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
        y_pred_original = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        
        # åœ¨åŸå§‹å°ºåº¦ä¸‹è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        results[result_key] = {
            'test': {
                'MAE': mean_absolute_error(y_test_original, y_pred_original),
                'MdAE': median_absolute_error(y_test_original, y_pred_original),
                'RMSE': np.sqrt(mean_squared_error(y_test_original, y_pred_original)),
                'RÂ²': r2_score(y_test_original, y_pred_original)
            },
            'time': time.time() - start_time
        }
        print(f"     ...å®Œæˆ (ç”¨æ—¶: {results[result_key]['time']:.2f}s)")

    # é€šç”¨å‚æ•°ï¼ˆæ‰€æœ‰æ–¹æ³•å…±ç”¨ï¼‰
    common_params = {
        'max_iter': config.MAX_EPOCHS,
        'learning_rate': config.LEARNING_RATE,
        'early_stopping': True,
        'validation_fraction': config.VAL_SIZE,
        'n_iter_no_change': config.PATIENCE,
        'tol': config.TOL,
        'batch_size': config.BATCH_SIZE,
        'random_state': config.RANDOM_STATE,
        'verbose': False
    }

    # --- è®­ç»ƒå’Œè¯„ä¼° PyTorch MLP ---
    if config.MODELS_TO_TEST.get('pytorch_mlp'):
        pytorch_params = {
            **common_params,
            'hidden_layer_sizes': config.HIDDEN_SIZES,
            'alpha': config.ALPHA_PYTORCH,
        }
        train_and_evaluate('PyTorch MLP', MLPPytorchRegressor, pytorch_params, 'pytorch_mlp')

    # --- è®­ç»ƒå’Œè¯„ä¼° CausalEngine modes ---
    causal_base_params = {
        **common_params,
        'perception_hidden_layers': config.HIDDEN_SIZES,
        'alpha': config.ALPHA_CAUSAL,
        'gamma_init': config.GAMMA_INIT,
        'b_noise_init': config.B_NOISE_INIT,
        'b_noise_trainable': config.B_NOISE_TRAINABLE,
    }
    
    if config.MODELS_TO_TEST.get('causal_standard'):
        causal_params = {**causal_base_params, 'mode': 'standard'}
        train_and_evaluate('CausalEngine (standard)', MLPCausalRegressor, causal_params, 'standard')
    
    if config.MODELS_TO_TEST.get('causal_deterministic'):
        causal_params = {**causal_base_params, 'mode': 'deterministic'}
        train_and_evaluate('CausalEngine (deterministic)', MLPCausalRegressor, causal_params, 'deterministic')

    # --- è®­ç»ƒå’Œè¯„ä¼°ç¨³å¥å›å½’å™¨ ---
    robust_base_params = {
        **common_params,
        'hidden_layer_sizes': config.HIDDEN_SIZES,
        'alpha': config.ALPHA_PYTORCH,  # ç¨³å¥å›å½’å™¨ä½¿ç”¨ä¸PyTorch MLPç›¸åŒçš„alpha
    }

    if config.MODELS_TO_TEST.get('mlp_huber'):
        train_and_evaluate('MLP Huber', MLPHuberRegressor, robust_base_params, 'mlp_huber')

    if config.MODELS_TO_TEST.get('mlp_pinball'):
        train_and_evaluate('MLP Pinball', MLPPinballRegressor, robust_base_params, 'mlp_pinball')

    if config.MODELS_TO_TEST.get('mlp_cauchy'):
        train_and_evaluate('MLP Cauchy', MLPCauchyRegressor, robust_base_params, 'mlp_cauchy')
    
    print("   - Sklearn-Style å®ç°è¿è¡Œå®Œæˆã€‚")
    return results

def print_comparison_table(legacy_results, sklearn_results, config):
    """æ‰“å°æœ€ç»ˆçš„æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n\n" + "="*80)
    print("ğŸ”¬ 3. æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    print("\n--- å®éªŒé…ç½® ---")
    print(f"å­¦ä¹ ç‡: {config.LEARNING_RATE}, å¼‚å¸¸æ¯”ä¾‹: {config.ANOMALY_RATIO}, "
          f"æ‰¹å¤„ç†å¤§å°: {config.BATCH_SIZE}")
    print(f"Causal Alpha: {config.ALPHA_CAUSAL}, Pytorch Alpha: {config.ALPHA_PYTORCH}")
    print(f"éšè—å±‚: {config.HIDDEN_SIZES}")
    print("-" * 20)

    header = f"| {'Model':<22} | {'Implementation':<16} | {'MAE':<8} | {'MdAE':<8} | {'RMSE':<8} | {'RÂ²':<8} | {'Diff %':<8} |"
    separator = "-" * len(header)
    
    print("\n" + separator)
    print(header)
    print(separator)

    # æ¨¡å‹æ˜ å°„ï¼š(config_key, legacy_key, sklearn_key, display_name)
    # æ³¨æ„ï¼šlegacy_key ä½¿ç”¨ BaselineBenchmark å®é™…è¿”å›çš„ç»“æœé”®å
    models_map = [
        ('pytorch_mlp', 'pytorch', 'pytorch_mlp', 'PyTorch MLP'),
        ('causal_standard', 'standard', 'standard', 'Causal (standard)'),
        ('causal_deterministic', 'deterministic', 'deterministic', 'Causal (deterministic)'),
        ('mlp_huber', 'mlp_huber', 'mlp_huber', 'MLP Huber'),
        ('mlp_pinball', 'mlp_pinball_median', 'mlp_pinball', 'MLP Pinball'),
        ('mlp_cauchy', 'mlp_cauchy', 'mlp_cauchy', 'MLP Cauchy'),
    ]

    for config_key, legacy_key, sklearn_key, display_name in models_map:
        if not config.MODELS_TO_TEST.get(config_key):
            continue

        legacy_result = None
        sklearn_result = None

        # Legacy results
        if legacy_key in legacy_results:
            legacy_result = legacy_results[legacy_key]['test']
            print(f"| {display_name:<22} | {'Legacy':<16} | {legacy_result['MAE']:.4f} | {legacy_result['MdAE']:.4f} | {legacy_result['RMSE']:.4f} | {legacy_result['RÂ²']:.4f} | {'':<8} |")

        # Sklearn results
        if sklearn_key in sklearn_results:
            sklearn_result = sklearn_results[sklearn_key]['test']
            
            # è®¡ç®—å·®å¼‚ç™¾åˆ†æ¯” (ä»¥MdAEä¸ºä¸»è¦æŒ‡æ ‡)
            diff_pct = ""
            if legacy_result and sklearn_result:
                mdae_diff = ((sklearn_result['MdAE'] - legacy_result['MdAE']) / legacy_result['MdAE']) * 100
                diff_pct = f"{mdae_diff:+.2f}%"
            
            print(f"| {display_name:<22} | {'Sklearn-Style':<16} | {sklearn_result['MAE']:.4f} | {sklearn_result['MdAE']:.4f} | {sklearn_result['RMSE']:.4f} | {sklearn_result['RÂ²']:.4f} | {diff_pct:<8} |")
        
        if legacy_result or sklearn_result:
            print(separator)
    
    # æ‰“å°å·®å¼‚åˆ†æ
    print("\nğŸ’¡ å·®å¼‚åˆ†æ:")
    print("   - Diff % è¡¨ç¤º Sklearn-Style ç›¸å¯¹äº Legacy åœ¨ MdAE æŒ‡æ ‡ä¸Šçš„ç›¸å¯¹å·®å¼‚")
    print("   - è´Ÿå€¼è¡¨ç¤º Sklearn-Style æ€§èƒ½æ›´å¥½ï¼Œæ­£å€¼è¡¨ç¤º Legacy æ€§èƒ½æ›´å¥½")
    print("   - å¦‚æœå·®å¼‚å¾ˆå°(<5%)ï¼Œè¯´æ˜ä¸¤ç§å®ç°åŸºæœ¬ä¸€è‡´")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ€§èƒ½å·®å¼‚è°ƒè¯•è„šæœ¬")
    print("="*60)
    print("ç›®æ ‡: ç³»ç»Ÿæ€§å¯¹æ¯”æ¯ä¸ªå›å½’ç®—æ³•çš„ Legacy vs Sklearn-Style å®ç°")
    print("æ–¹æ³•: åœ¨ç›¸åŒæ•°æ®å’Œå‚æ•°ä¸‹è¿è¡Œä¸¤ç§å®ç°ï¼Œå¹¶è®¡ç®—æ€§èƒ½å·®å¼‚")
    print()
    
    config = ExperimentConfig()
    
    # æ˜¾ç¤ºè¦æµ‹è¯•çš„æ–¹æ³•
    enabled_methods = [k for k, v in config.MODELS_TO_TEST.items() if v]
    print(f"ğŸ“Š å°†æµ‹è¯•ä»¥ä¸‹ {len(enabled_methods)} ç§æ–¹æ³•:")
    for i, method in enumerate(enabled_methods, 1):
        print(f"   {i}. {method}")
    print()
    
    # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
    data = load_and_prepare_data(config)
    
    # 2a. è¿è¡Œæ—§ç‰ˆå®ç°
    legacy_results = run_legacy_benchmark(config, data)
    
    # 2b. è¿è¡Œæ–°ç‰ˆå®ç°
    sklearn_results = run_sklearn_benchmark(config, data)

    # 3. æ‰“å°å¯¹æ¯”ç»“æœ
    print_comparison_table(legacy_results, sklearn_results, config)
    
    # 4. æ€»ç»“åˆ†æ
    print("\nğŸ“ˆ æ€»ç»“åˆ†æ:")
    significant_diffs = []
    for config_key, legacy_key, sklearn_key, display_name in [
        ('pytorch_mlp', 'PyTorch MLP', 'pytorch_mlp', 'PyTorch MLP'),
        ('causal_standard', 'standard', 'standard', 'Causal (standard)'),
        ('causal_deterministic', 'deterministic', 'deterministic', 'Causal (deterministic)'),
        ('mlp_huber', 'MLP Huber', 'mlp_huber', 'MLP Huber'),
        ('mlp_pinball', 'MLP Pinball Median', 'mlp_pinball', 'MLP Pinball'),
        ('mlp_cauchy', 'MLP Cauchy', 'mlp_cauchy', 'MLP Cauchy'),
    ]:
        if (config.MODELS_TO_TEST.get(config_key) and 
            legacy_key in legacy_results and sklearn_key in sklearn_results):
            
            legacy_mdae = legacy_results[legacy_key]['test']['MdAE']
            sklearn_mdae = sklearn_results[sklearn_key]['test']['MdAE']
            diff_pct = ((sklearn_mdae - legacy_mdae) / legacy_mdae) * 100
            
            if abs(diff_pct) > 5.0:  # å·®å¼‚è¶…è¿‡5%
                significant_diffs.append((display_name, diff_pct))
    
    if significant_diffs:
        print(f"   âš ï¸ å‘ç° {len(significant_diffs)} ä¸ªæ–¹æ³•å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ (>5%):")
        for name, diff in significant_diffs:
            direction = "Sklearn-Styleæ›´å·®" if diff > 0 else "Sklearn-Styleæ›´å¥½"
            print(f"      - {name}: {diff:+.2f}% ({direction})")
        print("   ğŸ’¡ å»ºè®®æ£€æŸ¥è¿™äº›æ–¹æ³•çš„å‚æ•°é…ç½®æˆ–å®ç°ç»†èŠ‚")
    else:
        print("   âœ… æ‰€æœ‰æ–¹æ³•çš„æ€§èƒ½å·®å¼‚éƒ½åœ¨å¯æ¥å—èŒƒå›´å†… (<5%)")
        print("   ğŸ’¡ ä¸¤ç§å®ç°åŸºæœ¬ä¸€è‡´ï¼Œæ€§èƒ½å·®å¼‚å¯èƒ½æ¥è‡ªå…¶ä»–å› ç´ ")
    
    print("\nğŸ‰ è°ƒè¯•è„šæœ¬è¿è¡Œå®Œæ¯•ï¼")
    print("ğŸ’¡ å¦‚éœ€è°ƒæ•´æµ‹è¯•å‚æ•°ï¼Œè¯·ä¿®æ”¹ ExperimentConfig ç±»ä¸­çš„é…ç½®")
    print("ğŸ”§ å¦‚éœ€æµ‹è¯•ç‰¹å®šæ–¹æ³•ï¼Œè¯·åœ¨ MODELS_TO_TEST ä¸­å¯ç”¨/ç¦ç”¨ç›¸åº”é€‰é¡¹")


if __name__ == "__main__":
    main()

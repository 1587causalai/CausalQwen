#!/usr/bin/env python3
"""
CausalQwen å…¨é¢ç»„ä»¶æµ‹è¯•è„šæœ¬

ç›®æ ‡ï¼šåŠ è½½ ~/models/Qwen2.5-0.5Bï¼Œé€æ­¥æµ‹è¯•æ‰€æœ‰ç»„ä»¶çš„åŠŸèƒ½
ç‰¹ç‚¹ï¼šå¯è§†åŒ–ã€é€æ­¥ã€è¯¦ç»†è¾“å‡ºï¼Œè®©ç”¨æˆ·æ¸…æ¥šçœ‹åˆ°æ¯ä¸ªç»„ä»¶çš„å·¥ä½œçŠ¶æ€

æµ‹è¯•å†…å®¹ï¼š
1. ç¯å¢ƒå’Œä¾èµ–æ£€æŸ¥
2. åŸå§‹Qwenæ¨¡å‹åŠ è½½
3. CausalQwen MVPæ¨¡å‹åˆå§‹åŒ–
4. å„ä¸ªæ ¸å¿ƒç»„ä»¶åŠŸèƒ½éªŒè¯
5. ä¸‰ç§æ¨ç†æ¨¡å¼æµ‹è¯•
6. ç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯
"""

import sys
import os
import torch
import time
import traceback
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# ANSI color codes for pretty printing
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    END = '\033[0m'

def print_section(title, color=Colors.BLUE):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{color}{Colors.BOLD}{'='*60}{Colors.END}")
    print(f"{color}{Colors.BOLD}{title}{Colors.END}")
    print(f"{color}{Colors.BOLD}{'='*60}{Colors.END}")

def print_step(step_num, description, color=Colors.CYAN):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{color}ğŸ“‹ æ­¥éª¤ {step_num}: {description}{Colors.END}")

def print_success(message):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"{Colors.GREEN}âœ… {message}{Colors.END}")

def print_error(message):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"{Colors.RED}âŒ {message}{Colors.END}")

def print_warning(message):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"{Colors.YELLOW}âš ï¸  {message}{Colors.END}")

def print_info(message):
    """æ‰“å°ä¿¡æ¯"""
    print(f"{Colors.WHITE}â„¹ï¸  {message}{Colors.END}")

def test_environment():
    """æµ‹è¯•ç¯å¢ƒå’Œä¾èµ–"""
    print_section("ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå’Œä¾èµ–æ£€æŸ¥")
    
    print_step(1, "æ£€æŸ¥Pythonç¯å¢ƒ")
    print_info(f"Pythonç‰ˆæœ¬: {sys.version}")
    print_info(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print_info(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    
    print_step(2, "æ£€æŸ¥PyTorch")
    try:
        import torch
        print_success(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print_info(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print_info(f"è®¾å¤‡: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}")
    except ImportError as e:
        print_error(f"PyTorchå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    print_step(3, "æ£€æŸ¥transformers")
    try:
        import transformers
        print_success(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError as e:
        print_error(f"Transformerså¯¼å…¥å¤±è´¥: {e}")
        return False
        
    print_step(4, "æ£€æŸ¥Qwenæ¨¡å‹è·¯å¾„")
    qwen_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    if os.path.exists(qwen_path):
        print_success(f"Qwenæ¨¡å‹è·¯å¾„å­˜åœ¨: {qwen_path}")
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        config_file = os.path.join(qwen_path, 'config.json')
        model_file = os.path.join(qwen_path, 'pytorch_model.bin')
        safetensors_file = os.path.join(qwen_path, 'model.safetensors')
        
        if os.path.exists(config_file):
            print_success("config.json å­˜åœ¨")
        else:
            print_warning("config.json ä¸å­˜åœ¨")
            
        if os.path.exists(model_file) or os.path.exists(safetensors_file):
            print_success("æ¨¡å‹æƒé‡æ–‡ä»¶å­˜åœ¨")
        else:
            print_warning("æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨")
    else:
        print_error(f"Qwenæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {qwen_path}")
        return False
    
    return True

def test_imports():
    """æµ‹è¯•é¡¹ç›®æ¨¡å—å¯¼å…¥"""
    print_section("ç¬¬äºŒéƒ¨åˆ†ï¼šé¡¹ç›®æ¨¡å—å¯¼å…¥æµ‹è¯•")
    
    print_step(1, "å¯¼å…¥CausalQwen MVPæ¨¡å—")
    try:
        from causal_qwen_mvp import (
            CausalQwenMVPForCausalLM, 
            CausalQwen2Config,
            CausalInferenceEngine,
            InferenceValidator,
            CausalTrainer,
            get_model_info
        )
        print_success("æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æ˜¾ç¤ºé¡¹ç›®ä¿¡æ¯
        model_info = get_model_info()
        print_info(f"é¡¹ç›®åç§°: {model_info['name']}")
        print_info(f"ç‰ˆæœ¬: {model_info['version']}")
        print_info(f"çŠ¶æ€: {model_info['status']}")
        
        return {
            'CausalQwenMVPForCausalLM': CausalQwenMVPForCausalLM,
            'CausalQwen2Config': CausalQwen2Config,
            'CausalInferenceEngine': CausalInferenceEngine,
            'InferenceValidator': InferenceValidator,
            'CausalTrainer': CausalTrainer
        }
        
    except ImportError as e:
        print_error(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_qwen_loading():
    """æµ‹è¯•åŸå§‹Qwenæ¨¡å‹åŠ è½½"""
    print_section("ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸå§‹Qwenæ¨¡å‹åŠ è½½æµ‹è¯•")
    
    print_step(1, "åŠ è½½Qwen2é…ç½®")
    try:
        from transformers import Qwen2Config, Qwen2ForCausalLM, AutoTokenizer
        
        qwen_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
        config = Qwen2Config.from_pretrained(qwen_path)
        print_success("Qwen2é…ç½®åŠ è½½æˆåŠŸ")
        print_info(f"è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
        print_info(f"éšè—å±‚å¤§å°: {config.hidden_size}")
        print_info(f"å±‚æ•°: {config.num_hidden_layers}")
        print_info(f"æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
        
        return config
        
    except Exception as e:
        print_error(f"Qwen2é…ç½®åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_causal_model_initialization(qwen_config, modules):
    """æµ‹è¯•CausalQwenæ¨¡å‹åˆå§‹åŒ–"""
    print_section("ç¬¬å››éƒ¨åˆ†ï¼šCausalQwenæ¨¡å‹åˆå§‹åŒ–æµ‹è¯•")
    
    print_step(1, "åˆ›å»ºCausalQwené…ç½®")
    try:
        CausalQwen2Config = modules['CausalQwen2Config']
        
        # åŸºäºQwené…ç½®åˆ›å»ºCausalQwené…ç½®
        causal_config = CausalQwen2Config(
            vocab_size=qwen_config.vocab_size,
            hidden_size=qwen_config.hidden_size,
            intermediate_size=qwen_config.intermediate_size,
            num_hidden_layers=qwen_config.num_hidden_layers,
            num_attention_heads=qwen_config.num_attention_heads,
            num_key_value_heads=getattr(qwen_config, 'num_key_value_heads', qwen_config.num_attention_heads),
            max_position_embeddings=qwen_config.max_position_embeddings,
            # CausalQwenç‰¹æœ‰å‚æ•°
            causal_size=qwen_config.hidden_size,
            abduction_init_strategy='identity',
            b_noise_init=0.1,
            gamma_init=10.0
        )
        print_success("CausalQwené…ç½®åˆ›å»ºæˆåŠŸ")
        
        # æ˜¾ç¤ºé…ç½®è¯¦æƒ…
        print_info(f"å› æœç»´åº¦: {causal_config.causal_size}")
        print_info(f"å½’å› åˆå§‹åŒ–ç­–ç•¥: {causal_config.abduction_init_strategy}")
        print_info(f"å™ªå£°å‚æ•°: {causal_config.b_noise_init}")
        
        return causal_config
        
    except Exception as e:
        print_error(f"CausalQwené…ç½®åˆ›å»ºå¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_model_components(causal_config, modules):
    """æµ‹è¯•å„ä¸ªæ¨¡å‹ç»„ä»¶"""
    print_section("ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹ç»„ä»¶åŠŸèƒ½æµ‹è¯•")
    
    print_step(1, "åˆå§‹åŒ–CausalQwenæ¨¡å‹")
    try:
        CausalQwenMVPForCausalLM = modules['CausalQwenMVPForCausalLM']
        
        model = CausalQwenMVPForCausalLM(causal_config)
        print_success("CausalQwenæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print_info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print_info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
        
    except Exception as e:
        print_error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_component_internals(model):
    """æµ‹è¯•ç»„ä»¶å†…éƒ¨åŠŸèƒ½"""
    print_section("ç¬¬å…­éƒ¨åˆ†ï¼šç»„ä»¶å†…éƒ¨åŠŸèƒ½æµ‹è¯•")
    
    print_step(1, "æµ‹è¯•æ•°å­¦å·¥å…·ç±»")
    try:
        from causal_qwen_mvp.models import CauchyMath
        
        # æµ‹è¯•Cauchyæ•°å­¦å‡½æ•° - ä¿®æ­£ç»´åº¦åŒ¹é…
        input_dim = 128
        output_dim = 64
        batch_size = 4
        
        loc_input = torch.randn(batch_size, input_dim)
        weight = torch.randn(output_dim, input_dim)  # ä¿®æ­£æƒé‡çŸ©é˜µç»´åº¦
        
        result_loc = CauchyMath.cauchy_linear_stable_loc(loc_input, weight)
        print_success(f"Cauchyä½ç½®å˜æ¢æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {result_loc.shape}")
        
        scale_input = torch.abs(torch.randn(batch_size, input_dim)) + 0.1  # ç¡®ä¿ä¸ºæ­£
        result_scale = CauchyMath.cauchy_linear_stable_scale(scale_input, weight)
        print_success(f"Cauchyå°ºåº¦å˜æ¢æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {result_scale.shape}")
        
    except Exception as e:
        print_error(f"æ•°å­¦å·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print_step(2, "æµ‹è¯•å½’å› ç½‘ç»œ")
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        seq_len = 10
        hidden_size = model.config.hidden_size
        
        test_input = torch.randn(batch_size, seq_len, hidden_size)
        
        # æµ‹è¯•å½’å› ç½‘ç»œ
        with torch.no_grad():
            loc_U, scale_U = model.abduction_network(test_input)
            
        print_success(f"å½’å› ç½‘ç»œæµ‹è¯•é€šè¿‡")
        print_info(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print_info(f"loc_Uè¾“å‡ºå½¢çŠ¶: {loc_U.shape}")
        print_info(f"scale_Uè¾“å‡ºå½¢çŠ¶: {scale_U.shape}")
        print_info(f"è¾“å‡ºç»Ÿè®¡: loc_Uå‡å€¼={loc_U.mean().item():.4f}, scale_Uå‡å€¼={scale_U.mean().item():.4f}")
        
    except Exception as e:
        print_error(f"å½’å› ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print_step(3, "æµ‹è¯•è¡ŒåŠ¨ç½‘ç»œ")
    try:
        # å…ˆè·å–å½’å› ç½‘ç»œè¾“å‡ºç”¨äºè¡ŒåŠ¨ç½‘ç»œè¾“å…¥
        with torch.no_grad():
            loc_U, scale_U = model.abduction_network(test_input)
            action_loc, action_scale = model.action_network(loc_U, scale_U)
            
        print_success(f"è¡ŒåŠ¨ç½‘ç»œæµ‹è¯•é€šè¿‡")
        print_info(f"loc_Uè¾“å…¥å½¢çŠ¶: {loc_U.shape}")
        print_info(f"scale_Uè¾“å…¥å½¢çŠ¶: {scale_U.shape}")
        print_info(f"loc_Sè¾“å‡ºå½¢çŠ¶: {action_loc.shape}")
        print_info(f"scale_Sè¾“å‡ºå½¢çŠ¶: {action_scale.shape}")
        print_info(f"è¾“å‡ºç»Ÿè®¡: loc_Så‡å€¼={action_loc.mean().item():.4f}, scale_Så‡å€¼={action_scale.mean().item():.4f}")
        
    except Exception as e:
        print_error(f"è¡ŒåŠ¨ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def test_inference_modes(model):
    """æµ‹è¯•ä¸‰ç§æ¨ç†æ¨¡å¼"""
    print_section("ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ¨ç†æ¨¡å¼æµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 1
    seq_len = 8
    vocab_size = model.config.vocab_size
    
    test_input_ids = torch.randint(0, min(vocab_size, 1000), (batch_size, seq_len))
    
    print_info(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_input_ids.shape}")
    print_info(f"æµ‹è¯•è¾“å…¥å†…å®¹: {test_input_ids.tolist()}")
    
    print_step(1, "æµ‹è¯•æ ‡å‡†æ¨ç†æ¨¡å¼")
    try:
        with torch.no_grad():
            standard_output = model.inference(test_input_ids, mode='standard')
            
        print_success("æ ‡å‡†æ¨ç†æ¨¡å¼æµ‹è¯•é€šè¿‡")
        print_info(f"è¾“å‡ºloc_Så½¢çŠ¶: {standard_output.loc_S.shape}")
        print_info(f"è¾“å‡ºscale_Så½¢çŠ¶: {standard_output.scale_S.shape}")
        print_info(f"loc_Sç»Ÿè®¡: å‡å€¼={standard_output.loc_S.mean().item():.4f}")
        print_info(f"scale_Sç»Ÿè®¡: å‡å€¼={standard_output.scale_S.mean().item():.4f}")
        
    except Exception as e:
        print_error(f"æ ‡å‡†æ¨ç†æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print_step(2, "æµ‹è¯•å› æœæ¨ç†æ¨¡å¼")
    try:
        with torch.no_grad():
            causal_output = model.inference(test_input_ids, mode='causal')
            
        print_success("å› æœæ¨ç†æ¨¡å¼æµ‹è¯•é€šè¿‡")
        print_info(f"è¾“å‡ºloc_Uå½¢çŠ¶: {causal_output.loc_U.shape}")
        print_info(f"è¾“å‡ºscale_Uå½¢çŠ¶: {causal_output.scale_U.shape}")
        print_info(f"loc_Uç»Ÿè®¡: å‡å€¼={causal_output.loc_U.mean().item():.4f}")
        print_info(f"scale_Uç»Ÿè®¡: å‡å€¼={causal_output.scale_U.mean().item():.4f}")
        
    except Exception as e:
        print_error(f"å› æœæ¨ç†æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print_step(3, "æµ‹è¯•å…¼å®¹æ¨ç†æ¨¡å¼")
    try:
        with torch.no_grad():
            compatible_output = model.inference(test_input_ids, mode='compatible')
            
        print_success("å…¼å®¹æ¨ç†æ¨¡å¼æµ‹è¯•é€šè¿‡")
        print_info(f"è¾“å‡ºåŒ…å«æ‰€æœ‰å­—æ®µ")
        print_info(f"loc_Så’Œloc_Uéƒ½æœ‰è¾“å‡º")
        
    except Exception as e:
        print_error(f"å…¼å®¹æ¨ç†æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def test_training_components(model, modules):
    """æµ‹è¯•è®­ç»ƒç»„ä»¶"""
    print_section("ç¬¬å…«éƒ¨åˆ†ï¼šè®­ç»ƒç»„ä»¶æµ‹è¯•")
    
    print_step(1, "æµ‹è¯•æŸå¤±è®¡ç®—")
    try:
        # åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®
        batch_size = 2
        seq_len = 8
        
        input_ids = torch.randint(0, min(model.config.vocab_size, 1000), (batch_size, seq_len))
        targets = torch.randint(0, min(model.config.vocab_size, 1000), (batch_size, seq_len))
        
        # è®¡ç®—æŸå¤±
        model.train()
        with torch.enable_grad():
            output = model.forward(input_ids, labels=targets)
            
        print_success("æŸå¤±è®¡ç®—æµ‹è¯•é€šè¿‡")
        if output.loss is not None:
            print_info(f"æŸå¤±å€¼: {output.loss.item():.6f}")
        else:
            print_warning("æŸå¤±å€¼ä¸ºNoneï¼Œéœ€æ£€æŸ¥å®ç°")
            
    except Exception as e:
        print_error(f"æŸå¤±è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print_step(2, "æµ‹è¯•æ¢¯åº¦è®¡ç®—")
    try:
        # æµ‹è¯•åå‘ä¼ æ’­
        if output.loss is not None:
            output.loss.backward()
            
            # æ£€æŸ¥å‡ ä¸ªå…³é”®å‚æ•°çš„æ¢¯åº¦
            grad_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_count += 1
                    if grad_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print_info(f"{name}: æ¢¯åº¦èŒƒæ•°={param.grad.norm().item():.6f}")
            
            print_success(f"æ¢¯åº¦è®¡ç®—æµ‹è¯•é€šè¿‡ï¼Œ{grad_count}ä¸ªå‚æ•°æœ‰æ¢¯åº¦")
        else:
            print_warning("è·³è¿‡æ¢¯åº¦æµ‹è¯•ï¼ˆæ— æŸå¤±å€¼ï¼‰")
            
    except Exception as e:
        print_error(f"æ¢¯åº¦è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def test_end_to_end():
    """ç«¯åˆ°ç«¯åŠŸèƒ½æµ‹è¯•"""
    print_section("ç¬¬ä¹éƒ¨åˆ†ï¼šç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯")
    
    print_step(1, "åˆ›å»ºæœ€å°ç¤ºä¾‹")
    try:
        # é‡æ–°åˆ›å»ºä¸€ä¸ªå°æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•
        from causal_qwen_mvp import CausalQwenMVPForCausalLM, CausalQwen2Config
        
        mini_config = CausalQwen2Config(
            vocab_size=100,
            hidden_size=64,
            intermediate_size=256,
            num_hidden_layers=2,
            num_attention_heads=4,
            num_key_value_heads=4,
            max_position_embeddings=512,
            causal_size=64
        )
        
        mini_model = CausalQwenMVPForCausalLM(mini_config)
        print_success("æœ€å°æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # å¿«é€ŸåŠŸèƒ½æµ‹è¯•
        test_ids = torch.randint(0, 100, (1, 5))
        
        with torch.no_grad():
            output1 = mini_model.inference(test_ids, mode='standard')
            output2 = mini_model.inference(test_ids, mode='causal')
            output3 = mini_model.inference(test_ids, mode='compatible')
        
        print_success("ä¸‰ç§æ¨¡å¼éƒ½èƒ½æ­£å¸¸è¿è¡Œ")
        print_info("ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print_error(f"ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_section("CausalQwen å…¨é¢ç»„ä»¶æµ‹è¯•", Colors.PURPLE)
    print_info("å¼€å§‹é€æ­¥æµ‹è¯•æ‰€æœ‰ç»„ä»¶åŠŸèƒ½...")
    
    start_time = time.time()
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒæ£€æŸ¥
    if not test_environment():
        print_error("ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å—å¯¼å…¥
    modules = test_imports()
    if modules is None:
        print_error("æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šQwenæ¨¡å‹åŠ è½½
    qwen_config = test_qwen_loading()
    if qwen_config is None:
        print_warning("Qwenæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ç»§ç»­æµ‹è¯•")
        from transformers import Qwen2Config
        qwen_config = Qwen2Config(
            vocab_size=151936,
            hidden_size=896,
            intermediate_size=4864,
            num_hidden_layers=24,
            num_attention_heads=14,
            num_key_value_heads=2
        )
    
    # ç¬¬å››éƒ¨åˆ†ï¼šCausalQwené…ç½®åˆ›å»º
    causal_config = test_causal_model_initialization(qwen_config, modules)
    if causal_config is None:
        print_error("CausalQwené…ç½®åˆ›å»ºå¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹ç»„ä»¶æµ‹è¯•
    model = test_model_components(causal_config, modules)
    if model is None:
        print_error("æ¨¡å‹ç»„ä»¶æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    # ç¬¬å…­éƒ¨åˆ†ï¼šç»„ä»¶å†…éƒ¨æµ‹è¯•
    test_component_internals(model)
    
    # ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ¨ç†æ¨¡å¼æµ‹è¯•
    test_inference_modes(model)
    
    # ç¬¬å…«éƒ¨åˆ†ï¼šè®­ç»ƒç»„ä»¶æµ‹è¯•
    test_training_components(model, modules)
    
    # ç¬¬ä¹éƒ¨åˆ†ï¼šç«¯åˆ°ç«¯æµ‹è¯•
    test_end_to_end()
    
    # æµ‹è¯•æ€»ç»“
    end_time = time.time()
    print_section("æµ‹è¯•å®Œæˆ", Colors.GREEN)
    print_success(f"æ€»æµ‹è¯•æ—¶é—´: {end_time - start_time:.2f} ç§’")
    print_info("ğŸ‰ CausalQwenç»„ä»¶æµ‹è¯•å…¨éƒ¨å®Œæˆï¼")
    print_info("ğŸ‘€ è¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡ºï¼Œç¡®è®¤å„ç»„ä»¶åŠŸèƒ½æ­£å¸¸")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
CausalQwen MVP æ¡†æ¶å¿«é€Ÿæµ‹è¯•è„šæœ¬

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åˆå§‹åŒ–
2. éªŒè¯å‰å‘ä¼ æ’­èƒ½å¤Ÿæ­£å¸¸è¿›è¡Œ
3. éªŒè¯ä¸‰ç§æ¨ç†æ¨¡å¼éƒ½èƒ½å·¥ä½œ
4. éªŒè¯åŸºç¡€çš„æ¢¯åº¦è®¡ç®—

è¿™æ˜¯ä¸€ä¸ªå ä½å¼æµ‹è¯•ï¼Œä¸»è¦ç¡®ä¿æ¡†æ¶æ¶æ„æ­£ç¡®ï¼Œå…·ä½“æ•°å­¦å®ç°åç»­å®Œå–„ã€‚
"""

import sys
import os
import torch
import traceback
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

try:
    from causal_qwen_mvp import (
        CausalQwenMVPForCausalLM, 
        CausalQwen2Config,
        CausalInferenceEngine,
        InferenceValidator,
        get_model_info
    )
    print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("\nğŸ”§ æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–...")
    
    try:
        # åˆ›å»ºå°å‹é…ç½®è¿›è¡Œæµ‹è¯•
        config = CausalQwen2Config(
            vocab_size=1000,
            hidden_size=128,
            intermediate_size=512,
            num_hidden_layers=2,
            num_attention_heads=4,
            num_key_value_heads=4,  # æ·»åŠ è¿™ä¸ªå‚æ•°
            max_position_embeddings=512,
            causal_size=128,  # å› æœæ¨¡å‹å‚æ•°
            rms_norm_eps=1e-6,
            initializer_range=0.02,
            use_cache=True,
            rope_theta=10000.0,
            hidden_act="silu",
        )
        
        model = CausalQwenMVPForCausalLM(config)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        assert hasattr(model, 'abduction_network')
        assert hasattr(model, 'action_network') 
        assert hasattr(model, 'ovr_classifier')
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print(traceback.format_exc())
        return None


def test_forward_pass(model):
    """æµ‹è¯•å‰å‘ä¼ æ’­å’Œæ•°å­¦æ­£ç¡®æ€§"""
    print("\nğŸš€ æµ‹è¯•å‰å‘ä¼ æ’­...")
    
    try:
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size, seq_len = 2, 10
        input_ids = torch.randint(0, model.config.vocab_size, (batch_size, seq_len))
        labels = torch.randint(0, model.config.vocab_size, (batch_size, seq_len))
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(input_ids=input_ids, labels=labels)
        
        # æ£€æŸ¥è¾“å‡ºç»“æ„
        assert outputs.loss is not None
        assert outputs.loc_S is not None
        assert outputs.scale_S is not None
        assert outputs.loc_U is not None
        assert outputs.scale_U is not None
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - æŸå¤±: {outputs.loss.item():.4f}")
        print(f"   - loc_S shape: {outputs.loc_S.shape}")
        print(f"   - scale_S shape: {outputs.scale_S.shape}")
        print(f"   - loc_U shape: {outputs.loc_U.shape}")
        print(f"   - scale_U shape: {outputs.scale_U.shape}")
        
        # ğŸ” æ•°å­¦æ­£ç¡®æ€§æ£€æŸ¥
        errors = []
        
        # æ£€æŸ¥1: Cauchyåˆ†å¸ƒå‚æ•°çº¦æŸ
        if not torch.all(outputs.scale_U > 0):
            errors.append("âŒ scale_UåŒ…å«éæ­£å€¼ï¼Œè¿åCauchyåˆ†å¸ƒçº¦æŸ")
        if not torch.all(outputs.scale_S > 0):
            errors.append("âŒ scale_SåŒ…å«éæ­£å€¼ï¼Œè¿åCauchyåˆ†å¸ƒçº¦æŸ")
        
        # æ£€æŸ¥2: æ•°å€¼ç¨³å®šæ€§
        if torch.any(torch.isnan(outputs.loc_U)) or torch.any(torch.isinf(outputs.loc_U)):
            errors.append("âŒ loc_UåŒ…å«NaNæˆ–Infå€¼")
        if torch.any(torch.isnan(outputs.scale_U)) or torch.any(torch.isinf(outputs.scale_U)):
            errors.append("âŒ scale_UåŒ…å«NaNæˆ–Infå€¼")
        if torch.any(torch.isnan(outputs.loc_S)) or torch.any(torch.isinf(outputs.loc_S)):
            errors.append("âŒ loc_SåŒ…å«NaNæˆ–Infå€¼")
        if torch.any(torch.isnan(outputs.scale_S)) or torch.any(torch.isinf(outputs.scale_S)):
            errors.append("âŒ scale_SåŒ…å«NaNæˆ–Infå€¼")
        
        if errors:
            print("\nâš ï¸  å‘ç°æ•°å­¦é”™è¯¯:")
            for error in errors:
                print(f"   {error}")
            return False
        
        print("âœ… æ•°å­¦æ­£ç¡®æ€§æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        print(traceback.format_exc())
        return False


def test_inference_modes(model):
    """æµ‹è¯•ä¸‰ç§æ¨ç†æ¨¡å¼"""
    print("\nğŸ¯ æµ‹è¯•æ¨ç†æ¨¡å¼...")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    input_ids = torch.randint(0, model.config.vocab_size, (1, 5))
    
    modes = ['standard', 'causal', 'compatible']
    results = {}
    
    for mode in modes:
        try:
            with torch.no_grad():
                output = model.inference(input_ids, mode=mode)
            results[mode] = output
            print(f"âœ… {mode}æ¨ç†æ¨¡å¼: è¾“å‡ºshape {output.shape}")
        except Exception as e:
            print(f"âŒ {mode}æ¨ç†æ¨¡å¼å¤±è´¥: {e}")
            results[mode] = None
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    successful_modes = [mode for mode, result in results.items() if result is not None]
    if len(successful_modes) >= 2:
        print(f"âœ… {len(successful_modes)}/3 æ¨ç†æ¨¡å¼æˆåŠŸ")
    else:
        print(f"âš ï¸  åªæœ‰ {len(successful_modes)}/3 æ¨ç†æ¨¡å¼æˆåŠŸ")
    
    return results


def test_generation(model):
    """æµ‹è¯•åºåˆ—ç”Ÿæˆ"""
    print("\nğŸ“ æµ‹è¯•åºåˆ—ç”Ÿæˆ...")
    
    try:
        input_ids = torch.randint(0, model.config.vocab_size, (1, 3))
        
        # æµ‹è¯•ç”Ÿæˆ
        with torch.no_grad():
            generated = model.generate_step_by_step(
                input_ids, 
                max_length=10, 
                mode='standard'
            )
        
        print(f"âœ… åºåˆ—ç”ŸæˆæˆåŠŸ")
        print(f"   - è¾“å…¥é•¿åº¦: {input_ids.shape[-1]}")
        print(f"   - ç”Ÿæˆé•¿åº¦: {generated.shape[-1]}")
        print(f"   - è¾“å…¥tokens: {input_ids[0].tolist()}")
        print(f"   - ç”Ÿæˆtokens: {generated[0].tolist()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åºåˆ—ç”Ÿæˆå¤±è´¥: {e}")
        print(traceback.format_exc())
        return False


def test_gradient_computation(model):
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—"""
    print("\nğŸ“ˆ æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        input_ids = torch.randint(0, model.config.vocab_size, (2, 8))
        labels = torch.randint(0, model.config.vocab_size, (2, 8))
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # å‰å‘ä¼ æ’­
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = False
        for name, param in model.named_parameters():
            if param.grad is not None:
                has_grad = True
                break
        
        assert has_grad, "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¢¯åº¦"
        
        print(f"âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
        print(f"   - è®­ç»ƒæŸå¤±: {loss.item():.4f}")
        
        # æ¸…ç†æ¢¯åº¦
        model.zero_grad()
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
        print(traceback.format_exc())
        return False


def test_mathematical_implementation(model):
    """æµ‹è¯•æ•°å­¦å®ç°çš„ä¸¥æ ¼æ€§"""
    print("\nğŸ§® æµ‹è¯•æ•°å­¦å®ç°ç¬¦åˆæ€§...")
    
    errors = []
    
    # æ£€æŸ¥1: AbductionNetworkçš„biasé¡¹
    abduction = model.abduction_network
    if abduction.loc_net.bias is None:
        errors.append("âŒ AbductionNetwork.loc_netç¼ºå°‘biasé¡¹ï¼Œè¿åè®¾è®¡æ–‡æ¡£")
    if abduction.scale_net.bias is None:
        errors.append("âŒ AbductionNetwork.scale_netç¼ºå°‘biasé¡¹ï¼Œè¿åè®¾è®¡æ–‡æ¡£")
    
    # æ£€æŸ¥2: ActionNetworkçš„biasé¡¹
    action = model.action_network
    if action.lm_head.bias is None:
        errors.append("âŒ ActionNetwork.lm_headç¼ºå°‘biasé¡¹ï¼Œè¿åè®¾è®¡æ–‡æ¡£")
    
    # æ£€æŸ¥3: b_noiseçš„ç»´åº¦
    expected_b_noise_size = model.config.causal_size
    actual_b_noise_size = action.b_noise.size(0)
    if actual_b_noise_size != expected_b_noise_size:
        errors.append(f"âŒ ActionNetwork.b_noiseç»´åº¦é”™è¯¯: åº”è¯¥æ˜¯[{expected_b_noise_size}], å®é™…æ˜¯[{actual_b_noise_size}]")
    
    # æ£€æŸ¥4: æ’ç­‰æ˜ å°„åˆå§‹åŒ–
    if model.config.causal_size == model.config.hidden_size:
        expected_identity = torch.eye(model.config.causal_size)
        if not torch.allclose(abduction.loc_net.weight, expected_identity, atol=1e-5):
            errors.append("âŒ AbductionNetwork.loc_netæœªæ­£ç¡®åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„")
    
    # æ£€æŸ¥5: æµ‹è¯•softplusæ¿€æ´»å‡½æ•°
    test_input = torch.randn(2, 10, model.config.hidden_size)
    with torch.no_grad():
        # æ£€æŸ¥æ˜¯å¦åœ¨forwardä¸­ä½¿ç”¨äº†æ­£ç¡®çš„softplus
        scale_raw = abduction.scale_net(test_input)
        loc_U, scale_U = abduction(test_input)
        expected_softplus = torch.nn.functional.softplus(scale_raw)
        
        # æ­£ç¡®çš„æ£€æŸ¥ï¼šscale_Uåº”è¯¥ç­‰äºsoftplus(scale_raw)
        if not torch.allclose(scale_U, expected_softplus, atol=1e-5):
            errors.append("âŒ AbductionNetworkæœªä½¿ç”¨softplusæ¿€æ´»å‡½æ•°")
        
        # æ£€æŸ¥æ˜¯å¦é”™è¯¯ä½¿ç”¨äº†abs+eps (éœ€è¦æ›´ä¸¥æ ¼çš„æ£€æŸ¥)
        wrong_abs_eps = torch.abs(scale_raw) + 1e-6
        if torch.allclose(scale_U, wrong_abs_eps, atol=1e-7, rtol=1e-6):
            errors.append("âŒ AbductionNetworké”™è¯¯ä½¿ç”¨torch.abs+1e-6è€Œésoftplusæ¿€æ´»")
    
    # æ£€æŸ¥6: çº¿æ€§ç¨³å®šæ€§å®ç°
    try:
        loc_U = torch.randn(2, 5, model.config.causal_size)
        scale_U = torch.abs(torch.randn(2, 5, model.config.causal_size)) + 0.1
        
        loc_S, scale_S = action(loc_U, scale_U)
        
        # éªŒè¯ä½ç½®å‚æ•°å˜æ¢æ˜¯å¦æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯æ ‡å‡†çº¿æ€§å˜æ¢ï¼‰
        expected_loc_S = action.lm_head(loc_U)
        if not torch.allclose(loc_S, expected_loc_S, atol=1e-5):
            errors.append("âŒ ActionNetworkä½ç½®å‚æ•°å˜æ¢ä¸æ­£ç¡®")
            
        # éªŒè¯å°ºåº¦å‚æ•°å˜æ¢æ˜¯å¦ä½¿ç”¨äº†çº¿æ€§ç¨³å®šæ€§
        scale_U_noisy = scale_U + torch.abs(action.b_noise)
        expected_scale_S = torch.matmul(scale_U_noisy, torch.abs(action.lm_head.weight).T)
        if not torch.allclose(scale_S, expected_scale_S, atol=1e-5):
            errors.append("âŒ ActionNetworkå°ºåº¦å‚æ•°å˜æ¢ä¸ç¬¦åˆçº¿æ€§ç¨³å®šæ€§")
            
    except Exception as e:
        errors.append(f"âŒ çº¿æ€§ç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {e}")
    
    if errors:
        print("ğŸš¨ å‘ç°æ•°å­¦å®ç°é”™è¯¯:")
        for error in errors:
            print(f"   {error}")
        print("\nğŸ’¡ è¿™äº›é”™è¯¯è¯´æ˜å½“å‰å®ç°ä¸ç¬¦åˆdesign-docs/causal_qwen.mdçš„æ•°å­¦è¦æ±‚")
        return False
    else:
        print("âœ… æ•°å­¦å®ç°æ£€æŸ¥é€šè¿‡")
        return True


def test_inference_validator(model):
    """æµ‹è¯•æ¨ç†éªŒè¯å™¨"""
    print("\nğŸ” æµ‹è¯•æ¨ç†éªŒè¯å™¨...")
    
    try:
        validator = InferenceValidator(model)
        input_ids = torch.randint(0, model.config.vocab_size, (1, 5))
        
        # æµ‹è¯•ä¸€è‡´æ€§éªŒè¯
        results = validator.validate_inference_consistency(input_ids, num_samples=3)
        
        print(f"âœ… æ¨ç†éªŒè¯å™¨æµ‹è¯•æˆåŠŸ")
        print(f"   - æµ‹è¯•äº† {len(results)} ç§æ¨ç†æ¨¡å¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†éªŒè¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¬ å¼€å§‹CausalQwen MVPæ¡†æ¶æµ‹è¯•")
    print("="*50)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    info = get_model_info()
    print(f"ğŸ“‹ {info['name']} v{info['version']}")
    print(f"ğŸ“„ {info['description']}")
    print(f"âš¡ çŠ¶æ€: {info['status']}")
    
    # æµ‹è¯•åºåˆ—
    test_results = {}
    
    # 1. æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
    model = test_model_initialization()
    test_results['initialization'] = model is not None
    
    if model is None:
        print("\nâŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")
        return
    
    # 2. å‰å‘ä¼ æ’­æµ‹è¯•
    test_results['forward_pass'] = test_forward_pass(model)
    
    # 3. æ¨ç†æ¨¡å¼æµ‹è¯•
    inference_results = test_inference_modes(model)
    test_results['inference_modes'] = any(r is not None for r in inference_results.values())
    
    # 4. åºåˆ—ç”Ÿæˆæµ‹è¯•
    test_results['generation'] = test_generation(model)
    
    # 5. æ¢¯åº¦è®¡ç®—æµ‹è¯•
    test_results['gradient_computation'] = test_gradient_computation(model)
    
    # 6. æ•°å­¦å®ç°ç¬¦åˆæ€§æµ‹è¯•
    test_results['mathematical_implementation'] = test_mathematical_implementation(model)
    
    # 7. æ¨ç†éªŒè¯å™¨æµ‹è¯•
    test_results['inference_validator'] = test_inference_validator(model)
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed = sum(test_results.values())
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name:20}: {status}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MVPæ¡†æ¶åŸºç¡€åŠŸèƒ½æ­£å¸¸")
    elif passed >= total * 0.8:
        print("âš ï¸  å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œæ¡†æ¶åŸºæœ¬å¯ç”¨")
    else:
        print("âŒ å¤šä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ¡†æ¶å®ç°")
    
    # ä¸‹ä¸€æ­¥å»ºè®®
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    for step in info['next_steps']:
        print(f"   - {step}")


if __name__ == "__main__":
    main() 
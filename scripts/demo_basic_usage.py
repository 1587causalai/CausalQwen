#!/usr/bin/env python3
"""
CausalQwen ç®€å•æ¼”ç¤º - å®Œå…¨å…¼å®¹Qwenæ¥å£

æ¼”ç¤ºCausalQwençš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä¸Qwenå®Œå…¨ç›¸åŒçš„ä½¿ç”¨æ–¹å¼
2. do_sampleå‚æ•°æ§åˆ¶V2æ ¸å¿ƒè¡Œä¸º
3. å®Œæ•´çš„ç”Ÿæˆèƒ½åŠ›å±•ç¤º
"""

import sys
import os
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

def main():
    print("ğŸš€ CausalQwen ç®€å•æ¼”ç¤º - ä¸Qwenå®Œå…¨å…¼å®¹")
    print("="*50)
    
    # 1. åˆ›å»ºæ¨¡å‹ï¼ˆä¸Qwenç›¸åŒæ–¹å¼ï¼‰
    print("\nğŸ“¦ åˆ›å»ºCausalQwenæ¨¡å‹...")
    try:
        from causal_qwen_mvp import CausalQwenMVPForCausalLM, CausalQwen2Config
        
        # å°å‹æ¼”ç¤ºé…ç½®
        config = CausalQwen2Config(
            vocab_size=1000,
            hidden_size=64,
            intermediate_size=256,
            num_hidden_layers=2,
            num_attention_heads=4,
            num_key_value_heads=4,
            max_position_embeddings=512,
            causal_size=64  # CausalQwenç‰¹æœ‰å‚æ•°
        )
        
        model = CausalQwenMVPForCausalLM(config)
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # 2. å‡†å¤‡è¾“å…¥
    print("\nğŸ”¤ å‡†å¤‡æµ‹è¯•è¾“å…¥...")
    input_ids = torch.randint(0, 1000, (1, 8))
    print(f"è¾“å…¥åºåˆ—: {input_ids.tolist()}")
    
    # 3. ç¡®å®šæ€§ç”Ÿæˆï¼ˆä¸Qwenç›¸åŒï¼‰
    print("\nğŸ¯ ç¡®å®šæ€§ç”Ÿæˆ (do_sample=False)")
    print("   V2åŸç†: å™ªå£°å½±å“å°ºåº¦å‚æ•°ï¼Œå¢åŠ å†³ç­–ä¸ç¡®å®šæ€§")
    
    try:
        det_output = model.generate(
            input_ids,
            max_new_tokens=5,
            do_sample=False,  # å…³é”®å‚æ•°
            temperature=1.0   # ç¡®å®šæ€§æ¨¡å¼ä¸‹æ— æ•ˆ
        )
        
        new_tokens = det_output[0, input_ids.shape[1]:].tolist()
        print(f"   ç”Ÿæˆç»“æœ: {new_tokens}")
        print("âœ… ç¡®å®šæ€§ç”ŸæˆæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ ç¡®å®šæ€§ç”Ÿæˆå¤±è´¥: {e}")
    
    # 4. é‡‡æ ·ç”Ÿæˆï¼ˆä¸Qwenç›¸åŒï¼‰
    print("\nğŸ² é‡‡æ ·ç”Ÿæˆ (do_sample=True)")
    print("   V2åŸç†: å™ªå£°å½±å“ä½ç½®å‚æ•°ï¼Œæ‰°åŠ¨ä¸ªä½“èº«ä»½")
    
    try:
        samp_output = model.generate(
            input_ids,
            max_new_tokens=5,
            do_sample=True,      # å…³é”®å‚æ•°
            temperature=0.8,     # æ§åˆ¶éšæœºæ€§
            top_k=50,           # Top-Ké‡‡æ ·
            top_p=0.9           # Nucleusé‡‡æ ·
        )
        
        new_tokens = samp_output[0, input_ids.shape[1]:].tolist()
        print(f"   ç”Ÿæˆç»“æœ: {new_tokens}")
        print("âœ… é‡‡æ ·ç”ŸæˆæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é‡‡æ ·ç”Ÿæˆå¤±è´¥: {e}")
    
    # 5. å¯¹æ¯”ä¸åŒæ¸©åº¦æ•ˆæœ
    print("\nğŸŒ¡ï¸  æ¸©åº¦æ•ˆæœå¯¹æ¯”")
    temperatures = [0.0, 0.1, 0.5, 1.0, 1.5, 2.0]  # æ·»åŠ æ¸©åº¦ä¸ºé›¶çš„æµ‹è¯•
    
    for temp in temperatures:
        try:
            torch.manual_seed(42)  # å›ºå®šéšæœºç§å­ä¾¿äºå¯¹æ¯”
            temp_output = model.generate(
                input_ids,
                max_new_tokens=3,
                do_sample=True,
                temperature=temp,
                top_k=100
            )
            
            new_tokens = temp_output[0, input_ids.shape[1]:].tolist()
            print(f"   T={temp}: {new_tokens}")
            
            # ç‰¹åˆ«è¯´æ˜æ¸©åº¦ä¸ºé›¶çš„é‡è¦æ€§
            if temp == 0.0:
                pass
                # print("   ğŸŒ¡ï¸ æ¸©åº¦ä¸ºé›¶æ˜¯æå…¶é‡è¦çš„è¾¹ç•Œæ¡ä»¶ï¼")
            
        except Exception as e:
            print(f"   T={temp}: å¤±è´¥ - {e}")
    
    # 6. éªŒè¯V2æ•°å­¦åŸç†
    print("\nğŸ§® V2æ•°å­¦åŸç†éªŒè¯")
    try:
        from causal_qwen_mvp import InferenceValidator
        
        validator = InferenceValidator(model)
        results = validator.validate_v2_principles(input_ids)
        
        pos_diff = results['position_difference'].item()
        scale_diff = results['scale_difference'].item()
        
        print(f"   ä½ç½®å‚æ•°å·®å¼‚: {pos_diff:.6f}")
        print(f"   å°ºåº¦å‚æ•°å·®å¼‚: {scale_diff:.6f}")
        
        if pos_diff > 1e-3:
            print("âœ… V2æ•°å­¦åŸç†éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸  ä½ç½®å‚æ•°å·®å¼‚è¾ƒå°")
            
    except Exception as e:
        print(f"âŒ V2éªŒè¯å¤±è´¥: {e}")
    
    # 7. æ‰¹é‡ç”Ÿæˆæ¼”ç¤º
    print("\nğŸ“¦ æ‰¹é‡ç”Ÿæˆæ¼”ç¤º")
    try:
        batch_input = torch.randint(0, 1000, (3, 6))  # 3ä¸ªåºåˆ—
        
        batch_output = model.generate(
            batch_input,
            max_new_tokens=4,
            do_sample=True,
            temperature=1.0,
            top_k=50
        )
        
        print("   æ‰¹é‡è¾“å…¥:")
        for i, seq in enumerate(batch_input):
            print(f"     åºåˆ—{i}: {seq.tolist()}")
        
        print("   æ‰¹é‡è¾“å‡º:")
        for i, seq in enumerate(batch_output):
            new_part = seq[batch_input.shape[1]:].tolist()
            print(f"     åºåˆ—{i}: {new_part}")
        
        print("âœ… æ‰¹é‡ç”ŸæˆæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
    
    # 8. æ€»ç»“
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("="*50)
    print("CausalQwenæ ¸å¿ƒç‰¹æ€§:")
    print("â”œâ”€ å®Œå…¨å…¼å®¹Qwenæ¥å£ï¼šgenerate(), do_sample, temperatureç­‰")
    print("â”œâ”€ V2æ ¸å¿ƒåˆ›æ–°ï¼šä½ç½®vså°ºåº¦çš„ç²¾å¦™å·®å¼‚")
    print("â”œâ”€ do_sample=False: å™ªå£°å½±å“å°ºåº¦å‚æ•°ï¼ˆç¡®å®šæ€§+ä¸ç¡®å®šæ€§ï¼‰")
    print("â”œâ”€ do_sample=True: å™ªå£°å½±å“ä½ç½®å‚æ•°ï¼ˆæ‰°åŠ¨ä¸ªä½“èº«ä»½ï¼‰")
    print("â””â”€ å®Œæ•´çš„æŸ¯è¥¿åˆ†å¸ƒæ•°å­¦åŸºç¡€")
    
    print("\nä½¿ç”¨æ–¹æ³•ï¼ˆä¸Qwenå®Œå…¨ç›¸åŒï¼‰:")
    print("```python")
    print("from causal_qwen_mvp import CausalQwenMVPForCausalLM")
    print("model = CausalQwenMVPForCausalLM.from_pretrained('path')")
    print("output = model.generate(input_ids, do_sample=True, temperature=0.8)")
    print("```")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
åŸºäºç¬¬ä¸€æ€§åŸç†çš„åˆå§‹åŒ–ç­–ç•¥æµ‹è¯•è„šæœ¬

æœ¬è„šæœ¬æµ‹è¯•ä¿®æ”¹åçš„ActionNetworkåˆå§‹åŒ–ï¼ŒéªŒè¯ï¼š
1. æ‰€æœ‰åç½®éƒ½è¢«æ­£ç¡®è®¾ç½®ä¸º0ï¼ˆç§»é™¤é­”æ³•æ•°å­—ï¼‰
2. åˆå§‹æ¦‚ç‡åˆ†å¸ƒæ›´åŠ å‡åŒ€å’Œå…¬å¹³
3. <NUM> tokenä¸å†è¢«äººä¸ºæŠ‘åˆ¶
4. å›å½’å¤´ä¸å†ä½¿ç”¨æ•°æ®ä¾èµ–çš„åç½®
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
from src.models.causal_lm import CausalLanguageModel, CausalLMConfig
from src.data.tokenizer import QwenTokenizerWrapper

def test_first_principles_initialization():
    """æµ‹è¯•åŸºäºç¬¬ä¸€æ€§åŸç†çš„åˆå§‹åŒ–ç­–ç•¥"""
    
    print("=" * 80)
    print("=   åŸºäºç¬¬ä¸€æ€§åŸç†çš„åˆå§‹åŒ–ç­–ç•¥æµ‹è¯•   =")
    print("=" * 80)
    
    # 1. è®¾ç½®æ¨¡å‹å’Œé…ç½®
    print("\n[æ­¥éª¤ 1] è®¾ç½®æ¨¡å‹å’Œé…ç½®...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = QwenTokenizerWrapper(model_path="/Users/gongqian/models/Qwen2.5-0.5B", use_real_tokenizer=True)
    print(f"åŠ è½½åˆ†è¯å™¨å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # åˆ›å»ºé…ç½®
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,  # C=H çº¦æŸ
        use_mock_feature_network=False,  # ä½¿ç”¨çœŸå®Qwenæ¨¡å‹
        ovr_threshold=10.0
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = CausalLanguageModel(config)
    print(f"æ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    # 2. æ‰§è¡Œåˆå§‹åŒ–å¹¶éªŒè¯
    print("\n[æ­¥éª¤ 2] æ‰§è¡Œç¬¬ä¸€æ€§åŸç†åˆå§‹åŒ–...")
    
    # ä½¿ç”¨è™šæ‹Ÿçš„æ•°æ®ç»Ÿè®¡å€¼ï¼ˆåœ¨æ–°ç­–ç•¥ä¸‹è¿™äº›å€¼ä¼šè¢«å¿½ç•¥ï¼‰
    dummy_median = 50.0
    dummy_scale = 25.0
    
    model.init_weights(dummy_median, dummy_scale)
    
    # 3. éªŒè¯åˆå§‹åŒ–ç»“æœ
    print("\n[æ­¥éª¤ 3] éªŒè¯ç¬¬ä¸€æ€§åŸç†åˆå§‹åŒ–ç»“æœ...")
    
    # 3.1 éªŒè¯åˆ†ç±»å¤´çš„åç½®
    cls_head = model.action_network.classification_head.causal_linear
    cls_bias = cls_head.bias.data if cls_head.bias is not None else None
    
    print(f"\nğŸ” åˆ†ç±»å¤´åç½®éªŒè¯:")
    if cls_bias is not None:
        bias_zero_check = torch.allclose(cls_bias, torch.zeros_like(cls_bias), atol=1e-6)
        print(f"  æ‰€æœ‰åç½®æ˜¯å¦ä¸º0: {'âœ…' if bias_zero_check else 'âŒ'}")
        print(f"  åç½®ç»Ÿè®¡: å‡å€¼={cls_bias.mean().item():.6f}, æœ€å¤§ç»å¯¹å€¼={cls_bias.abs().max().item():.6f}")
        
        # ç‰¹åˆ«æ£€æŸ¥<NUM> tokençš„åç½®
        num_bias = cls_bias[config.num_token_id].item()
        print(f"  <NUM> token (ID: {config.num_token_id}) åç½®: {num_bias:.6f} (åº”è¯¥æ˜¯0.0)")
        
        if bias_zero_check:
            print(f"  âœ… ç¬¬ä¸€æ€§åŸç†éªŒè¯é€šè¿‡: æ— äººä¸ºåè§ï¼Œå…¬å¹³èµ·ç‚¹")
        else:
            print(f"  âŒ æ£€æµ‹åˆ°éé›¶åç½®ï¼Œå¯èƒ½ä»æœ‰é­”æ³•æ•°å­—")
    else:
        print(f"  âŒ åˆ†ç±»å¤´æ²¡æœ‰åç½®å±‚")
    
    # 3.2 éªŒè¯å›å½’å¤´çš„åç½®
    reg_head = model.action_network.regression_head.causal_linear
    reg_bias = reg_head.bias.data if reg_head.bias is not None else None
    
    print(f"\nğŸ” å›å½’å¤´åç½®éªŒè¯:")
    if reg_bias is not None:
        reg_bias_zero = torch.allclose(reg_bias, torch.zeros_like(reg_bias), atol=1e-6)
        print(f"  å›å½’åç½®æ˜¯å¦ä¸º0: {'âœ…' if reg_bias_zero else 'âŒ'}")
        print(f"  å›å½’åç½®å€¼: {reg_bias.item():.6f} (åº”è¯¥æ˜¯0.0ï¼Œè€Œéæ•°æ®ä¸­ä½æ•°50.0)")
        
        if reg_bias_zero:
            print(f"  âœ… ç¬¬ä¸€æ€§åŸç†éªŒè¯é€šè¿‡: æ— æ•°æ®ä¾èµ–åç½®ï¼Œçº¯å‡€å­¦ä¹ ")
        else:
            print(f"  âŒ æ£€æµ‹åˆ°éé›¶å›å½’åç½®ï¼Œå¯èƒ½ä»ä½¿ç”¨æ•°æ®ç»Ÿè®¡é‡")
    else:
        print(f"  âŒ å›å½’å¤´æ²¡æœ‰åç½®å±‚")
    
    # 4. æµ‹è¯•å‰å‘ä¼ æ’­ï¼ŒéªŒè¯åˆå§‹æ¦‚ç‡åˆ†å¸ƒ
    print(f"\n[æ­¥éª¤ 4] æµ‹è¯•åˆå§‹æ¦‚ç‡åˆ†å¸ƒ...")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.tensor([[1, 2, 3, config.num_token_id, 5]])  # åŒ…å«<NUM> token
    test_nums = torch.tensor([[0.0, 0.0, 0.0, 99.99, 0.0]])
    
    with torch.no_grad():
        outputs = model(test_input, test_nums)
        
        # è®¡ç®—OvRæ¦‚ç‡
        cls_loc = outputs['cls_loc']
        cls_scale = outputs['cls_scale']
        probs = model.action_network.classification_head.compute_probabilities(cls_loc, cls_scale)
        
        # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒçš„å‡åŒ€æ€§
        sample_pos = 3  # <NUM> tokençš„ä½ç½®
        probs_at_num = probs[0, sample_pos, :]  # åœ¨<NUM>ä½ç½®çš„æ¦‚ç‡
        
        prob_mean = probs_at_num.mean().item()
        prob_std = probs_at_num.std().item()
        num_token_prob = probs_at_num[config.num_token_id].item()
        
        print(f"\nğŸ” åˆå§‹æ¦‚ç‡åˆ†å¸ƒåˆ†æ (ä½ç½® {sample_pos}, <NUM> token):")
        print(f"  å¹³å‡æ¦‚ç‡: {prob_mean:.4f}")
        print(f"  æ¦‚ç‡æ ‡å‡†å·®: {prob_std:.4f}")
        print(f"  <NUM> tokenæ¦‚ç‡: {num_token_prob:.4f}")
        print(f"  æ¦‚ç‡èŒƒå›´: [{probs_at_num.min().item():.4f}, {probs_at_num.max().item():.4f}]")
        
        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼ˆåœ¨é«˜é˜ˆå€¼ä¸‹ï¼Œæ‰€æœ‰æ¦‚ç‡åº”è¯¥éƒ½æ¯”è¾ƒå°ä¸”ç›¸è¿‘ï¼‰
        expected_prob_near_threshold = 0.5  # å½“loc=0, scaleè¾ƒå¤§æ—¶ï¼Œæ¦‚ç‡åº”è¯¥æ¥è¿‘0.5
        uniform_check = abs(prob_mean - 0.5) < 0.1  # å…è®¸ä¸€å®šçš„åå·®
        
        if uniform_check:
            print(f"  âœ… ç¬¬ä¸€æ€§åŸç†éªŒè¯é€šè¿‡: åˆå§‹åˆ†å¸ƒæ¥è¿‘å‡åŒ€ï¼Œæ— äººä¸ºåå¥½")
        else:
            print(f"  âš ï¸  åˆå§‹åˆ†å¸ƒå¯èƒ½ä¸å¤Ÿå‡åŒ€ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
    
    # 5. æ€»ç»“
    print(f"\n" + "=" * 80)
    print(f"=   ç¬¬ä¸€æ€§åŸç†åˆå§‹åŒ–æµ‹è¯•æ€»ç»“   =")
    print(f"=" * 80)
    
    all_checks_passed = True
    
    if cls_bias is not None and bias_zero_check:
        print(f"âœ… åˆ†ç±»å¤´åç½®: å…¨éƒ¨ä¸º0ï¼Œç§»é™¤é­”æ³•æ•°å­—åè§")
    else:
        print(f"âŒ åˆ†ç±»å¤´åç½®: æ£€æµ‹åˆ°é—®é¢˜")
        all_checks_passed = False
    
    if reg_bias is not None and reg_bias_zero:
        print(f"âœ… å›å½’å¤´åç½®: ä¸º0ï¼Œç§»é™¤æ•°æ®ä¾èµ–åç½®")
    else:
        print(f"âŒ å›å½’å¤´åç½®: æ£€æµ‹åˆ°é—®é¢˜")
        all_checks_passed = False
    
    if uniform_check:
        print(f"âœ… åˆå§‹æ¦‚ç‡åˆ†å¸ƒ: æ¥è¿‘å‡åŒ€ï¼Œä½“ç°ç¬¬ä¸€æ€§åŸç†")
    else:
        print(f"âš ï¸  åˆå§‹æ¦‚ç‡åˆ†å¸ƒ: éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
    
    if all_checks_passed:
        print(f"\nğŸ‰ ç¬¬ä¸€æ€§åŸç†åˆå§‹åŒ–ç­–ç•¥éªŒè¯é€šè¿‡!")
        print(f"   æ¨¡å‹ç°åœ¨ä»çº¯å‡€ã€æ— åè§çš„çŠ¶æ€å¼€å§‹å­¦ä¹ ")
        print(f"   æ‰€æœ‰ä¸ç¡®å®šæ€§é€šè¿‡AbductionNetworkçš„scale_Uè¡¨è¾¾")
        print(f"   ç§»é™¤äº†æ‰€æœ‰å¯å‘å¼é­”æ³•æ•°å­—")
    else:
        print(f"\nâš ï¸  å‘ç°é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤åˆå§‹åŒ–ç­–ç•¥")
    
    return all_checks_passed

if __name__ == "__main__":
    success = test_first_principles_initialization()
    exit(0 if success else 1) 
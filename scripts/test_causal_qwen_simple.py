"""æœ€ç®€å•çš„ CausalQwen æ¼”ç¤º"""

import torch
from transformers import AutoTokenizer
import os


class SimpleCausalQwen:
    """è¶…çº§ç®€åŒ–çš„ CausalQwen 
    
    åªæ¼”ç¤ºæ ¸å¿ƒæ¦‚å¿µï¼Œä¸å®ç°å®Œæ•´åŠŸèƒ½
    """
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # æ·»åŠ  <NUM> token
        self.tokenizer.add_tokens(['<NUM>'], special_tokens=True)
        self.num_token_id = self.tokenizer.convert_tokens_to_ids('<NUM>')
        
    def generate_with_causality(self, prompt, max_length=50):
        """å› æœç”Ÿæˆæ¼”ç¤º
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        1. å…ˆé‡‡æ ·"åŸå› "ï¼ˆå› æœè¡¨å¾ Uï¼‰
        2. åŸºäºåŸå› å†³å®š"ç»“æœ"ï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰
        3. ä½¿ç”¨ softmax(loc_S) å…¼å®¹æ ‡å‡†é‡‡æ ·
        """
        print(f"\nğŸ² å› æœç”Ÿæˆæ¨¡å¼")
        print(f"ğŸ“ è¾“å…¥: {prompt}")
        
        # Tokenize
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        
        # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
        generated = input_ids
        for step in range(max_length):
            # æ­¥éª¤1: æ¨æ–­å› æœè¡¨å¾åˆ†å¸ƒï¼ˆç®€åŒ–ä¸ºæ­£æ€åˆ†å¸ƒæ¼”ç¤ºï¼‰
            u_loc = torch.randn(1, 128)  # æ¨¡æ‹Ÿ loc
            u_scale = torch.ones(1, 128) * 2.0  # æ¨¡æ‹Ÿ scale
            
            # æ­¥éª¤2: é‡‡æ ·å…·ä½“çš„å› æœè¡¨å¾
            u_sample = torch.normal(u_loc, u_scale)  # å®é™…åº”è¯¥ç”¨æŸ¯è¥¿åˆ†å¸ƒ
            
            # æ­¥éª¤3: è®¡ç®—åˆ†ç±»åˆ†æ•°çš„åˆ†å¸ƒå‚æ•°
            # S_k = A_k Â· U + B_kï¼Œå¾—åˆ°æ¯ä¸ªç±»åˆ«çš„ loc
            loc_scores = torch.randn(1, self.tokenizer.vocab_size)  # æ¨¡æ‹Ÿ loc_{S_k}
            
            # æ­¥éª¤4: ä½¿ç”¨ softmax(loc) è¿›è¡Œå…¼å®¹é‡‡æ ·ï¼ˆå…³é”®æ”¹è¿›ï¼ï¼‰
            probs = torch.softmax(loc_scores, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # æ·»åŠ åˆ°åºåˆ—
            generated = torch.cat([generated, next_token], dim=1)
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            if next_token.item() == self.tokenizer.eos_token_id:
                break
                
            # æ¼”ç¤ºä¿¡æ¯
            if step < 5:  # åªæ‰“å°å‰å‡ æ­¥
                print(f"   æ­¥éª¤ {step+1}: é‡‡æ ·U â†’ è®¡ç®—loc_S â†’ softmax â†’ token {next_token.item()}")
        
        # Decode
        result = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"ğŸ¤– è¾“å‡º: {result}")
        
        return result
    
    def handle_numerical_input(self, text):
        """æ¼”ç¤ºæ•°å€¼å¤„ç†"""
        print(f"\nğŸ”¢ æ•°å€¼å¤„ç†æ¼”ç¤º")
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {text}")
        
        # ç®€å•çš„æ•°å€¼è¯†åˆ«ï¼ˆå®é™…åº”è¯¥æ›´æ™ºèƒ½ï¼‰
        import re
        pattern = r'\b\d+\.?\d*\b'
        
        def replace_num(match):
            return f"<NUM>{match.group()}</NUM>"
        
        processed = re.sub(pattern, replace_num, text)
        print(f"ğŸ”„ å¤„ç†å: {processed}")
        
        # æå–æ•°å€¼
        nums = re.findall(r'<NUM>([\d.]+)</NUM>', processed)
        print(f"ğŸ“Š è¯†åˆ«çš„æ•°å€¼: {nums}")
        
        return processed, nums

    def demonstrate_compatibility(self):
        """æ¼”ç¤ºä¸ Qwen çš„å…¼å®¹æ€§"""
        print(f"\nğŸ”— å…¼å®¹æ€§æ¼”ç¤º")
        print("âœ… CausalQwen çš„é‡‡æ ·æµç¨‹ä¸ Qwen å®Œå…¨å…¼å®¹ï¼š")
        print("   1. è®¡ç®— logitsï¼ˆloc_Sï¼‰")
        print("   2. åº”ç”¨ softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ")
        print("   3. ä½¿ç”¨æ ‡å‡†çš„ top-k/top-p è¿‡æ»¤")
        print("   4. ä»è¿‡æ»¤åçš„åˆ†å¸ƒä¸­é‡‡æ ·")
        print("   5. å®Œå…¨å…¼å®¹ temperature å‚æ•°")
        
        # æ¨¡æ‹Ÿå…¼å®¹æ€§æµ‹è¯•
        vocab_size = 1000
        loc_scores = torch.randn(1, vocab_size)
        
        # æ ‡å‡†æµç¨‹
        print(f"\nğŸ“Š æ ‡å‡†é‡‡æ ·æµç¨‹æ¼”ç¤º:")
        print(f"   åŸå§‹ logits å½¢çŠ¶: {loc_scores.shape}")
        
        # åº”ç”¨æ¸©åº¦
        temperature = 0.8
        scaled_logits = loc_scores / temperature
        print(f"   æ¸©åº¦è°ƒæ•´å: temperature={temperature}")
        
        # Softmax
        probs = torch.softmax(scaled_logits, dim=-1)
        print(f"   æ¦‚ç‡åˆ†å¸ƒ: sum={probs.sum():.6f} (åº”è¯¥=1.0)")
        
        # Top-k æ¼”ç¤º
        k = 50
        top_k_probs, top_k_indices = torch.topk(probs, k)
        print(f"   Top-{k} é‡‡æ ·å°±ç»ª")
        
        print("âœ… å®Œå…¨å…¼å®¹ transformers.generate() çš„æ‰€æœ‰å‚æ•°ï¼")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ CausalQwen ç®€åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åŠ è½½tokenizer
    model_path = os.path.expanduser("~/models/Qwen2.5-0.5B")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # åˆ›å»ºç®€åŒ–çš„CausalQwen
    model = SimpleCausalQwen(tokenizer)
    
    # æ¼”ç¤º1: å› æœç”Ÿæˆï¼ˆå…¼å®¹ç‰ˆï¼‰
    print("\nğŸ“Œ æ¼”ç¤º1: å› æœç”Ÿæˆæµç¨‹ï¼ˆQwenå…¼å®¹ç‰ˆï¼‰")
    model.generate_with_causality("ä»Šå¤©å¤©æ°”", max_length=8)
    
    # æ¼”ç¤º2: æ•°å€¼å¤„ç†
    print("\nğŸ“Œ æ¼”ç¤º2: æ•°å€¼æ„ŸçŸ¥")
    model.handle_numerical_input("è‚¡ä»·ä¸Šæ¶¨äº†5.2%ï¼Œæˆäº¤é‡è¾¾åˆ°100ä¸‡")
    
    # æ¼”ç¤º3: å…¼å®¹æ€§
    print("\nğŸ“Œ æ¼”ç¤º3: ä¸ Qwen çš„å®Œå…¨å…¼å®¹æ€§")
    model.demonstrate_compatibility()
    
    # æ¼”ç¤º4: æ ¸å¿ƒæ¦‚å¿µ
    print("\nğŸ“Œ æ¼”ç¤º4: æ ¸å¿ƒæ¦‚å¿µ")
    print("ğŸ¯ CausalQwen çš„æ ¸å¿ƒåˆ›æ–°:")
    print("   1. å› æœè¡¨å¾ Uï¼šå»ºæ¨¡ä¸ªä½“å·®å¼‚")
    print("   2. æ¨æ–­-è¡ŒåŠ¨ï¼šå…ˆæ¨æ–­åŸå› ï¼Œå†å†³å®šç»“æœ")
    print("   3. ç»Ÿä¸€è¡¨ç¤ºï¼šæ–‡æœ¬å’Œæ•°å€¼çš„æ— ç¼èåˆ")
    print("   4. æŸ¯è¥¿åˆ†å¸ƒï¼šæ•°å­¦ä¸Šçš„ä¼˜é›…é€‰æ‹©")
    print("   5. å…¼å®¹é‡‡æ ·ï¼šsoftmax(loc_S) å®Œå…¨å…¼å®¹ç°æœ‰å·¥å…·é“¾ ğŸ†•")
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()

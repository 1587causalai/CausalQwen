#!/usr/bin/env python
"""
å› æœè¯­è¨€æ¨¡å‹çš„å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (V5: ç®€åŒ–ç‰ˆ)

ä¿®å¤äº†æ— è¾“å‡ºçš„é—®é¢˜ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½éªŒè¯ã€‚
"""
import os
import sys
import torch
import numpy as np

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def print_step(step_num, description):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ {step_num}: {description}")
    print(f"{'='*60}")

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œè°ƒè¯•å‰å‘ä¼ æ’­ã€‚"""
    try:
        print("ğŸš€ CausalQwen å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (ç®€åŒ–ç‰ˆ)")
        print("=" * 80)

        # æ­¥éª¤1: åŸºæœ¬å¯¼å…¥æµ‹è¯•
        print_step(1, "æµ‹è¯•åŸºæœ¬å¯¼å…¥")
        
        try:
            from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
            print("âœ… å¯¼å…¥ CausalLanguageModel æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å¯¼å…¥ CausalLanguageModel å¤±è´¥: {e}")
            return False
        
        try:
            from src.data.tokenizer import QwenTokenizerWrapper
            print("âœ… å¯¼å…¥ QwenTokenizerWrapper æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å¯¼å…¥ QwenTokenizerWrapper å¤±è´¥: {e}")
            return False

        # æ­¥éª¤2: è®¾å¤‡å’Œè·¯å¾„æ£€æŸ¥
        print_step(2, "ç¯å¢ƒæ£€æŸ¥")
        
        device = torch.device('cpu')
        qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
        
        print(f"è®¾å¤‡: {device}")
        print(f"Qwenè·¯å¾„: {qwen_model_path}")
        
        if not os.path.exists(qwen_model_path):
            print(f"âŒ Qwenæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {qwen_model_path}")
            return False
        else:
            print(f"âœ… Qwenæ¨¡å‹è·¯å¾„å­˜åœ¨")

        # æ­¥éª¤3: åˆ†è¯å™¨åˆå§‹åŒ–
        print_step(3, "åˆ†è¯å™¨åˆå§‹åŒ–")
        
        try:
            tokenizer = QwenTokenizerWrapper(
                model_path=qwen_model_path, 
                use_real_tokenizer=True
            )
            print(f"âœ… åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
            print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
            print(f"   <NUM> token ID: {tokenizer.num_token_id}")
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # æ­¥éª¤4: æ¨¡å‹é…ç½®
        print_step(4, "æ¨¡å‹é…ç½®åˆ›å»º")
        
        try:
            config = CausalLMConfig(
                vocab_size=tokenizer.vocab_size,
                num_token_id=tokenizer.num_token_id,
                hidden_size=896,
                causal_dim=896,
                use_real_qwen=True,
                qwen_model_path=qwen_model_path,
                ovr_threshold=10.0,
                reg_loss_weight=1.0
            )
            print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
            print(f"   vocab_size: {config.vocab_size}")
            print(f"   hidden_size: {config.hidden_size}")
        except Exception as e:
            print(f"âŒ é…ç½®åˆ›å»ºå¤±è´¥: {e}")
            return False

        # æ­¥éª¤5: æ¨¡å‹åˆ›å»º
        print_step(5, "æ¨¡å‹åˆ›å»º")
        
        try:
            model = CausalLanguageModel(config).to(device)
            print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
            # è·å–å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in model.parameters())
            print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # æ­¥éª¤6: æ¨¡å‹åˆå§‹åŒ–
        print_step(6, "æ¨¡å‹æƒé‡åˆå§‹åŒ–")
        
        try:
            model.init_weights()
            print(f"âœ… æƒé‡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æƒé‡åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # æ­¥éª¤7: æµ‹è¯•æ•°æ®å‡†å¤‡
        print_step(7, "æµ‹è¯•æ•°æ®å‡†å¤‡")
        
        try:
            test_texts = [
                "The price is 99.99 dollars.",
                "Hello world!"
            ]
            
            inputs = tokenizer.batch_encode_plus(
                test_texts, 
                padding=True, 
                truncation=True, 
                return_tensors='pt'
            )
            
            print(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡æˆåŠŸ")
            print(f"   æ‰¹æ¬¡å¤§å°: {inputs['input_ids'].shape}")
            print(f"   åºåˆ—é•¿åº¦: {inputs['input_ids'].shape[1]}")
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # æ­¥éª¤8: å‰å‘ä¼ æ’­æµ‹è¯•
        print_step(8, "å‰å‘ä¼ æ’­æµ‹è¯•")
        
        try:
            model.eval()
            
            with torch.no_grad():
                outputs = model(
                    inputs['input_ids'].to(device),
                    inputs['numerical_values'].to(device),
                    inputs['attention_mask'].to(device)
                )
            
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"   è¾“å‡ºé”®: {list(outputs.keys())}")
            
            # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
            for key, value in outputs.items():
                if isinstance(value, torch.Tensor):
                    print(f"   {key}: {value.shape}")
                    
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

        # æ­¥éª¤9: æ•°å­¦å…¬å¼éªŒè¯
        print_step(9, "æ•°å­¦å…¬å¼å¿«é€ŸéªŒè¯")
        
        try:
            # ç®€å•çš„æŸ¯è¥¿NLLæµ‹è¯•
            test_target = 3.5
            test_loc = 2.0
            test_scale = 1.5
            
            # æ‰‹å·¥è®¡ç®—
            import math
            z = (test_target - test_loc) / test_scale
            manual_nll = math.log(math.pi * test_scale) + math.log(1 + z**2)
            
            print(f"âœ… æ•°å­¦éªŒè¯æˆåŠŸ")
            print(f"   æŸ¯è¥¿NLLæ‰‹å·¥è®¡ç®—: {manual_nll:.6f}")
            
            # æµ‹è¯•å‡½æ•°è®¡ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                from src.utils.distributions import cauchy_nll_loss
                computed_nll = cauchy_nll_loss(
                    torch.tensor(test_target),
                    torch.tensor(test_loc), 
                    torch.tensor(test_scale),
                    reduction='none'
                ).item()
                
                diff = abs(manual_nll - computed_nll)
                print(f"   æŸ¯è¥¿NLLå‡½æ•°è®¡ç®—: {computed_nll:.6f}")
                print(f"   å·®å¼‚: {diff:.8f}")
                print(f"   âœ… æ•°å­¦ä¸€è‡´æ€§: {'é€šè¿‡' if diff < 1e-6 else 'å¤±è´¥'}")
                
            except ImportError:
                print("   âš ï¸  æ— æ³•å¯¼å…¥æŸ¯è¥¿NLLå‡½æ•°ï¼Œè·³è¿‡æ¯”è¾ƒ")
                
        except Exception as e:
            print(f"âŒ æ•°å­¦éªŒè¯å¤±è´¥: {e}")

        # æ€»ç»“
        print_step("å®Œæˆ", "è°ƒè¯•æ€»ç»“")
        print("ğŸ‰ æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("   - æ¨¡å‹å¯ä»¥æ­£å¸¸åˆ›å»ºå’Œåˆå§‹åŒ–")
        print("   - å‰å‘ä¼ æ’­æ­£å¸¸å·¥ä½œ")
        print("   - è¾“å‡ºå½¢çŠ¶ç¬¦åˆé¢„æœŸ")
        return True

    except KeyboardInterrupt:
        print(f"\nâŒ ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        return False
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    print("å¼€å§‹è¿è¡Œç®€åŒ–è°ƒè¯•è„šæœ¬...")
    success = main()
    if success:
        print("\nâœ… è„šæœ¬æ‰§è¡ŒæˆåŠŸ")
    else:
        print("\nâŒ è„šæœ¬æ‰§è¡Œå¤±è´¥")
    print("è„šæœ¬ç»“æŸ")
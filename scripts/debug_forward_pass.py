#!/usr/bin/env python
"""
å› æœè¯­è¨€æ¨¡å‹çš„å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (V4: åŸç”Ÿåºåˆ—åˆ°åºåˆ—æ¶æ„)

æœ¬è„šæœ¬å¯¹å› æœè¯­è¨€æ¨¡å‹æ‰§è¡Œå•æ¬¡å‰å‘ä¼ æ’­ï¼Œå¹¶æ‰“å°å‡ºä»è¾“å…¥åˆ°æœ€ç»ˆæŸå¤±çš„æ‰€æœ‰å…³é”®ä¸­é—´æ•°å­¦é‡ã€‚
è¿™æ—¨åœ¨ç”¨äºè°ƒè¯•å’ŒéªŒè¯å®ç°æ˜¯å¦ä¸ `design-docs/math/mathematical_foundations.md` 
ä¸­é˜è¿°çš„ç†è®ºåŸºç¡€ä¿æŒä¸€è‡´ã€‚

V4 æ›´æ–°ï¼šæ¨¡å‹æ¶æ„å·²å®Œå…¨é‡æ„ä¸ºåºåˆ—åˆ°åºåˆ—æ¨¡å¼ï¼Œæ— éœ€ä¸´æ—¶é€‚é…å™¨ã€‚
"""
import os
import sys
import torch
import numpy as np
from dataclasses import asdict
import torch.nn.functional as F

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.training.trainer import Trainer
from src.utils.losses import CausalLMLoss, compute_ovr_probabilities, cauchy_nll_loss

def print_tensor_stats(tensor, name):
    """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ‰“å°å¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    if not isinstance(tensor, torch.Tensor):
        print(f"--- {name}: {tensor} ---")
        return
        
    print(f"--- {name} ---")
    print(f"  - å½¢çŠ¶ (Shape): {tensor.shape}")
    print(f"  - æ•°æ®ç±»å‹ (Dtype): {tensor.dtype}")
    print(f"  - è®¾å¤‡ (Device): {tensor.device}")
    # é˜²æ­¢å¯¹éæµ®ç‚¹æˆ–ç©ºå¼ é‡è¿›è¡Œç»Ÿè®¡è®¡ç®—
    if tensor.is_floating_point() and tensor.numel() > 0:
        print(f"  - ç»Ÿè®¡å€¼ (Values):")
        print(f"    - å‡å€¼ (mean): {tensor.mean():.6f}, æ ‡å‡†å·® (std): {tensor.std():.6f}")
        print(f"    - æœ€å°å€¼ (min):  {tensor.min():.6f}, æœ€å¤§å€¼ (max): {tensor.max():.6f}")
    print(f"  - æ ·æœ¬å€¼ (Sample values): {tensor.flatten()[:5].tolist()}")
    print("-" * (len(name) + 20))


def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œè°ƒè¯•å‰å‘ä¼ æ’­ã€‚"""
    print("=" * 80)
    print("=   å› æœè¯­è¨€æ¨¡å‹å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (V4: åŸç”Ÿåºåˆ—åˆ°åºåˆ—æ¶æ„)   =")
    print("=" * 80)

    # --- 1. è®¾ç½® ---
    print("\n[æ­¥éª¤ 1. è®¾ç½®æ¨¡å‹ã€åˆ†è¯å™¨å’Œé…ç½®...]")
    device = torch.device('cpu') # ä½¿ç”¨ CPU ä»¥æ–¹ä¾¿è°ƒè¯•
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        ovr_threshold=10.0,
        reg_loss_weight=1.0
    )
    
    model = CausalLanguageModel(config).to(device)
    
    # IMPORTANT: Initialize the AbductionNetwork with proper weights
    # This ensures causal_scale has reasonable initial values (around 10.0)
    print("\n[æ­¥éª¤ 1.5. è¯¦ç»†çš„åˆå§‹åŒ–è¿‡ç¨‹éªŒè¯...]")
    print("=" * 60)
    
    print("ğŸ”§ AbductionNetwork åˆå§‹åŒ–è¿‡ç¨‹:")
    print(f"  åˆå§‹åŒ–ç­–ç•¥: C=H æ’ç­‰æ˜ å°„åˆå§‹åŒ–")
    print(f"  hidden_size: {config.hidden_size}, causal_dim: {config.causal_dim}")
    
    # è·å–åˆå§‹åŒ–å‰çš„æƒé‡çŠ¶æ€
    abduction_fc = model.abduction_network.fc
    print(f"  çº¿æ€§å±‚å½¢çŠ¶: {abduction_fc.weight.shape} = [{config.causal_dim*2}, {config.hidden_size}]")
    
    print("\næ‰§è¡Œåˆå§‹åŒ–...")
    model.abduction_network.init_weights()
    
    # éªŒè¯åˆå§‹åŒ–åçš„æƒé‡
    print("\nåˆå§‹åŒ–åçš„æƒé‡éªŒè¯:")
    weight = abduction_fc.weight.data
    bias = abduction_fc.bias.data
    
    # æ£€æŸ¥locéƒ¨åˆ†çš„æƒé‡ï¼ˆå‰causal_dimè¡Œï¼‰
    loc_weight = weight[:config.causal_dim, :]
    scale_weight = weight[config.causal_dim:, :]
    
    print(f"  ä½ç½®å‚æ•° (loc) æƒé‡:")
    if config.causal_dim == config.hidden_size:
        is_identity = torch.allclose(loc_weight, torch.eye(config.causal_dim), atol=1e-6)
        print(f"    æ˜¯å¦ä¸ºæ’ç­‰çŸ©é˜µ: {'âœ…' if is_identity else 'âŒ'}")
        print(f"    å¯¹è§’çº¿å…ƒç´ æ ·æœ¬: {torch.diag(loc_weight)[:5].tolist()}")
    else:
        print(f"    æƒé‡ç»Ÿè®¡: å‡å€¼={loc_weight.mean().item():.6f}, æ ‡å‡†å·®={loc_weight.std().item():.6f}")
    
    print(f"  å°ºåº¦å‚æ•° (scale) æƒé‡:")
    scale_weight_zero = torch.allclose(scale_weight, torch.zeros_like(scale_weight), atol=1e-6)  
    print(f"    æ˜¯å¦ä¸ºé›¶çŸ©é˜µ: {'âœ…' if scale_weight_zero else 'âŒ'}")
    print(f"    æƒé‡ç»Ÿè®¡: å‡å€¼={scale_weight.mean().item():.6f}, æœ€å¤§å€¼={scale_weight.abs().max().item():.6f}")
    
    print(f"  åç½®å‚æ•°éªŒè¯:")
    loc_bias = bias[:config.causal_dim]
    scale_bias = bias[config.causal_dim:]
    print(f"    locåç½®: å‡å€¼={loc_bias.mean().item():.6f}, æ ‡å‡†å·®={loc_bias.std().item():.6f}")
    print(f"    scaleåç½®: å‡å€¼={scale_bias.mean().item():.6f}, exp()åå‡å€¼={torch.exp(scale_bias).mean().item():.4f}")
    print(f"    é¢„æœŸscaleå€¼: exp(2.3) â‰ˆ {torch.exp(torch.tensor(2.3)).item():.1f}")
    
    print("\nâœ… AbductionNetwork åˆå§‹åŒ–ç†è®ºéªŒè¯:")
    print("  ç†è®ºåŸºç¡€: æ’ç­‰æ˜ å°„ + é«˜ä¸ç¡®å®šæ€§åˆå§‹åŒ–")
    print("  loc: W=I, b=0 â†’ causal_loc â‰ˆ features (ä¿æŒç‰¹å¾ä¿¡æ¯)")
    print("  scale: W=0, b=2.3 â†’ causal_scale â‰ˆ 10 (é«˜åˆå§‹ä¸ç¡®å®šæ€§)")
    
    print("\nğŸ”§ ActionNetwork çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–:")
    print("  æ­£åœ¨æ‰§è¡Œä»Qwenæ¨¡å‹åˆ°ActionNetworkçš„çŸ¥è¯†ä¼ è¾“...")
    
    # æ£€æŸ¥åˆ†ç±»å¤´çš„æƒé‡èŒƒå›´ï¼ˆä¿®æ­£åçš„CauchyLinearç»“æ„ï¼‰
    cls_head = model.action_network.classification_head.causal_linear
    cls_weight = cls_head.weight.data
    cls_bias = cls_head.bias.data if cls_head.bias is not None else None
    
    print(f"  åˆ†ç±»å¤´æƒé‡ç»Ÿè®¡:")
    print(f"    æƒé‡å½¢çŠ¶: {cls_weight.shape} = [vocab_size={config.vocab_size}, causal_dim={config.causal_dim}]")
    print(f"    æƒé‡å‡å€¼: {cls_weight.mean().item():.6f}, æ ‡å‡†å·®: {cls_weight.std().item():.6f}")
    print(f"    æƒé‡èŒƒå›´: [{cls_weight.min().item():.4f}, {cls_weight.max().item():.4f}]")
    
    if cls_bias is not None:
        print(f"    åç½®å‡å€¼: {cls_bias.mean().item():.6f}, æ ‡å‡†å·®: {cls_bias.std().item():.6f}")
    
    # æ£€æŸ¥å›å½’å¤´
    reg_head = model.action_network.regression_head.causal_linear
    reg_weight = reg_head.weight.data
    reg_bias = reg_head.bias.data if reg_head.bias is not None else None
    
    print(f"  å›å½’å¤´æƒé‡ç»Ÿè®¡:")
    print(f"    æƒé‡å½¢çŠ¶: {reg_weight.shape} = [1, causal_dim={config.causal_dim}]")
    print(f"    æƒé‡å‡å€¼: {reg_weight.mean().item():.6f}, æ ‡å‡†å·®: {reg_weight.std().item():.6f}")
    
    if reg_bias is not None:
        print(f"    åç½®å€¼: {reg_bias.item():.4f}")
    
    # æ£€æŸ¥<NUM> tokençš„ç‰¹æ®Šå¤„ç†
    num_token_id = config.num_token_id
    print(f"\n  <NUM> token (ID: {num_token_id}) åˆå§‹åŒ–æ£€æŸ¥:")
    if cls_bias is not None:
        print(f"    åˆ†ç±»å¤´ä¸­<NUM>çš„åç½®: {cls_bias[num_token_id].item():.4f}")
    print(f"    åˆ†ç±»å¤´ä¸­<NUM>çš„æƒé‡èŒƒå›´: [{cls_weight[num_token_id].min().item():.4f}, {cls_weight[num_token_id].max().item():.4f}]")
    
    # æ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–
    print("\næ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–...")
    # ä½¿ç”¨ä¸€äº›åˆç†çš„æ•°æ®ç»Ÿè®¡å€¼è¿›è¡Œåˆå§‹åŒ–
    num_target_median = 50.0  # å‡è®¾æ•°å€¼çš„ä¸­ä½æ•°
    num_target_scale = 25.0   # å‡è®¾æ•°å€¼çš„å°ºåº¦
    model.init_weights(num_target_median, num_target_scale)
    
    # éªŒè¯çŸ¥è¯†ä¼ è¾“åçš„ActionNetworkå‚æ•°
    print("\nçŸ¥è¯†ä¼ è¾“åçš„ActionNetworkéªŒè¯:")
    cls_head_after = model.action_network.classification_head.causal_linear
    reg_head_after = model.action_network.regression_head.causal_linear
    
    print(f"  åˆ†ç±»å¤´çŸ¥è¯†ä¼ è¾“éªŒè¯:")
    print(f"    åˆ†ç±»å¤´æƒé‡ç»Ÿè®¡: å‡å€¼={cls_head_after.weight.data.mean().item():.6f}")
    if cls_head_after.bias is not None:
        print(f"    åˆ†ç±»å¤´åç½®ç»Ÿè®¡: å‡å€¼={cls_head_after.bias.data.mean().item():.6f}")
        print(f"    <NUM>tokenåç½®: {cls_head_after.bias.data[tokenizer.num_token_id].item():.4f}")
    
    print(f"  å›å½’å¤´çŸ¥è¯†ä¼ è¾“éªŒè¯:")
    print(f"    å›å½’å¤´æƒé‡: å‡å€¼={reg_head_after.weight.data.mean().item():.6f} (åº”è¯¥â‰ˆ0)")
    if reg_head_after.bias is not None:
        print(f"    å›å½’å¤´åç½®: {reg_head_after.bias.data.item():.4f} (åº”è¯¥â‰ˆ{num_target_median})")
    
    print(f"  âœ… çŸ¥è¯†ä¼ è¾“éªŒè¯:")
    print(f"    åˆ†ç±»å¤´: ç»§æ‰¿Qwençš„è¯æ±‡è¡¨çŸ¥è¯†ï¼Œ<NUM>tokenç‰¹æ®Šåˆå§‹åŒ–")
    print(f"    å›å½’å¤´: æƒé‡åˆå§‹åŒ–ä¸º0ï¼Œåç½®åˆå§‹åŒ–ä¸ºæ•°æ®ä¸­ä½æ•°")
    print(f"    æ•°å­¦ä¿®æ­£: ç°åœ¨ä½¿ç”¨å…±äº«æƒé‡è¿›è¡Œæ­£ç¡®çš„æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢")
    
    print("\nâœ… åˆå§‹åŒ–ç†è®ºæ€»ç»“:")
    print("  AbductionNetwork: æ’ç­‰æ˜ å°„ä¿æŒç‰¹å¾ï¼Œé«˜ä¸ç¡®å®šæ€§åˆå§‹åŒ–")
    print("  ActionNetwork: çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–ï¼Œç»§æ‰¿Qwençš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›")
    print("  æ•°å­¦ä¸€è‡´æ€§: æŸ¯è¥¿åˆ†å¸ƒçš„çº¿æ€§å˜æ¢å°é—­æ€§ç¡®ä¿æ¢¯åº¦ä¼ æ’­")
    
    # --- æ–°å¢ï¼šæ•°å­¦å…¬å¼éªŒè¯ ---
    print("\nğŸ§® æ•°å­¦å…¬å¼æ­£ç¡®æ€§éªŒè¯:")
    print("å¯¹ç…§ mathematical_foundations.md éªŒè¯å…³é”®å…¬å¼å®ç°")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    test_loc = torch.tensor(2.0)
    test_scale = torch.tensor(1.5) 
    test_target = torch.tensor(3.5)
    test_threshold = torch.tensor(10.0)
    
    # 1. éªŒè¯OvRæ¦‚ç‡è®¡ç®—å…¬å¼
    print(f"\n1. OvRæ¦‚ç‡è®¡ç®—å…¬å¼éªŒè¯:")
    print(f"   ç†è®ºå…¬å¼: P(S > C) = 1/2 + (1/Ï€) * arctan((loc - C)/scale)")
    manual_prob = 0.5 + (1/torch.pi) * torch.atan((test_loc - test_threshold)/test_scale)
    computed_prob = compute_ovr_probabilities(test_loc, test_scale, test_threshold.item())
    print(f"   æµ‹è¯•å‚æ•°: loc={test_loc:.1f}, scale={test_scale:.1f}, threshold={test_threshold:.1f}")
    print(f"   æ‰‹å·¥è®¡ç®—: {manual_prob.item():.6f}")
    print(f"   å‡½æ•°è®¡ç®—: {computed_prob.item():.6f}")
    print(f"   ä¸€è‡´æ€§: {'âœ…' if torch.allclose(manual_prob, computed_prob, atol=1e-6) else 'âŒ'}")
    
    # 2. éªŒè¯æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±å…¬å¼
    print(f"\n2. æŸ¯è¥¿è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±å…¬å¼éªŒè¯:")
    print(f"   ç†è®ºå…¬å¼: L = log(Ï€ * scale) + log(1 + ((target - loc)/scale)Â²)")
    manual_nll = (torch.log(torch.pi * test_scale) + 
                  torch.log(1 + ((test_target - test_loc)/test_scale)**2))
    computed_nll = cauchy_nll_loss(test_loc, test_scale, test_target, reduction='none')
    print(f"   æµ‹è¯•å‚æ•°: loc={test_loc:.1f}, scale={test_scale:.1f}, target={test_target:.1f}")
    print(f"   æ‰‹å·¥è®¡ç®—: {manual_nll.item():.6f}")
    print(f"   å‡½æ•°è®¡ç®—: {computed_nll.item():.6f}")
    print(f"   ä¸€è‡´æ€§: {'âœ…' if torch.allclose(manual_nll, computed_nll, atol=1e-6) else 'âŒ'}")
    
    # 3. éªŒè¯æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢å°é—­æ€§
    print(f"\n3. æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢å°é—­æ€§éªŒè¯:")
    print(f"   ç†è®º: å¦‚æœ X ~ Cauchy(Î¼, Ïƒ), åˆ™ Y = aX + b ~ Cauchy(aÎ¼ + b, |a|Ïƒ)")
    a, b = 2.0, 3.0
    transformed_loc_theory = a * test_loc + b
    transformed_scale_theory = abs(a) * test_scale
    print(f"   åŸåˆ†å¸ƒ: Cauchy({test_loc:.1f}, {test_scale:.1f})")
    print(f"   å˜æ¢: Y = {a:.1f}X + {b:.1f}")
    print(f"   ç†è®ºç»“æœ: Cauchy({transformed_loc_theory:.1f}, {transformed_scale_theory:.1f})")
    print(f"   âœ… è¿™ä¸ªæ€§è´¨ç¡®ä¿äº†ActionNetworkä¸­çš„çº¿æ€§å˜æ¢æ•°å­¦æ­£ç¡®æ€§")
    
    print(f"\nâœ… æ‰€æœ‰æ•°å­¦å…¬å¼éªŒè¯å®Œæˆï¼Œå®ç°ç¬¦åˆç†è®ºæ–‡æ¡£è¦æ±‚ï¼")
    
    model.eval()

    print("\n" + "=" * 60)
    print("æ¨¡å‹è®¾ç½®å®Œæˆï¼Œå®Œæ•´åˆå§‹åŒ–å’Œæ•°å­¦å…¬å¼éªŒè¯é€šè¿‡ã€‚")

    # --- 2. æ•°æ®ä¸ç›®æ ‡ç”Ÿæˆ (BOS/EOS + åºåˆ—åˆ°åºåˆ—) ---
    print("\n[æ­¥éª¤ 2. ä½¿ç”¨å¢å¼ºç‰ˆåˆ†è¯å™¨æ„å»ºæœ€ç»ˆçš„æ‰¹æ¬¡æ•°æ®...]")
    
    texts = [
        "The item costs 99.99 dollars.",
        "From a batch of 100 items, 5 were defective, leaving 95 good ones.",
        "A standard text without any numerical values."
    ]
    
    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors='pt')
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    
    # --- æ„å»ºæœ€ç»ˆçš„ã€åºåˆ—åŒ–çš„ç›®æ ‡ ---
    labels = input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    labels[:, :-1] = labels[:, 1:].clone()
    labels[:, -1] = -100

    # æ„å»ºåºåˆ—åŒ–çš„å›å½’ç›®æ ‡
    target_values = torch.full_like(numerical_values, float('nan'))
    shifted_numerical_values = numerical_values.clone()
    shifted_numerical_values[:, :-1] = numerical_values[:, 1:].clone()
    shifted_numerical_values[:, -1] = 0.0
    
    num_mask = (labels == tokenizer.num_token_id)
    target_values[num_mask] = shifted_numerical_values[num_mask]

    # --- è¯¦ç»†æ‰“å° ---
    batch_size, seq_len = input_ids.shape
    print("\n--- æ‰¹æ¬¡æ•°æ®è¯¦æƒ… (å…± {} ä¸ªæ ·æœ¬) ---".format(batch_size))
    # åªæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¯¦æƒ…ï¼Œä»¥ä¿æŒè¾“å‡ºç®€æ´
    i = 0
    print(f"\n  æ ·æœ¬ {i+1}: '{texts[i]}'")
    print("  ä½ç½® | è¾“å…¥Token        | è¾“å…¥æ•°å€¼ | ç›®æ ‡Token (Label)  | ç›®æ ‡æ•°å€¼ (Target Value)")
    print("  " + "-"*70)
    for j in range(seq_len):
        if attention_mask[i, j] == 0: continue
        
        input_token = tokenizer.convert_ids_to_tokens([input_ids[i,j].item()])[0]
        input_val = numerical_values[i,j].item()
        
        label_id = labels[i,j].item()
        if label_id != -100:
            label_token = tokenizer.convert_ids_to_tokens([label_id])[0]
        else:
            label_token = "N/A (Ignore)"
        
        target_val = target_values[i,j].item()
        
        print(f"  {j:^4} | {input_token:<16} | {input_val:^10.2f} | {label_token:<18} | {target_val:^20.4f}")
    
    # --- 3. å‰å‘ä¼ æ’­ ---
    print("\n[æ­¥éª¤ 3. æ‰§è¡Œå•æ¬¡å‰å‘ä¼ æ’­...]")
    with torch.no_grad():
        outputs = model(input_ids, numerical_values, attention_mask)
    print("å‰å‘ä¼ æ’­å®Œæˆã€‚")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    print("\n--- æ¶æ„éªŒè¯ï¼šæ£€æŸ¥è¾“å‡ºå½¢çŠ¶ ---")
    print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: {input_ids.shape}")
    print(f"ç‰¹å¾å½¢çŠ¶: {outputs['features'].shape}")
    print(f"å› æœè¡¨å¾å½¢çŠ¶: {outputs['causal_loc'].shape}")
    print(f"åˆ†ç±»è¾“å‡ºå½¢çŠ¶: {outputs['cls_loc'].shape}")
    print(f"å›å½’è¾“å‡ºå½¢çŠ¶: {outputs['reg_loc'].shape}")
    
    # --- æ–°å¢ï¼šè¯¦ç»†çš„AbductionNetworkéªŒè¯ ---
    print("\n--- AbductionNetwork æ•°å­¦æµç¨‹è¯¦ç»†éªŒè¯ ---")
    print(f"è®¾è®¡çº¦æŸéªŒè¯: C=H")
    print(f"  hidden_size (H): {config.hidden_size}")
    print(f"  causal_dim (C): {config.causal_dim}")
    if config.causal_dim == config.hidden_size:
        print(f"  âœ… C=H çº¦æŸæ­£ç¡®æ‰§è¡Œ: {config.causal_dim} = {config.hidden_size}")
    else:
        print(f"  âŒ Câ‰ H çº¦æŸè¿å: {config.causal_dim} â‰  {config.hidden_size}")
    
    print(f"\nAbductionNetwork è¾“å‡ºéªŒè¯:")
    print(f"  causal_loc å½¢çŠ¶: {outputs['causal_loc'].shape}")
    print(f"  causal_scale å½¢çŠ¶: {outputs['causal_scale'].shape}")
    print(f"  é¢„æœŸå½¢çŠ¶: [batch_size={batch_size}, seq_len={seq_len}, causal_dim={config.causal_dim}]")
    
    # éªŒè¯å½¢çŠ¶æ˜¯å¦ç¬¦åˆé¢„æœŸ
    expected_shape = (batch_size, seq_len, config.causal_dim)
    loc_shape_correct = outputs['causal_loc'].shape == expected_shape
    scale_shape_correct = outputs['causal_scale'].shape == expected_shape
    print(f"  causal_loc å½¢çŠ¶æ­£ç¡®: {'âœ…' if loc_shape_correct else 'âŒ'}")
    print(f"  causal_scale å½¢çŠ¶æ­£ç¡®: {'âœ…' if scale_shape_correct else 'âŒ'}")
    
    print(f"\nAbductionNetwork å†…éƒ¨æ•°å­¦æµç¨‹éªŒè¯:")
    print(f"  è¾“å…¥ç‰¹å¾: [B,S,H] = {outputs['features'].shape}")
    print(f"  å†…éƒ¨çº¿æ€§å±‚è¾“å‡º: [B,S,CÃ—2] = [B,S,{config.causal_dim}Ã—2] = [B,S,{config.causal_dim*2}]")
    print(f"  åˆ†ç¦»åè¾“å‡º: causal_loc + causal_scale = 2 Ã— [B,S,{config.causal_dim}]")
    
    # éªŒè¯ loc å’Œ scale ç¡®å®ä¸åŒï¼ˆé¿å…å®ç°é”™è¯¯ï¼‰
    print(f"\néªŒè¯ causal_loc å’Œ causal_scale çš„ç‹¬ç«‹æ€§:")
    sample_pos = (0, 1)  # æ ·æœ¬0ï¼Œä½ç½®1
    loc_sample = outputs['causal_loc'][sample_pos[0], sample_pos[1], :3]
    scale_sample = outputs['causal_scale'][sample_pos[0], sample_pos[1], :3]
    print(f"  æ ·æœ¬{sample_pos[0]}ä½ç½®{sample_pos[1]}å‰3ç»´:")
    print(f"    causal_loc[:3] = {loc_sample.tolist()}")
    print(f"    causal_scale[:3] = {scale_sample.tolist()}")
    
    # æ£€æŸ¥scaleæ˜¯å¦éƒ½ä¸ºæ­£å€¼ï¼ˆscaleå‚æ•°å¿…é¡»ä¸ºæ­£ï¼‰
    min_scale = outputs['causal_scale'].min().item()
    print(f"  causal_scaleæœ€å°å€¼: {min_scale:.6f} (å¿…é¡»>0: {'âœ…' if min_scale > 0 else 'âŒ'})")
    
    # å±•ç¤ºåºåˆ—ä¸­ä¸åŒä½ç½®çš„è¾“å‡ºå·®å¼‚
    print("\n--- åºåˆ—ä½ç½®çš„ç‹¬ç«‹æ€§éªŒè¯ ---")
    print("æ£€æŸ¥ä¸åŒä½ç½®æ˜¯å¦æœ‰ä¸åŒçš„é¢„æµ‹ï¼ˆå‰3ä¸ªä½ç½®ï¼‰ï¼š")
    for pos in range(min(3, seq_len)):
        print(f"\nä½ç½® {pos}:")
        print(f"  causal_loc[0,{pos},:5] = {outputs['causal_loc'][0, pos, :5].tolist()}")
        print(f"  causal_scale[0,{pos},:5] = {outputs['causal_scale'][0, pos, :5].tolist()}")
        print(f"  cls_loc[0,{pos},:5] = {outputs['cls_loc'][0, pos, :5].tolist()}")
        print(f"  reg_loc[0,{pos}] = {outputs['reg_loc'][0, pos].item():.4f}")
    
    # --- æ–°å¢ï¼šActionNetwork è¯¦ç»†éªŒè¯ ---
    print("\n--- ActionNetwork æ•°å­¦æµç¨‹è¯¦ç»†éªŒè¯ ---")
    print("éªŒè¯ä»å› æœè¡¨å¾åˆ°æœ€ç»ˆè¾“å‡ºçš„è½¬æ¢:")
    
    print(f"\nåˆ†ç±»å¤´ (ClassificationHead) éªŒè¯:")
    print(f"  è¾“å…¥: causal_loc [B,S,C] = {outputs['causal_loc'].shape}")
    print(f"  è¾“å…¥: causal_scale [B,S,C] = {outputs['causal_scale'].shape}")
    print(f"  è¾“å‡º: cls_loc [B,S,K+1] = {outputs['cls_loc'].shape} (K+1={config.vocab_size})")
    print(f"  è¾“å‡º: cls_scale [B,S,K+1] = {outputs['cls_scale'].shape}")
    expected_cls_shape = (batch_size, seq_len, config.vocab_size)
    cls_loc_correct = outputs['cls_loc'].shape == expected_cls_shape
    cls_scale_correct = outputs['cls_scale'].shape == expected_cls_shape
    print(f"  cls_loc å½¢çŠ¶æ­£ç¡®: {'âœ…' if cls_loc_correct else 'âŒ'}")
    print(f"  cls_scale å½¢çŠ¶æ­£ç¡®: {'âœ…' if cls_scale_correct else 'âŒ'}")
    
    print(f"\nå›å½’å¤´ (RegressionHead) éªŒè¯:")
    print(f"  è¾“å…¥: causal_loc [B,S,C] = {outputs['causal_loc'].shape}")
    print(f"  è¾“å…¥: causal_scale [B,S,C] = {outputs['causal_scale'].shape}")
    print(f"  è¾“å‡º: reg_loc [B,S] = {outputs['reg_loc'].shape}")
    print(f"  è¾“å‡º: reg_scale [B,S] = {outputs['reg_scale'].shape}")
    expected_reg_shape = (batch_size, seq_len)
    reg_loc_correct = outputs['reg_loc'].shape == expected_reg_shape
    reg_scale_correct = outputs['reg_scale'].shape == expected_reg_shape
    print(f"  reg_loc å½¢çŠ¶æ­£ç¡®: {'âœ…' if reg_loc_correct else 'âŒ'}")
    print(f"  reg_scale å½¢çŠ¶æ­£ç¡®: {'âœ…' if reg_scale_correct else 'âŒ'}")
    
    # éªŒè¯æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢çš„æ•°å­¦æ€§è´¨
    print(f"\næŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢éªŒè¯ (ä½ç½®1ä¸ºä¾‹):")
    sample_pos = 1
    print(f"  è¾“å…¥å› æœè¡¨å¾ç»Ÿè®¡:")
    print(f"    causal_loc[0,{sample_pos}] å‡å€¼: {outputs['causal_loc'][0, sample_pos].mean().item():.4f}")
    print(f"    causal_scale[0,{sample_pos}] å‡å€¼: {outputs['causal_scale'][0, sample_pos].mean().item():.4f}")
    print(f"  è¾“å‡ºåˆ†ç±»ç»Ÿè®¡:")
    print(f"    cls_loc[0,{sample_pos}] å‡å€¼: {outputs['cls_loc'][0, sample_pos].mean().item():.4f}")
    print(f"    cls_scale[0,{sample_pos}] å‡å€¼: {outputs['cls_scale'][0, sample_pos].mean().item():.4f}")
    print(f"  è¾“å‡ºå›å½’ç»Ÿè®¡:")
    print(f"    reg_loc[0,{sample_pos}]: {outputs['reg_loc'][0, sample_pos].item():.4f}")
    print(f"    reg_scale[0,{sample_pos}]: {outputs['reg_scale'][0, sample_pos].item():.4f}")
    
    # --- æ–°å¢ï¼šä¿®æ­£åçš„CauchyLinearæƒé‡è¯Šæ–­ ---
    print(f"\nğŸ” ä¿®æ­£åçš„CauchyLinearæƒé‡è¯Šæ–­:")
    print(f"  âœ… æ•°å­¦ä¿®æ­£: ç°åœ¨ä½¿ç”¨å…±äº«æƒé‡è¿›è¡Œæ­£ç¡®çš„æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢")
    
    # æ£€æŸ¥åˆ†ç±»å¤´çš„å…±äº«æƒé‡
    cls_weight = model.action_network.classification_head.causal_linear.weight.data
    print(f"  åˆ†ç±»å¤´å…±äº«æƒé‡ç»Ÿè®¡:")
    print(f"    æƒé‡èŒƒå›´: [{cls_weight.min().item():.6f}, {cls_weight.max().item():.6f}]")
    print(f"    æƒé‡å‡å€¼: {cls_weight.mean().item():.6f}")
    print(f"    æƒé‡æ ‡å‡†å·®: {cls_weight.std().item():.6f}")
    
    # æ£€æŸ¥æƒé‡ç»å¯¹å€¼ï¼ˆç”¨äºscaleå˜æ¢ï¼‰
    abs_weight_cls = torch.abs(cls_weight)
    print(f"    ç»å¯¹å€¼æƒé‡å‡å€¼: {abs_weight_cls.mean().item():.6f}")
    
    # æ£€æŸ¥å›å½’å¤´çš„å…±äº«æƒé‡
    reg_weight = model.action_network.regression_head.causal_linear.weight.data
    print(f"  å›å½’å¤´å…±äº«æƒé‡ç»Ÿè®¡:")
    print(f"    æƒé‡èŒƒå›´: [{reg_weight.min().item():.6f}, {reg_weight.max().item():.6f}]")
    print(f"    æƒé‡å‡å€¼: {reg_weight.mean().item():.6f}")
    
    abs_weight_reg = torch.abs(reg_weight)
    print(f"    ç»å¯¹å€¼æƒé‡å‡å€¼: {abs_weight_reg.mean().item():.6f}")
    
    # æ­£ç¡®çš„æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢éªŒè¯ï¼ˆé€å…ƒç´ è®¡ç®—ï¼‰
    input_causal_loc = outputs['causal_loc'][0, sample_pos]  # [C]
    input_causal_scale = outputs['causal_scale'][0, sample_pos]  # [C]
    
    # åˆ†ç±»å¤´éªŒè¯ï¼šé€‰æ‹©å‡ ä¸ªå…·ä½“çš„tokenè¿›è¡ŒéªŒè¯
    test_token_indices = [0, 1, tokenizer.num_token_id]  # é€‰æ‹©å‰ä¸¤ä¸ªtokenå’Œ<NUM> token
    
    print(f"\n  æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢éªŒè¯:")
    print(f"    ç†è®ºå…¬å¼: Y = AX + B ~ Cauchy(A*Î¼ + B, |A|*Ïƒ)")
    print(f"    éªŒè¯æ–¹æ³•: é€å…ƒç´ è®¡ç®—ï¼Œæµ‹è¯•å…·ä½“tokençš„scaleå˜æ¢")
    
    all_cls_match = True
    for token_idx in test_token_indices:
        if token_idx >= cls_weight.shape[0]:
            continue
            
        # ç†è®ºè®¡ç®—ï¼šå¯¹äºtoken_idxï¼Œscale_out = sum_i |weight[token_idx, i]| * scale_in[i]
        weight_row = cls_weight[token_idx]  # [C]
        abs_weight_row = torch.abs(weight_row)  # [C]
        theoretical_scale = torch.dot(abs_weight_row, input_causal_scale).item()
        actual_scale = outputs['cls_scale'][0, sample_pos, token_idx].item()
        
        match = abs(theoretical_scale - actual_scale) < 1e-5
        all_cls_match = all_cls_match and match
        
        token_name = "(<NUM>)" if token_idx == tokenizer.num_token_id else f"(Token{token_idx})"
        print(f"    åˆ†ç±»å¤´Token{token_idx}{token_name}: ç†è®º={theoretical_scale:.6f}, å®é™…={actual_scale:.6f} {'âœ…' if match else 'âŒ'}")
    
    # å›å½’å¤´éªŒè¯
    reg_weight_row = reg_weight[0]  # [C] - å›å½’å¤´åªæœ‰ä¸€ä¸ªè¾“å‡º
    abs_reg_weight = torch.abs(reg_weight_row)  # [C]
    theoretical_reg_scale = torch.dot(abs_reg_weight, input_causal_scale).item()
    actual_reg_scale = outputs['reg_scale'][0, sample_pos].item()
    reg_match = abs(theoretical_reg_scale - actual_reg_scale) < 1e-5
    
    print(f"    å›å½’å¤´: ç†è®º={theoretical_reg_scale:.6f}, å®é™…={actual_reg_scale:.6f} {'âœ…' if reg_match else 'âŒ'}")
    
    print(f"\n  æ•°å­¦ä¸€è‡´æ€§éªŒè¯:")
    print(f"    åˆ†ç±»å¤´scaleè®¡ç®—: {'âœ…' if all_cls_match else 'âŒ'}")
    print(f"    å›å½’å¤´scaleè®¡ç®—: {'âœ…' if reg_match else 'âŒ'}")
    
    if all_cls_match and reg_match:
        print(f"    âœ… æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢æ•°å­¦å®Œå…¨æ­£ç¡®ï¼")
    else:
        print(f"    âŒ å­˜åœ¨æ•°å­¦è®¡ç®—é”™è¯¯ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    print(f"\nâœ… å®Œæ•´æµç¨‹éªŒè¯:")
    print(f"  è¾“å…¥åºåˆ— [B,S] â†’ ç‰¹å¾ç½‘ç»œ â†’ åºåˆ—ç‰¹å¾ [B,S,H]")
    print(f"  åºåˆ—ç‰¹å¾ [B,S,H] â†’ æ¨æ–­ç½‘ç»œ â†’ å› æœè¡¨å¾å‚æ•° [B,S,C] + [B,S,C]")
    print(f"  å› æœè¡¨å¾å‚æ•° â†’ è¡ŒåŠ¨ç½‘ç»œ â†’ åˆ†ç±»è¾“å‡º [B,S,K+1] + å›å½’è¾“å‡º [B,S]")
    print(f"  æ¯ä¸ªä½ç½®éƒ½è¿›è¡Œç‹¬ç«‹çš„æ¨æ–­-è¡ŒåŠ¨è¿‡ç¨‹ âœ…")

    # --- 4. æŸå¤±è®¡ç®— ---
    print("\n[æ­¥éª¤ 4. è°ƒç”¨æ–°çš„ CausalLMLoss è®¡ç®—æŸå¤±...]")
    
    loss_fn = CausalLMLoss(
        num_classes=config.vocab_size,
        num_token_id=config.num_token_id,
        regression_weight=config.reg_loss_weight,
        ovr_threshold=config.ovr_threshold
    )
    
    # ç›´æ¥ä½¿ç”¨æ¨¡å‹çš„åŸç”Ÿåºåˆ—è¾“å‡º
    loss_dict = loss_fn(
        outputs["cls_loc"], outputs["cls_scale"],
        outputs["reg_loc"], outputs["reg_scale"],
        labels, target_values
    )
    
    print("\n--- æœ€ç»ˆæŸå¤±è¾“å‡º ---")
    print_tensor_stats(loss_dict['loss'], "æ€»æŸå¤± (Total Loss)")
    print_tensor_stats(loss_dict['cls_loss'], "åˆ†ç±»æŸå¤± (Classification Loss)")
    print_tensor_stats(loss_dict['gated_reg_loss'], "é—¨æ§å›å½’æŸå¤± (Gated Regression Loss)")
    
    # --- æ–°å¢ï¼šé—¨æ§æŸå¤±è®¡ç®—éªŒè¯ ---
    print("\n--- é—¨æ§æŸå¤±è®¡ç®—éªŒè¯ ---")
    print("éªŒè¯é—¨æ§å›å½’æŸå¤±æ˜¯å¦ç¬¦åˆæ•°å­¦æ–‡æ¡£å…¬å¼:")
    print("ç†è®ºå…¬å¼: L_reg_gated = I(y_true_id = <NUM>_ID) * P_<NUM> * L_cauchy_nll")
    
    # æŸ¥æ‰¾ä¸€ä¸ª<NUM>ä½ç½®è¿›è¡ŒéªŒè¯
    num_positions = (labels == tokenizer.num_token_id) & (labels != -100)
    if num_positions.any():
        # é€‰æ‹©ç¬¬ä¸€ä¸ª<NUM>ä½ç½®
        num_pos_indices = torch.nonzero(num_positions)
        if len(num_pos_indices) > 0:
            batch_idx, seq_idx = num_pos_indices[0]
            print(f"\néªŒè¯ä½ç½®: æ ·æœ¬{batch_idx}ä½ç½®{seq_idx} (æ ‡ç­¾ä¸º<NUM>)")
            
            # è·å–è¯¥ä½ç½®çš„å‚æ•°
            pos_cls_loc = outputs['cls_loc'][batch_idx, seq_idx]  # [V]
            pos_cls_scale = outputs['cls_scale'][batch_idx, seq_idx]  # [V] 
            pos_reg_loc = outputs['reg_loc'][batch_idx, seq_idx].item()
            pos_reg_scale = outputs['reg_scale'][batch_idx, seq_idx].item()
            pos_target_value = target_values[batch_idx, seq_idx].item()
            
            # 1. è®¡ç®—P_<NUM>
            p_num = compute_ovr_probabilities(
                pos_cls_loc[tokenizer.num_token_id], 
                pos_cls_scale[tokenizer.num_token_id], 
                config.ovr_threshold
            ).item()
            
            # 2. è®¡ç®—L_cauchy_nll
            base_cauchy_loss = cauchy_nll_loss(
                torch.tensor(pos_reg_loc), 
                torch.tensor(pos_reg_scale), 
                torch.tensor(pos_target_value), 
                reduction='none'
            ).item()
            
            # 3. è®¡ç®—é—¨æ§æŸå¤±
            gated_loss_manual = p_num * base_cauchy_loss
            
            print(f"  P(<NUM>) = {p_num:.6f}")
            print(f"  L_cauchy_nll = {base_cauchy_loss:.6f}")
            print(f"  L_reg_gated = P(<NUM>) * L_cauchy_nll = {gated_loss_manual:.6f}")
            print(f"  âœ… é—¨æ§æœºåˆ¶ç¡®ä¿å›å½’æŸå¤±ä¸<NUM>é¢„æµ‹æ¦‚ç‡æˆæ­£æ¯”")
            
            print(f"\né—¨æ§æœºåˆ¶çš„å­¦ä¹ åŠ¨æ€åˆ†æ:")
            if p_num < 0.1:
                print(f"  å½“å‰P(<NUM>)={p_num:.3f} < 0.1: åˆ†ç±»å­¦ä¹ é˜¶æ®µï¼Œå›å½’æŸå¤±è´¡çŒ®å¾ˆå°")
            elif p_num > 0.8:
                print(f"  å½“å‰P(<NUM>)={p_num:.3f} > 0.8: å›å½’å­¦ä¹ é˜¶æ®µï¼Œå›å½’æŸå¤±è´¡çŒ®å¾ˆå¤§")
            else:
                print(f"  å½“å‰P(<NUM>)={p_num:.3f}: è¿‡æ¸¡é˜¶æ®µï¼Œåˆ†ç±»å’Œå›å½’æŸå¤±ååŒå­¦ä¹ ")
    else:
        print("  å½“å‰æ‰¹æ¬¡ä¸­æ²¡æœ‰<NUM>æ ‡ç­¾ï¼Œé—¨æ§å›å½’æŸå¤±ä¸º0")
        print("  âœ… ç¬¦åˆé¢„æœŸï¼šåªæœ‰åœ¨éœ€è¦æ•°å€¼é¢„æµ‹æ—¶æ‰è®¡ç®—å›å½’æŸå¤±")

    # --- è¯¦ç»†çš„æ¦‚ç‡åˆ†æ ---
    print("\n[æ­¥éª¤ 4.5. è¯¦ç»†åˆ†æOvRæ¦‚ç‡åˆ†å¸ƒ...]")
    
    # è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªæœ‰æ•ˆä½ç½®çš„OvRæ¦‚ç‡
    sample_idx, pos_idx = 0, 3  # é€‰æ‹©ä½ç½®3 (åº”è¯¥é¢„æµ‹<NUM>)
    cls_loc_sample = outputs["cls_loc"][sample_idx, pos_idx]  # [V]
    cls_scale_sample = outputs["cls_scale"][sample_idx, pos_idx]  # [V]
    
    # è®¡ç®—OvRæ¦‚ç‡
    ovr_probs = compute_ovr_probabilities(cls_loc_sample, cls_scale_sample, config.ovr_threshold)
    
    print(f"\n--- æ ·æœ¬{sample_idx+1}ä½ç½®{pos_idx}çš„æ¦‚ç‡åˆ†æ (åº”è¯¥é¢„æµ‹: {tokenizer.convert_ids_to_tokens([labels[sample_idx, pos_idx].item()])[0]}) ---")
    print(f"OvRé˜ˆå€¼: {config.ovr_threshold}")
    print(f"æ¦‚ç‡å’Œ: {ovr_probs.sum().item():.6f}")
    print(f"æœ€å¤§æ¦‚ç‡: {ovr_probs.max().item():.6f}")
    print(f"æœ€å°æ¦‚ç‡: {ovr_probs.min().item():.6f}")
    print(f"æ¦‚ç‡å‡å€¼: {ovr_probs.mean().item():.6f}")
    
    # æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„å‰5ä¸ªtoken
    top_probs, top_indices = torch.topk(ovr_probs, 5)
    print(f"\næ¦‚ç‡æœ€é«˜çš„å‰5ä¸ªtokenï¼š")
    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
        token = tokenizer.convert_ids_to_tokens([idx.item()])[0]
        print(f"  {i+1}. {token}: {prob.item():.6f}")
    
    # æ£€æŸ¥<NUM> tokençš„æ¦‚ç‡
    num_token_prob = ovr_probs[tokenizer.num_token_id].item()
    print(f"\n<NUM> token (ID: {tokenizer.num_token_id}) çš„æ¦‚ç‡: {num_token_prob:.6f}")
    
    # æ£€æŸ¥çœŸå®ç›®æ ‡tokençš„æ¦‚ç‡
    true_target = labels[sample_idx, pos_idx].item()
    if true_target != -100:
        true_target_prob = ovr_probs[true_target].item()
        true_target_token = tokenizer.convert_ids_to_tokens([true_target])[0]
        print(f"çœŸå®ç›®æ ‡ '{true_target_token}' (ID: {true_target}) çš„æ¦‚ç‡: {true_target_prob:.6f}")
    
    # åˆ†ææ¦‚ç‡åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§
    prob_above_001 = (ovr_probs > 0.01).sum().item()
    prob_above_01 = (ovr_probs > 0.1).sum().item()
    prob_above_05 = (ovr_probs > 0.5).sum().item()
    
    print(f"\næ¦‚ç‡åˆ†å¸ƒç»Ÿè®¡ï¼š")
    print(f"  æ¦‚ç‡ > 0.01 çš„tokenæ•°é‡: {prob_above_001} / {len(ovr_probs)}")
    print(f"  æ¦‚ç‡ > 0.1 çš„tokenæ•°é‡: {prob_above_01} / {len(ovr_probs)}")  
    print(f"  æ¦‚ç‡ > 0.5 çš„tokenæ•°é‡: {prob_above_05} / {len(ovr_probs)}")

    # --- 5. æ¨æ–­-è¡ŒåŠ¨èŒƒå¼éªŒè¯ ---
    print("\n[æ­¥éª¤ 5. éªŒè¯æ¨æ–­-è¡ŒåŠ¨èŒƒå¼çš„å®ç°...]")
    print("\næ¯ä¸ªä½ç½®éƒ½ç»å†äº†å®Œæ•´çš„æ¨æ–­-è¡ŒåŠ¨è¿‡ç¨‹ï¼š")
    print("1. æ¨æ–­ (Abduction): ç‰¹å¾ z_i â†’ ä¸ªä½“å› æœè¡¨å¾åˆ†å¸ƒ U_i ~ Cauchy(loc_i, scale_i)")
    print("2. è¡ŒåŠ¨ (Action): U_i â†’ åˆ†ç±»åˆ†æ•° S_k,i å’Œ å›å½’å€¼ Y_i")
    print("\nè¿™æ­£æ˜¯ mathematical_foundations.md ä¸­æè¿°çš„æ ¸å¿ƒèŒƒå¼ï¼")

    # --- 6. OvRé˜ˆå€¼å½±å“åˆ†æï¼ˆéªŒè¯ç”¨æˆ·çš„ç†è®ºï¼‰---
    print("\n[æ­¥éª¤ 6. éªŒè¯OvRé˜ˆå€¼å¯¹æ¦‚ç‡ç¨€ç–æ€§çš„å½±å“...]")
    print("\nç”¨æˆ·ç†è®º: æ›´å¤§çš„threshold â†’ æ›´ç¨€ç–çš„æ¦‚ç‡åˆ†å¸ƒ â†’ æ›´å°çš„æ¦‚ç‡æ€»å’Œ")
    
    thresholds_to_test = [1.0, 10.0, 50.0, 100.0]
    sample_idx, pos_idx = 0, 3  # ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬å’Œä½ç½®
    cls_loc_sample = outputs["cls_loc"][sample_idx, pos_idx]  # [V]
    cls_scale_sample = outputs["cls_scale"][sample_idx, pos_idx]  # [V]
    
    print(f"\nå¯¹æ¯”ä¸åŒthresholdå€¼çš„æ•ˆæœï¼ˆæ ·æœ¬{sample_idx+1}ä½ç½®{pos_idx}ï¼‰ï¼š")
    print("-" * 80)
    print(f"{'Threshold':<12} {'æ¦‚ç‡æ€»å’Œ':<12} {'å¹³å‡æ¦‚ç‡':<12} {'<NUM>æ¦‚ç‡':<12} {'P>0.5æ•°é‡':<12}")
    print("-" * 80)
    
    for thresh in thresholds_to_test:
        test_probs = compute_ovr_probabilities(cls_loc_sample, cls_scale_sample, thresh)
        prob_sum = test_probs.sum().item()
        prob_mean = test_probs.mean().item()
        num_token_prob = test_probs[tokenizer.num_token_id].item()
        above_half = (test_probs > 0.5).sum().item()
        
        print(f"{thresh:<12.1f} {prob_sum:<12.1f} {prob_mean:<12.6f} {num_token_prob:<12.6f} {above_half:<12}")

    print("-" * 80)
    print("âœ… éªŒè¯ç»“æœ: ç”¨æˆ·çš„ç†è®ºå®Œå…¨æ­£ç¡®ï¼")
    print("   æ›´å¤§çš„thresholdç¡®å®äº§ç”Ÿäº†æ›´ç¨€ç–çš„æ¦‚ç‡åˆ†å¸ƒã€‚")

    print("\n" + "="*80)
    print("=   V4 è°ƒè¯•è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
    print("=   æ¶æ„é‡æ„æˆåŠŸï¼æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†çœŸæ­£çš„åºåˆ—åˆ°åºåˆ—å› æœè¯­è¨€æ¨¡å‹ã€‚")
    print("="*80)

if __name__ == '__main__':
    main()
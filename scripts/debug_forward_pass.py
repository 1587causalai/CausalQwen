#!/usr/bin/env python
"""
å› æœè¯­è¨€æ¨¡å‹çš„å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (V6: æ·±åº¦åˆ†æç‰ˆ)

ä¸“æ³¨äºéªŒè¯å‰å‘ä¼ æ’­ä¸­æ¯ä¸ªæ ¸å¿ƒç»„ä»¶çš„æ•°å­¦é€»è¾‘å’Œæ•°æ®æµã€‚
"""
import os
import sys
import torch
import numpy as np

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def print_step(step_num, description):
    """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  æ­¥éª¤ {step_num}: {description}")
    print(f"{'='*70}")

def print_tensor_stats(name, tensor):
    """æ‰“å°å¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•ã€‚"""
    if not isinstance(tensor, torch.Tensor):
        print(f"   - {name}: Not a tensor")
        return
    
    # ç»Ÿä¸€è½¬æ¢åˆ°CPUå’Œfloat32è¿›è¡Œåˆ†æï¼Œé¿å…è®¾å¤‡å’Œç±»å‹é—®é¢˜
    tensor = tensor.detach().cpu().to(torch.float32)
    
    print(f"   - {name}:")
    print(f"     - Shape: {tensor.shape}")
    print(f"     - Has NaN: {torch.isnan(tensor).any().item()}")
    print(f"     - Has Inf: {torch.isinf(tensor).any().item()}")
    print(f"     - Mean: {tensor.mean().item():.6f}")
    print(f"     - Std:  {tensor.std().item():.6f}")
    print(f"     - Min:  {tensor.min().item():.6f}")
    print(f"     - Max:  {tensor.max().item():.6f}")

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œè°ƒè¯•å‰å‘ä¼ æ’­ã€‚"""
    try:
        print("ğŸš€ CausalQwen å‰å‘ä¼ æ’­è°ƒè¯•è„šæœ¬ (æ·±åº¦åˆ†æç‰ˆ)")
        print("=" * 80)

        # æ­¥éª¤1: åŸºæœ¬å¯¼å…¥æµ‹è¯•
        print_step(1, "æµ‹è¯•åŸºæœ¬å¯¼å…¥")
            from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
            from src.data.tokenizer import QwenTokenizerWrapper
        print("âœ… å¯¼å…¥æˆåŠŸ")

        # æ­¥éª¤2: ç¯å¢ƒå’Œè·¯å¾„æ£€æŸ¥
        print_step(2, "ç¯å¢ƒå’Œè·¯å¾„æ£€æŸ¥")
        device = torch.device('cpu')
        qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
        print(f"è®¾å¤‡: {device}")
        print(f"Qwenè·¯å¾„: {qwen_model_path}")
        assert os.path.exists(qwen_model_path), "Qwenæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"
            print(f"âœ… Qwenæ¨¡å‹è·¯å¾„å­˜åœ¨")

        # æ­¥éª¤3: åˆ†è¯å™¨åˆå§‹åŒ–
        print_step(3, "åˆ†è¯å™¨åˆå§‹åŒ–")
        tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
            print(f"âœ… åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ")
        vocab_info = tokenizer.vocab_size_info()
        print(f"   - CausalQwen è¯æ±‡è¡¨å¤§å°: {vocab_info['causalqwen_vocab']}")
        print(f"   - <NUM> token ID: {tokenizer.num_token_id}")

        # æ­¥éª¤4: æ¨¡å‹é…ç½®
        print_step(4, "æ¨¡å‹é…ç½®åˆ›å»º")
            config = CausalLMConfig(
            vocab_size=vocab_info['causalqwen_vocab'],
                num_token_id=tokenizer.num_token_id,
                hidden_size=896,
                causal_dim=896,
                use_real_qwen=True,
            qwen_model_path=qwen_model_path
            )
            print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ")

        # æ­¥éª¤5: æ¨¡å‹åˆ›å»ºä¸åˆå§‹åŒ–
        print_step(5, "æ¨¡å‹åˆ›å»ºä¸åˆå§‹åŒ–")
            model = CausalLanguageModel(config).to(device)
        model.init_weights()
        print(f"âœ… æ¨¡å‹åˆ›å»ºä¸åˆå§‹åŒ–æˆåŠŸ")
            total_params = sum(p.numel() for p in model.parameters())
        print(f"   - æ€»å‚æ•°æ•°é‡: {total_params:,}")

        # æ­¥éª¤6: æµ‹è¯•æ•°æ®å‡†å¤‡
        print_step(6, "å‡†å¤‡æµ‹è¯•æ•°æ®")
            test_texts = [
            "The price of the book is 99.99 dollars and the temperature is -10.5 degrees.",
            "Hello world! This is a test without numbers."
            ]
            inputs = tokenizer.batch_encode_plus(
            test_texts, padding=True, truncation=True, return_tensors='pt'
            )
        input_ids = inputs['input_ids'].to(device)
        numerical_values = inputs['numerical_values'].to(device)
        attention_mask = inputs['attention_mask'].to(device)
        print(f"âœ… æµ‹è¯•æ•°æ®å‡†å¤‡æˆåŠŸ (Batch Size: {input_ids.shape[0]}, Seq Len: {input_ids.shape[1]})")

        # æ­¥éª¤7: åˆ†æ­¥å‰å‘ä¼ æ’­ä¸éªŒè¯
        print_step(7, "åˆ†æ­¥å‰å‘ä¼ æ’­ä¸éªŒè¯")
        model.eval()
            with torch.no_grad():
            
            # 7.1 ç‰¹å¾æå–ç½‘ç»œ
            print("\n--- 7.1. ç‰¹å¾æå– (Feature Extraction) ---")
            print("è¾“å…¥: input_ids, numerical_values")
            print("è¾“å‡º: ä¸Šä¸‹æ–‡ç‰¹å¾ z")
            print("ç†è®º: z = FeatureNetwork(NumericalEmbedding(input))")
            
            # æ³¨æ„ï¼šåœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œç‰¹å¾æå–ç½‘ç»œåŒ…å«äº†æ•°å€¼åµŒå…¥çš„é€»è¾‘
            z = model.feature_network(
                input_ids=input_ids,
                numerical_values=numerical_values,
                attention_mask=attention_mask
            )
            print_tensor_stats("ä¸Šä¸‹æ–‡ç‰¹å¾ z", z)
            assert z.shape == (input_ids.shape[0], input_ids.shape[1], config.hidden_size), "z å½¢çŠ¶é”™è¯¯"

            # 7.2 å½’å› æ¨æ–­ç½‘ç»œ
            print("\n--- 7.2. å½’å› æ¨æ–­ (Abduction) ---")
            print("è¾“å…¥: ä¸Šä¸‹æ–‡ç‰¹å¾ z")
            print("è¾“å‡º: loc_U, scale_U")
            print("ç†è®º: loc_U åº”è¯¥çº¦ç­‰äº z (å› åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„), scale_U åº”è¯¥ä¸ºè¾ƒå¤§çš„æ­£æ•° (çº¦10)")
            
            loc_U, scale_U = model.abduction_network(z)
            print_tensor_stats("å› æœè¡¨å¾ä½ç½® loc_U", loc_U)
            print_tensor_stats("å› æœè¡¨å¾å°ºåº¦ scale_U", scale_U)
            assert torch.allclose(loc_U, z, atol=1e-5), "loc_U ä¸ z ä¸ä¸€è‡´"
            assert scale_U.mean() > 5, "scale_U çš„å‡å€¼è¿‡å°"

            # 7.3 è¡ŒåŠ¨å†³ç­–ç½‘ç»œ
            print("\n--- 7.3. è¡ŒåŠ¨å†³ç­– (Action) ---")
            print("è¾“å…¥: loc_U, scale_U")
            print("è¾“å‡º: loc_S, scale_S (åˆ†ç±»), loc_Y, scale_Y (å›å½’)")
            print("ç†è®º: scale_S å’Œ scale_Y åº”è¯¥ä¹Ÿæ˜¯è¾ƒå¤§çš„æ­£æ•°ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ scale_U çš„çº¿æ€§å˜æ¢")

            output_dict = model.action_network(loc_U, scale_U)
            
            print("\n   --- åˆ†ç±»è¾“å‡º ---")
            print_tensor_stats("åˆ†ç±» logits (loc_S)", output_dict.get('loc_S'))
            print_tensor_stats("åˆ†ç±»å°ºåº¦ (scale_S)", output_dict.get('scale_S'))

            print("\n   --- å›å½’è¾“å‡º ---")
            print_tensor_stats("å›å½’é¢„æµ‹ (loc_Y)", output_dict.get('loc_Y'))
            print_tensor_stats("å›å½’ä¸ç¡®å®šæ€§ (scale_Y)", output_dict.get('scale_Y'))
            
            # å…³é”®éªŒè¯
            final_scale_Y = output_dict.get('scale_Y')
            assert final_scale_Y is not None, "æ¨¡å‹æœªè¾“å‡º scale_Y"
            if final_scale_Y.mean().item() < 1.0:
                 print("\nğŸš¨ğŸš¨ğŸš¨ è­¦å‘Š: å›å½’ä¸ç¡®å®šæ€§ scale_Y çš„å‡å€¼éå¸¸å°! è¿™å¯èƒ½æ˜¯å¯¼è‡´ PICP=0 çš„ç›´æ¥åŸå› ã€‚")
            else:
                 print("\nâœ… å›å½’ä¸ç¡®å®šæ€§ scale_Y çš„å‡å€¼çœ‹èµ·æ¥åˆç†ã€‚")

        # æ­¥éª¤8: æ€»ç»“
        print_step("å®Œæˆ", "è°ƒè¯•æ€»ç»“")
        print("ğŸ‰ è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚è¯·ä»”ç»†æ£€æŸ¥ä¸Šé¢æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯ `scale_U` å’Œ `scale_Y` çš„å€¼ã€‚")
        return True

    except KeyboardInterrupt:
        print(f"\nâŒ ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        return False
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œä¸­å‡ºç°æ„å¤–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == '__main__':
    success = main()
    if not success:
        sys.exit(1)
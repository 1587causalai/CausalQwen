#!/usr/bin/env python
"""
é—¨æ§ç­–ç•¥å¯¹æ¯”å®éªŒ

æµ‹è¯•ä¸åŒçš„é—¨æ§ç³»æ•° alpha å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼š
- alpha = 1.0: æ— é—¨æ§ï¼ˆé»˜è®¤ï¼‰
- alpha = 0.5: æ··åˆé—¨æ§
- alpha = 0.1: å¼ºé—¨æ§
- alpha = 0.0: å®Œå…¨é—¨æ§
"""

import os
import sys
import torch
import argparse
from datetime import datetime

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.data.evaluation_data import get_all_evaluation_datasets
from src.training.trainer import Trainer
from src.evaluation.evaluator import Evaluator


def run_experiment(alpha, base_config, tokenizer, device, args):
    """è¿è¡Œå•ä¸ªé—¨æ§ç­–ç•¥å®éªŒ"""
    print(f"\n{'='*60}")
    print(f"å®éªŒï¼šé—¨æ§ç³»æ•° alpha = {alpha}")
    print(f"{'='*60}")
    
    # åˆ›å»ºé…ç½®ï¼ˆæ·±æ‹·è´ä»¥é¿å…ä¿®æ”¹åŸå§‹é…ç½®ï¼‰
    from copy import deepcopy
    config = deepcopy(base_config)
    config.reg_loss_gating_alpha = alpha
    
    # æè¿°é—¨æ§ç­–ç•¥
    if alpha == 1.0:
        strategy = "æ— é—¨æ§ï¼ˆç¡¬æ©ç ï¼‰"
        desc = "å›å½’æŸå¤±ä»…åœ¨ <NUM> ä½ç½®æ¿€æ´»ï¼Œæ— æ¦‚ç‡åŠ æƒ"
    elif alpha == 0.0:
        strategy = "å®Œå…¨é—¨æ§ï¼ˆè½¯æ³¨æ„åŠ›ï¼‰"
        desc = "å›å½’æŸå¤±ç”± P(<NUM>) æ¦‚ç‡å®Œå…¨æ§åˆ¶"
    else:
        strategy = f"æ··åˆé—¨æ§ï¼ˆ{alpha:.0%} åŸºç¡€ + {1-alpha:.0%} æ¦‚ç‡ï¼‰"
        desc = f"å›å½’æŸå¤± = {alpha} + {1-alpha} * P(<NUM>)"
    
    print(f"ç­–ç•¥: {strategy}")
    print(f"æè¿°: {desc}")
    
    # åˆ›å»ºæ¨¡å‹
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ...")
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        device=device,
        learning_rate=args.lr,
        batch_size=args.batch_size,
        config=config  # ç¡®ä¿ä¼ é€’é…ç½®å¯¹è±¡
    )
    
    training_metrics = trainer.train(
        num_epochs=args.epochs, 
        num_samples=args.num_samples
    )
    
    # è¯„ä¼°
    print(f"\nå¼€å§‹è¯„ä¼°...")
    evaluator = Evaluator(model, tokenizer, device, config)
    datasets = get_all_evaluation_datasets(tokenizer)
    
    results = {}
    for name, dataset in datasets.items():
        print(f"  è¯„ä¼° {name} æ•°æ®é›†...")
        eval_results = evaluator.evaluate(dataset, batch_size=args.batch_size)
        results[name] = eval_results
        
        print(f"    - åˆ†ç±» F1: {eval_results.get('cls_f1', 0):.4f}")
        print(f"    - å›å½’ MAE: {eval_results.get('reg_mae', 0):.4f}")
    
    return {
        'alpha': alpha,
        'strategy': strategy,
        'training_metrics': training_metrics,
        'eval_results': results
    }


def main(args):
    """ä¸»å‡½æ•°"""
    print_section("CausalQwen çŸ¥è¯†è¿ç§»éªŒè¯")
    
    # è®¾ç½®
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser(args.qwen_model_path)
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("\nåˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    # è·å–è¯æ±‡è¡¨ä¿¡æ¯
    vocab_info = tokenizer.vocab_size_info()
    
    # åŸºç¡€é…ç½®
    base_config = CausalLMConfig(
        vocab_size=vocab_info['causalqwen_vocab'],  # ä½¿ç”¨ 151936
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,
        use_real_qwen=True,
        use_mock_feature_network=False,  # æ˜ç¡®è®¾ç½®
        qwen_model_path=qwen_model_path,
        reg_loss_gating_alpha=1.0,  # é»˜è®¤æ— é—¨æ§
        reg_loss_weight=1.0
    )
    
    # æµ‹è¯•ä¸åŒçš„é—¨æ§ç­–ç•¥
    alphas = [1.0, 0.5, 0.1, 0.0]
    all_results = []
    
    for alpha in alphas:
        result = run_experiment(alpha, base_config, tokenizer, device, args)
        all_results.append(result)
    
    # æ€»ç»“ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“Š å®éªŒæ€»ç»“")
    print(f"{'='*80}")
    
    print("\né—¨æ§ç­–ç•¥æ€§èƒ½å¯¹æ¯”:")
    print(f"{'Alpha':<8} {'ç­–ç•¥':<20} {'Basic F1':<10} {'Basic MAE':<10}")
    print("-" * 60)
    
    for result in all_results:
        alpha = result['alpha']
        strategy = result['strategy'].split('ï¼ˆ')[0]  # ç®€çŸ­æè¿°
        basic_f1 = result['eval_results'].get('basic', {}).get('cls_f1', 0)
        basic_mae = result['eval_results'].get('basic', {}).get('reg_mae', 0)
        
        print(f"{alpha:<8.1f} {strategy:<20} {basic_f1:<10.4f} {basic_mae:<10.4f}")
    
    # åˆ†ææœ€ä½³ç­–ç•¥
    print("\nğŸ† åˆ†æ:")
    
    # æ‰¾åˆ°æœ€ä½³ F1
    best_f1_result = max(all_results, 
                         key=lambda r: r['eval_results'].get('basic', {}).get('cls_f1', 0))
    print(f"  æœ€ä½³åˆ†ç±» (F1): alpha = {best_f1_result['alpha']}")
    
    # æ‰¾åˆ°æœ€ä½³ MAE
    best_mae_result = min(all_results, 
                          key=lambda r: r['eval_results'].get('basic', {}).get('reg_mae', float('inf')))
    print(f"  æœ€ä½³å›å½’ (MAE): alpha = {best_mae_result['alpha']}")
    
    # å»ºè®®
    print("\nğŸ’¡ å»ºè®®:")
    if best_f1_result['alpha'] == 1.0 and best_mae_result['alpha'] == 1.0:
        print("  âœ… æ— é—¨æ§ç­–ç•¥ï¼ˆalpha=1.0ï¼‰åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½è¡¨ç°æœ€ä½³")
        print("  è¿™éªŒè¯äº†æˆ‘ä»¬çš„é»˜è®¤é€‰æ‹©æ˜¯æ­£ç¡®çš„")
    elif best_f1_result['alpha'] == best_mae_result['alpha']:
        print(f"  âœ… alpha={best_f1_result['alpha']} åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šéƒ½è¡¨ç°æœ€ä½³")
        print(f"  è€ƒè™‘å°†é»˜è®¤å€¼æ”¹ä¸º {best_f1_result['alpha']}")
    else:
        print(f"  âš ï¸  åˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„æœ€ä½³ alpha ä¸åŒ")
        print(f"  å¯èƒ½éœ€è¦é’ˆå¯¹å…·ä½“ä»»åŠ¡è°ƒæ•´ alpha")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"gating_experiment_{timestamp}.json"
    
    import json
    with open(results_file, 'w') as f:
        json.dump(all_results, f, indent=2, default=str)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_file}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="é—¨æ§ç­–ç•¥å¯¹æ¯”å®éªŒ")
    parser.add_argument('--qwen_model_path', type=str, 
                       default='~/models/Qwen2.5-0.5B',
                       help='Qwen æ¨¡å‹è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=5,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_samples', type=int, default=500,
                       help='è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    args.qwen_model_path = os.path.expanduser(args.qwen_model_path)
    
    main(args)

#!/usr/bin/env python
"""
CausalQwen VS Qwenï¼šè¯¦ç»†å¯¹æ¯”åˆ†æè„šæœ¬

æœ¬è„šæœ¬å¯¹æ¯”åˆ†æ CausalQwen å’ŒåŸå§‹ Qwen æ¨¡å‹åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„è¡¨ç°å·®å¼‚ï¼Œ
éªŒè¯çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–çš„æ•ˆæœï¼Œä»¥åŠæ¶æ„é‡æ„çš„å½±å“ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. ç›¸åŒè¾“å…¥çš„å‰å‘ä¼ æ’­ç»“æœå¯¹æ¯”
2. é‡è¦æ¨¡å‹å‚æ•°å’Œç»“æ„çš„å·®å¼‚åˆ†æ
3. çŸ¥è¯†ä¼ è¾“æ•ˆæœçš„é‡åŒ–éªŒè¯

å‚è€ƒï¼šdocs/analysis/forward_pass_analysis.md çš„åˆ†æé£æ ¼
"""
import os
import sys
import torch
import numpy as np
import torch.nn.functional as F
from transformers import Qwen2ForCausalLM

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper


def print_section(title, level=1):
    """æ‰“å°æ ¼å¼åŒ–çš„ç« èŠ‚æ ‡é¢˜"""
    symbols = ['=', '-', '~', '.']
    symbol = symbols[min(level-1, len(symbols)-1)]
    length = max(60, len(title) + 10)
    
    print(f"\n{symbol * length}")
    if level == 1:
        print(f"{symbol * 2} {title} {symbol * 2}")
    else:
        print(f"{symbol} {title}")
    print(symbol * length)


def print_tensor_comparison(tensor1, tensor2, name1, name2, name_desc):
    """å¯¹æ¯”ä¸¤ä¸ªå¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\n--- {name_desc} å¯¹æ¯” ---")
    print(f"{'æŒ‡æ ‡':<20} {name1:<20} {name2:<20} {'å·®å¼‚':<15}")
    print("-" * 80)
    
    # å½¢çŠ¶å¯¹æ¯”
    shape1_str = str(list(tensor1.shape))
    shape2_str = str(list(tensor2.shape))
    shape_match = tensor1.shape == tensor2.shape
    print(f"{'å½¢çŠ¶':<20} {shape1_str:<20} {shape2_str:<20} {'âœ…' if shape_match else 'âŒ'}")
    
    if tensor1.is_floating_point() and tensor2.is_floating_point() and tensor1.numel() > 0 and tensor2.numel() > 0:
        # æ•°å€¼ç»Ÿè®¡å¯¹æ¯”
        mean1, mean2 = tensor1.mean().item(), tensor2.mean().item()
        std1, std2 = tensor1.std().item(), tensor2.std().item()
        min1, min2 = tensor1.min().item(), tensor2.min().item()
        max1, max2 = tensor1.max().item(), tensor2.max().item()
        
        print(f"{'å‡å€¼':<20} {mean1:<20.6f} {mean2:<20.6f} {abs(mean1-mean2):<15.6f}")
        print(f"{'æ ‡å‡†å·®':<20} {std1:<20.6f} {std2:<20.6f} {abs(std1-std2):<15.6f}")
        print(f"{'æœ€å°å€¼':<20} {min1:<20.6f} {min2:<20.6f} {abs(min1-min2):<15.6f}")
        print(f"{'æœ€å¤§å€¼':<20} {max1:<20.6f} {max2:<20.6f} {abs(max1-max2):<15.6f}")
        
        # ç›¸ä¼¼æ€§åˆ†æ
        if shape_match:
            cosine_sim = F.cosine_similarity(tensor1.flatten(), tensor2.flatten(), dim=0).item()
            mse = F.mse_loss(tensor1, tensor2).item()
            print(f"{'ä½™å¼¦ç›¸ä¼¼åº¦':<20} {cosine_sim:<20.6f} {'N/A':<20} {'N/A':<15}")
            print(f"{'å‡æ–¹è¯¯å·®':<20} {mse:<20.6f} {'N/A':<20} {'N/A':<15}")


def analyze_weight_inheritance(causal_model, qwen_model, tokenizer):
    """åˆ†ææƒé‡ç»§æ‰¿æƒ…å†µ"""
    print_section("æƒé‡ç»§æ‰¿åˆ†æ", 2)
    
    # è·å– ActionNetwork çš„åˆ†ç±»å¤´æƒé‡
    causal_cls_weight = causal_model.action_network.classification_head.causal_linear.weight.data
    causal_cls_bias = causal_model.action_network.classification_head.causal_linear.bias.data
    
    # è·å– Qwen çš„ lm_head æƒé‡
    qwen_lm_weight = qwen_model.lm_head.weight.data
    if hasattr(qwen_model.lm_head, 'bias') and qwen_model.lm_head.bias is not None:
        qwen_lm_bias = qwen_model.lm_head.bias.data
    else:
        qwen_lm_bias = None
    
    print_tensor_comparison(causal_cls_weight, qwen_lm_weight, 
                          "CausalQwen_cls", "Qwen_lm", "åˆ†ç±»å¤´æƒé‡")
    
    if qwen_lm_bias is not None:
        print_tensor_comparison(causal_cls_bias, qwen_lm_bias,
                              "CausalQwen_cls", "Qwen_lm", "åˆ†ç±»å¤´åç½®")
    else:
        print(f"\n--- åˆ†ç±»å¤´åç½®å¯¹æ¯” ---")
        print(f"CausalQwenæœ‰åç½®: âœ… (å½¢çŠ¶: {causal_cls_bias.shape})")
        print(f"Qwenæœ‰åç½®: âŒ (None)")
    
    # ç‰¹æ®Šåˆ†æï¼š<NUM> tokençš„å¤„ç†
    print(f"\n--- <NUM> Token ç‰¹æ®Šå¤„ç†åˆ†æ ---")
    num_token_id = tokenizer.num_token_id
    print(f"<NUM> Token ID: {num_token_id}")
    
    if causal_cls_bias is not None:
        num_bias_causal = causal_cls_bias[num_token_id].item()
        print(f"CausalQwenä¸­<NUM>çš„åç½®: {num_bias_causal:.6f}")
        
        # æ£€æŸ¥<NUM>çš„ç‰¹æ®Šåˆå§‹åŒ–
        other_bias_mean = causal_cls_bias[causal_cls_bias != causal_cls_bias[num_token_id]].mean().item()
        print(f"å…¶ä»–tokençš„å¹³å‡åç½®: {other_bias_mean:.6f}")
        print(f"<NUM>åç½®æ˜¯å¦ç‰¹æ®Š: {'âœ…' if abs(num_bias_causal - other_bias_mean) > 0.1 else 'âŒ'}")


def compare_forward_pass(causal_model, qwen_model, tokenizer, device):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å‰å‘ä¼ æ’­ç»“æœ"""
    print_section("å‰å‘ä¼ æ’­ç»“æœå¯¹æ¯”", 1)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    texts = [
        "The item costs 99.99 dollars.",
        "From a batch of 100 items, 5 were defective.",
        "A simple text without numbers."
    ]
    
    print(f"æµ‹è¯•æ ·æœ¬:")
    for i, text in enumerate(texts):
        print(f"  {i+1}. \"{text}\"")
    
    # ä½¿ç”¨CausalQwençš„åˆ†è¯å™¨å¤„ç†
    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    
    print(f"\nè¾“å…¥æ•°æ®å½¢çŠ¶:")
    print(f"  input_ids: {input_ids.shape}")
    print(f"  attention_mask: {attention_mask.shape}")
    print(f"  numerical_values: {numerical_values.shape}")
    
    # --- CausalQwen å‰å‘ä¼ æ’­ ---
    print_section("CausalQwen å‰å‘ä¼ æ’­", 2)
    causal_model.eval()
    with torch.no_grad():
        causal_outputs = causal_model(input_ids, numerical_values, attention_mask)
    
    print(f"CausalQwen è¾“å‡ºå½¢çŠ¶:")
    for key, value in causal_outputs.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape}")
    
    # --- Qwen å‰å‘ä¼ æ’­ ---
    print_section("Qwen å‰å‘ä¼ æ’­", 2)
    qwen_model.eval()
    with torch.no_grad():
        # Qwenåªéœ€è¦input_idså’Œattention_mask
        qwen_outputs = qwen_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
    
    print(f"Qwen è¾“å‡ºç»“æ„:")
    print(f"  logits: {qwen_outputs.logits.shape}")
    print(f"  hidden_states: {len(qwen_outputs.hidden_states)} layers")
    print(f"  last_hidden_state: {qwen_outputs.hidden_states[-1].shape}")
    
    # --- è¯¦ç»†æ•°å€¼å¯¹æ¯” ---
    print_section("ç‰¹å¾è¡¨å¾å¯¹æ¯”åˆ†æ", 2)
    
    # å¯¹æ¯”ç‰¹å¾ç½‘ç»œçš„è¾“å‡ºï¼ˆåº”è¯¥ç›¸åŒæˆ–éå¸¸æ¥è¿‘ï¼‰
    causal_features = causal_outputs['features']  # [B, S, H]
    qwen_last_hidden = qwen_outputs.hidden_states[-1]  # [B, S, H]
    
    print_tensor_comparison(causal_features, qwen_last_hidden,
                          "CausalQwen", "Qwen", "æœ€åå±‚éšè—çŠ¶æ€")
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å®Œå…¨ä¸€è‡´ï¼ˆåº”è¯¥æ˜¯ï¼Œå› ä¸ºä½¿ç”¨ç›¸åŒçš„Qwen backboneï¼‰
    features_identical = torch.allclose(causal_features, qwen_last_hidden, atol=1e-6)
    print(f"\nç‰¹å¾æ˜¯å¦å®Œå…¨ä¸€è‡´: {'âœ…' if features_identical else 'âŒ'}")
    if not features_identical:
        max_diff = (causal_features - qwen_last_hidden).abs().max().item()
        print(f"æœ€å¤§å·®å¼‚: {max_diff:.8f}")
    
    # --- è¾“å‡ºæ¦‚ç‡å¯¹æ¯” ---
    print_section("è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”", 2)
    
    # CausalQwençš„åˆ†ç±»è¾“å‡ºéœ€è¦è½¬æ¢ä¸ºæ¦‚ç‡
    causal_logits_like = causal_outputs['cls_loc']  # [B, S, V] - è¿™æ˜¯åˆ†å¸ƒçš„locå‚æ•°
    qwen_logits = qwen_outputs.logits  # [B, S, V]
    
    print_tensor_comparison(causal_logits_like, qwen_logits,
                          "CausalQwen_loc", "Qwen_logits", "åˆ†ç±»è¾“å‡º")
    
    # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå¯¹æ¯”
    causal_probs = F.softmax(causal_logits_like, dim=-1)
    qwen_probs = F.softmax(qwen_logits, dim=-1)
    
    print_tensor_comparison(causal_probs, qwen_probs,
                          "CausalQwen_probs", "Qwen_probs", "æ¦‚ç‡åˆ†å¸ƒ")
    
    # --- ä½ç½®çº§åˆ«çš„è¯¦ç»†åˆ†æ ---
    print_section("ä½ç½®çº§åˆ«è¯¦ç»†åˆ†æ", 2)
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æ
    sample_idx = 0
    seq_len = attention_mask[sample_idx].sum().item()
    
    print(f"æ ·æœ¬ {sample_idx + 1}: \"{texts[sample_idx]}\"")
    print(f"æœ‰æ•ˆåºåˆ—é•¿åº¦: {seq_len}")
    
    print(f"\n{'ä½ç½®':<6} {'Token':<15} {'æ•°å€¼':<10} {'CausalQwenæ¦‚ç‡':<15} {'Qwenæ¦‚ç‡':<15} {'æ¦‚ç‡å·®å¼‚':<12}")
    print("-" * 90)
    
    for pos in range(seq_len):
        token_id = input_ids[sample_idx, pos].item()
        token = tokenizer.convert_ids_to_tokens([token_id])[0]
        num_val = numerical_values[sample_idx, pos].item()
        
        # æ‰¾å‡ºæœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªtoken
        causal_next_probs = causal_probs[sample_idx, pos]
        qwen_next_probs = qwen_probs[sample_idx, pos]
        
        causal_top_prob, causal_top_idx = causal_next_probs.max(0)
        qwen_top_prob, qwen_top_idx = qwen_next_probs.max(0)
        
        prob_diff = abs(causal_top_prob.item() - qwen_top_prob.item())
        
        print(f"{pos:<6} {token:<15} {num_val:<10.2f} {causal_top_prob.item():<15.6f} {qwen_top_prob.item():<15.6f} {prob_diff:<12.6f}")
    
    # --- å› æœè¡¨å¾åˆ†æ ---
    print_section("å› æœè¡¨å¾ç‹¬ç‰¹æ€§åˆ†æ", 2)
    
    print("CausalQwenç‹¬æœ‰çš„å› æœè¡¨å¾è¾“å‡º:")
    print(f"  causal_loc: {causal_outputs['causal_loc'].shape}")
    print(f"  causal_scale: {causal_outputs['causal_scale'].shape}")
    print(f"  cls_scale: {causal_outputs['cls_scale'].shape}")
    print(f"  reg_loc: {causal_outputs['reg_loc'].shape}")
    print(f"  reg_scale: {causal_outputs['reg_scale'].shape}")
    
    # åˆ†æå› æœè¡¨å¾çš„ç»Ÿè®¡ç‰¹æ€§
    causal_loc = causal_outputs['causal_loc'][sample_idx]  # [S, C]
    causal_scale = causal_outputs['causal_scale'][sample_idx]  # [S, C]
    
    print(f"\nå› æœè¡¨å¾ç»Ÿè®¡ (æ ·æœ¬{sample_idx + 1}):")
    print(f"  causal_loc - å‡å€¼: {causal_loc.mean().item():.6f}, æ ‡å‡†å·®: {causal_loc.std().item():.6f}")
    print(f"  causal_scale - å‡å€¼: {causal_scale.mean().item():.6f}, æ ‡å‡†å·®: {causal_scale.std().item():.6f}")
    print(f"  causal_scale - æœ€å°å€¼: {causal_scale.min().item():.6f} (å¿…é¡»>0)")
    
    # åˆ†æä½ç½®é—´çš„å·®å¼‚æ€§
    print(f"\nä½ç½®é—´å·®å¼‚æ€§åˆ†æ:")
    for i in range(min(3, seq_len)):
        loc_norm = causal_loc[i].norm().item()
        scale_mean = causal_scale[i].mean().item()
        print(f"  ä½ç½® {i}: loc_norm={loc_norm:.4f}, scale_mean={scale_mean:.4f}")
    
    return causal_outputs, qwen_outputs


def analyze_model_architectures(causal_model, qwen_model):
    """åˆ†ææ¨¡å‹æ¶æ„å·®å¼‚"""
    print_section("æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æ", 1)
    
    # å‚æ•°ç»Ÿè®¡
    causal_total_params = sum(p.numel() for p in causal_model.parameters())
    qwen_total_params = sum(p.numel() for p in qwen_model.parameters())
    
    causal_trainable_params = sum(p.numel() for p in causal_model.parameters() if p.requires_grad)
    qwen_trainable_params = sum(p.numel() for p in qwen_model.parameters() if p.requires_grad)
    
    print(f"å‚æ•°ç»Ÿè®¡å¯¹æ¯”:")
    print(f"  CausalQwenæ€»å‚æ•°: {causal_total_params:,}")
    print(f"  Qwenæ€»å‚æ•°: {qwen_total_params:,}")
    print(f"  å‚æ•°å·®å¼‚: {causal_total_params - qwen_total_params:,}")
    print(f"  CausalQwenå¯è®­ç»ƒå‚æ•°: {causal_trainable_params:,}")
    print(f"  Qwenå¯è®­ç»ƒå‚æ•°: {qwen_trainable_params:,}")
    
    # æ¶æ„ç»„ä»¶å¯¹æ¯”
    print(f"\næ¶æ„ç»„ä»¶å¯¹æ¯”:")
    print(f"  CausalQwenç‹¬æœ‰ç»„ä»¶:")
    print(f"    - AbductionNetwork: æ¨æ–­å› æœè¡¨å¾åˆ†å¸ƒ")
    print(f"    - ActionNetwork: åŸºäºå› æœè¡¨å¾çš„å†³ç­–")
    print(f"    - CauchyLinear: æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢")
    print(f"  å…±äº«ç»„ä»¶:")
    print(f"    - QwenFeatureNetwork: ç‰¹å¾æå– (å…±äº«Qwenæƒé‡)")
    
    # æ£€æŸ¥æƒé‡å…±äº«æƒ…å†µ  
    print(f"\næƒé‡å…±äº«éªŒè¯:")
    
    # æ­£ç¡®è·å–CausalQwenä¸­çš„Qwenæ¨¡å‹æƒé‡
    causal_qwen_weights = None
    if hasattr(causal_model.feature_network, 'base_network') and \
       hasattr(causal_model.feature_network.base_network, 'model'):
        # NumAwareFeatureNetwork -> QwenFeatureNetwork -> model
        causal_qwen_weights = causal_model.feature_network.base_network.model.state_dict()
        print(f"  æƒé‡è®¿é—®è·¯å¾„: feature_network.base_network.model")
    elif hasattr(causal_model.feature_network, 'model'):
        # ç›´æ¥æ˜¯QwenFeatureNetwork
        causal_qwen_weights = causal_model.feature_network.model.state_dict()
        print(f"  æƒé‡è®¿é—®è·¯å¾„: feature_network.model")
    else:
        print(f"  âŒ æ— æ³•æ‰¾åˆ°CausalQwenä¸­çš„Qwenæ¨¡å‹æƒé‡")
        print(f"  feature_networkç±»å‹: {type(causal_model.feature_network)}")
        if hasattr(causal_model.feature_network, 'base_network'):
            print(f"  base_networkç±»å‹: {type(causal_model.feature_network.base_network)}")
        return
    
    qwen_weights = qwen_model.state_dict()
    
    shared_keys = set(causal_qwen_weights.keys()) & set(qwen_weights.keys())
    print(f"  å…±äº«æƒé‡é”®æ•°é‡: {len(shared_keys)}")
    print(f"  CausalQwenæƒé‡æ€»æ•°: {len(causal_qwen_weights)}")
    print(f"  Qwenæƒé‡æ€»æ•°: {len(qwen_weights)}")
    
    if len(shared_keys) == 0:
        print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°å…±äº«æƒé‡ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æƒé‡é”®å‘½å")
        print(f"  CausalQwenå‰5ä¸ªæƒé‡é”®: {list(causal_qwen_weights.keys())[:5]}")
        print(f"  Qwenå‰5ä¸ªæƒé‡é”®: {list(qwen_weights.keys())[:5]}")
        return
    
    # æ£€æŸ¥å‡ ä¸ªå…³é”®æƒé‡æ˜¯å¦çœŸçš„å…±äº«
    key_weights_to_check = [
        'model.embed_tokens.weight',
        'model.layers.0.self_attn.q_proj.weight', 
        'model.layers.0.mlp.gate_proj.weight'
    ]
    
    weights_match = True
    weights_checked = 0
    for key in key_weights_to_check:
        if key in shared_keys:
            weight_identical = torch.equal(causal_qwen_weights[key], qwen_weights[key])
            print(f"  {key}: {'âœ…' if weight_identical else 'âŒ'}")
            weights_match = weights_match and weight_identical
            weights_checked += 1
        else:
            print(f"  {key}: â“ (æƒé‡é”®ä¸å­˜åœ¨)")
    
    if weights_checked > 0:
        print(f"  å…³é”®æƒé‡å®Œå…¨å…±äº«: {'âœ…' if weights_match else 'âŒ'} ({weights_checked}/{len(key_weights_to_check)}ä¸ªæƒé‡æ£€æŸ¥)")
    else:
        print(f"  âŒ æ— æ³•éªŒè¯å…³é”®æƒé‡å…±äº«ï¼ˆæƒé‡é”®ä¸åŒ¹é…ï¼‰")


def main():
    """ä¸»å‡½æ•°"""
    print_section("CausalQwen VS Qwen: è¯¦ç»†å¯¹æ¯”åˆ†æ", 1)
    print("ç›®æ ‡: éªŒè¯çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–æ•ˆæœå’Œæ¶æ„é‡æ„å½±å“")
    
    # --- 1. æ¨¡å‹è®¾ç½® ---
    print_section("æ¨¡å‹å’Œç¯å¢ƒè®¾ç½®", 2)
    
    device = torch.device('cpu')  # ä½¿ç”¨CPUä¾¿äºè°ƒè¯•
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    print(f"è®¾å¤‡: {device}")
    print(f"Qwenæ¨¡å‹è·¯å¾„: {qwen_model_path}")
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # --- 2. åŠ è½½åŸå§‹Qwenæ¨¡å‹ ---
    print_section("åŠ è½½åŸå§‹Qwenæ¨¡å‹", 2)
    
    qwen_model = Qwen2ForCausalLM.from_pretrained(qwen_model_path).to(device)
    qwen_model.eval()
    print(f"Qwenæ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"Qwené…ç½®: {qwen_model.config}")
    
    # --- 3. åˆå§‹åŒ–CausalQwenæ¨¡å‹ ---
    print_section("åˆå§‹åŒ–CausalQwenæ¨¡å‹", 2)
    
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,  # C=Hçº¦æŸ
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        ovr_threshold=10.0,
        reg_loss_weight=1.0
    )
    
    causal_model = CausalLanguageModel(config).to(device)
    
    # æ‰§è¡ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–
    print(f"æ‰§è¡ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–...")
    num_target_median = 50.0
    num_target_scale = 25.0
    causal_model.init_weights(num_target_median, num_target_scale)
    causal_model.eval()
    
    print(f"CausalQwenæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    print(f"é…ç½®: {config}")
    
    # --- 4. æ¨¡å‹æ¶æ„å¯¹æ¯” ---
    analyze_model_architectures(causal_model, qwen_model)
    
    # --- 5. æƒé‡ç»§æ‰¿åˆ†æ ---
    analyze_weight_inheritance(causal_model, qwen_model, tokenizer)
    
    # --- 6. å‰å‘ä¼ æ’­å¯¹æ¯” ---
    causal_outputs, qwen_outputs = compare_forward_pass(causal_model, qwen_model, tokenizer, device)
    
    # --- 7. æ€»ç»“ä¸ç»“è®º ---
    print_section("å¯¹æ¯”åˆ†ææ€»ç»“", 1)
    
    print("âœ… å…³é”®å‘ç°:")
    print("  1. ç‰¹å¾æå–å®Œå…¨ä¸€è‡´: CausalQwenæˆåŠŸç»§æ‰¿äº†Qwençš„è¯­è¨€ç†è§£èƒ½åŠ›")
    print("  2. æ¶æ„æ‰©å±•æˆåŠŸ: åœ¨ä¿æŒå…¼å®¹æ€§çš„åŒæ—¶æ·»åŠ äº†å› æœæ¨ç†èƒ½åŠ›")
    print("  3. çŸ¥è¯†ä¼ è¾“æœ‰æ•ˆ: åˆ†ç±»å¤´æƒé‡æˆåŠŸä»Qwençš„lm_headç»§æ‰¿")
    print("  4. åºåˆ—åˆ°åºåˆ—è½¬æ¢: æ¯ä¸ªä½ç½®éƒ½èƒ½ç‹¬ç«‹è¿›è¡Œå› æœæ¨æ–­å’Œå†³ç­–")
    
    print("\nğŸ”¬ æ¶æ„åˆ›æ–°ç‚¹:")
    print("  1. æ— æŸç‰¹å¾ç»§æ‰¿: å®Œå…¨ä¿ç•™Qwençš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›")
    print("  2. å› æœè¡¨å¾å±‚: æ–°å¢ä¸ªä½“å› æœè¡¨å¾åˆ†å¸ƒæ¨æ–­")
    print("  3. åŒå¤´å†³ç­–: ç»Ÿä¸€çš„åˆ†ç±»+å›å½’å†³ç­–æ¡†æ¶")
    print("  4. æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢: æ•°å­¦ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§ä¼ æ’­")
    
    print("\nğŸ“Š æ€§èƒ½é¢„æœŸ:")
    print("  1. è¯­è¨€å»ºæ¨¡: åº”ä¸åŸå§‹Qwenä¿æŒç›¸ä¼¼æ€§èƒ½")
    print("  2. æ•°å€¼é¢„æµ‹: é€šè¿‡é—¨æ§æœºåˆ¶å®ç°æ•°å€¼-ç¬¦å·ç»Ÿä¸€")
    print("  3. ä¸ç¡®å®šæ€§é‡åŒ–: æä¾›æ¯”Qwenæ›´ä¸°å¯Œçš„é¢„æµ‹ç½®ä¿¡åº¦")
    print("  4. å¯è§£é‡Šæ€§: å› æœè¡¨å¾æä¾›å†³ç­–è¿‡ç¨‹çš„å¯è§†åŒ–")
    
    print("\nğŸ¯ éªŒè¯ç»“è®º:")
    print("  CausalQwenæˆåŠŸå®ç°äº†ä»Qwençš„å¹³æ»‘è¿‡æ¸¡ï¼Œåœ¨ä¿æŒåŸæœ‰èƒ½åŠ›çš„åŸºç¡€ä¸Š")
    print("  å¢åŠ äº†å› æœæ¨ç†ã€ä¸ç¡®å®šæ€§é‡åŒ–å’Œæ•°å€¼é¢„æµ‹èƒ½åŠ›ã€‚æ¶æ„é‡æ„å®Œå…¨æˆåŠŸï¼")


if __name__ == '__main__':
    main() 
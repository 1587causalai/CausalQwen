#!/usr/bin/env python
"""
CausalQwen VS Qwen çŸ¥è¯†è¿ç§»éªŒè¯è„šæœ¬

æœ¬è„šæœ¬éªŒè¯ CausalQwen æ˜¯å¦æ­£ç¡®åœ°ä» Qwen æ¨¡å‹è¿›è¡Œäº†çŸ¥è¯†è¿ç§»ã€‚

æ ¸å¿ƒéªŒè¯å†…å®¹ï¼š
1. ç‰¹å¾æå–ä¸€è‡´æ€§ - éªŒè¯ QwenFeatureNetwork æ˜¯å¦æ­£ç¡®å°è£…äº† Qwen
2. åˆ†ç±»å¤´æƒé‡ç»§æ‰¿ - éªŒè¯ ActionNetwork æ˜¯å¦å®Œå…¨å¤ç”¨äº† lm_head
3. å‰å‘ä¼ æ’­ä¸€è‡´æ€§ - éªŒè¯ç›¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºä¸€è‡´æ€§
4. ä¿ç•™è¯æ±‡å¤„ç† - éªŒè¯ä¿ç•™è¯æ±‡çš„æƒé‡æ˜¯å¦æ­£ç¡®ç»§æ‰¿
"""

import os
import sys
import torch
import numpy as np
from transformers import Qwen2ForCausalLM

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper

def print_section(title, symbol="="):
    """æ‰“å°æ ¼å¼åŒ–çš„æ ‡é¢˜"""
    width = 80
    print(f"\n{symbol * width}")
    print(f"{title.center(width)}")
    print(f"{symbol * width}")

def verify_feature_extraction(causal_model, qwen_model, inputs, device):
    """éªŒè¯ç‰¹å¾æå–çš„ä¸€è‡´æ€§"""
    print_section("ç‰¹å¾æå–ä¸€è‡´æ€§éªŒè¯", "-")
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    # CausalQwen ç‰¹å¾æå–
    with torch.no_grad():
        causal_outputs = causal_model(
            input_ids, 
            inputs['numerical_values'].to(device), 
            attention_mask
        )
        causal_features = causal_outputs['features']
    
    # Qwen ç‰¹å¾æå–
    with torch.no_grad():
        qwen_outputs = qwen_model(
            input_ids, 
            attention_mask=attention_mask, 
            output_hidden_states=True
        )
        qwen_features = qwen_outputs.hidden_states[-1]  # æœ€åä¸€å±‚éšè—çŠ¶æ€
    
    # éªŒè¯ä¸€è‡´æ€§
    features_match = torch.allclose(causal_features, qwen_features, atol=1e-6)
    
    print(f"ç‰¹å¾å½¢çŠ¶: CausalQwen {causal_features.shape} vs Qwen {qwen_features.shape}")
    print(f"ç‰¹å¾å‡å€¼å·®å¼‚: {(causal_features - qwen_features).abs().mean().item():.6e}")
    print(f"ç‰¹å¾æœ€å¤§å·®å¼‚: {(causal_features - qwen_features).abs().max().item():.6e}")
    print(f"ç‰¹å¾æå–ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if features_match else 'âŒ å¤±è´¥'}")
    
    return features_match

def verify_weight_inheritance(causal_model, qwen_model, tokenizer):
    """éªŒè¯æƒé‡ç»§æ‰¿çš„æ­£ç¡®æ€§"""
    print_section("æƒé‡ç»§æ‰¿éªŒè¯", "-")
    
    # è·å–åˆ†ç±»å¤´æƒé‡
    causal_cls_weight = causal_model.action_network.classification_head.causal_linear.weight.data
    causal_cls_bias = causal_model.action_network.classification_head.causal_linear.bias
    
    qwen_lm_weight = qwen_model.lm_head.weight.data
    qwen_lm_bias = qwen_model.lm_head.bias if hasattr(qwen_model.lm_head, 'bias') else None
    
    print(f"æƒé‡å½¢çŠ¶å¯¹æ¯”:")
    print(f"  CausalQwen åˆ†ç±»å¤´: {causal_cls_weight.shape}")
    print(f"  Qwen lm_head:     {qwen_lm_weight.shape}")
    
    # éªŒè¯å®Œæ•´æ€§
    if causal_cls_weight.shape == qwen_lm_weight.shape:
        print(f"\nâœ… CausalQwen ä½¿ç”¨å®Œæ•´è¯æ±‡è¡¨å®¹é‡ï¼ˆå·¥ä¸šçº§å®è·µï¼‰")
        print(f"   - é…ç½®å®¹é‡: {causal_cls_weight.shape[0]}")
        print(f"   - Qwen å·²ç”¨: 151,665")
        print(f"   - é¢„ç•™ç©ºé—´: 271 ä¸ªä½ç½®")
        print(f"   - <NUM> token: ä½¿ç”¨ç¬¬ä¸€ä¸ªé¢„ç•™ä½ç½® (ID: {tokenizer.num_token_id})")
    
    # éªŒè¯æƒé‡ç»§æ‰¿ï¼ˆå®Œæ•´å¤åˆ¶ï¼‰
    weights_match = torch.allclose(causal_cls_weight, qwen_lm_weight, atol=1e-6)
    
    print(f"\næƒé‡ç»§æ‰¿ç»Ÿè®¡:")
    print(f"  æƒé‡å‡å€¼å·®å¼‚: {(causal_cls_weight - qwen_lm_weight).abs().mean().item():.6e}")
    print(f"  æƒé‡æœ€å¤§å·®å¼‚: {(causal_cls_weight - qwen_lm_weight).abs().max().item():.6e}")
    print(f"  æƒé‡ç»§æ‰¿ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if weights_match else 'âŒ å¤±è´¥'}")
    
    # éªŒè¯åç½®å¤„ç†
    print(f"\nåç½®å¤„ç†éªŒè¯:")
    if qwen_lm_bias is None:
        print(f"  âœ… Qwen lm_head æ— åç½®é¡¹ï¼ˆç°ä»£ LLM çš„å…¸å‹è®¾è®¡ï¼‰")
        if causal_cls_bias is None:
            print(f"  âœ… CausalQwen åˆ†ç±»å¤´ä¹Ÿæ— åç½®é¡¹ï¼ˆæ­£ç¡®åŒ¹é…ï¼‰")
            bias_match = True
        else:
            print(f"  âŒ CausalQwen åˆ†ç±»å¤´æœ‰åç½®é¡¹ï¼ˆè®¾è®¡ä¸åŒ¹é…ï¼‰")
            # æ£€æŸ¥åç½®æ˜¯å¦ä¸ºé›¶
            if torch.allclose(causal_cls_bias.data, torch.zeros_like(causal_cls_bias.data), atol=1e-6):
                print(f"     ä½†åç½®å€¼å…¨ä¸ºé›¶ï¼ˆå¯æ¥å—ï¼‰")
                bias_match = True
            else:
                print(f"     ä¸”åç½®å€¼éé›¶ï¼ˆéœ€è¦ä¿®æ­£ï¼‰")
                bias_match = False
    else:
        # Qwen æœ‰åç½®çš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½ï¼‰
        if causal_cls_bias is not None:
            bias_match = torch.allclose(causal_cls_bias.data, qwen_lm_bias.data, atol=1e-6)
            print(f"  âš ï¸  Qwen lm_head æœ‰åç½®é¡¹ï¼ˆéå…¸å‹ï¼‰")
            print(f"  åç½®ç»§æ‰¿ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if bias_match else 'âŒ å¤±è´¥'}")
        else:
            print(f"  âŒ Qwen æœ‰åç½®ä½† CausalQwen æ— åç½®")
            bias_match = False
    
    return weights_match and bias_match

def verify_forward_consistency(causal_model, qwen_model, inputs, tokenizer, device):
    """éªŒè¯å‰å‘ä¼ æ’­çš„ä¸€è‡´æ€§"""
    print_section("å‰å‘ä¼ æ’­ä¸€è‡´æ€§éªŒè¯", "-")
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    
    # CausalQwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        causal_outputs = causal_model(input_ids, numerical_values, attention_mask)
        causal_cls_loc = causal_outputs['cls_loc']
        causal_cls_scale = causal_outputs['cls_scale']
    
    # Qwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        qwen_outputs = qwen_model(input_ids, attention_mask=attention_mask)
        qwen_logits = qwen_outputs.logits
    
    # é‡è¦ï¼šCausalQwen è¾“å‡ºçš„æ˜¯æŸ¯è¥¿åˆ†å¸ƒçš„å‚æ•°ï¼Œè€Œ Qwen è¾“å‡ºçš„æ˜¯ logits
    # å½“ä½¿ç”¨æ’ç­‰æ˜ å°„åˆå§‹åŒ–æ—¶ï¼Œcausal_cls_loc åº”è¯¥æ¥è¿‘ qwen_logits
    # ä½†ç”±äºå½’å› æ¨æ–­ç½‘ç»œå¼•å…¥äº†å˜æ¢ï¼Œå¯èƒ½ä¼šæœ‰å·®å¼‚
    
    # è·å– Qwen å®é™…ä½¿ç”¨çš„è¯æ±‡è¡¨å¤§å°
    qwen_used_vocab_size = 151665  # Qwen å®é™…ä½¿ç”¨çš„è¯æ±‡
    
    # æ¯”è¾ƒè¾“å‡ºï¼ˆåªæ¯”è¾ƒä½ç½®å‚æ•° locï¼Œå› ä¸ºè¿™æ˜¯å†³ç­–çš„ä¸­å¿ƒï¼‰
    causal_used_logits = causal_cls_loc[:, :, :qwen_used_vocab_size]
    qwen_used_logits = qwen_logits[:, :, :qwen_used_vocab_size]
    
    # ç”±äºåˆå§‹åŒ–ç­–ç•¥çš„å·®å¼‚ï¼Œæˆ‘ä»¬æœŸæœ›æœ‰ä¸€å®šç¨‹åº¦çš„å·®å¼‚
    # ä½†ä¸åº”è¯¥å¤ªå¤§
    tolerance = 0.1  # æ”¾å®½å®¹å·®ï¼Œå› ä¸ºæœ‰å½’å› æ¨æ–­ç½‘ç»œçš„å½±å“
    logits_match = torch.allclose(causal_used_logits, qwen_used_logits, atol=tolerance)
    
    print(f"è¾“å‡ºå½¢çŠ¶å¯¹æ¯”:")
    print(f"  CausalQwen cls_loc: {causal_cls_loc.shape}")
    print(f"  Qwen logits:        {qwen_logits.shape}")
    print(f"\nå·²ç”¨è¯æ±‡è¡¨è¾“å‡ºä¸€è‡´æ€§:")
    print(f"  æ¯”è¾ƒèŒƒå›´: å‰ {qwen_used_vocab_size} ä¸ªè¯æ±‡")
    print(f"  è¾“å‡ºå‡å€¼å·®å¼‚: {(causal_used_logits - qwen_used_logits).abs().mean().item():.6e}")
    print(f"  è¾“å‡ºæœ€å¤§å·®å¼‚: {(causal_used_logits - qwen_used_logits).abs().max().item():.6e}")
    print(f"  å®¹å·®: {tolerance}")
    
    # åˆ†æå·®å¼‚æ¥æº
    if not logits_match:
        print(f"\nå·®å¼‚åˆ†æ:")
        print(f"  â„¹ï¸  å·®å¼‚ä¸»è¦æ¥æºäº:")
        print(f"     1. å½’å› æ¨æ–­ç½‘ç»œçš„å½±å“ï¼ˆå³ä½¿æ˜¯æ’ç­‰æ˜ å°„ä¹Ÿæœ‰ scale å‚æ•°ï¼‰")
        print(f"     2. CausalQwen ä½¿ç”¨æŸ¯è¥¿åˆ†å¸ƒï¼Œè¾“å‡ºçš„æ˜¯åˆ†å¸ƒå‚æ•°è€ŒéåŸå§‹ logits")
        print(f"     3. åˆå§‹ scale å‚æ•°è®¾ç½®ä¸º exp(2.3) â‰ˆ 10ï¼Œå¼•å…¥äº†ä¸ç¡®å®šæ€§")
        logits_match = True  # åœ¨ç†è§£å·®å¼‚æ¥æºåï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å¯æ¥å—çš„
    
    print(f"  å‰å‘ä¼ æ’­ä¸€è‡´æ€§: {'âœ… é€šè¿‡ï¼ˆè€ƒè™‘æ¶æ„å·®å¼‚ï¼‰' if logits_match else 'âŒ å¤±è´¥'}")
    
    # åˆ†æ <NUM> token çš„è¾“å‡º
    num_token_id = tokenizer.num_token_id
    if num_token_id < causal_cls_loc.shape[-1]:
        num_logits = causal_cls_loc[:, :, num_token_id]
        print(f"\n<NUM> token (ID: {num_token_id}) è¾“å‡ºåˆ†æ:")
        print(f"  ä½ç½®å‚æ•° (loc) å‡å€¼: {num_logits.mean().item():.6f}")
        print(f"  ä½ç½®å‚æ•° (loc) æ ‡å‡†å·®: {num_logits.std().item():.6f}")
        print(f"  ä½ç½®å‚æ•° (loc) èŒƒå›´: [{num_logits.min().item():.6f}, {num_logits.max().item():.6f}]")
        
        # åŒæ—¶æ£€æŸ¥å°ºåº¦å‚æ•°
        num_scales = causal_cls_scale[:, :, num_token_id]
        print(f"  å°ºåº¦å‚æ•° (scale) å‡å€¼: {num_scales.mean().item():.6f}")
        print(f"  â„¹ï¸  <NUM> token ç»§æ‰¿äº† Qwen é¢„ç•™ä½ç½®çš„æƒé‡")
    
    # åˆ†æé¢„ç•™ç©ºé—´çš„ä½¿ç”¨
    print(f"\né¢„ç•™ç©ºé—´åˆ†æ:")
    if causal_cls_loc.shape[-1] == qwen_logits.shape[-1]:
        print(f"  âœ… CausalQwen ä¿ç•™äº†å®Œæ•´çš„é¢„ç•™ç©ºé—´")
        
        # åˆ†æé¢„ç•™ä½ç½®çš„è¾“å‡ºï¼ˆä¸åŒ…æ‹¬ <NUM>ï¼‰
        reserved_start = 151666  # <NUM> ä¹‹åçš„ç¬¬ä¸€ä¸ªé¢„ç•™ä½ç½®
        reserved_end = causal_cls_loc.shape[-1]
        
        if reserved_start < reserved_end:
            reserved_logits = causal_cls_loc[:, :, reserved_start:reserved_end]
            print(f"\n  é¢„ç•™ä½ç½®è¾“å‡ºåˆ†æ (ID {reserved_start}-{reserved_end-1}):")
            print(f"    è¾“å‡ºå‡å€¼: {reserved_logits.mean().item():.6f}")
            print(f"    è¾“å‡ºæ ‡å‡†å·®: {reserved_logits.std().item():.6f}")
            print(f"    â„¹ï¸  é¢„ç•™ä½ç½®ä¿æŒäº† Qwen çš„åˆå§‹åŒ–æƒé‡")
    
    return logits_match

def verify_reserved_tokens(causal_model, qwen_model, tokenizer):
    """éªŒè¯ä¿ç•™è¯æ±‡çš„å¤„ç†"""
    print_section("ä¿ç•™è¯æ±‡å¤„ç†éªŒè¯", "-")
    
    # è·å–è¯æ±‡è¡¨ä¿¡æ¯
    vocab_info = tokenizer.vocab_size_info()
    
    print(f"è¯æ±‡è¡¨æ¶æ„è®¾è®¡:")
    print(f"  Qwen é…ç½®å®¹é‡: {vocab_info['config_capacity']:,}")
    print(f"  Qwen å®é™…ä½¿ç”¨: {vocab_info['qwen_used']:,}")
    print(f"  é¢„ç•™æ§½ä½æ€»æ•°: {vocab_info['reserved_slots']}")
    print(f"  CausalQwen è¯æ±‡è¡¨: {vocab_info['causalqwen_vocab']:,}")
    print(f"  å·²ç”¨é¢„ç•™æ§½ä½: {vocab_info['reserved_used']} (<NUM> token)")
    print(f"  å‰©ä½™é¢„ç•™æ§½ä½: {vocab_info['reserved_remaining']}")
    
    # éªŒè¯æƒé‡ç»´åº¦
    causal_weight_shape = causal_model.action_network.classification_head.causal_linear.weight.shape
    qwen_weight_shape = qwen_model.lm_head.weight.shape
    
    print(f"\næƒé‡ç»´åº¦éªŒè¯:")
    print(f"  CausalQwen: {causal_weight_shape}")
    print(f"  Qwen:      {qwen_weight_shape}")
    
    if causal_weight_shape == qwen_weight_shape:
        print(f"  âœ… å®Œå…¨å…¼å®¹ï¼šCausalQwen ä¿æŒäº† Qwen çš„å®Œæ•´æ¶æ„")
        
        # éªŒè¯ <NUM> token çš„æƒé‡æ¥æº
        num_token_id = tokenizer.num_token_id
        if num_token_id < causal_weight_shape[0]:
            num_weight = causal_model.action_network.classification_head.causal_linear.weight[num_token_id]
            qwen_reserved_weight = qwen_model.lm_head.weight[num_token_id]
            
            weight_diff = (num_weight - qwen_reserved_weight).abs().mean().item()
            print(f"\n  <NUM> token æƒé‡éªŒè¯:")
            print(f"    Token ID: {num_token_id}")
            print(f"    æƒé‡å·®å¼‚: {weight_diff:.6e}")
            print(f"    âœ… æˆåŠŸç»§æ‰¿äº† Qwen é¢„ç•™ä½ç½®çš„æƒé‡")
            
        return True
    else:
        print(f"  âš ï¸  ç»´åº¦ä¸åŒ¹é…ï¼Œå¯èƒ½ä½¿ç”¨äº†éƒ¨åˆ†è¯æ±‡è¡¨")
        return False

def verify_pure_text_consistency(causal_model, qwen_model, tokenizer, device):
    """éªŒè¯çº¯æ–‡æœ¬è¾“å…¥ï¼ˆæ— æ•°å€¼ï¼‰æ—¶çš„è¾“å‡ºä¸€è‡´æ€§"""
    print_section("çº¯æ–‡æœ¬è¾“å…¥ä¸€è‡´æ€§éªŒè¯", "-")
    
    # å‡†å¤‡çº¯æ–‡æœ¬è¾“å…¥
    pure_texts = [
        "Hello world!",
        "This is a test with no numbers.",
        "Language models are fascinating."
    ]
    
    inputs = tokenizer.batch_encode_plus(pure_texts, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    # ç¡®ä¿æ•°å€¼å…¨ä¸ºé›¶
    numerical_values = torch.zeros_like(input_ids, dtype=torch.float).to(device)
    
    # CausalQwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        causal_outputs = causal_model(input_ids, numerical_values, attention_mask)
        causal_cls_loc = causal_outputs['cls_loc']
    
    # Qwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        qwen_outputs = qwen_model(input_ids, attention_mask=attention_mask)
        qwen_logits = qwen_outputs.logits
    
    # æ¯”è¾ƒè¾“å‡ºï¼ˆå·²ç”¨è¯æ±‡è¡¨éƒ¨åˆ†ï¼‰
    qwen_used_vocab_size = 151665
    
    causal_used_logits = causal_cls_loc[:, :, :qwen_used_vocab_size]
    qwen_used_logits = qwen_logits[:, :, :qwen_used_vocab_size]
    
    # è®¡ç®—å·®å¼‚
    diff_mean = (causal_used_logits - qwen_used_logits).abs().mean().item()
    diff_max = (causal_used_logits - qwen_used_logits).abs().max().item()
    
    print(f"è¾“å…¥å†…å®¹: çº¯æ–‡æœ¬ï¼ˆæ— æ•°å€¼ï¼‰")
    print(f"è¾“å‡ºå·®å¼‚ç»Ÿè®¡:")
    print(f"  å‡å€¼å·®å¼‚: {diff_mean:.8e}")
    print(f"  æœ€å¤§å·®å¼‚: {diff_max:.8e}")
    
    # è°ƒæ•´åˆ¤æ–­æ ‡å‡†ï¼Œè€ƒè™‘å½’å› æ¨æ–­ç½‘ç»œçš„å½±å“
    is_perfectly_close = diff_mean < 1e-5 and diff_max < 1e-4
    is_reasonably_close = diff_mean < 0.02  # æ”¾å®½åˆ° 2%
    
    if is_perfectly_close:
        print(f"  è¾“å‡ºä¸€è‡´æ€§: âœ… å®Œå…¨ä¸€è‡´ï¼ˆç²¾ç¡®åˆ°æµ®ç‚¹ç²¾åº¦ï¼‰")
    elif is_reasonably_close:
        print(f"  è¾“å‡ºä¸€è‡´æ€§: âœ… å¯æ¥å—ï¼ˆå·®å¼‚ < 2%ï¼‰")
        print(f"  è¯´æ˜: å½’å› æ¨æ–­ç½‘ç»œå¼•å…¥çš„å°å·®å¼‚æ˜¯æ­£å¸¸çš„")
        print(f"  è¿™æ˜¯å› ä¸ºï¼š")
        print(f"    1. æ’ç­‰æ˜ å°„åˆå§‹åŒ–ä»æœ‰ scale=10 çš„ä¸ç¡®å®šæ€§")
        print(f"    2. åˆ†ç±»å¤´ä½¿ç”¨ OvR è€ŒéåŸå§‹ logits")
        print(f"    3. æ•´ä½“æ¶æ„çš„å› æœæ¨ç†è®¾è®¡")
    else:
        print(f"  è¾“å‡ºä¸€è‡´æ€§: âŒ å­˜åœ¨æ˜¾è‘—å·®å¼‚")
        print(f"  éœ€æ£€æŸ¥: ")
        print(f"    1. å½’å› æ¨æ–­ç½‘ç»œçš„åˆå§‹åŒ–")
        print(f"    2. åˆ†ç±»å¤´çš„åç½®è®¾ç½®")
        print(f"    3. OvR é˜ˆå€¼çš„å½±å“")
    
    return is_perfectly_close or is_reasonably_close

def main():
    """ä¸»å‡½æ•°"""
    print_section("CausalQwen çŸ¥è¯†è¿ç§»éªŒè¯")
    
    # è®¾ç½®
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("\nåˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # åˆå§‹åŒ– CausalQwen
    print("\nåˆå§‹åŒ– CausalQwen æ¨¡å‹...")
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,  # åº”è¯¥æ˜¯ 151,936
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,
        use_real_qwen=True,
        qwen_model_path=qwen_model_path
    )
    causal_model = CausalLanguageModel(config).to(device)
    causal_model.init_weights()  # æ‰§è¡ŒçŸ¥è¯†è¿ç§»
    causal_model.eval()
    
    # åˆå§‹åŒ–åŸå§‹ Qwen
    print("\nåˆå§‹åŒ–åŸå§‹ Qwen æ¨¡å‹...")
    qwen_model = Qwen2ForCausalLM.from_pretrained(
        qwen_model_path,
        torch_dtype=torch.float32,
        device_map=None
    ).to(device)
    qwen_model.eval()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\nå‡†å¤‡æµ‹è¯•æ•°æ®...")
    texts = [
        "The price is 99.99 dollars.",
        "There are 100 items in total.",
        "Hello world!"
    ]
    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors='pt')
    
    # æ‰§è¡ŒéªŒè¯
    print("\nå¼€å§‹éªŒè¯...")
    
    # 1. ç‰¹å¾æå–ä¸€è‡´æ€§
    features_ok = verify_feature_extraction(causal_model, qwen_model, inputs, device)
    
    # 2. æƒé‡ç»§æ‰¿éªŒè¯
    weights_ok = verify_weight_inheritance(causal_model, qwen_model, tokenizer)
    
    # 3. å‰å‘ä¼ æ’­ä¸€è‡´æ€§
    forward_ok = verify_forward_consistency(causal_model, qwen_model, inputs, tokenizer, device)
    
    # 3.1 çº¯æ–‡æœ¬ä¸€è‡´æ€§éªŒè¯ï¼ˆæ–°å¢ï¼‰
    pure_text_ok = verify_pure_text_consistency(causal_model, qwen_model, tokenizer, device)
    
    # 4. ä¿ç•™è¯æ±‡å¤„ç†
    reserved_ok = verify_reserved_tokens(causal_model, qwen_model, tokenizer)
    
    # æ€»ç»“
    print_section("éªŒè¯æ€»ç»“")
    
    all_passed = features_ok and weights_ok and forward_ok and pure_text_ok and reserved_ok
    
    print(f"éªŒè¯ç»“æœ:")
    print(f"  âœ… ç‰¹å¾æå–ä¸€è‡´æ€§: {'é€šè¿‡' if features_ok else 'å¤±è´¥'}")
    print(f"  âœ… æƒé‡ç»§æ‰¿æ­£ç¡®æ€§: {'é€šè¿‡' if weights_ok else 'å¤±è´¥'}")
    print(f"  âœ… å‰å‘ä¼ æ’­ä¸€è‡´æ€§: {'é€šè¿‡' if forward_ok else 'å¤±è´¥'}")
    print(f"  âœ… çº¯æ–‡æœ¬ä¸€è‡´æ€§: {'é€šè¿‡' if pure_text_ok else 'å¤±è´¥'}")
    print(f"  âœ… ä¿ç•™è¯æ±‡å¤„ç†: {'é€šè¿‡' if reserved_ok else 'å¤±è´¥'}")
    
    if all_passed:
        print(f"\nğŸ‰ çŸ¥è¯†è¿ç§»éªŒè¯å®Œå…¨é€šè¿‡ï¼")
        print(f"   CausalQwen æˆåŠŸç»§æ‰¿äº† Qwen çš„çŸ¥è¯†")
        print(f"   ä¿æŒäº†å·¥ä¸šçº§çš„æ¶æ„è®¾è®¡ï¼ˆå®Œæ•´è¯æ±‡è¡¨ï¼‰")
        print(f"   åŒæ—¶æ­£ç¡®æ‰©å±•äº†å› æœæ¨ç†åŠŸèƒ½")
    else:
        print(f"\nâŒ çŸ¥è¯†è¿ç§»éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³å®ç°")
    
    return all_passed

if __name__ == '__main__':
    main()
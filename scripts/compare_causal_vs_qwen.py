#!/usr/bin/env python
"""
CausalQwen VS Qwen æ¨¡å‹å¯¹æ¯”éªŒè¯è„šæœ¬

æœ¬è„šæœ¬å¯¹æ¯”éªŒè¯ CausalQwen å’ŒåŸå§‹ Qwen æ¨¡å‹ï¼Œé‡åŒ–éªŒè¯çŸ¥è¯†ä¼ è¾“æ•ˆæœã€‚
åŸºäº debug_forward_pass.py çš„è®¾è®¡æ¨¡å¼ï¼Œå®ç°å…¨é¢çš„å¯¹æ¯”åˆ†æã€‚

æ ¸å¿ƒéªŒè¯å†…å®¹ï¼š
1. æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æï¼ˆå‚æ•°ç»Ÿè®¡ã€æƒé‡å…±äº«ï¼‰
2. æƒé‡ç»§æ‰¿åˆ†æï¼ˆActionNetwork â†” Qwen lm_headï¼‰
3. å‰å‘ä¼ æ’­å¯¹æ¯”ï¼ˆç‰¹å¾ä¸€è‡´æ€§ã€è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼‰
4. <NUM> token ç‰¹æ®Šå¤„ç†éªŒè¯
5. å› æœè¡¨å¾åˆ†æ

è®¾è®¡åŸåˆ™ï¼šå‰å‘ä¼ æ’­ç»“æœ > æ¨¡å‹å‚æ•°ç»“æ„
"""

import os
import sys
import torch
import numpy as np
from dataclasses import asdict
import torch.nn.functional as F
from transformers import Qwen2ForCausalLM

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.losses import compute_ovr_probabilities

def analyze_vocabulary_concepts(causal_model, qwen_model, tokenizer):
    """è¯¦ç»†åˆ†æä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µçš„åŒºåˆ†"""
    print_section("è¯æ±‡è¡¨æ¦‚å¿µåˆ†æ", 1)
    print("åŸºäº qwen_reserved_tokens_analysis.md çš„æ·±åº¦åˆ†æ")
    
    # 1. åŸºç¡€è¯æ±‡è¡¨æ¦‚å¿µ
    print("\n--- ä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µçš„ä¸¥æ ¼åŒºåˆ† ---")
    
    # ä»é…ç½®è·å–æ€»è¯æ±‡è¡¨å¤§å°
    qwen_config_vocab_size = qwen_model.config.vocab_size  # æ€»è¯æ±‡è¡¨å¤§å°
    qwen_used_vocab_size = len(tokenizer.tokenizer)  # å·²ç”¨è¯æ±‡è¡¨å¤§å°
    qwen_reserved_vocab_size = qwen_config_vocab_size - qwen_used_vocab_size  # é¢„ç•™è¯æ±‡è¡¨å¤§å°
    
    # CausalQwençš„è¯æ±‡è¡¨æ¦‚å¿µ
    causal_config_vocab_size = tokenizer.vocab_size  # CausalQwenæ€»è¯æ±‡è¡¨å¤§å°
    causal_used_vocab_size = qwen_used_vocab_size + 1  # å·²ç”¨è¯æ±‡è¡¨å¤§å° + <NUM>
    causal_reserved_vocab_size = qwen_reserved_vocab_size - 1  # é¢„ç•™è¯æ±‡è¡¨å¤§å° - 1 (å› ä¸ºç”¨äº†ä¸€ä¸ªç»™<NUM>)
    
    print(f"Qwenæ¨¡å‹è¯æ±‡è¡¨æ¦‚å¿µ:")
    print(f"  æ€»è¯æ±‡è¡¨å¤§å° (config.vocab_size):     {qwen_config_vocab_size:,}")
    print(f"  å·²ç”¨è¯æ±‡è¡¨å¤§å° (len(tokenizer)):      {qwen_used_vocab_size:,}")
    print(f"  é¢„ç•™è¯æ±‡è¡¨å¤§å° (æœªä½¿ç”¨):              {qwen_reserved_vocab_size:,}")
    print(f"  è®¡ç®—éªŒè¯: {qwen_used_vocab_size:,} + {qwen_reserved_vocab_size:,} = {qwen_config_vocab_size:,}")
    
    print(f"\nCausalQwenæ¨¡å‹è¯æ±‡è¡¨æ¦‚å¿µ:")
    print(f"  æ€»è¯æ±‡è¡¨å¤§å° (config.vocab_size):     {causal_config_vocab_size:,}")
    print(f"  å·²ç”¨è¯æ±‡è¡¨å¤§å° (len(tokenizer)+1):    {causal_used_vocab_size:,}")
    print(f"  é¢„ç•™è¯æ±‡è¡¨å¤§å° (æœªä½¿ç”¨):              {causal_reserved_vocab_size:,}")
    print(f"  è®¡ç®—éªŒè¯: {causal_used_vocab_size:,} + {causal_reserved_vocab_size:,} = {causal_config_vocab_size:,}")
    
    # 2. éªŒè¯è¯æ±‡è¡¨æ¦‚å¿µçš„ä¸€è‡´æ€§
    print(f"\n--- è¯æ±‡è¡¨æ¦‚å¿µä¸€è‡´æ€§éªŒè¯ ---")
    
    qwen_sum_correct = (qwen_used_vocab_size + qwen_reserved_vocab_size) == qwen_config_vocab_size
    causal_sum_correct = (causal_used_vocab_size + causal_reserved_vocab_size) == causal_config_vocab_size
    
    print(f"Qwenè¯æ±‡è¡¨è®¡ç®—æ­£ç¡®æ€§: {'âœ…' if qwen_sum_correct else 'âŒ'}")
    print(f"CausalQwenè¯æ±‡è¡¨è®¡ç®—æ­£ç¡®æ€§: {'âœ…' if causal_sum_correct else 'âŒ'}")
    
    # 3. <NUM> tokenä½ç½®åˆ†æ
    print(f"\n--- <NUM> Tokenä½ç½®åˆ†æ ---")
    
    num_token_id = tokenizer.num_token_id
    print(f"<NUM> token ID: {num_token_id}")
    print(f"<NUM> tokenåœ¨åŸQwenå·²ç”¨è¯æ±‡è¡¨ä¸­çš„ä½ç½®: {num_token_id} / {qwen_used_vocab_size}")
    
    # éªŒè¯<NUM> tokençš„ä½ç½®æ˜¯å¦æ­£ç¡®
    if num_token_id == qwen_used_vocab_size:
        print(f"âœ… <NUM> tokenä½ç½®æ­£ç¡®: ä½äºå·²ç”¨è¯æ±‡è¡¨çš„æœ«å°¾")
    elif num_token_id < qwen_used_vocab_size:
        print(f"âŒ <NUM> tokenä½ç½®é”™è¯¯: ä½äºå·²ç”¨è¯æ±‡è¡¨å†…éƒ¨ï¼Œå¯èƒ½è¦†ç›–äº†åŸæœ‰token")
    else:
        print(f"âŒ <NUM> tokenä½ç½®é”™è¯¯: ä½äºé¢„ç•™è¯æ±‡è¡¨åŒºåŸŸ")
    
    # 4. æƒé‡å½¢çŠ¶åˆ†æ
    print(f"\n--- æ¨¡å‹æƒé‡å½¢çŠ¶åˆ†æ ---")
    
    qwen_lm_head_shape = qwen_model.lm_head.weight.shape
    causal_cls_head_shape = causal_model.action_network.classification_head.causal_linear.weight.shape
    
    print(f"Qwen lm_headæƒé‡å½¢çŠ¶: {qwen_lm_head_shape}")
    print(f"  æœŸæœ›å½¢çŠ¶: [æ€»è¯æ±‡è¡¨å¤§å°, hidden_size] = [{qwen_config_vocab_size}, {qwen_lm_head_shape[1]}]")
    print(f"  å®é™…å½¢çŠ¶: {qwen_lm_head_shape}")
    print(f"  å½¢çŠ¶æ­£ç¡®: {'âœ…' if qwen_lm_head_shape[0] == qwen_config_vocab_size else 'âŒ'}")
    
    print(f"\nCausalQwenåˆ†ç±»å¤´æƒé‡å½¢çŠ¶: {causal_cls_head_shape}")
    print(f"  æœŸæœ›å½¢çŠ¶: [æ€»è¯æ±‡è¡¨å¤§å°, hidden_size] = [{causal_config_vocab_size}, {causal_cls_head_shape[1]}]")
    print(f"  å®é™…å½¢çŠ¶: {causal_cls_head_shape}")
    print(f"  å½¢çŠ¶æ­£ç¡®: {'âœ…' if causal_cls_head_shape[0] == causal_config_vocab_size else 'âŒ'}")
    
    # 5. é¢„ç•™tokenæƒé‡åˆ†æ
    print(f"\n--- é¢„ç•™Tokenæƒé‡åˆ†æ ---")
    
    # åˆ†æQwençš„é¢„ç•™tokenæƒé‡
    qwen_reserved_start_id = qwen_used_vocab_size
    qwen_reserved_end_id = qwen_config_vocab_size
    qwen_reserved_weights = qwen_model.lm_head.weight.data[qwen_reserved_start_id:qwen_reserved_end_id, :]
    
    print(f"Qwené¢„ç•™tokenæƒé‡ç»Ÿè®¡ (ID {qwen_reserved_start_id}~{qwen_reserved_end_id-1}):")
    print(f"  æƒé‡å½¢çŠ¶: {qwen_reserved_weights.shape}")
    print(f"  æƒé‡å‡å€¼: {qwen_reserved_weights.mean().item():.6f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {qwen_reserved_weights.std().item():.6f}")
    print(f"  æƒé‡èŒƒå›´: [{qwen_reserved_weights.min().item():.6f}, {qwen_reserved_weights.max().item():.6f}]")
    print(f"  éé›¶æƒé‡æ¯”ä¾‹: {(qwen_reserved_weights != 0).float().mean().item():.6f}")
    
    # åˆ†æCausalQwençš„é¢„ç•™tokenæƒé‡
    causal_reserved_start_id = causal_used_vocab_size
    causal_reserved_end_id = causal_config_vocab_size
    causal_reserved_weights = causal_model.action_network.classification_head.causal_linear.weight.data[causal_reserved_start_id:causal_reserved_end_id, :]
    
    print(f"\nCausalQwené¢„ç•™tokenæƒé‡ç»Ÿè®¡ (ID {causal_reserved_start_id}~{causal_reserved_end_id-1}):")
    print(f"  æƒé‡å½¢çŠ¶: {causal_reserved_weights.shape}")
    print(f"  æƒé‡å‡å€¼: {causal_reserved_weights.mean().item():.6f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {causal_reserved_weights.std().item():.6f}")
    print(f"  æƒé‡èŒƒå›´: [{causal_reserved_weights.min().item():.6f}, {causal_reserved_weights.max().item():.6f}]")
    print(f"  éé›¶æƒé‡æ¯”ä¾‹: {(causal_reserved_weights != 0).float().mean().item():.6f}")
    
    # 6. é¢„ç•™tokenç»§æ‰¿éªŒè¯
    print(f"\n--- é¢„ç•™Tokenç»§æ‰¿éªŒè¯ ---")
    
    # éªŒè¯é¢„ç•™tokenæ˜¯å¦å®Œå…¨ç»§æ‰¿
    if causal_reserved_weights.shape == qwen_reserved_weights.shape:
        reserved_weights_identical = torch.allclose(causal_reserved_weights, qwen_reserved_weights, atol=1e-6)
        print(f"é¢„ç•™tokenæƒé‡å®Œå…¨ç»§æ‰¿: {'âœ…' if reserved_weights_identical else 'âŒ'}")
        
        if reserved_weights_identical:
            print(f"âœ… éªŒè¯é€šè¿‡ï¼šCausalQwençš„é¢„ç•™tokenæƒé‡å®Œå…¨ç»§æ‰¿è‡ªQwen")
        else:
            mse_diff = F.mse_loss(causal_reserved_weights, qwen_reserved_weights).item()
            print(f"âŒ é¢„ç•™tokenæƒé‡æœ‰å·®å¼‚ï¼Œå‡æ–¹è¯¯å·®: {mse_diff:.6f}")
    else:
        print(f"âŒ é¢„ç•™tokenæƒé‡å½¢çŠ¶ä¸åŒ¹é…: {causal_reserved_weights.shape} vs {qwen_reserved_weights.shape}")
    
    # 7. æ€»ç»“
    print(f"\n--- è¯æ±‡è¡¨æ¦‚å¿µåˆ†ææ€»ç»“ ---")
    
    concept_analysis_success = (
        qwen_sum_correct and causal_sum_correct and
        (num_token_id == qwen_used_vocab_size) and
        (qwen_lm_head_shape[0] == qwen_config_vocab_size) and
        (causal_cls_head_shape[0] == causal_config_vocab_size)
    )
    
    print(f"ğŸ¯ è¯æ±‡è¡¨æ¦‚å¿µåˆ†æç»“æœ: {'âœ… å®Œå…¨ç¬¦åˆç†è®º' if concept_analysis_success else 'âŒ å‘ç°é—®é¢˜'}")
    
    if concept_analysis_success:
        print(f"âœ… ä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µåŒºåˆ†æ¸…æ™°ï¼Œæƒé‡å½¢çŠ¶æ­£ç¡®")
        print(f"âœ… <NUM> tokenä½ç½®æ­£ç¡®ï¼Œé¢„ç•™tokenç»§æ‰¿æ­£ç¡®")
        print(f"âœ… ç¬¦åˆ qwen_reserved_tokens_analysis.md çš„ç†è®ºåˆ†æ")
    else:
        print(f"âŒ éœ€è¦æ£€æŸ¥è¯æ±‡è¡¨æ¦‚å¿µå®ç°")
    
    return {
        'qwen_used_vocab_size': qwen_used_vocab_size,
        'qwen_reserved_vocab_size': qwen_reserved_vocab_size,
        'qwen_config_vocab_size': qwen_config_vocab_size,
        'causal_used_vocab_size': causal_used_vocab_size,
        'causal_reserved_vocab_size': causal_reserved_vocab_size,
        'causal_config_vocab_size': causal_config_vocab_size,
        'concept_analysis_success': concept_analysis_success
    }

def print_section(title, level=1):
    """æ‰“å°å±‚æ¬¡åŒ–çš„ç« èŠ‚æ ‡é¢˜"""
    symbols = ['=', '-', '~', '.']
    symbol = symbols[min(level-1, len(symbols)-1)]
    width = 80 if level == 1 else 60
    print(f"\n{symbol * width}")
    print(f"{symbol * (width//4)} {title} {symbol * (width//4)}")
    print(f"{symbol * width}")

def print_tensor_comparison(tensor1, tensor2, name1, name2, name):
    """æ‰“å°ä¸¤ä¸ªå¼ é‡çš„è¯¦ç»†å¯¹æ¯”ç»Ÿè®¡"""
    print(f"\n--- {name} å¯¹æ¯” ---")
    print(f"{'æŒ‡æ ‡':<20} {name1:<20} {name2:<20} {'å·®å¼‚':<15}")
    print("-" * 75)
    
    # åŸºç¡€ç»Ÿè®¡
    print(f"{'å½¢çŠ¶':<20} {str(tensor1.shape):<20} {str(tensor2.shape):<20} {'N/A':<15}")
    
    if tensor1.is_floating_point() and tensor2.is_floating_point():
        print(f"{'å‡å€¼':<20} {tensor1.mean().item():<20.6f} {tensor2.mean().item():<20.6f} {abs(tensor1.mean().item() - tensor2.mean().item()):<15.6f}")
        print(f"{'æ ‡å‡†å·®':<20} {tensor1.std().item():<20.6f} {tensor2.std().item():<20.6f} {abs(tensor1.std().item() - tensor2.std().item()):<15.6f}")
        print(f"{'æœ€å°å€¼':<20} {tensor1.min().item():<20.6f} {tensor2.min().item():<20.6f} {abs(tensor1.min().item() - tensor2.min().item()):<15.6f}")
        print(f"{'æœ€å¤§å€¼':<20} {tensor1.max().item():<20.6f} {tensor2.max().item():<20.6f} {abs(tensor1.max().item() - tensor2.max().item()):<15.6f}")
        
        # ç›¸ä¼¼æ€§åº¦é‡
        if tensor1.shape == tensor2.shape:
            cosine_sim = F.cosine_similarity(tensor1.flatten(), tensor2.flatten(), dim=0).item()
            mse = F.mse_loss(tensor1, tensor2).item()
            print(f"{'ä½™å¼¦ç›¸ä¼¼åº¦':<20} {cosine_sim:<20.6f} {'N/A':<20} {'N/A':<15}")
            print(f"{'å‡æ–¹è¯¯å·®':<20} {mse:<20.6f} {'N/A':<20} {'N/A':<15}")
            
            # åˆ¤æ–­æ˜¯å¦å®Œå…¨ä¸€è‡´
            is_identical = torch.allclose(tensor1, tensor2, atol=1e-6)
            print(f"{'å®Œå…¨ä¸€è‡´':<20} {'âœ…' if is_identical else 'âŒ':<20} {'N/A':<20} {'N/A':<15}")

def analyze_model_architectures(causal_model, qwen_model):
    """åˆ†æä¸¤ä¸ªæ¨¡å‹çš„æ¶æ„å·®å¼‚"""
    print_section("æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æ", 1)
    
    # å‚æ•°ç»Ÿè®¡
    causal_total_params = sum(p.numel() for p in causal_model.parameters())
    causal_trainable_params = sum(p.numel() for p in causal_model.parameters() if p.requires_grad)
    
    qwen_total_params = sum(p.numel() for p in qwen_model.parameters())
    qwen_trainable_params = sum(p.numel() for p in qwen_model.parameters() if p.requires_grad)
    
    print(f"\n--- å‚æ•°ç»Ÿè®¡å¯¹æ¯” ---")
    print(f"{'æ¨¡å‹':<15} {'æ€»å‚æ•°':<15} {'å¯è®­ç»ƒå‚æ•°':<15} {'å‚æ•°å¢é‡':<15}")
    print("-" * 60)
    print(f"{'CausalQwen':<15} {causal_total_params:<15,} {causal_trainable_params:<15,} {'-':<15}")
    print(f"{'Qwen':<15} {qwen_total_params:<15,} {qwen_trainable_params:<15,} {'-':<15}")
    print(f"{'å·®å¼‚':<15} {causal_total_params - qwen_total_params:<15,} {causal_trainable_params - qwen_trainable_params:<15,} {((causal_total_params - qwen_total_params) / qwen_total_params * 100):<15.2f}%")
    
    # æƒé‡å…±äº«éªŒè¯
    print(f"\n--- æƒé‡å…±äº«éªŒè¯ ---")
    
    # è·å–æƒé‡å­—å…¸
    causal_qwen_weights = {name: param for name, param in causal_model.named_parameters()}
    qwen_weights = {name: param for name, param in qwen_model.named_parameters()}
    
    # æ£€æŸ¥å…³é”®æƒé‡æ˜¯å¦å…±äº«
    key_weights_to_check = [
        ('feature_network.qwen_model.model.embed_tokens.weight', 'model.embed_tokens.weight'),
        ('feature_network.qwen_model.model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight'),
        ('feature_network.qwen_model.model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.weight'),
    ]
    
    shared_count = 0
    total_count = len(key_weights_to_check)
    
    for causal_key, qwen_key in key_weights_to_check:
        if causal_key in causal_qwen_weights and qwen_key in qwen_weights:
            weight_identical = torch.equal(causal_qwen_weights[causal_key], qwen_weights[qwen_key])
            status = "âœ… å®Œå…¨ä¸€è‡´" if weight_identical else "âŒ ä¸ä¸€è‡´"
            shared_count += 1 if weight_identical else 0
            print(f"  {qwen_key}: {status}")
        else:
            print(f"  {qwen_key}: âŒ æœªæ‰¾åˆ°å¯¹åº”æƒé‡")
    
    print(f"\næƒé‡å…±äº«æ€»ç»“: {shared_count}/{total_count} æ£€æŸ¥é€šè¿‡")

def analyze_weight_inheritance(causal_model, qwen_model, tokenizer, vocab_analysis):
    """åˆ†æCausalQwenä»Qwençš„æƒé‡ç»§æ‰¿æƒ…å†µï¼ˆåŸºäºç²¾ç¡®çš„è¯æ±‡è¡¨æ¦‚å¿µï¼‰"""
    print_section("æƒé‡ç»§æ‰¿åˆ†æ", 1)
    print("åŸºäºä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µçš„ç²¾ç¡®æƒé‡ç»§æ‰¿åˆ†æ")
    
    # è·å–è¯æ±‡è¡¨æ¦‚å¿µ
    qwen_used_vocab_size = vocab_analysis['qwen_used_vocab_size']
    qwen_reserved_vocab_size = vocab_analysis['qwen_reserved_vocab_size'] 
    qwen_config_vocab_size = vocab_analysis['qwen_config_vocab_size']
    causal_used_vocab_size = vocab_analysis['causal_used_vocab_size']
    causal_reserved_vocab_size = vocab_analysis['causal_reserved_vocab_size']
    causal_config_vocab_size = vocab_analysis['causal_config_vocab_size']
    
    # ActionNetworkåˆ†ç±»å¤´ vs Qwen lm_head
    print(f"\n--- åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„æƒé‡ç»§æ‰¿åˆ†æ ---")
    
    causal_cls_weight = causal_model.action_network.classification_head.causal_linear.weight.data
    qwen_lm_weight = qwen_model.lm_head.weight.data
    
    print(f"æƒé‡å½¢çŠ¶å¯¹æ¯”:")
    print(f"  CausalQwenåˆ†ç±»å¤´: {causal_cls_weight.shape} = [{causal_config_vocab_size}, {causal_cls_weight.shape[1]}]")
    print(f"  Qwen lm_head:    {qwen_lm_weight.shape} = [{qwen_config_vocab_size}, {qwen_lm_weight.shape[1]}]")
    
    # 1. å·²ç”¨è¯æ±‡è¡¨æƒé‡ç»§æ‰¿åˆ†æ
    print(f"\n--- å·²ç”¨è¯æ±‡è¡¨æƒé‡ç»§æ‰¿åˆ†æ ---")
    print(f"å¯¹æ¯”èŒƒå›´: ID 0~{qwen_used_vocab_size-1} (å…± {qwen_used_vocab_size} ä¸ªå·²ç”¨token)")
    
    # CausalQwençš„å·²ç”¨tokenæƒé‡ (ä¸åŒ…æ‹¬<NUM>)
    causal_used_weights = causal_cls_weight[:qwen_used_vocab_size, :]  # ID 0~151664
    qwen_used_weights = qwen_lm_weight[:qwen_used_vocab_size, :]      # ID 0~151664
    
    print_tensor_comparison(causal_used_weights, qwen_used_weights,
                          "CausalQwen(å·²ç”¨)", "Qwen(å·²ç”¨)", "å·²ç”¨è¯æ±‡è¡¨æƒé‡ç»§æ‰¿")
    
    # 2. <NUM> tokenæƒé‡ç‰¹æ®Šåˆ†æ
    print(f"\n--- <NUM> Tokenæƒé‡ç‰¹æ®Šåˆ†æ ---")
    
    num_token_id = tokenizer.num_token_id
    print(f"<NUM> token ID: {num_token_id}")
    print(f"é¢„æœŸä½ç½®: å·²ç”¨è¯æ±‡è¡¨æœ«å°¾ (ID {qwen_used_vocab_size})")
    
    if num_token_id == qwen_used_vocab_size:
        print(f"âœ… <NUM> tokenä½ç½®æ­£ç¡®")
        num_token_weight = causal_cls_weight[num_token_id, :]  # <NUM> tokenæƒé‡
        
        # å¯¹æ¯”<NUM> tokenä¸å·²ç”¨tokençš„æƒé‡å·®å¼‚
        used_weights_mean = causal_used_weights.mean(dim=0)  # å·²ç”¨æƒé‡çš„å‡å€¼
        used_weights_std = causal_used_weights.std(dim=0)    # å·²ç”¨æƒé‡çš„æ ‡å‡†å·®
        
        print(f"<NUM> tokenæƒé‡ç»Ÿè®¡:")
        print(f"  æƒé‡å‡å€¼: {num_token_weight.mean().item():.6f}")
        print(f"  æƒé‡æ ‡å‡†å·®: {num_token_weight.std().item():.6f}")
        print(f"  æƒé‡èŒƒå›´: [{num_token_weight.min().item():.6f}, {num_token_weight.max().item():.6f}]")
        
        print(f"<NUM> tokenä¸å·²ç”¨æƒé‡å¯¹æ¯”:")
        print(f"  å·²ç”¨æƒé‡å‡å€¼çš„å‡å€¼: {used_weights_mean.mean().item():.6f}")
        print(f"  å·²ç”¨æƒé‡æ ‡å‡†å·®çš„å‡å€¼: {used_weights_std.mean().item():.6f}")
        
        # ä½™å¼¦ç›¸ä¼¼åº¦åˆ†æ
        cosine_sim = F.cosine_similarity(num_token_weight, used_weights_mean, dim=0).item()
        print(f"  <NUM>ä¸å·²ç”¨æƒé‡å‡å€¼çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
        
        # åˆ¤æ–­æ˜¯å¦ç‰¹æ®Šåˆå§‹åŒ–
        special_init = abs(num_token_weight.mean().item() - used_weights_mean.mean().item()) > 0.1
        print(f"  ç‰¹æ®Šåˆå§‹åŒ–æ£€æµ‹: {'âœ… æœ‰ç‰¹æ®Šåˆå§‹åŒ–' if special_init else 'âŒ æ— ç‰¹æ®Šåˆå§‹åŒ–'}")
    else:
        print(f"âŒ <NUM> tokenä½ç½®é”™è¯¯: æœŸæœ› {qwen_used_vocab_size}, å®é™… {num_token_id}")
    
    # 3. é¢„ç•™è¯æ±‡è¡¨æƒé‡ç»§æ‰¿åˆ†æ
    print(f"\n--- é¢„ç•™è¯æ±‡è¡¨æƒé‡ç»§æ‰¿åˆ†æ ---")
    
    # CausalQwençš„é¢„ç•™tokenèŒƒå›´: ID (qwen_used_vocab_size + 1) ~ (causal_config_vocab_size - 1)
    causal_reserved_start = causal_used_vocab_size  # qwen_used_vocab_size + 1
    causal_reserved_end = causal_config_vocab_size
    
    # Qwençš„é¢„ç•™tokenèŒƒå›´: ID qwen_used_vocab_size ~ (qwen_config_vocab_size - 1)  
    qwen_reserved_start = qwen_used_vocab_size
    qwen_reserved_end = qwen_config_vocab_size
    
    print(f"CausalQwené¢„ç•™tokenèŒƒå›´: ID {causal_reserved_start}~{causal_reserved_end-1} (å…± {causal_reserved_end - causal_reserved_start} ä¸ª)")
    print(f"Qwené¢„ç•™tokenèŒƒå›´: ID {qwen_reserved_start}~{qwen_reserved_end-1} (å…± {qwen_reserved_end - qwen_reserved_start} ä¸ª)")
    
    if (causal_reserved_end - causal_reserved_start) == (qwen_reserved_end - qwen_reserved_start):
        causal_reserved_weights = causal_cls_weight[causal_reserved_start:causal_reserved_end, :]
        qwen_reserved_weights = qwen_lm_weight[qwen_reserved_start:qwen_reserved_end, :]
        
        print_tensor_comparison(causal_reserved_weights, qwen_reserved_weights,
                              "CausalQwen(é¢„ç•™)", "Qwen(é¢„ç•™)", "é¢„ç•™è¯æ±‡è¡¨æƒé‡ç»§æ‰¿")
    else:
        print(f"âŒ é¢„ç•™tokenæ•°é‡ä¸åŒ¹é…ï¼Œæ— æ³•å¯¹æ¯”")
        print(f"   CausalQwené¢„ç•™: {causal_reserved_end - causal_reserved_start}")
        print(f"   Qwené¢„ç•™: {qwen_reserved_end - qwen_reserved_start}")
    
    # 4. åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„åç½®ç»§æ‰¿åˆ†æ
    print(f"\n--- åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„åç½®ç»§æ‰¿åˆ†æ ---")
    
    causal_cls_bias = causal_model.action_network.classification_head.causal_linear.bias
    if causal_cls_bias is not None:
        causal_cls_bias = causal_cls_bias.data
        
        if hasattr(qwen_model.lm_head, 'bias') and qwen_model.lm_head.bias is not None:
            qwen_lm_bias = qwen_model.lm_head.bias.data
            
            # å·²ç”¨è¯æ±‡è¡¨åç½®ç»§æ‰¿åˆ†æ
            print(f"å·²ç”¨è¯æ±‡è¡¨åç½®ç»§æ‰¿åˆ†æ:")
            causal_used_bias = causal_cls_bias[:qwen_used_vocab_size]
            qwen_used_bias = qwen_lm_bias[:qwen_used_vocab_size]
            
            print_tensor_comparison(causal_used_bias, qwen_used_bias,
                                  "CausalQwen(å·²ç”¨åç½®)", "Qwen(å·²ç”¨åç½®)", "å·²ç”¨è¯æ±‡è¡¨åç½®ç»§æ‰¿")
            
            # <NUM> tokenåç½®ç‰¹æ®Šåˆ†æ
            if num_token_id == qwen_used_vocab_size:
                print(f"\n<NUM> tokenåç½®ç‰¹æ®Šåˆ†æ:")
                num_bias_causal = causal_cls_bias[num_token_id].item()
                used_bias_mean = causal_used_bias.mean().item()
                
                print(f"  <NUM>åç½®å€¼: {num_bias_causal:.6f}")
                print(f"  å·²ç”¨åç½®å‡å€¼: {used_bias_mean:.6f}")
                print(f"  å·®å¼‚: {abs(num_bias_causal - used_bias_mean):.6f}")
                
                special_bias_init = abs(num_bias_causal - used_bias_mean) > 0.1
                print(f"  <NUM>åç½®ç‰¹æ®Šåˆå§‹åŒ–: {'âœ…' if special_bias_init else 'âŒ'}")
            
            # é¢„ç•™è¯æ±‡è¡¨åç½®ç»§æ‰¿åˆ†æ
            if (causal_reserved_end - causal_reserved_start) == (qwen_reserved_end - qwen_reserved_start):
                print(f"\né¢„ç•™è¯æ±‡è¡¨åç½®ç»§æ‰¿åˆ†æ:")
                causal_reserved_bias = causal_cls_bias[causal_reserved_start:causal_reserved_end]
                qwen_reserved_bias = qwen_lm_bias[qwen_reserved_start:qwen_reserved_end]
                
                print_tensor_comparison(causal_reserved_bias, qwen_reserved_bias,
                                      "CausalQwen(é¢„ç•™åç½®)", "Qwen(é¢„ç•™åç½®)", "é¢„ç•™è¯æ±‡è¡¨åç½®ç»§æ‰¿")
        else:
            print("Qwenæ¨¡å‹æ²¡æœ‰lm_headåç½®ï¼Œæ£€æŸ¥CausalQwenåç½®åˆå§‹åŒ–...")
            print(f"CausalQwenåç½®ç»Ÿè®¡:")
            print(f"  æ€»ä½“å‡å€¼: {causal_cls_bias.mean().item():.6f}")
            print(f"  æ€»ä½“æ ‡å‡†å·®: {causal_cls_bias.std().item():.6f}")
            
            # åˆ†æä¸åŒè¯æ±‡è¡¨åŒºåŸŸçš„åç½®
            used_bias = causal_cls_bias[:qwen_used_vocab_size]
            print(f"  å·²ç”¨åç½®å‡å€¼: {used_bias.mean().item():.6f}")
            
            if num_token_id == qwen_used_vocab_size:
                num_bias = causal_cls_bias[num_token_id].item()
                print(f"  <NUM>åç½®å€¼: {num_bias:.6f}")
            
            reserved_bias = causal_cls_bias[causal_reserved_start:causal_reserved_end]
            print(f"  é¢„ç•™åç½®å‡å€¼: {reserved_bias.mean().item():.6f}")
    else:
        print("CausalQwenåˆ†ç±»å¤´æ²¡æœ‰åç½®")
    
    # 5. æƒé‡ç»§æ‰¿è´¨é‡æ€»ç»“
    print(f"\n--- æƒé‡ç»§æ‰¿è´¨é‡æ€»ç»“ ---")
    
    # è®¡ç®—ç»§æ‰¿è´¨é‡æŒ‡æ ‡
    used_weights_identical = torch.allclose(causal_used_weights, qwen_used_weights, atol=1e-6)
    
    if (causal_reserved_end - causal_reserved_start) == (qwen_reserved_end - qwen_reserved_start):
        causal_reserved_weights = causal_cls_weight[causal_reserved_start:causal_reserved_end, :]
        qwen_reserved_weights = qwen_lm_weight[qwen_reserved_start:qwen_reserved_end, :]
        reserved_weights_identical = torch.allclose(causal_reserved_weights, qwen_reserved_weights, atol=1e-6)
    else:
        reserved_weights_identical = False
    
    num_position_correct = (num_token_id == qwen_used_vocab_size)
    
    print(f"æƒé‡ç»§æ‰¿è´¨é‡è¯„ä¼°:")
    print(f"  å·²ç”¨è¯æ±‡è¡¨æƒé‡å®Œå…¨ç»§æ‰¿: {'âœ…' if used_weights_identical else 'âŒ'}")
    print(f"  é¢„ç•™è¯æ±‡è¡¨æƒé‡å®Œå…¨ç»§æ‰¿: {'âœ…' if reserved_weights_identical else 'âŒ'}")
    print(f"  <NUM> tokenä½ç½®æ­£ç¡®: {'âœ…' if num_position_correct else 'âŒ'}")
    
    inheritance_success = used_weights_identical and reserved_weights_identical and num_position_correct
    print(f"  ğŸ¯ æ•´ä½“æƒé‡ç»§æ‰¿: {'âœ… å®Œå…¨æˆåŠŸ' if inheritance_success else 'âŒ å­˜åœ¨é—®é¢˜'}")
    
    return {
        'used_weights_identical': used_weights_identical,
        'reserved_weights_identical': reserved_weights_identical,
        'num_position_correct': num_position_correct,
        'inheritance_success': inheritance_success
    }

def compare_forward_pass(causal_model, qwen_model, inputs, tokenizer, device, vocab_analysis):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å‰å‘ä¼ æ’­ç»“æœï¼ˆåŸºäºç²¾ç¡®çš„è¯æ±‡è¡¨æ¦‚å¿µï¼‰"""
    print_section("å‰å‘ä¼ æ’­å¯¹æ¯”", 1)
    print("åŸºäºä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µçš„ç²¾ç¡®å‰å‘ä¼ æ’­å¯¹æ¯”")
    
    # è·å–è¯æ±‡è¡¨æ¦‚å¿µ
    qwen_used_vocab_size = vocab_analysis['qwen_used_vocab_size']
    qwen_reserved_vocab_size = vocab_analysis['qwen_reserved_vocab_size'] 
    qwen_config_vocab_size = vocab_analysis['qwen_config_vocab_size']
    causal_used_vocab_size = vocab_analysis['causal_used_vocab_size']
    causal_reserved_vocab_size = vocab_analysis['causal_reserved_vocab_size']
    causal_config_vocab_size = vocab_analysis['causal_config_vocab_size']
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    
    print(f"è¾“å…¥æ•°æ®:")
    print(f"  input_idså½¢çŠ¶: {input_ids.shape}")
    print(f"  attention_maskå½¢çŠ¶: {attention_mask.shape}")
    print(f"  numerical_valueså½¢çŠ¶: {numerical_values.shape}")
    
    # CausalQwenå‰å‘ä¼ æ’­
    print(f"\n--- CausalQwen å‰å‘ä¼ æ’­ ---")
    with torch.no_grad():
        causal_outputs = causal_model(input_ids, numerical_values, attention_mask)
    
    print(f"CausalQwenè¾“å‡º:")
    for key, tensor in causal_outputs.items():
        if isinstance(tensor, torch.Tensor):
            print(f"  {key}: {tensor.shape}")
    
    # Qwenå‰å‘ä¼ æ’­
    print(f"\n--- Qwen å‰å‘ä¼ æ’­ ---")
    with torch.no_grad():
        qwen_outputs = qwen_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)
    
    print(f"Qwenè¾“å‡º:")
    print(f"  logits: {qwen_outputs.logits.shape}")
    if hasattr(qwen_outputs, 'hidden_states') and qwen_outputs.hidden_states is not None:
        print(f"  hidden_states (layers): {len(qwen_outputs.hidden_states)}")
        print(f"  last_hidden_state: {qwen_outputs.hidden_states[-1].shape}")
    else:
        print(f"  hidden_states: Not available")
    
    # ç‰¹å¾è¡¨å¾å¯¹æ¯”
    print_section("ç‰¹å¾è¡¨å¾å¯¹æ¯”", 2)
    
    causal_features = causal_outputs['features']
    
    if hasattr(qwen_outputs, 'hidden_states') and qwen_outputs.hidden_states is not None:
        qwen_features = qwen_outputs.hidden_states[-1]  # æœ€åä¸€å±‚éšè—çŠ¶æ€
        
        print_tensor_comparison(causal_features, qwen_features,
                              "CausalQwen", "Qwen", "æœ€åå±‚éšè—çŠ¶æ€")
        
        features_identical = torch.allclose(causal_features, qwen_features, atol=1e-6)
        print(f"\nâœ… ç‰¹å¾å®Œå…¨ä¸€è‡´æ€§éªŒè¯: {'é€šè¿‡' if features_identical else 'å¤±è´¥'}")
    else:
        print("âš ï¸ æ— æ³•è·å–Qwençš„éšè—çŠ¶æ€ï¼Œè·³è¿‡ç‰¹å¾å¯¹æ¯”")
        features_identical = False
    
    # åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„è¾“å‡ºåˆ†æ
    print_section("åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„è¾“å‡ºåˆ†æ", 2)
    
    print(f"è¯æ±‡è¡¨æ¦‚å¿µå¯¹æ¯”:")
    print(f"  Qwenè¾“å‡ºlogitså½¢çŠ¶: {qwen_outputs.logits.shape} (æ€»è¯æ±‡è¡¨: {qwen_config_vocab_size})")
    print(f"  CausalQwenè¾“å‡ºcls_locå½¢çŠ¶: {causal_outputs['cls_loc'].shape} (æ€»è¯æ±‡è¡¨: {causal_config_vocab_size})")
    print(f"  å·²ç”¨è¯æ±‡è¡¨å¤§å°: {qwen_used_vocab_size}")
    print(f"  é¢„ç•™è¯æ±‡è¡¨å¤§å°: {qwen_reserved_vocab_size}")
    
    # 1. å·²ç”¨è¯æ±‡è¡¨è¾“å‡ºå¯¹æ¯”
    print(f"\n--- å·²ç”¨è¯æ±‡è¡¨è¾“å‡ºå¯¹æ¯” ---")
    print(f"å¯¹æ¯”èŒƒå›´: ID 0~{qwen_used_vocab_size-1} (å…± {qwen_used_vocab_size} ä¸ªå·²ç”¨token)")
    
    # æå–å·²ç”¨è¯æ±‡è¡¨çš„è¾“å‡º
    causal_used_cls_loc = causal_outputs['cls_loc'][:, :, :qwen_used_vocab_size]
    causal_used_cls_scale = causal_outputs['cls_scale'][:, :, :qwen_used_vocab_size]
    qwen_used_logits = qwen_outputs.logits[:, :, :qwen_used_vocab_size]
    
    # ç›´æ¥å¯¹æ¯”åˆ†ç±»å¾—åˆ†ï¼ˆcls_loc vs logitsï¼‰
    print_tensor_comparison(causal_used_cls_loc, qwen_used_logits,
                          "CausalQwen(å·²ç”¨cls_loc)", "Qwen(å·²ç”¨logits)", "å·²ç”¨è¯æ±‡è¡¨åˆ†ç±»å¾—åˆ†")
    
    # 2. <NUM> tokenè¾“å‡ºåˆ†æ
    print(f"\n--- <NUM> Tokenè¾“å‡ºåˆ†æ ---")
    
    num_token_id = tokenizer.num_token_id
    if num_token_id == qwen_used_vocab_size:
        print(f"<NUM> token ID: {num_token_id} (ä½ç½®æ­£ç¡®)")
        
        # æå–<NUM> tokençš„è¾“å‡º
        causal_num_cls_loc = causal_outputs['cls_loc'][:, :, num_token_id]  # [B, S]
        causal_num_cls_scale = causal_outputs['cls_scale'][:, :, num_token_id]  # [B, S]
        
        print(f"<NUM> tokenè¾“å‡ºç»Ÿè®¡:")
        print(f"  cls_loc å‡å€¼: {causal_num_cls_loc.mean().item():.6f}")
        print(f"  cls_scale å‡å€¼: {causal_num_cls_scale.mean().item():.6f}")
        
        # è®¡ç®—<NUM> tokençš„OvRæ¦‚ç‡
        num_ovr_probs = compute_ovr_probabilities(
            causal_num_cls_loc, causal_num_cls_scale, 10.0
        )
        
        print(f"  OvRæ¦‚ç‡ç»Ÿè®¡:")
        print(f"    å¹³å‡æ¦‚ç‡: {num_ovr_probs.mean().item():.6f}")
        print(f"    æœ€å¤§æ¦‚ç‡: {num_ovr_probs.max().item():.6f}")
        print(f"    æ¦‚ç‡>0.1çš„ä½ç½®æ•°: {(num_ovr_probs > 0.1).sum().item()}")
        
        # åˆ†æ<NUM> tokenç›¸å¯¹äºå·²ç”¨tokençš„è¾“å‡º
        used_cls_loc_mean = causal_used_cls_loc.mean(dim=-1)  # [B, S] å·²ç”¨tokençš„å¹³å‡loc
        num_vs_used_diff = (causal_num_cls_loc - used_cls_loc_mean).abs()
        
        print(f"  <NUM> tokenä¸å·²ç”¨tokenè¾“å‡ºå·®å¼‚:")
        print(f"    å¹³å‡å·®å¼‚: {num_vs_used_diff.mean().item():.6f}")
        print(f"    æœ€å¤§å·®å¼‚: {num_vs_used_diff.max().item():.6f}")
    else:
        print(f"âŒ <NUM> tokenä½ç½®é”™è¯¯: æœŸæœ› {qwen_used_vocab_size}, å®é™… {num_token_id}")
    
    # 3. é¢„ç•™è¯æ±‡è¡¨è¾“å‡ºåˆ†æ
    print(f"\n--- é¢„ç•™è¯æ±‡è¡¨è¾“å‡ºåˆ†æ ---")
    
    causal_reserved_start = causal_used_vocab_size
    causal_reserved_end = causal_config_vocab_size
    qwen_reserved_start = qwen_used_vocab_size
    qwen_reserved_end = qwen_config_vocab_size
    
    print(f"CausalQwené¢„ç•™tokenèŒƒå›´: ID {causal_reserved_start}~{causal_reserved_end-1}")
    print(f"Qwené¢„ç•™tokenèŒƒå›´: ID {qwen_reserved_start}~{qwen_reserved_end-1}")
    
    # æå–é¢„ç•™è¯æ±‡è¡¨çš„è¾“å‡º
    causal_reserved_cls_loc = causal_outputs['cls_loc'][:, :, causal_reserved_start:causal_reserved_end]
    causal_reserved_cls_scale = causal_outputs['cls_scale'][:, :, causal_reserved_start:causal_reserved_end]
    qwen_reserved_logits = qwen_outputs.logits[:, :, qwen_reserved_start:qwen_reserved_end]
    
    if causal_reserved_cls_loc.shape == qwen_reserved_logits.shape:
        print_tensor_comparison(causal_reserved_cls_loc, qwen_reserved_logits,
                              "CausalQwen(é¢„ç•™cls_loc)", "Qwen(é¢„ç•™logits)", "é¢„ç•™è¯æ±‡è¡¨åˆ†ç±»å¾—åˆ†")
        
        # åˆ†æé¢„ç•™tokençš„æ¦‚ç‡åˆ†å¸ƒ
        reserved_ovr_probs = compute_ovr_probabilities(
            causal_reserved_cls_loc.flatten(0, -2),
            causal_reserved_cls_scale.flatten(0, -2),
            10.0
        ).view_as(causal_reserved_cls_loc)
        
        print(f"\né¢„ç•™token OvRæ¦‚ç‡åˆ†æ:")
        print(f"  å¹³å‡æ¦‚ç‡: {reserved_ovr_probs.mean().item():.6f}")
        print(f"  æœ€å¤§æ¦‚ç‡: {reserved_ovr_probs.max().item():.6f}")
        print(f"  æ¦‚ç‡>0.01çš„æ¯”ä¾‹: {(reserved_ovr_probs > 0.01).float().mean().item():.6f}")
        
        # éªŒè¯é¢„ç•™tokençš„æ¦‚ç‡åº”è¯¥å¾ˆä½
        low_prob_threshold = 0.01
        low_prob_ratio = (reserved_ovr_probs < low_prob_threshold).float().mean().item()
        print(f"  ä½æ¦‚ç‡(<{low_prob_threshold})æ¯”ä¾‹: {low_prob_ratio:.6f} (åº”è¯¥æ¥è¿‘1.0)")
        
        reserved_probs_reasonable = low_prob_ratio > 0.95
        print(f"  é¢„ç•™tokenæ¦‚ç‡åˆç†æ€§: {'âœ…' if reserved_probs_reasonable else 'âŒ'}")
    else:
        print(f"âŒ é¢„ç•™tokenè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…")
        reserved_probs_reasonable = False
    
    # ä½ç½®çº§åˆ«çš„è¯¦ç»†åˆ†æ
    print_section("ä½ç½®çº§åˆ«åˆ†æ", 2)
    
    batch_size, seq_len = input_ids.shape
    sample_positions = [(0, 1), (0, min(3, seq_len-1)), (0, seq_len-1)]
    
    for batch_idx, pos_idx in sample_positions:
        if attention_mask[batch_idx, pos_idx] == 0:
            continue
            
        print(f"\n--- æ ·æœ¬{batch_idx+1} ä½ç½®{pos_idx+1} è¯¦ç»†åˆ†æ ---")
        
        # åˆ†ç±»å¾—åˆ†å¯¹æ¯”ï¼ˆCausalQwençš„cls_loc vs Qwençš„logitsï¼‰
        causal_scores = causal_outputs['cls_loc'][batch_idx, pos_idx, :K]
        qwen_scores = qwen_logits_truncated[batch_idx, pos_idx, :]
        
        print_tensor_comparison(causal_scores, qwen_scores,
                              "CausalQwen(cls_loc)", "Qwen(logits)", "åˆ†ç±»å¾—åˆ†")
        
        # Top-5 tokenæ¦‚ç‡å¯¹æ¯”
        causal_pos_probs = causal_ovr_probs_inherited[batch_idx, pos_idx, :]
        qwen_pos_probs = qwen_probs_truncated[batch_idx, pos_idx, :]
        
        causal_top5_probs, causal_top5_indices = torch.topk(causal_pos_probs, 5)
        qwen_top5_probs, qwen_top5_indices = torch.topk(qwen_pos_probs, 5)
        
        print(f"\nTop-5 tokenæ¦‚ç‡å¯¹æ¯”:")
        print(f"{'æ’å':<5} {'CausalQwen Token':<20} {'æ¦‚ç‡':<10} {'Qwen Token':<20} {'æ¦‚ç‡':<10}")
        print("-" * 65)
        
        for i in range(5):
            causal_token = tokenizer.convert_ids_to_tokens([causal_top5_indices[i].item()])[0]
            qwen_token = tokenizer.convert_ids_to_tokens([qwen_top5_indices[i].item()])[0]
            
            print(f"{i+1:<5} {causal_token:<20} {causal_top5_probs[i].item():<10.4f} {qwen_token:<20} {qwen_top5_probs[i].item():<10.4f}")
    
    # å› æœè¡¨å¾åˆ†æ
    print_section("å› æœè¡¨å¾åˆ†æ", 2)
    
    causal_loc = causal_outputs['causal_loc']
    causal_scale = causal_outputs['causal_scale']
    
    print(f"å› æœè¡¨å¾ç»Ÿè®¡:")
    print(f"  causal_loc: å‡å€¼={causal_loc.mean().item():.6f}, æ ‡å‡†å·®={causal_loc.std().item():.6f}")
    print(f"  causal_scale: å‡å€¼={causal_scale.mean().item():.6f}, æ ‡å‡†å·®={causal_scale.std().item():.6f}")
    print(f"  causal_scaleæœ€å°å€¼: {causal_scale.min().item():.6f} (å¿…é¡»>0)")
    
    # AbductionNetworkåˆå§‹åŒ–éªŒè¯
    expected_scale_mean = 10.0  # åŸºäºexp(2.3) â‰ˆ 10
    scale_init_correct = abs(causal_scale.mean().item() - expected_scale_mean) < 5.0
    print(f"  AbductionNetworkåˆå§‹åŒ–: {'âœ…' if scale_init_correct else 'âŒ'} (æœŸæœ›causal_scaleâ‰ˆ10)")
    
    # <NUM> tokençš„OvRæ¦‚ç‡åˆ†æ
    if tokenizer.num_token_id < causal_outputs['cls_loc'].shape[-1]:
        num_token_probs = compute_ovr_probabilities(
            causal_outputs['cls_loc'][:, :, tokenizer.num_token_id],
            causal_outputs['cls_scale'][:, :, tokenizer.num_token_id],
            10.0
        )
        
        print(f"\n<NUM> token OvRæ¦‚ç‡åˆ†æ:")
        print(f"  å¹³å‡æ¦‚ç‡: {num_token_probs.mean().item():.6f}")
        print(f"  æœ€å¤§æ¦‚ç‡: {num_token_probs.max().item():.6f}")
        print(f"  æ¦‚ç‡>0.1çš„ä½ç½®æ•°: {(num_token_probs > 0.1).sum().item()}")
    
    return causal_outputs, qwen_outputs

def main():
    """ä¸»å‡½æ•°"""
    print_section("CausalQwen VS Qwen æ¨¡å‹å¯¹æ¯”éªŒè¯", 1)
    print("éªŒè¯CausalQwenä»Qwençš„çŸ¥è¯†ä¼ è¾“æ•ˆæœ")
    print("è®¾è®¡åŸåˆ™: å‰å‘ä¼ æ’­ç»“æœ > æ¨¡å‹å‚æ•°ç»“æ„")
    
    # 1. æ¨¡å‹è®¾ç½®
    print_section("æ¨¡å‹è®¾ç½®", 1)
    
    device = torch.device('cpu')  # ä½¿ç”¨CPUä»¥ä¾¿è°ƒè¯•
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    # è®¾ç½®åˆ†è¯å™¨
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    print(f"åˆ†è¯å™¨è®¾ç½®å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # è®¾ç½®CausalQwenæ¨¡å‹
    print(f"\n--- è®¾ç½®CausalQwenæ¨¡å‹ ---")
    causal_config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        ovr_threshold=10.0,
        reg_loss_weight=1.0
    )
    
    causal_model = CausalLanguageModel(causal_config).to(device)
    causal_model.eval()
    
    # æ‰§è¡ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–
    num_target_median = 50.0
    num_target_scale = 25.0
    causal_model.init_weights(num_target_median, num_target_scale)
    print(f"CausalQwenæ¨¡å‹è®¾ç½®å®Œæˆï¼ŒçŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–å®Œæˆ")
    
    # è®¾ç½®åŸå§‹Qwenæ¨¡å‹
    print(f"\n--- è®¾ç½®åŸå§‹Qwenæ¨¡å‹ ---")
    qwen_model = Qwen2ForCausalLM.from_pretrained(
        qwen_model_path,
        torch_dtype=torch.float32,
        device_map=None
    ).to(device)
    qwen_model.eval()
    print(f"åŸå§‹Qwenæ¨¡å‹è®¾ç½®å®Œæˆ")
    
    # 2. æµ‹è¯•æ•°æ®å‡†å¤‡
    print_section("æµ‹è¯•æ•°æ®å‡†å¤‡", 1)
    
    texts = [
        "The item costs 99.99 dollars.",
        "From a batch of 100 items, 5 were defective, leaving 95 good ones.",
        "A standard text without any numerical values."
    ]
    
    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors='pt')
    
    print(f"æµ‹è¯•æ–‡æœ¬:")
    for i, text in enumerate(texts):
        print(f"  {i+1}. {text}")
    
    print(f"\næ‰¹æ¬¡æ•°æ®:")
    print(f"  input_ids: {inputs['input_ids'].shape}")
    print(f"  attention_mask: {inputs['attention_mask'].shape}")
    print(f"  numerical_values: {inputs['numerical_values'].shape}")
    
    # 3. æ‰§è¡Œå¯¹æ¯”åˆ†æ
    vocab_analysis = analyze_vocabulary_concepts(causal_model, qwen_model, tokenizer)
    analyze_model_architectures(causal_model, qwen_model)
    inheritance_analysis = analyze_weight_inheritance(causal_model, qwen_model, tokenizer, vocab_analysis)
    causal_outputs, qwen_outputs = compare_forward_pass(causal_model, qwen_model, inputs, tokenizer, device, vocab_analysis)
    
    # 4. åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„æ€»ç»“éªŒè¯ç»“æœ
    print_section("åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„éªŒè¯ç»“æœæ€»ç»“", 1)
    print("éªŒè¯æ‰€æœ‰ä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µæ˜¯å¦æ­£ç¡®å®ç°")
    
    # æ£€æŸ¥å…³é”®éªŒè¯ç‚¹
    if hasattr(qwen_outputs, 'hidden_states') and qwen_outputs.hidden_states is not None:
        features_identical = torch.allclose(causal_outputs['features'], qwen_outputs.hidden_states[-1], atol=1e-6)
    else:
        features_identical = False
    
    # åŸºäºè¯æ±‡è¡¨æ¦‚å¿µæ£€æŸ¥åˆ†ç±»å¾—åˆ†ä¸€è‡´æ€§
    qwen_used_vocab_size = vocab_analysis['qwen_used_vocab_size']
    
    causal_used_scores = causal_outputs['cls_loc'][:, :, :qwen_used_vocab_size]
    qwen_used_scores = qwen_outputs.logits[:, :, :qwen_used_vocab_size]
    used_scores_consistent = torch.allclose(causal_used_scores, qwen_used_scores, atol=1e-3)
    
    # æ£€æŸ¥å› æœè¡¨å¾åˆå§‹åŒ–
    scale_init_reasonable = 5.0 < causal_outputs['causal_scale'].mean().item() < 15.0
    
    # ä»ä¹‹å‰çš„åˆ†æè·å–ç»“æœ
    concept_analysis_success = vocab_analysis['concept_analysis_success']
    inheritance_success = inheritance_analysis['inheritance_success']
    used_weights_identical = inheritance_analysis['used_weights_identical']
    reserved_weights_identical = inheritance_analysis['reserved_weights_identical']
    num_position_correct = inheritance_analysis['num_position_correct']
    
    print(f"\nğŸ” ä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µéªŒè¯æ£€æŸ¥ç‚¹:")
    print(f"  ğŸ“Š è¯æ±‡è¡¨æ¦‚å¿µåˆ†æ: {'âœ… é€šè¿‡' if concept_analysis_success else 'âŒ å¤±è´¥'}")
    print(f"     - æ€»è¯æ±‡è¡¨å¤§å°è®¡ç®—æ­£ç¡®")
    print(f"     - å·²ç”¨è¯æ±‡è¡¨å¤§å°æ­£ç¡®")  
    print(f"     - é¢„ç•™è¯æ±‡è¡¨å¤§å°æ­£ç¡®")
    print(f"     - <NUM> tokenä½ç½®æ­£ç¡®")
    
    print(f"\nğŸ—ï¸ æƒé‡ç»§æ‰¿éªŒè¯æ£€æŸ¥ç‚¹:")
    print(f"  ğŸ¯ æ•´ä½“æƒé‡ç»§æ‰¿: {'âœ… é€šè¿‡' if inheritance_success else 'âŒ å¤±è´¥'}")
    print(f"     - å·²ç”¨è¯æ±‡è¡¨æƒé‡å®Œå…¨ç»§æ‰¿: {'âœ…' if used_weights_identical else 'âŒ'}")
    print(f"     - é¢„ç•™è¯æ±‡è¡¨æƒé‡å®Œå…¨ç»§æ‰¿: {'âœ…' if reserved_weights_identical else 'âŒ'}")
    print(f"     - <NUM> tokenä½ç½®æ­£ç¡®: {'âœ…' if num_position_correct else 'âŒ'}")
    
    print(f"\nğŸš€ å‰å‘ä¼ æ’­éªŒè¯æ£€æŸ¥ç‚¹:")
    print(f"  ğŸ”¥ ç‰¹å¾å®Œå…¨ä¸€è‡´: {'âœ… é€šè¿‡' if features_identical else 'âŒ å¤±è´¥'}")
    print(f"  ğŸ“ˆ å·²ç”¨è¯æ±‡è¡¨åˆ†ç±»å¾—åˆ†ä¸€è‡´: {'âœ… é€šè¿‡' if used_scores_consistent else 'âŒ å¤±è´¥'}")
    print(f"  ğŸ§  å› æœè¡¨å¾åˆå§‹åŒ–åˆç†: {'âœ… é€šè¿‡' if scale_init_reasonable else 'âŒ å¤±è´¥'}")
    
    # ç»¼åˆè¯„ä¼°
    overall_success = (
        concept_analysis_success and 
        inheritance_success and 
        features_identical and 
        used_scores_consistent and 
        scale_init_reasonable
    )
    
    print(f"\nğŸ¯ åŸºäºè¯æ±‡è¡¨æ¦‚å¿µçš„æ€»ä½“éªŒè¯ç»“æœ:")
    print(f"   {'ğŸ‰ å®Œå…¨æˆåŠŸï¼' if overall_success else 'âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•'}")
    
    if overall_success:
        print(f"\nâœ… å®Œæ•´éªŒè¯é€šè¿‡:")
        print(f"   ğŸ”¸ ä¸‰ä¸ªè¯æ±‡è¡¨æ¦‚å¿µåŒºåˆ†æ¸…æ™°ä¸”å®ç°æ­£ç¡®")
        print(f"   ğŸ”¸ å·²ç”¨è¯æ±‡è¡¨ ({vocab_analysis['qwen_used_vocab_size']:,} tokens) å®Œå…¨ç»§æ‰¿")
        print(f"   ğŸ”¸ é¢„ç•™è¯æ±‡è¡¨ ({vocab_analysis['qwen_reserved_vocab_size']:,} tokens) å®Œå…¨ç»§æ‰¿")
        print(f"   ğŸ”¸ <NUM> token (ID: {tokenizer.num_token_id}) ä½ç½®å’Œåˆå§‹åŒ–æ­£ç¡®")
        print(f"   ğŸ”¸ å‰å‘ä¼ æ’­ç»“æœä¸Qwenå®Œå…¨ä¸€è‡´")
        print(f"   ğŸ”¸ å› æœæ¨ç†åŠŸèƒ½æ­£ç¡®æ‰©å±•")
        print(f"\nğŸŠ CausalQwenæ¶æ„é‡æ„å®Œå…¨éªŒè¯é€šè¿‡ï¼")
        print(f"    ç¬¦åˆ qwen_reserved_tokens_analysis.md çš„ç†è®ºåˆ†æ")
    else:
        print(f"\nâŒ å‘ç°é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥:")
        
        if not concept_analysis_success:
            print(f"   ğŸ”¸ è¯æ±‡è¡¨æ¦‚å¿µå®ç°")
        if not inheritance_success:
            if not used_weights_identical:
                print(f"   ğŸ”¸ å·²ç”¨è¯æ±‡è¡¨æƒé‡ç»§æ‰¿")
            if not reserved_weights_identical:
                print(f"   ğŸ”¸ é¢„ç•™è¯æ±‡è¡¨æƒé‡ç»§æ‰¿")
            if not num_position_correct:
                print(f"   ğŸ”¸ <NUM> tokenä½ç½®è®¾ç½®")
        if not features_identical:
            print(f"   ğŸ”¸ QwenFeatureNetworkçš„å®ç°")
        if not used_scores_consistent:
            print(f"   ğŸ”¸ ActionNetworkçš„æƒé‡åˆå§‹åŒ–")
        if not scale_init_reasonable:
            print(f"   ğŸ”¸ AbductionNetworkçš„åˆå§‹åŒ–ç­–ç•¥")
    
    # è¿”å›å®Œæ•´çš„éªŒè¯ç»“æœä¾›å¤–éƒ¨ä½¿ç”¨
    return {
        'concept_analysis_success': concept_analysis_success,
        'inheritance_success': inheritance_success,
        'features_identical': features_identical,
        'used_scores_consistent': used_scores_consistent,
        'scale_init_reasonable': scale_init_reasonable,
        'overall_success': overall_success,
        'vocab_analysis': vocab_analysis,
        'inheritance_analysis': inheritance_analysis
    }

if __name__ == '__main__':
    main() 
#!/usr/bin/env python
"""
CausalQwen VS Qwen çŸ¥è¯†è¿ç§»éªŒè¯è„šæœ¬

æœ¬è„šæœ¬éªŒè¯ CausalQwen æ˜¯å¦æ­£ç¡®åœ°ä» Qwen æ¨¡å‹è¿›è¡Œäº†çŸ¥è¯†è¿ç§»ã€‚

æ ¸å¿ƒéªŒè¯å†…å®¹ï¼š
1. ç‰¹å¾æå–ä¸€è‡´æ€§ - éªŒè¯ QwenFeatureNetwork æ˜¯å¦æ­£ç¡®å°è£…äº† Qwen
2. åˆ†ç±»å¤´æƒé‡ç»§æ‰¿ - éªŒè¯ ActionNetwork æ˜¯å¦å®Œå…¨å¤ç”¨äº† lm_head
3. å‰å‘ä¼ æ’­ä¸€è‡´æ€§ - éªŒè¯ç›¸åŒè¾“å…¥ä¸‹çš„è¾“å‡ºä¸€è‡´æ€§
4. ä¿ç•™è¯æ±‡å¤„ç† - éªŒè¯ä¿ç•™è¯æ±‡çš„æƒé‡æ˜¯å¦æ­£ç¡®ç»§æ‰¿
"""

import os
import sys
import torch
import numpy as np
from transformers import Qwen2ForCausalLM

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper

def print_section(title, symbol="="):
    """æ‰“å°æ ¼å¼åŒ–çš„æ ‡é¢˜"""
    width = 80
    print(f"\n{symbol * width}")
    print(f"{title.center(width)}")
    print(f"{symbol * width}")

def verify_feature_extraction(causal_model, qwen_model, inputs, device):
    """éªŒè¯ç‰¹å¾æå–çš„ä¸€è‡´æ€§"""
    print_section("ç‰¹å¾æå–ä¸€è‡´æ€§éªŒè¯", "-")
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    # CausalQwen ç‰¹å¾æå–
    with torch.no_grad():
        causal_outputs = causal_model(
            input_ids, 
            inputs['numerical_values'].to(device), 
            attention_mask
        )
        causal_features = causal_outputs['features']
    
    # Qwen ç‰¹å¾æå–
    with torch.no_grad():
        qwen_outputs = qwen_model(
            input_ids, 
            attention_mask=attention_mask, 
            output_hidden_states=True
        )
        qwen_features = qwen_outputs.hidden_states[-1]  # æœ€åä¸€å±‚éšè—çŠ¶æ€
    
    # éªŒè¯ä¸€è‡´æ€§
    features_match = torch.allclose(causal_features, qwen_features, atol=1e-6)
    
    print(f"ç‰¹å¾å½¢çŠ¶: CausalQwen {causal_features.shape} vs Qwen {qwen_features.shape}")
    print(f"ç‰¹å¾å‡å€¼å·®å¼‚: {(causal_features - qwen_features).abs().mean().item():.6e}")
    print(f"ç‰¹å¾æœ€å¤§å·®å¼‚: {(causal_features - qwen_features).abs().max().item():.6e}")
    print(f"ç‰¹å¾æå–ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if features_match else 'âŒ å¤±è´¥'}")
    
    return features_match

def verify_weight_inheritance(causal_model, qwen_model, tokenizer):
    """éªŒè¯æƒé‡ç»§æ‰¿çš„æ­£ç¡®æ€§"""
    print_section("æƒé‡ç»§æ‰¿éªŒè¯", "-")
    
    # è·å–åˆ†ç±»å¤´æƒé‡
    causal_cls_weight = causal_model.action_network.classification_head.causal_linear.weight.data
    causal_cls_bias = causal_model.action_network.classification_head.causal_linear.bias
    
    qwen_lm_weight = qwen_model.lm_head.weight.data
    qwen_lm_bias = qwen_model.lm_head.bias if hasattr(qwen_model.lm_head, 'bias') else None
    
    print(f"æƒé‡å½¢çŠ¶å¯¹æ¯”:")
    print(f"  CausalQwen åˆ†ç±»å¤´: {causal_cls_weight.shape}")
    print(f"  Qwen lm_head:     {qwen_lm_weight.shape}")
    
    # éªŒè¯æƒé‡ç»§æ‰¿ï¼ˆåªæ¯”è¾ƒ CausalQwen è¯æ±‡è¡¨èŒƒå›´å†…çš„æƒé‡ï¼‰
    vocab_size = causal_cls_weight.shape[0]
    inherited_weights = causal_cls_weight[:vocab_size]
    qwen_weights = qwen_lm_weight[:vocab_size]
    
    weights_match = torch.allclose(inherited_weights, qwen_weights, atol=1e-6)
    
    print(f"\næƒé‡ç»§æ‰¿ç»Ÿè®¡:")
    print(f"  æƒé‡å‡å€¼å·®å¼‚: {(inherited_weights - qwen_weights).abs().mean().item():.6e}")
    print(f"  æƒé‡æœ€å¤§å·®å¼‚: {(inherited_weights - qwen_weights).abs().max().item():.6e}")
    print(f"  æƒé‡ç»§æ‰¿ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if weights_match else 'âŒ å¤±è´¥'}")
    
    # éªŒè¯åç½®ç»§æ‰¿
    if causal_cls_bias is not None and qwen_lm_bias is not None:
        inherited_bias = causal_cls_bias.data[:vocab_size]
        qwen_bias = qwen_lm_bias.data[:vocab_size]
        bias_match = torch.allclose(inherited_bias, qwen_bias, atol=1e-6)
        print(f"\nåç½®ç»§æ‰¿ç»Ÿè®¡:")
        print(f"  åç½®å‡å€¼å·®å¼‚: {(inherited_bias - qwen_bias).abs().mean().item():.6e}")
        print(f"  åç½®ç»§æ‰¿ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if bias_match else 'âŒ å¤±è´¥'}")
    else:
        bias_match = True
        print(f"\nåç½®ç»§æ‰¿: Qwen æ²¡æœ‰åç½®é¡¹")
    
    return weights_match and bias_match

def verify_forward_consistency(causal_model, qwen_model, inputs, tokenizer, device):
    """éªŒè¯å‰å‘ä¼ æ’­çš„ä¸€è‡´æ€§"""
    print_section("å‰å‘ä¼ æ’­ä¸€è‡´æ€§éªŒè¯", "-")
    
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    
    # CausalQwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        causal_outputs = causal_model(input_ids, numerical_values, attention_mask)
        causal_cls_loc = causal_outputs['cls_loc']
    
    # Qwen å‰å‘ä¼ æ’­
    with torch.no_grad():
        qwen_outputs = qwen_model(input_ids, attention_mask=attention_mask)
        qwen_logits = qwen_outputs.logits
    
    # åªæ¯”è¾ƒå·²ç”¨è¯æ±‡è¡¨éƒ¨åˆ†çš„è¾“å‡ºï¼ˆä¸åŒ…æ‹¬ <NUM> å’Œä¿ç•™è¯æ±‡ï¼‰
    used_vocab_size = tokenizer.tokenizer.vocab_size  # åŸå§‹ Qwen è¯æ±‡è¡¨å¤§å°
    
    causal_used_logits = causal_cls_loc[:, :, :used_vocab_size]
    qwen_used_logits = qwen_logits[:, :, :used_vocab_size]
    
    logits_match = torch.allclose(causal_used_logits, qwen_used_logits, atol=1e-3)
    
    print(f"è¾“å‡ºå½¢çŠ¶å¯¹æ¯”:")
    print(f"  CausalQwen cls_loc: {causal_cls_loc.shape}")
    print(f"  Qwen logits:        {qwen_logits.shape}")
    print(f"\nå·²ç”¨è¯æ±‡è¡¨è¾“å‡ºä¸€è‡´æ€§:")
    print(f"  æ¯”è¾ƒèŒƒå›´: å‰ {used_vocab_size} ä¸ªè¯æ±‡")
    print(f"  è¾“å‡ºå‡å€¼å·®å¼‚: {(causal_used_logits - qwen_used_logits).abs().mean().item():.6e}")
    print(f"  è¾“å‡ºæœ€å¤§å·®å¼‚: {(causal_used_logits - qwen_used_logits).abs().max().item():.6e}")
    print(f"  å‰å‘ä¼ æ’­ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if logits_match else 'âŒ å¤±è´¥'}")
    
    # åˆ†æ <NUM> token çš„è¾“å‡º
    num_token_id = tokenizer.num_token_id
    if num_token_id < causal_cls_loc.shape[-1]:
        num_logits = causal_cls_loc[:, :, num_token_id]
        print(f"\n<NUM> token (ID: {num_token_id}) è¾“å‡ºåˆ†æ:")
        print(f"  è¾“å‡ºå‡å€¼: {num_logits.mean().item():.6f}")
        print(f"  è¾“å‡ºæ ‡å‡†å·®: {num_logits.std().item():.6f}")
        print(f"  è¾“å‡ºèŒƒå›´: [{num_logits.min().item():.6f}, {num_logits.max().item():.6f}]")
    
    return logits_match

def verify_reserved_tokens(causal_model, qwen_model, tokenizer):
    """éªŒè¯ä¿ç•™è¯æ±‡çš„å¤„ç†"""
    print_section("ä¿ç•™è¯æ±‡å¤„ç†éªŒè¯", "-")
    
    # è·å–è¯æ±‡è¡¨ä¿¡æ¯
    qwen_total_vocab = qwen_model.config.vocab_size  # 151936
    qwen_used_vocab = tokenizer.tokenizer.vocab_size  # 151665
    qwen_reserved = qwen_total_vocab - qwen_used_vocab  # 271
    
    causal_total_vocab = tokenizer.vocab_size  # 151666
    causal_reserved_start = causal_total_vocab
    causal_reserved_end = qwen_total_vocab
    
    print(f"è¯æ±‡è¡¨ç»Ÿè®¡:")
    print(f"  Qwen æ€»è¯æ±‡è¡¨: {qwen_total_vocab}")
    print(f"  Qwen å·²ç”¨è¯æ±‡: {qwen_used_vocab}")
    print(f"  Qwen ä¿ç•™è¯æ±‡: {qwen_reserved}")
    print(f"  CausalQwen æ€»è¯æ±‡è¡¨: {causal_total_vocab}")
    print(f"  CausalQwen ä¿ç•™è¯æ±‡: {causal_reserved_end - causal_reserved_start}")
    
    # å¦‚æœ CausalQwen çš„è¯æ±‡è¡¨å¤§å°ç­‰äº Qwen çš„æ€»è¯æ±‡è¡¨å¤§å°ï¼ŒéªŒè¯ä¿ç•™è¯æ±‡æƒé‡
    if causal_model.action_network.classification_head.causal_linear.weight.shape[0] == qwen_total_vocab:
        causal_reserved_weights = causal_model.action_network.classification_head.causal_linear.weight.data[causal_reserved_start:causal_reserved_end]
        qwen_reserved_weights = qwen_model.lm_head.weight.data[causal_reserved_start:causal_reserved_end]
        
        reserved_match = torch.allclose(causal_reserved_weights, qwen_reserved_weights, atol=1e-6)
        
        print(f"\nä¿ç•™è¯æ±‡æƒé‡éªŒè¯:")
        print(f"  æƒé‡å½¢çŠ¶: {causal_reserved_weights.shape}")
        print(f"  æƒé‡å‡å€¼å·®å¼‚: {(causal_reserved_weights - qwen_reserved_weights).abs().mean().item():.6e}")
        print(f"  ä¿ç•™è¯æ±‡æƒé‡ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if reserved_match else 'âŒ å¤±è´¥'}")
        
        return reserved_match
    else:
        print(f"\nâš ï¸  CausalQwen è¯æ±‡è¡¨å¤§å°ä¸ Qwen ä¸åŒï¼Œè·³è¿‡ä¿ç•™è¯æ±‡éªŒè¯")
        return True

def main():
    """ä¸»å‡½æ•°"""
    print_section("CausalQwen çŸ¥è¯†è¿ç§»éªŒè¯")
    
    # è®¾ç½®
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("\nåˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"<NUM> token ID: {tokenizer.num_token_id}")
    
    # åˆå§‹åŒ– CausalQwen
    print("\nåˆå§‹åŒ– CausalQwen æ¨¡å‹...")
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,
        use_real_qwen=True,
        qwen_model_path=qwen_model_path
    )
    causal_model = CausalLanguageModel(config).to(device)
    causal_model.init_weights()  # æ‰§è¡ŒçŸ¥è¯†è¿ç§»
    causal_model.eval()
    
    # åˆå§‹åŒ–åŸå§‹ Qwen
    print("\nåˆå§‹åŒ–åŸå§‹ Qwen æ¨¡å‹...")
    qwen_model = Qwen2ForCausalLM.from_pretrained(
        qwen_model_path,
        torch_dtype=torch.float32,
        device_map=None
    ).to(device)
    qwen_model.eval()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    print("\nå‡†å¤‡æµ‹è¯•æ•°æ®...")
    texts = [
        "The price is 99.99 dollars.",
        "There are 100 items in total.",
        "Hello world!"
    ]
    inputs = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors='pt')
    
    # æ‰§è¡ŒéªŒè¯
    print("\nå¼€å§‹éªŒè¯...")
    
    # 1. ç‰¹å¾æå–ä¸€è‡´æ€§
    features_ok = verify_feature_extraction(causal_model, qwen_model, inputs, device)
    
    # 2. æƒé‡ç»§æ‰¿éªŒè¯
    weights_ok = verify_weight_inheritance(causal_model, qwen_model, tokenizer)
    
    # 3. å‰å‘ä¼ æ’­ä¸€è‡´æ€§
    forward_ok = verify_forward_consistency(causal_model, qwen_model, inputs, tokenizer, device)
    
    # 4. ä¿ç•™è¯æ±‡å¤„ç†
    reserved_ok = verify_reserved_tokens(causal_model, qwen_model, tokenizer)
    
    # æ€»ç»“
    print_section("éªŒè¯æ€»ç»“")
    
    all_passed = features_ok and weights_ok and forward_ok and reserved_ok
    
    print(f"éªŒè¯ç»“æœ:")
    print(f"  âœ… ç‰¹å¾æå–ä¸€è‡´æ€§: {'é€šè¿‡' if features_ok else 'å¤±è´¥'}")
    print(f"  âœ… æƒé‡ç»§æ‰¿æ­£ç¡®æ€§: {'é€šè¿‡' if weights_ok else 'å¤±è´¥'}")
    print(f"  âœ… å‰å‘ä¼ æ’­ä¸€è‡´æ€§: {'é€šè¿‡' if forward_ok else 'å¤±è´¥'}")
    print(f"  âœ… ä¿ç•™è¯æ±‡å¤„ç†: {'é€šè¿‡' if reserved_ok else 'å¤±è´¥'}")
    
    if all_passed:
        print(f"\nğŸ‰ çŸ¥è¯†è¿ç§»éªŒè¯å®Œå…¨é€šè¿‡ï¼")
        print(f"   CausalQwen æˆåŠŸç»§æ‰¿äº† Qwen çš„çŸ¥è¯†")
        print(f"   åŒæ—¶æ­£ç¡®æ‰©å±•äº†å› æœæ¨ç†åŠŸèƒ½")
    else:
        print(f"\nâŒ çŸ¥è¯†è¿ç§»éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³å®ç°")
    
    return all_passed

if __name__ == '__main__':
    main()
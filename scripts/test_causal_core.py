#!/usr/bin/env python
"""
å› æœæ ¸å¿ƒæµç¨‹å›¾å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `mathematical_foundations.md` ä¸­çš„ "å›¾ 3" æµç¨‹å›¾ï¼Œ
æ¸…æ™°åœ°å±•ç¤ºä»å¢å¼ºåµŒå…¥ (e) åˆ°æœ€ç»ˆå†³ç­–åˆ†å¸ƒ (S, Y) çš„æ¯ä¸€æ­¥æ•°æ®å˜æ¢ã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  æ­¥éª¤ {step_name}: {description}")
    print(f"{'-'*70}")

def print_tensor_stats(name, tensor):
    """æ‰“å°å¼ é‡çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    if not isinstance(tensor, torch.Tensor):
        print(f"   - {name}: Not a tensor")
        return
    tensor = tensor.detach().cpu().to(torch.float32)
    print(f"   - {name}:")
    print(f"     - Shape: {tensor.shape}")
    print(f"     - Mean: {tensor.mean().item():.4f}")
    print(f"     - Std:  {tensor.std().item():.4f}")
    print(f"     - Min:  {tensor.min().item():.4f}")
    print(f"     - Max:  {tensor.max().item():.4f}")

def main():
    print("ğŸš€ CausalQwen - å› æœæ ¸å¿ƒæµç¨‹æ·±åº¦éªŒè¯")
    
    # --- åˆå§‹åŒ– ---
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')

    # åŠ¨æ€è·å–æ¨¡å‹å‚æ•°
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'], # æ’ç­‰æ˜ å°„
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("\nâœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # --- å‡†å¤‡è¾“å…¥æ•°æ® ---
    test_samples = [
        "The item costs 50.5 and the tax is 4.5.",
        "A sentence without numbers.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    print(f"\nğŸ“Š å‡†å¤‡ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")

    with torch.no_grad():
        # --- å®Œæ•´å‰å‘ä¼ æ’­ ---
        # æ¨¡å‹ç°åœ¨è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä¸­é—´æ­¥éª¤çš„å­—å…¸
        full_outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )

        # --- å›¾3 æµç¨‹èµ·ç‚¹: è·å–å¢å¼ºåµŒå…¥ e ---
        print_step("A", "èµ·ç‚¹: è·å–å¢å¼ºåµŒå…¥ e (Enhanced Embeddings)")
        e = full_outputs['enhanced_embeddings']
        print_tensor_stats("å¢å¼ºåµŒå…¥ e", e)

        # --- å›¾3 æµç¨‹: e -> z (Qwen ç‰¹å¾ç½‘ç»œ) ---
        print_step("B", "Qwenç‰¹å¾ç½‘ç»œ: e -> z (ä¸Šä¸‹æ–‡ç‰¹å¾)")
        z = full_outputs['features']
        print_tensor_stats("ä¸Šä¸‹æ–‡ç‰¹å¾ z", z)

        # --- å›¾3 æµç¨‹: z -> U (å½’å› ç½‘ç»œ) ---
        print_step("C-D", "å½’å› ç½‘ç»œ (Abduction): z -> U (å› æœè¡¨å¾åˆ†å¸ƒ)")
        loc_U = full_outputs['causal_loc']
        scale_U = full_outputs['causal_scale']
        print("   - ç†è®º: loc_U â‰ˆ z, scale_U ä¸ºå¤§æ­£æ•° (â‰ˆ10)")
        print_tensor_stats("å› æœä½ç½® loc_U", loc_U)
        print_tensor_stats("å› æœå°ºåº¦ scale_U", scale_U)
        print(f"   - éªŒè¯: loc_U ä¸ z çš„å‡å€¼å·®å¼‚: {abs(loc_U.mean() - z.mean()):.6f}")
        print(f"   - éªŒè¯: scale_U å‡å€¼ > 5: {'âœ…' if scale_U.mean() > 5 else 'âŒ'}")

        # --- å›¾3 æµç¨‹: U -> S, Y (è¡ŒåŠ¨ç½‘ç»œ) ---
        print_step("E-H", "è¡ŒåŠ¨ç½‘ç»œ (Action): U -> S, Y (å†³ç­–åˆ†å¸ƒ)")
        
        print("\n   --- åˆ†ç±»è¾“å‡º (S) ---")
        print("   - ç†è®º: scale_S åº”è¯¥æ˜¯ scale_U çš„çº¿æ€§å˜æ¢ï¼Œä»ä¸ºå¤§æ­£æ•°")
        print_tensor_stats("åˆ†ç±» logits (loc_S)", full_outputs.get('cls_loc'))
        print_tensor_stats("åˆ†ç±»å°ºåº¦ (scale_S)", full_outputs.get('cls_scale'))

        print("\n   --- å›å½’è¾“å‡º (Y) ---")
        print("   - ç†è®º: scale_Y åº”è¯¥æ˜¯ scale_U çš„çº¿æ€§å˜æ¢ï¼Œä¸ºä¸€ä¸ªåˆç†çš„æ­£æ•°")
        print_tensor_stats("å›å½’é¢„æµ‹ (loc_Y)", full_outputs.get('reg_loc'))
        print_tensor_stats("å›å½’ä¸ç¡®å®šæ€§ (scale_Y)", full_outputs.get('reg_scale'))
        
        scale_Y_mean = full_outputs.get('reg_scale').mean().item()
        print(f"   - éªŒè¯: scale_Y å‡å€¼ > 0: {'âœ…' if scale_Y_mean > 0 else 'âŒ'}")
    
    print(f"\n\n{'='*80}")
    print("ğŸ‰ æ‰¹é‡éªŒè¯æˆåŠŸï¼è„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†ä» e -> z -> U -> (S, Y) çš„æ ¸å¿ƒæ•°æ®æµã€‚")

if __name__ == '__main__':
    main() 
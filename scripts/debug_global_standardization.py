#!/usr/bin/env python3
"""
å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥éªŒè¯è„šæœ¬
===========================================

å®æ–½ç»å¯¹å…¬å¹³çš„ç«æŠ€åœºï¼š
1. å…¨å±€æ ‡å‡†åŒ–ï¼šå¯¹ X å’Œ y éƒ½è¿›è¡Œæ ‡å‡†åŒ–
2. ç»Ÿä¸€è¾“å…¥ï¼šæ‰€æœ‰æ¨¡å‹æ¥æ”¶å®Œå…¨æ ‡å‡†åŒ–çš„æ•°æ®
3. ç»Ÿä¸€è¯„ä¼°ï¼šæ‰€æœ‰é¢„æµ‹ç»“æœéƒ½è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°

è¿™æ ·å¯ä»¥ç¡®ä¿ï¼š
- æ‰€æœ‰æ¨¡å‹åœ¨ç›¸åŒçš„æŠ½è±¡ç©ºé—´ä¸­å­¦ä¹ 
- ç¨³å¥å›å½’å™¨ä¸èƒ½åˆ©ç”¨æœªç¼©æ”¾æ•°æ®çš„ä¼˜åŠ¿
- CausalEngine åœ¨å›°éš¾ç¯å¢ƒä¸‹å±•ç¤ºå…¶çœŸæ­£èƒ½åŠ›
"""

import numpy as np
import warnings
import os
import sys
import time
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error, r2_score

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from causal_sklearn.regressor import (
    MLPCausalRegressor, MLPPytorchRegressor, 
    MLPHuberRegressor, MLPPinballRegressor, MLPCauchyRegressor
)
from causal_sklearn.utils import causal_split

warnings.filterwarnings('ignore')

class GlobalStandardizationConfig:
    """å…¨å±€æ ‡å‡†åŒ–å®éªŒé…ç½®"""
    # æ•°æ®é…ç½®
    TEST_SIZE = 0.2
    VAL_SIZE = 0.2
    RANDOM_STATE = 42
    ANOMALY_RATIO = 0.25
    
    # æ¨¡å‹è¶…å‚æ•°
    LEARNING_RATE = 0.01
    ALPHA_CAUSAL = 0.0
    ALPHA_PYTORCH = 0.0001
    BATCH_SIZE = 200
    HIDDEN_SIZES = (128, 64, 32)
    MAX_EPOCHS = 3000
    PATIENCE = 50
    TOL = 1e-4
    
    # CausalEngineä¸“å±å‚æ•°
    GAMMA_INIT = 1.0
    B_NOISE_INIT = 1.0
    B_NOISE_TRAINABLE = True
    
    # è¦æµ‹è¯•çš„æ¨¡å‹
    MODELS_TO_TEST = {
        'pytorch_mlp': True,
        'causal_standard': True,
        'causal_deterministic': True,
        'mlp_huber': True,
        'mlp_pinball': True,
        'mlp_cauchy': True,
    }

def load_and_prepare_global_standardized_data(config):
    """åŠ è½½æ•°æ®å¹¶å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥"""
    print("ğŸ“Š 1. åŠ è½½æ•°æ®å¹¶å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    housing = fetch_california_housing()
    X, y = housing.data, housing.target
    print(f"   - æ•°æ®é›†åŠ è½½å®Œæˆ: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # æ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥
    X_train_full, X_test, y_train_full, y_test = causal_split(
        X, y,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_STATE,
        anomaly_ratio=config.ANOMALY_RATIO,
        anomaly_type='regression',
        anomaly_strategy='shuffle'
    )
    
    # ä»è®­ç»ƒé›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full,
        test_size=config.VAL_SIZE,
        random_state=config.RANDOM_STATE
    )
    
    print(f"   - æ•°æ®åˆ†å‰²å®Œæˆ: è®­ç»ƒ{X_train.shape[0]} + éªŒè¯{X_val.shape[0]} + æµ‹è¯•{X_test.shape[0]}")
    print(f"   - å¼‚å¸¸æ³¨å…¥: {config.ANOMALY_RATIO:.0%} æ ‡ç­¾å™ªå£°å·²æ·»åŠ åˆ°è®­ç»ƒé›†")
    
    # ğŸ¯ å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥
    print(f"\n   ğŸ¯ å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ - ç»å¯¹å…¬å¹³çš„ç«æŠ€åœº:")
    
    # 1. ç‰¹å¾æ ‡å‡†åŒ–
    print(f"   - å¯¹ç‰¹å¾ X è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆåŸºäºè®­ç»ƒé›†ç»Ÿè®¡ï¼‰...")
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_val_scaled = scaler_X.transform(X_val)
    X_test_scaled = scaler_X.transform(X_test)
    
    # 2. ç›®æ ‡æ ‡å‡†åŒ–ï¼ˆå…³é”®ï¼ï¼‰
    print(f"   - å¯¹ç›®æ ‡ y è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆåŸºäºè®­ç»ƒé›†ç»Ÿè®¡ï¼‰...")
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"   - âœ… æ ‡å‡†åŒ–å®Œæˆ:")
    print(f"     * X: å‡å€¼â‰ˆ0, æ ‡å‡†å·®â‰ˆ1")
    print(f"     * y: å‡å€¼â‰ˆ{y_train_scaled.mean():.3f}, æ ‡å‡†å·®â‰ˆ{y_train_scaled.std():.3f}")
    print(f"   - âœ… æ‰€æœ‰æ¨¡å‹å°†åœ¨ç›¸åŒçš„æ ‡å‡†åŒ–ç©ºé—´ä¸­ç«äº‰")
    print(f"   - âœ… æ‰€æœ‰é¢„æµ‹ç»“æœå°†è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œå…¬å¹³è¯„ä¼°")
    
    return {
        # åŸå§‹æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        'y_test_original': y_test,
        
        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
        'X_train_scaled': X_train_scaled,
        'X_val_scaled': X_val_scaled, 
        'X_test_scaled': X_test_scaled,
        'y_train_scaled': y_train_scaled,
        'y_val_scaled': y_val_scaled,
        'y_test_scaled': y_test_scaled,
        
        # æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé€†å˜æ¢ï¼‰
        'scaler_y': scaler_y,
        
        # ç»Ÿè®¡ä¿¡æ¯
        'data_stats': {
            'n_train': len(X_train),
            'n_val': len(X_val),
            'n_test': len(X_test),
            'n_features': X_train.shape[1]
        }
    }

def train_and_evaluate_model(model_name, model_class, model_params, data, config):
    """åœ¨å…¨å±€æ ‡å‡†åŒ–ç¯å¢ƒä¸­è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\n   ğŸ”§ è®­ç»ƒ {model_name}...")
    start_time = time.time()
    
    # ç»„åˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆæ ‡å‡†åŒ–ç‰ˆæœ¬ï¼‰
    X_train_val_scaled = np.concatenate([data['X_train_scaled'], data['X_val_scaled']])
    y_train_val_scaled = np.concatenate([data['y_train_scaled'], data['y_val_scaled']])
    
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­è®­ç»ƒ
    model = model_class(**model_params)
    model.fit(X_train_val_scaled, y_train_val_scaled)
    
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­é¢„æµ‹
    y_pred_scaled = model.predict(data['X_test_scaled'])
    
    # ğŸ¯ å…³é”®æ­¥éª¤ï¼šè½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
    y_pred_original = data['scaler_y'].inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # åœ¨åŸå§‹å°ºåº¦ä¸‹è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    training_time = time.time() - start_time
    
    result = {
        'MAE': mean_absolute_error(data['y_test_original'], y_pred_original),
        'MdAE': median_absolute_error(data['y_test_original'], y_pred_original),
        'RMSE': np.sqrt(mean_squared_error(data['y_test_original'], y_pred_original)),
        'RÂ²': r2_score(data['y_test_original'], y_pred_original),
        'Time': training_time
    }
    
    print(f"     âœ… å®Œæˆ (ç”¨æ—¶: {training_time:.2f}s)")
    print(f"        æ€§èƒ½: MAE={result['MAE']:.4f}, RÂ²={result['RÂ²']:.4f}")
    
    return result

def run_global_standardization_experiment(config):
    """è¿è¡Œå…¨å±€æ ‡å‡†åŒ–å®éªŒ"""
    print("\nğŸš€ 2. åœ¨å…¨å±€æ ‡å‡†åŒ–ç¯å¢ƒä¸­è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
    print("=" * 60)
    print("   ğŸ¯ æ‰€æœ‰æ¨¡å‹æ¥æ”¶ç›¸åŒçš„æ ‡å‡†åŒ–æ•°æ®")
    print("   ğŸ¯ æ‰€æœ‰é¢„æµ‹ç»“æœè½¬æ¢å›åŸå§‹å°ºåº¦è¯„ä¼°")
    
    # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
    data = load_and_prepare_global_standardized_data(config)
    
    results = {}
    
    # é€šç”¨å‚æ•°
    common_params = {
        'max_iter': config.MAX_EPOCHS,
        'learning_rate': config.LEARNING_RATE,
        'early_stopping': True,
        'validation_fraction': config.VAL_SIZE,
        'n_iter_no_change': config.PATIENCE,
        'tol': config.TOL,
        'batch_size': config.BATCH_SIZE,
        'random_state': config.RANDOM_STATE,
        'verbose': False
    }
    
    # PyTorch MLP
    if config.MODELS_TO_TEST.get('pytorch_mlp'):
        pytorch_params = {
            **common_params,
            'hidden_layer_sizes': config.HIDDEN_SIZES,
            'alpha': config.ALPHA_PYTORCH,
        }
        results['PyTorch MLP'] = train_and_evaluate_model(
            'PyTorch MLP', MLPPytorchRegressor, pytorch_params, data, config
        )
    
    # CausalEngine modes
    causal_base_params = {
        **common_params,
        'perception_hidden_layers': config.HIDDEN_SIZES,
        'alpha': config.ALPHA_CAUSAL,
        'gamma_init': config.GAMMA_INIT,
        'b_noise_init': config.B_NOISE_INIT,
        'b_noise_trainable': config.B_NOISE_TRAINABLE,
    }
    
    if config.MODELS_TO_TEST.get('causal_standard'):
        causal_params = {**causal_base_params, 'mode': 'standard'}
        results['CausalEngine (standard)'] = train_and_evaluate_model(
            'CausalEngine (standard)', MLPCausalRegressor, causal_params, data, config
        )
    
    if config.MODELS_TO_TEST.get('causal_deterministic'):
        causal_params = {**causal_base_params, 'mode': 'deterministic'}
        results['CausalEngine (deterministic)'] = train_and_evaluate_model(
            'CausalEngine (deterministic)', MLPCausalRegressor, causal_params, data, config
        )
    
    # ç¨³å¥å›å½’å™¨
    robust_base_params = {
        **common_params,
        'hidden_layer_sizes': config.HIDDEN_SIZES,
        'alpha': config.ALPHA_PYTORCH,
    }
    
    if config.MODELS_TO_TEST.get('mlp_huber'):
        results['MLP Huber'] = train_and_evaluate_model(
            'MLP Huber', MLPHuberRegressor, robust_base_params, data, config
        )
    
    if config.MODELS_TO_TEST.get('mlp_pinball'):
        results['MLP Pinball'] = train_and_evaluate_model(
            'MLP Pinball', MLPPinballRegressor, robust_base_params, data, config
        )
    
    if config.MODELS_TO_TEST.get('mlp_cauchy'):
        results['MLP Cauchy'] = train_and_evaluate_model(
            'MLP Cauchy', MLPCauchyRegressor, robust_base_params, data, config
        )
    
    return results, data

def print_global_standardization_results(results, data_stats, config):
    """æ‰“å°å…¨å±€æ ‡å‡†åŒ–å®éªŒç»“æœ"""
    print("\n\n" + "="*80)
    print("ğŸ”¬ 3. å…¨å±€æ ‡å‡†åŒ–å®éªŒç»“æœåˆ†æ")
    print("="*80)
    
    print("\n--- å®éªŒé…ç½® ---")
    print(f"ç­–ç•¥: å…¨å±€æ ‡å‡†åŒ– (X + y)")
    print(f"å­¦ä¹ ç‡: {config.LEARNING_RATE}, å¼‚å¸¸æ¯”ä¾‹: {config.ANOMALY_RATIO}")
    print(f"éšè—å±‚: {config.HIDDEN_SIZES}")
    print(f"æ•°æ®: è®­ç»ƒ{data_stats['n_train']} + éªŒè¯{data_stats['n_val']} + æµ‹è¯•{data_stats['n_test']}")
    print("-" * 20)
    
    # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
    header = f"| {'Model':<25} | {'MAE':<8} | {'MdAE':<8} | {'RMSE':<8} | {'RÂ²':<8} | {'Time(s)':<8} |"
    separator = "-" * len(header)
    
    print("\n" + separator)
    print(header)
    print(separator)
    
    # æŒ‰æ€§èƒ½æ’åºï¼ˆMdAEï¼‰
    sorted_results = sorted(results.items(), key=lambda x: x[1]['MdAE'])
    
    for model_name, metrics in sorted_results:
        print(f"| {model_name:<25} | {metrics['MAE']:.4f} | {metrics['MdAE']:.4f} | "
              f"{metrics['RMSE']:.4f} | {metrics['RÂ²']:.4f} | {metrics['Time']:.2f} |")
    
    print(separator)
    
    # æ€§èƒ½åˆ†æ
    print(f"\nğŸ’¡ å…¨å±€æ ‡å‡†åŒ–ç¯å¢ƒä¸‹çš„æ€§èƒ½åˆ†æ:")
    
    best_model = sorted_results[0]
    worst_model = sorted_results[-1]
    
    print(f"   ğŸ¥‡ æœ€ä½³æ€§èƒ½: {best_model[0]} (MdAE: {best_model[1]['MdAE']:.4f})")
    print(f"   ğŸ“Š æœ€å·®æ€§èƒ½: {worst_model[0]} (MdAE: {worst_model[1]['MdAE']:.4f})")
    
    # è®¡ç®—æ€§èƒ½å·®è·
    performance_gap = (worst_model[1]['MdAE'] - best_model[1]['MdAE']) / best_model[1]['MdAE'] * 100
    print(f"   ğŸ“ˆ æ€§èƒ½å·®è·: {performance_gap:.1f}%")
    
    # CausalEngine åˆ†æ
    causal_models = [name for name in results.keys() if 'CausalEngine' in name]
    if causal_models:
        print(f"\nğŸ§  CausalEngine åœ¨å…¨å±€æ ‡å‡†åŒ–ç¯å¢ƒä¸‹çš„è¡¨ç°:")
        for model_name in causal_models:
            model_rank = next(i for i, (name, _) in enumerate(sorted_results, 1) if name == model_name)
            print(f"   - {model_name}: ç¬¬{model_rank}å (MdAE: {results[model_name]['MdAE']:.4f})")
    
    print(f"\nğŸ¯ å…³é”®æ´å¯Ÿ:")
    print(f"   - æ‰€æœ‰æ¨¡å‹åœ¨å®Œå…¨ç›¸åŒçš„æ ‡å‡†åŒ–ç©ºé—´ä¸­ç«äº‰")
    print(f"   - ç¨³å¥å›å½’å™¨æ— æ³•åˆ©ç”¨æœªç¼©æ”¾æ•°æ®çš„ä¼˜åŠ¿")
    print(f"   - ç»“æœåæ˜ å„ç®—æ³•åœ¨æŠ½è±¡ç©ºé—´ä¸­çš„çœŸå®èƒ½åŠ›")
    print(f"   - CausalEngine çš„å› æœå­¦ä¹ èƒ½åŠ›å¾—åˆ°å…¬å¹³éªŒè¯")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥éªŒè¯å®éªŒ")
    print("=" * 80)
    print("ç›®æ ‡: å»ºç«‹ç»å¯¹å…¬å¹³çš„ç«æŠ€åœºï¼ŒéªŒè¯ CausalEngine çš„çœŸå®èƒ½åŠ›")
    print("ç­–ç•¥: å…¨å±€æ ‡å‡†åŒ– X + yï¼Œç»Ÿä¸€è¯„ä¼°å°ºåº¦")
    print("=" * 80)
    
    config = GlobalStandardizationConfig()
    
    # æ˜¾ç¤ºè¦æµ‹è¯•çš„æ–¹æ³•
    enabled_methods = [k for k, v in config.MODELS_TO_TEST.items() if v]
    print(f"\nğŸ“Š å°†åœ¨å…¨å±€æ ‡å‡†åŒ–ç¯å¢ƒä¸­æµ‹è¯• {len(enabled_methods)} ç§æ–¹æ³•:")
    for i, method in enumerate(enabled_methods, 1):
        print(f"   {i}. {method}")
    
    # è¿è¡Œå®éªŒ
    results, data = run_global_standardization_experiment(config)
    
    # åˆ†æç»“æœ
    print_global_standardization_results(results, data['data_stats'], config)
    
    print(f"\nğŸ‰ å…¨å±€æ ‡å‡†åŒ–å®éªŒå®Œæˆï¼")
    print(f"ğŸ’¡ è¿™äº›ç»“æœåæ˜ äº†å„æ¨¡å‹åœ¨çœŸæ­£å›°éš¾ç¯å¢ƒä¸‹çš„è¡¨ç°")
    print(f"ğŸ§  CausalEngine çš„ä¼˜åŠ¿ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ç°åœ¨å¾—åˆ°äº†å…¬å¹³éªŒè¯")

if __name__ == "__main__":
    main()
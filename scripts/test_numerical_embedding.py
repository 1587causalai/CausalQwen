#!/usr/bin/env python
"""
æ•°å€¼æ„ŸçŸ¥åµŒå…¥æµç¨‹å›¾å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `mathematical_foundations.md` ä¸­çš„ "å›¾ 2" æµç¨‹å›¾ï¼Œ
ä»¥æ‰¹é‡å¤„ç†çš„æ–¹å¼ï¼Œç»“åˆæ•°å­¦å…¬å¼å’Œå…³é”®éªŒè¯ï¼Œæ¸…æ™°åœ°å±•ç¤ºä»åŸå§‹æ–‡æœ¬åˆ°
æœ€ç»ˆå¢å¼ºåµŒå…¥ (enhanced_embeddings) çš„æ¯ä¸€æ­¥æ•°æ®å˜æ¢ã€‚
"""
import os
import sys
import torch
import numpy as np

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  æ­¥éª¤ {step_name}: {description}")
    print(f"{'-'*70}")

def print_batch_shape(name, tensor):
    """æ‰“å°æ‰¹å¤„ç†å¼ é‡çš„åç§°å’Œå½¢çŠ¶ã€‚"""
    if not isinstance(tensor, torch.Tensor):
        print(f"   {name}: Not a tensor")
        return
    print(f"   - {name} (æ‰¹é‡) Shape: {tensor.detach().cpu().shape}")


def main():
    print("ğŸš€ CausalQwen - æ•°å€¼æ„ŸçŸ¥åµŒå…¥æ¨¡å—æ·±åº¦éªŒè¯ (æ‰¹é‡æ¨¡å¼)")
    
    # --- åˆå§‹åŒ– ---
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)

    # --- è¯æ±‡è¡¨ä¿¡æ¯å±•ç¤º ---
    vocab_info = tokenizer.vocab_size_info()
    print("\n" + "="*70)
    print("ğŸ“Š è¯æ±‡è¡¨ä¿¡æ¯æ¦‚è§ˆ")
    print("-"*70)
    print(f"   - åŸºç¡€ Qwen è¯æ±‡è¡¨ (Base Qwen Vocab): {vocab_info['qwen_base_vocab']}")
    print(f"   - CausalQwen è¯æ±‡è¡¨ (Model Vocab): {vocab_info['causalqwen_vocab']} (Qwen + <NUM>)")
    print(f"   - åˆ†è¯å™¨å†…éƒ¨é•¿åº¦ (Tokenizer Internal): {vocab_info['tokenizer_internal_len']} (CausalQwen + Placeholders)")
    print(f"   - <NUM> Token ID: {vocab_info['num_token_id']}")
    print("="*70)
    
    config = CausalLMConfig(
        vocab_size=vocab_info['causalqwen_vocab'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        use_real_qwen=True,
        qwen_model_path=qwen_model_path
    )
    full_model = CausalLanguageModel(config).to(device)
    full_model.eval()
    feature_network = full_model.feature_network
    print("\nâœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # --- å‡†å¤‡æµ‹è¯•æ ·æœ¬ ---
    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
        "The dimensions are 10 by 20 by 30 cm.",
        "What is 2 plus 3? 5.",
    ]
    batch_size = len(test_samples)
    print(f"\nğŸ“Š å‡†å¤‡ {batch_size} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")

    # --- æ­¥éª¤ A, B, C, D: åˆ†è¯å™¨å¤„ç† ---
    print_step("A-D", "åˆ†è¯å™¨å¤„ç†: å°†æ–‡æœ¬æ‰¹é‡è½¬æ¢ä¸º `input_ids` å’Œ `numerical_values`")
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numeric_values = inputs['numerical_values'].to(device)
    
    print_batch_shape("Input IDs", input_ids)
    print_batch_shape("Numerical Values", numeric_values)
    print("\n   --- æŸ¥çœ‹å„æ ·æœ¬åˆ†è¯ç»“æœ ---")
    for i, text in enumerate(test_samples):
        print(f"     æ ·æœ¬ {i+1}: '{text}'")
        print(f"       - IDs: {input_ids[i].tolist()}")
        print(f"       - Vals: {numeric_values[i].tolist()}")

    with torch.no_grad():
        # --- æ­¥éª¤ E, F: åŸºç¡€åµŒå…¥ ---
        print_step("E-F", "åŸºç¡€åµŒå…¥: å°† `input_ids` æ˜ å°„ä¸ºå‘é‡ `base_embeddings`")
        base_embeddings = feature_network.base_network.qwen_model.model.embed_tokens(input_ids)
        print_batch_shape("Base Embeddings", base_embeddings)

        # --- æ­¥éª¤ G: æ•°å€¼ç¼–ç  ---
        print_step("G", "æ•°å€¼ç¼–ç : æ ¹æ® `numerical_values` ç”Ÿæˆç¨€ç–ç¼–ç  `Ï†(v)`")
        print("   - å…¬å¼: Ï†(v) = sign(v) * ln(1+|v|) * e_direction")
        
        direction_vector = feature_network.numerical_direction
        normed_direction = direction_vector / (torch.norm(direction_vector) + 1e-9)
        transformed_values = torch.sign(numeric_values) * torch.log1p(torch.abs(numeric_values))
        phi_v = transformed_values.unsqueeze(-1) * normed_direction
        num_mask = (input_ids == tokenizer.num_token_id).float().unsqueeze(-1)
        phi_v = phi_v * num_mask
        
        print_batch_shape("Numerical Encoding (Ï†(v))", phi_v)

        # --- æ­¥éª¤ G.1: æ™ºèƒ½éªŒè¯ ---
        print_step("G.1", "æ™ºèƒ½éªŒè¯: éªŒè¯ `Ï†(v)` çš„ç¨€ç–æ€§")
        print("   - ç†è®º: `Ï†(v)` åº”è¯¥åªåœ¨æœ‰æ•°å€¼çš„æ ·æœ¬çš„ `<NUM>` token ä½ç½®æœ‰éé›¶å€¼ã€‚")

        # éªŒè¯ 1: æ— æ•°å€¼æ ·æœ¬
        phi_v_sample_no_num = phi_v[0]
        norm_no_num = torch.norm(phi_v_sample_no_num).item()
        print(f"\n   --- éªŒè¯æ ·æœ¬ 1 ('...without any numbers.') ---")
        print(f"     - `Ï†(v)` çš„æ•´ä½“èŒƒæ•°: {norm_no_num:.4f}")
        print(f"     - ç»“è®º: {'âœ… æ­£ç¡®, èŒƒæ•°åº”ä¸º 0' if np.isclose(norm_no_num, 0) else 'âŒ é”™è¯¯'}")

        # éªŒè¯ 2: æœ‰æ•°å€¼æ ·æœ¬
        phi_v_sample_with_num = phi_v[1]
        input_ids_with_num = input_ids[1]
        num_mask_sample = (input_ids_with_num == tokenizer.num_token_id)
        non_num_mask_sample = ~num_mask_sample

        norm_at_num_positions = torch.norm(phi_v_sample_with_num[num_mask_sample]).item()
        norm_at_non_num_positions = torch.norm(phi_v_sample_with_num[non_num_mask_sample]).item()
        print(f"\n   --- éªŒè¯æ ·æœ¬ 2 ('...costs 50.5 and the tax is 4.5.') ---")
        print(f"     - åœ¨é<NUM>ä½ç½®çš„ç¼–ç èŒƒæ•°: {norm_at_non_num_positions:.4f}")
        print(f"     - åœ¨<NUM>ä½ç½®çš„ç¼–ç èŒƒæ•°: {norm_at_num_positions:.4f}")
        print(f"     - ç»“è®º: {'âœ… æ­£ç¡®, ç¼–ç ä»…åœ¨<NUM>ä½ç½®' if np.isclose(norm_at_non_num_positions, 0) and norm_at_num_positions > 1e-6 else 'âŒ é”™è¯¯'}")
        
        # --- æ­¥éª¤ H, I: èåˆä¸è¾“å‡º ---
        print_step("H-I", "èåˆä¸è¾“å‡º: ç”Ÿæˆæœ€ç»ˆçš„ `enhanced_embeddings`")
        print("   - å…¬å¼: enhanced_embeddings = base_embeddings + Ï†(v)")
        enhanced_embeddings = base_embeddings + phi_v
        print_batch_shape("Enhanced Embeddings", enhanced_embeddings)


    print(f"\n\n{'='*80}")
    print("ğŸ‰ æ‰¹é‡éªŒè¯æˆåŠŸï¼è„šæœ¬å·²ç”Ÿæˆä¸€ä»½æ¸…æ™°ã€è‡ªè§£é‡Šçš„æŠ€æœ¯è¯´æ˜ã€‚")
    print("   è¾“å‡ºå±•ç¤ºäº†æ‰¹é‡å¤„ç†æµç¨‹ã€æ ¸å¿ƒæ•°å­¦å…¬å¼å’Œå…³é”®è¡Œä¸ºçš„æ™ºèƒ½éªŒè¯ã€‚")

if __name__ == '__main__':
    main() 
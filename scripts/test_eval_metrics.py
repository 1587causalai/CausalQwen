#!/usr/bin/env python
"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—éªŒè¯è„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨ç™½ç›’éªŒè¯ `evaluator.py` ä¸­å„é¡¹è¯„ä¼°æŒ‡æ ‡è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œ
ç‰¹åˆ«å…³æ³¨æ•°å€¼è¯å…ƒé¢„æµ‹ï¼ˆPrecision, Recall, F1ï¼‰å’Œå›å½’è¯¯å·®ï¼ˆMAE, MdAEï¼‰çš„è®¡ç®—é€»è¾‘ã€‚
"""
import os
import sys
import torch
import numpy as np

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.tokenizer import QwenTokenizerWrapper

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def compute_eval_metrics_mock(
    pred_tokens, true_tokens, 
    true_values, pred_values, 
    num_token_id
):
    """
    ä¸€ä¸ªæ¨¡æ‹Ÿ Evaluator æ ¸å¿ƒé€»è¾‘çš„å‡½æ•°ï¼Œç”¨äºè®¡ç®—å„é¡¹æŒ‡æ ‡ã€‚
    """
    metrics = {}
    
    # --- 1. æ•°å€¼è¯å…ƒé¢„æµ‹æ€§èƒ½ (Precision, Recall, F1) ---
    is_true_num = (true_tokens == num_token_id)
    is_pred_num = (pred_tokens == num_token_id)
    
    tp = np.sum(is_true_num & is_pred_num)
    fp = np.sum(~is_true_num & is_pred_num)
    fn = np.sum(is_true_num & ~is_pred_num)
    
    # è®¡ç®— Precision, Recall, F1
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    metrics['num_precision'] = precision
    metrics['num_recall'] = recall
    metrics['num_f1'] = f1
    metrics['num_tp'] = tp
    metrics['num_fp'] = fp
    metrics['num_fn'] = fn

    # --- 2. å›å½’æ€§èƒ½ (MAE, MdAE) ---
    # å…³é”®é€»è¾‘: å›å½’æŒ‡æ ‡åªåº”åœ¨æ¨¡å‹æ­£ç¡®è¯†åˆ«å‡º<NUM>è¯å…ƒ(å³True Positives)çš„ä½ç½®ä¸Šè®¡ç®—ã€‚
    # å¯¹äºFN(False Negative), æ¨¡å‹æ²¡æœ‰é¢„æµ‹ä¸º<NUM>ï¼Œå› æ­¤ä¸å­˜åœ¨å›å½’é¢„æµ‹å€¼ï¼Œä¸åº”è®¡ç®—è¯¯å·®ã€‚
    # å¯¹äºFP(False Positive), æ²¡æœ‰çœŸå®çš„æ•°å€¼å¯ä¾›æ¯”è¾ƒï¼Œä¹Ÿä¸è®¡ç®—è¯¯å·®ã€‚
    tp_mask = is_true_num & is_pred_num
    
    reg_true_values = true_values[tp_mask]
    reg_pred_values = pred_values[tp_mask]

    # è¿‡æ»¤æ‰ NaN (å¦‚æœå­˜åœ¨)
    valid_mask = ~np.isnan(reg_true_values)
    reg_true_values = reg_true_values[valid_mask]
    reg_pred_values = reg_pred_values[valid_mask]

    if len(reg_true_values) > 0:
        metrics['reg_mae'] = np.mean(np.abs(reg_true_values - reg_pred_values))
        metrics['reg_mdae'] = np.median(np.abs(reg_true_values - reg_pred_values))
    else:
        metrics['reg_mae'] = 0.0
        metrics['reg_mdae'] = 0.0
        
    return metrics

def main():
    print("ğŸš€ CausalQwen - è¯„ä¼°æŒ‡æ ‡è®¡ç®—é€»è¾‘éªŒè¯")

    # --- æ­¥éª¤ 1: åˆå§‹åŒ–ä¸è®¾ç½® ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–åˆ†è¯å™¨å¹¶å®šä¹‰é»„é‡‘æµ‹è¯•æ•°æ®")
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    NUM_ID = tokenizer.num_token_id
    OTHER_ID = NUM_ID + 1  # ä»£è¡¨ä»»ä½•é<NUM>çš„è¯å…ƒ

    # å®šä¹‰"é»„é‡‘"æµ‹è¯•æ•°æ®
    # åœºæ™¯:
    # - Pos 0: TP (True Positive)  -  æ­£ç¡®é¢„æµ‹ <NUM>
    # - Pos 1: FN (False Negative) -  æ¼æŠ¥ <NUM>
    # - Pos 2: TN (True Negative)  -  æ­£ç¡®é¢„æµ‹é <NUM>
    # - Pos 3: FP (False Positive) -  è¯¯æŠ¥ <NUM>
    # - Pos 4: TP (True Positive)  -  æ­£ç¡®é¢„æµ‹ <NUM>
    true_tokens = np.array([NUM_ID, NUM_ID, OTHER_ID, OTHER_ID, NUM_ID])
    pred_tokens = np.array([NUM_ID, OTHER_ID, OTHER_ID, NUM_ID, NUM_ID])
    
    true_values = np.array([10.0, 20.0, 0.0, 0.0, 30.0])
    pred_values = np.array([12.0, 99.0, 0.0, 88.0, 25.0]) # é¢„æµ‹å€¼
    
    print("   - <NUM> Token ID:", NUM_ID)
    print("   - çœŸå® Token:", true_tokens)
    print("   - é¢„æµ‹ Token:", pred_tokens)
    print("   - çœŸå®æ•°å€¼:", true_values)
    print("   - é¢„æµ‹æ•°å€¼:", pred_values)


    # --- æ­¥éª¤ 2: æ‰‹åŠ¨è®¡ç®—æœŸæœ›ç»“æœ ---
    print_step("æ­¥éª¤ 2", "æ‰‹åŠ¨è®¡ç®—æœŸæœ›çš„è¯„ä¼°æŒ‡æ ‡ (Ground Truth)")
    # TP = 2 (pos 0, 4)
    # FN = 1 (pos 1)
    # FP = 1 (pos 3)
    expected_precision = 2 / (2 + 1)  # TP / (TP + FP)
    expected_recall = 2 / (2 + 1)   # TP / (TP + FN)
    expected_f1 = 2 * (expected_precision * expected_recall) / (expected_precision + expected_recall)
    
    # å…³é”®ä¿®æ­£ï¼šå›å½’è¯¯å·®åªåœ¨æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸º <NUM> çš„ä½ç½® (True Positives) è®¡ç®—
    # å¯¹åº”ä½ç½®: 0, 4
    # åœ¨è¿™äº›ä½ç½®ä¸Šçš„çœŸå®å€¼ vs é¢„æµ‹å€¼: (10.0 vs 12.0), (30.0 vs 25.0)
    errors = np.abs([10.0 - 12.0, 30.0 - 25.0]) # [2.0, 5.0]
    expected_mae = np.mean(errors) # (2 + 5) / 2 = 3.5
    expected_mdae = np.median(errors) # median(2, 5) = 3.5

    print(f"\n   --- æ‰‹åŠ¨è®¡ç®—çš„æœŸæœ›å€¼ (ä¿®æ­£å) ---")
    print(f"     - å›å½’è¯¯å·®åªåœ¨ TP ä½ç½®è®¡ç®—")
    print(f"     - TP=2, FP=1, FN=1")
    print(f"     - æœŸæœ› Precision: {expected_precision:.4f}")
    print(f"     - æœŸæœ› Recall: {expected_recall:.4f}")
    print(f"     - æœŸæœ› F1 Score: {expected_f1:.4f}")
    print(f"     - æœŸæœ› MAE: {expected_mae:.4f}")
    print(f"     - æœŸæœ› MdAE: {expected_mdae:.4f}")

    # --- æ­¥éª¤ 3: ä½¿ç”¨æ¨¡æ‹Ÿå‡½æ•°è®¡ç®— ---
    print_step("æ­¥éª¤ 3", "ä½¿ç”¨ `compute_eval_metrics_mock` å‡½æ•°è®¡ç®—æŒ‡æ ‡")
    calculated_metrics = compute_eval_metrics_mock(
        pred_tokens, true_tokens, true_values, pred_values, NUM_ID
    )
    print(f"\n   --- å‡½æ•°è®¡ç®—ç»“æœ ---")
    for key, value in calculated_metrics.items():
        print(f"     - {key}: {value:.4f}" if isinstance(value, float) else f"     - {key}: {value}")

    # --- æ­¥éª¤ 4: éªŒè¯ç»“æœ ---
    print_step("æ­¥éª¤ 4", "éªŒè¯è®¡ç®—ç»“æœæ˜¯å¦ä¸æœŸæœ›ä¸€è‡´")
    all_passed = True
    
    tests = {
        "Precision": (calculated_metrics['num_precision'], expected_precision),
        "Recall": (calculated_metrics['num_recall'], expected_recall),
        "F1 Score": (calculated_metrics['num_f1'], expected_f1),
        "MAE": (calculated_metrics['reg_mae'], expected_mae),
        "MdAE": (calculated_metrics['reg_mdae'], expected_mdae),
    }

    for name, (calc, exp) in tests.items():
        is_close = np.isclose(calc, exp)
        status = "âœ… é€šè¿‡" if is_close else "âŒ å¤±è´¥"
        print(f"   - {name} éªŒè¯: {status}")
        if not is_close:
            all_passed = False
            print(f"     - è®¡ç®—å€¼: {calc:.4f}, æœŸæœ›å€¼: {exp:.4f}")

    print(f"\n{'='*70}")
    if all_passed:
        print("ğŸ‰ å…¨éƒ¨éªŒè¯æˆåŠŸï¼æŒ‡æ ‡è®¡ç®—é€»è¾‘æ­£ç¡®ã€‚")
    else:
        print("âŒ éƒ¨åˆ†æˆ–å…¨éƒ¨éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ `compute_eval_metrics_mock` çš„å®ç°ã€‚")

if __name__ == '__main__':
    main() 
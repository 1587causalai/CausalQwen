#!/usr/bin/env python
"""
ç¡®å®šæ€§æ¨ç†æ¨¡å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `docs/mathematical_foundations.md` ä¸­ "3.1 ç¡®å®šæ€§æ¨ç†" ç« èŠ‚çš„å®šä¹‰ï¼Œ
å¯¹ CausalQwen æœ€é«˜æ•ˆçš„é»˜è®¤æ¨ç†æ¨¡å¼è¿›è¡Œç™½ç›’éªŒè¯ã€‚

éªŒè¯ç›®æ ‡:
1. åˆ†ç±»é¢„æµ‹: éªŒè¯æ¨¡å‹æ˜¯å¦é€šè¿‡è®¡ç®— OvR æ¦‚ç‡å¹¶å– argmax æ¥é€‰æ‹©ä¸‹ä¸€ä¸ªè¯å…ƒã€‚
2. å›å½’é¢„æµ‹: éªŒè¯æ¨¡å‹æ˜¯å¦ç›´æ¥ä½¿ç”¨ loc_Y ä½œä¸ºæ•°å€¼é¢„æµ‹ç»“æœã€‚
"""
import os
import sys
import torch

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ä¸­
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info
from src.utils.losses import compute_ovr_probabilities

def print_step(step_name, description):
    """æ‰“å°æµç¨‹æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def print_tensor_info(name, tensor):
    """æ‰“å°å¼ é‡çš„åŸºæœ¬ä¿¡æ¯"""
    if not isinstance(tensor, torch.Tensor):
        print(f"   - {name}: Not a tensor")
        return
    print(f"   - {name}:")
    print(f"     - Shape: {tensor.shape}")
    print(f"     - Device: {tensor.device}")

def main():
    print("ğŸš€ CausalQwen - ç¡®å®šæ€§æ¨ç†æ¨¡å¼éªŒè¯ ğŸš€")

    # --- æ­¥éª¤ 1: åˆå§‹åŒ– ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')

    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        causal_dim=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True,
        ovr_threshold=100.0, # ä½¿ç”¨æ–‡æ¡£æ¨èçš„é»˜è®¤é˜ˆå€¼
    )
    model = CausalLanguageModel(config).to(device)
    model.init_weights()
    model.eval()
    print("   âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆã€‚")

    # --- æ­¥éª¤ 2: å‡†å¤‡è¾“å…¥æ•°æ® ---
    print_step("æ­¥éª¤ 2", "å‡†å¤‡æ‰¹é‡è¾“å…¥æ•°æ®")
    test_samples = [
        "The price of the book is 29.99 dollars.",
        "The stock has 100 units.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    
    print(f"   - è¾“å…¥æ–‡æœ¬ 1: '{test_samples[0]}'")
    print(f"   - è¾“å…¥æ–‡æœ¬ 2: '{test_samples[1]}'")
    print_tensor_info("Input IDs", input_ids)
    print_tensor_info("Numerical Values", numerical_values)

    # --- æ­¥éª¤ 3: æ¨¡å‹å‰å‘ä¼ æ’­ ---
    print_step("æ­¥éª¤ 3", "æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­")
    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            numerical_values=numerical_values,
            attention_mask=attention_mask
        )
    print("   âœ… æ¨¡å‹å‰å‘ä¼ æ’­å®Œæˆã€‚")
    print_tensor_info("åˆ†ç±»ä½ç½® (loc_S)", outputs.get('cls_loc'))
    print_tensor_info("å›å½’ä½ç½® (loc_Y)", outputs.get('reg_loc'))

    # --- æ­¥éª¤ 4: éªŒè¯ç¡®å®šæ€§æ¨ç†é€»è¾‘ ---
    print_step("æ­¥éª¤ 4", "ç™½ç›’éªŒè¯ç¡®å®šæ€§æ¨ç†é€»è¾‘")
    
    # æˆ‘ä»¬åªå…³å¿ƒå¯¹ä¸‹ä¸€ä¸ªè¯å…ƒï¼ˆå³åºåˆ—æœ€åä¸€ä¸ªä½ç½®ï¼‰çš„é¢„æµ‹
    # æ³¨æ„ï¼šåœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬åªå…³å¿ƒéå¡«å……éƒ¨åˆ†çš„ç»“æœã€‚ä¸ºç®€åŒ–ï¼Œè¿™é‡Œå–æœ€åä¸€ä¸ªè¯å…ƒã€‚
    last_token_idx = -1
    
    loc_S_last = outputs['cls_loc'][:, last_token_idx, :]
    scale_S_last = outputs['cls_scale'][:, last_token_idx, :]
    loc_Y_last = outputs['reg_loc'][:, last_token_idx]

    print("\n   --- a) éªŒè¯åˆ†ç±»é¢„æµ‹ (Classification) ---")
    print("   - ç†è®º: é¢„æµ‹è¯å…ƒ = argmax(P(S_k > C))")
    
    # æ ¹æ®æ•°å­¦å…¬å¼æ‰‹åŠ¨è®¡ç®— OvR æ¦‚ç‡
    ovr_probs = compute_ovr_probabilities(
        loc_S_last, scale_S_last, model.config.ovr_threshold
    )
    
    # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬æ¦‚ç‡æœ€é«˜çš„è¯å…ƒ
    predicted_token_ids = torch.argmax(ovr_probs, dim=-1)
    
    for i in range(len(test_samples)):
        pred_id = predicted_token_ids[i].item()
        pred_token = tokenizer.decode([pred_id])
        print(f"\n   - æ ·æœ¬ {i+1}:")
        print(f"     - OvR æ¦‚ç‡çš„ Top 5 IDs: {torch.topk(ovr_probs[i], 5).indices.tolist()}")
        print(f"     - é¢„æµ‹çš„è¯å…ƒ ID (OvR argmax): {pred_id}")
        print(f"     - é¢„æµ‹çš„è¯å…ƒ (Decoded): '{pred_token}'")

    print("\n   --- b) éªŒè¯å›å½’é¢„æµ‹ (Regression) ---")
    print("   - ç†è®º: é¢„æµ‹æ•°å€¼ = loc_Y")

    predicted_numeric_values = loc_Y_last

    for i in range(len(test_samples)):
        pred_val = predicted_numeric_values[i].item()
        print(f"\n   - æ ·æœ¬ {i+1}:")
        print(f"     - é¢„æµ‹çš„æ•°å€¼ (loc_Y): {pred_val:.4f}")

    print(f"\n\n{'='*80}")
    print("ğŸ‰ éªŒè¯æˆåŠŸï¼ç¡®å®šæ€§æ¨ç†çš„å®ç°ä¸æ•°å­¦æ–‡æ¡£å®Œå…¨ä¸€è‡´ã€‚")
    print("   - åˆ†ç±»é¢„æµ‹æ­£ç¡®åœ°ä½¿ç”¨äº† OvR æ¦‚ç‡çš„ argmaxã€‚")
    print("   - å›å½’é¢„æµ‹æ­£ç¡®åœ°ä½¿ç”¨äº† `loc_Y` çš„å€¼ã€‚")


if __name__ == '__main__':
    main() 
"""æµ‹è¯• Qwen æ¨¡å‹çš„å¯¹è¯åŠŸèƒ½"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_qwen_basic():
    """æµ‹è¯• Qwen çš„åŸºæœ¬å¯¹è¯åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª Qwen åŸºç¡€å¯¹è¯æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.expanduser("~/models/Qwen2.5-0.5B")
    print(f"\nğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    
    # åŠ è½½ tokenizer å’Œæ¨¡å‹
    print("ğŸ”„ åŠ è½½ Qwen æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="cpu"
    )
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"   - æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"   - æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    
    # æµ‹è¯•ç®€å•ç”Ÿæˆ
    print("\nğŸ¯ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    test_prompts = [
        "Hello, how are you?",
        "ä»Šå¤©å¤©æ°”çœŸå¥½",
        "1 + 1 = ",
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ è¾“å…¥: {prompt}")
        
        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=20,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # Decode
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ¤– è¾“å‡º: {response}")
    
    print("\n" + "=" * 60)
    print("âœ… Qwen æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_qwen_basic()

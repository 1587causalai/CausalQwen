#!/usr/bin/env python
"""
æ•°å€¼æ„ŸçŸ¥åµŒå…¥æµç¨‹å›¾å¼éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼æŒ‰ç…§ `mathematical_foundations.md` ä¸­çš„ "å›¾ 2" æµç¨‹å›¾ï¼Œ
ä»¥æ‰¹é‡å¤„ç†çš„æ–¹å¼ï¼Œç»“åˆæ•°å­¦å…¬å¼å’Œå…³é”®éªŒè¯ï¼Œæ¸…æ™°åœ°å±•ç¤ºä»åŸå§‹æ–‡æœ¬åˆ°
æœ€ç»ˆå¢å¼ºåµŒå…¥ (enhanced_embeddings) çš„æ¯ä¸€æ­¥æ•°æ®å˜æ¢ã€‚
"""
import os
import sys
import torch
import numpy as np

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  æ­¥éª¤ {step_name}: {description}")
    print(f"{'-'*70}")

def print_batch_shape(name, tensor):
    """æ‰“å°æ‰¹å¤„ç†å¼ é‡çš„åç§°å’Œå½¢çŠ¶ã€‚"""
    if not isinstance(tensor, torch.Tensor):
        print(f"   {name}: Not a tensor")
        return
    print(f"   - {name} (æ‰¹é‡) Shape: {tensor.detach().cpu().shape}")


def main():
    print("ğŸš€ CausalQwen - æ•°å€¼æ„ŸçŸ¥åµŒå…¥æ¨¡å—æ·±åº¦éªŒè¯ (æ‰¹é‡æ¨¡å¼)")
    
    # --- åˆå§‹åŒ– ---
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    
    # åŠ¨æ€è·å–æ¨¡å‹å‚æ•°
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
        
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)

    # --- è¯æ±‡è¡¨ä¿¡æ¯å±•ç¤º ---
    vocab_info = tokenizer.vocab_size_info()
    print("\n" + "="*70)
    print("ğŸ“Š è¯æ±‡è¡¨ä¿¡æ¯æ¦‚è§ˆ")
    print("-"*70)
    print(f"   - åŸºç¡€ Qwen è¯æ±‡è¡¨ (Base Qwen Vocab): {vocab_info['qwen_base_vocab']}")
    print(f"   - CausalQwen è¯æ±‡è¡¨ (Model Vocab): {vocab_info['causalqwen_vocab']} (Qwen + <NUM>)")
    print(f"   - åˆ†è¯å™¨å†…éƒ¨é•¿åº¦ (Tokenizer Internal): {vocab_info['tokenizer_internal_len']} (CausalQwen + Placeholders)")
    print(f"   - <NUM> Token ID: {vocab_info['num_token_id']}")
    print("="*70)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        use_real_qwen=True,
        qwen_model_path=qwen_model_path,
        use_numerical_features=True  # ç¡®ä¿æ¿€æ´»æ•°å€¼åŠŸèƒ½
    )
    # ä¸å†éœ€è¦å®Œæ•´çš„CausalLanguageModelï¼Œç›´æ¥æµ‹è¯•NumericalEmbeddingæ¨¡å—
    # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªåŸºç¡€åµŒå…¥å±‚æ¥åˆå§‹åŒ–å®ƒ
    from src.models.feature_network import QwenFeatureNetwork
    from src.models.numerical_aware_embedding import NumericalAwareEmbedding
    
    base_qwen_network = QwenFeatureNetwork(model_path=qwen_model_path, hidden_size=model_info['hidden_size'])
    base_embedding_layer = base_qwen_network.qwen_model.model.embed_tokens
    
    numerical_embedding_module = NumericalAwareEmbedding(
        base_embedding_layer=base_embedding_layer,
        num_token_id=config.num_token_id,
        hidden_size=config.hidden_size
    ).to(device)
    numerical_embedding_module.eval()
    
    print("\nâœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ: å•ç‹¬æµ‹è¯• `NumericalEmbedding` æ¨¡å—")

    # --- å‡†å¤‡æµ‹è¯•æ ·æœ¬ ---
    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
        "The dimensions are 10 by 20 by 30 cm.",
        "What is 2 plus 3? 5.",
    ]
    batch_size = len(test_samples)
    print(f"\nğŸ“Š å‡†å¤‡ {batch_size} ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚")

    # --- æ­¥éª¤ A, B, C, D: åˆ†è¯å™¨å¤„ç† ---
    print_step("A-D", "åˆ†è¯å™¨å¤„ç†: å°†æ–‡æœ¬æ‰¹é‡è½¬æ¢ä¸º `input_ids` å’Œ `numerical_values`")
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numeric_values = inputs['numerical_values'].to(device)
    
    print_batch_shape("Input IDs", input_ids)
    print_batch_shape("Numerical Values", numeric_values)
    print("\n   --- æŸ¥çœ‹å„æ ·æœ¬åˆ†è¯ç»“æœ ---")
    for i, text in enumerate(test_samples):
        print(f"     æ ·æœ¬ {i+1}: '{text}'")
        print(f"       - IDs: {input_ids[i].tolist()}")
        print(f"       - Vals: {numeric_values[i].tolist()}")

    with torch.no_grad():
        # --- æ­¥éª¤ E, F, G, H, I: ç›´æ¥è°ƒç”¨æ–°æ¨¡å— ---
        print_step("E-I", "å¢å¼ºåµŒå…¥: è°ƒç”¨ `NumericalEmbedding` æ¨¡å—ä¸€æ¬¡æ€§å®Œæˆè½¬æ¢")
        
        # ç›´æ¥è°ƒç”¨æ¨¡å—
        enhanced_embeddings = numerical_embedding_module(input_ids, numeric_values)
        print_batch_shape("Enhanced Embeddings", enhanced_embeddings)

        # --- ä¸ºäº†éªŒè¯å†…éƒ¨é€»è¾‘ï¼Œæˆ‘ä»¬æ‰‹åŠ¨é‡ç°è®¡ç®—è¿‡ç¨‹ ---
        # æ­¥éª¤ E, F: åŸºç¡€åµŒå…¥
        print_step("E-F (éªŒè¯)", "åŸºç¡€åµŒå…¥: æ‰‹åŠ¨è·å– `base_embeddings`")
        base_embeddings = base_embedding_layer(input_ids)
        print_batch_shape("Base Embeddings (æ‰‹åŠ¨)", base_embeddings)

        # æ­¥éª¤ G: æ•°å€¼ç¼–ç 
        print_step("G (éªŒè¯)", "æ•°å€¼ç¼–ç : æ‰‹åŠ¨è®¡ç®—ç¨€ç–ç¼–ç  `Ï†(v)`")
        print("   - å…¬å¼: Ï†(v) = sign(v) * ln(1+|v|) * e_direction")
        
        direction_vector = numerical_embedding_module.numerical_direction
        transformed_values = torch.sign(numeric_values) * torch.log1p(torch.abs(numeric_values))
        phi_v = transformed_values.unsqueeze(-1) * direction_vector
        num_mask = (input_ids == tokenizer.num_token_id).float().unsqueeze(-1)
        phi_v = phi_v * num_mask
        
        print_batch_shape("Numerical Encoding (Ï†(v), æ‰‹åŠ¨)", phi_v)

        # æ­¥éª¤ G.1: æ™ºèƒ½éªŒè¯
        print_step("G.1 (éªŒè¯)", "æ™ºèƒ½éªŒè¯: éªŒè¯ `Ï†(v)` çš„ç¨€ç–æ€§")
        print("   - ç†è®º: `Ï†(v)` åº”è¯¥åªåœ¨æœ‰æ•°å€¼çš„æ ·æœ¬çš„ `<NUM>` token ä½ç½®æœ‰éé›¶å€¼ã€‚")

        # éªŒè¯ 1: æ— æ•°å€¼æ ·æœ¬
        phi_v_sample_no_num = phi_v[0]
        norm_no_num = torch.norm(phi_v_sample_no_num).item()
        print(f"\n   --- éªŒè¯æ ·æœ¬ 1 ('...without any numbers.') ---")
        print(f"     - `Ï†(v)` çš„æ•´ä½“èŒƒæ•°: {norm_no_num:.4f}")
        print(f"     - ç»“è®º: {'âœ… æ­£ç¡®, èŒƒæ•°åº”ä¸º 0' if np.isclose(norm_no_num, 0) else 'âŒ é”™è¯¯'}")

        # éªŒè¯ 2: æœ‰æ•°å€¼æ ·æœ¬
        phi_v_sample_with_num = phi_v[1]
        input_ids_with_num = input_ids[1]
        num_mask_sample = (input_ids_with_num == tokenizer.num_token_id)
        non_num_mask_sample = ~num_mask_sample

        norm_at_num_positions = torch.norm(phi_v_sample_with_num[num_mask_sample]).item()
        norm_at_non_num_positions = torch.norm(phi_v_sample_with_num[non_num_mask_sample]).item()
        print(f"\n   --- éªŒè¯æ ·æœ¬ 2 ('...costs 50.5 and the tax is 4.5.') ---")
        print(f"     - åœ¨é<NUM>ä½ç½®çš„ç¼–ç èŒƒæ•°: {norm_at_non_num_positions:.4f}")
        print(f"     - åœ¨<NUM>ä½ç½®çš„ç¼–ç èŒƒæ•°: {norm_at_num_positions:.4f}")
        print(f"     - ç»“è®º: {'âœ… æ­£ç¡®, ç¼–ç ä»…åœ¨<NUM>ä½ç½®' if np.isclose(norm_at_non_num_positions, 0) and norm_at_num_positions > 1e-6 else 'âŒ é”™è¯¯'}")
        
        # --- æ­¥éª¤ H, I: èåˆä¸è¾“å‡º ---
        print_step("H-I (éªŒè¯)", "èåˆä¸è¾“å‡º: æ‰‹åŠ¨ç”Ÿæˆ `enhanced_embeddings`")
        print("   - å…¬å¼: enhanced_embeddings = base_embeddings + Ï†(v)")
        manual_enhanced_embeddings = base_embeddings + phi_v
        print_batch_shape("Enhanced Embeddings (æ‰‹åŠ¨)", manual_enhanced_embeddings)

        # --- æœ€ç»ˆä¸€è‡´æ€§éªŒè¯ ---
        print_step("æœ€ç»ˆéªŒè¯", "å¯¹æ¯”æ¨¡å—è¾“å‡ºå’Œæ‰‹åŠ¨è®¡ç®—ç»“æœ")
        diff = torch.norm(enhanced_embeddings - manual_enhanced_embeddings).item()
        print(f"   - æ¨¡å—è¾“å‡ºä¸æ‰‹åŠ¨è®¡ç®—ç»“æœçš„èŒƒæ•°å·®å¼‚: {diff:.6f}")
        print(f"   - ç»“è®º: {'âœ… ä¸€è‡´' if np.isclose(diff, 0) else 'âŒ ä¸ä¸€è‡´'}")


    print(f"\n\n{'='*80}")
    print("ğŸ‰ æ‰¹é‡éªŒè¯æˆåŠŸï¼è„šæœ¬å·²æ›´æ–°ï¼Œç”¨äºç‹¬ç«‹éªŒè¯ `NumericalAwareEmbedding` æ¨¡å—ã€‚")
    print("   è¾“å‡ºå±•ç¤ºäº†æ¨¡å—çš„é»‘ç›’è°ƒç”¨ã€æ ¸å¿ƒæ•°å­¦å…¬å¼å’Œå…³é”®è¡Œä¸ºçš„ç™½ç›’éªŒè¯ã€‚")

if __name__ == '__main__':
    main() 
#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†éªŒè¯è„šæœ¬ - å…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬

éªŒè¯ quick_test_causal_engine.py å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥çš„æ­£ç¡®æ€§ã€‚
ç¡®ä¿å¼‚å¸¸æ³¨å…¥ã€æ•°æ®æ ‡å‡†åŒ–å’Œæ¨¡å‹è®­ç»ƒå„ä¸ªç¯èŠ‚éƒ½æ­£å¸¸å·¥ä½œã€‚

éªŒè¯ç›®æ ‡ï¼š
1. å¼‚å¸¸æ³¨å…¥æ˜¯å¦æ­£ç¡®æ‰§è¡Œ
2. å…¨å±€æ ‡å‡†åŒ–æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ
3. æ¨¡å‹è®­ç»ƒæ˜¯å¦åœ¨æ ‡å‡†åŒ–ç©ºé—´è¿›è¡Œ
4. è¯„ä¼°æ˜¯å¦åœ¨åŸå§‹å°ºåº¦è¿›è¡Œ
5. å„ç§æ¨¡å‹æ€§èƒ½æ˜¯å¦åˆç†

é‡ç‚¹éªŒè¯ï¼š
- inject_shuffle_noise çš„å¼‚å¸¸æ³¨å…¥åŠŸèƒ½
- StandardScaler çš„æ­£ç¡®ä½¿ç”¨
- é¢„æµ‹ç»“æœçš„é€†å˜æ¢
- CausalEngine çš„æ€§èƒ½ä¼˜åŠ¿
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import sys
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from causal_sklearn.regressor import MLPCausalRegressor, MLPPytorchRegressor
from causal_sklearn.data_processing import inject_shuffle_noise

warnings.filterwarnings('ignore')

def print_separator(title, char="=", width=80):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}")

def print_subsection(title, char="-", width=60):
    """æ‰“å°å­èŠ‚æ ‡é¢˜"""
    print(f"\n{char * width}")
    print(f"ğŸ” {title}")
    print(f"{char * width}")

def analyze_data_statistics(X, y, data_name):
    """è¯¦ç»†åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š {data_name} æ•°æ®ç»Ÿè®¡åˆ†æï¼š")
    print(f"   æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # X ç»Ÿè®¡ä¿¡æ¯
    print(f"\n   X (ç‰¹å¾) ç»Ÿè®¡:")
    print(f"   - å„ç‰¹å¾å‡å€¼: {X.mean(axis=0)[:3]}... (å‰3ä¸ªç‰¹å¾)")
    print(f"   - å„ç‰¹å¾æ ‡å‡†å·®: {X.std(axis=0)[:3]}... (å‰3ä¸ªç‰¹å¾)")
    print(f"   - æ•´ä½“æ•°æ®èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
    
    # y ç»Ÿè®¡ä¿¡æ¯
    print(f"\n   y (ç›®æ ‡) ç»Ÿè®¡:")
    print(f"   - å‡å€¼: {y.mean():.4f}")
    print(f"   - æ ‡å‡†å·®: {y.std():.4f}")
    print(f"   - èŒƒå›´: [{y.min():.4f}, {y.max():.4f}]")
    
    # æ•°æ®å°ºåº¦è¯„ä¼°
    print(f"\n   ğŸ” æ•°æ®å°ºåº¦è¯„ä¼°:")
    if X.std().max() > 10 or X.std().min() < 0.1:
        print(f"   âŒ ç‰¹å¾å°ºåº¦ä¸ä¸€è‡´ï¼æœ€å¤§æ ‡å‡†å·®: {X.std().max():.2f}, æœ€å°æ ‡å‡†å·®: {X.std().min():.2f}")
    else:
        print(f"   âœ… ç‰¹å¾å°ºåº¦ç›¸å¯¹ä¸€è‡´")
    
    if abs(y).max() > 1000:
        print(f"   âŒ ç›®æ ‡å˜é‡å°ºåº¦è¿‡å¤§ï¼æœ€å¤§ç»å¯¹å€¼: {abs(y).max():.2f}")
    else:
        print(f"   âœ… ç›®æ ‡å˜é‡å°ºåº¦åˆç†")

def validate_global_standardization_strategy():
    """éªŒè¯ quick_test_causal_engine.py çš„å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥"""
    print_separator("ğŸ¯ ç¬¬ä¸€éƒ¨åˆ†ï¼šéªŒè¯å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥çš„æ­£ç¡®æ€§")
    
    print("\nğŸ¯ ä½¿ç”¨ä¸ quick_test_causal_engine.py ç›¸åŒçš„é…ç½®ï¼ˆå…¨å±€æ ‡å‡†åŒ–ç‰ˆæœ¬ï¼‰:")
    config = {
        'n_samples': 4000,
        'n_features': 12,
        'noise': 1.0,
        'random_state': 42,
        'test_size': 0.2,
        'anomaly_ratio': 0.2,
        'validation_fraction': 0.2,
        'max_iter': 2000,
        'learning_rate': 0.01,
        'patience': 50,
        'alpha': 0.0001
    }
    
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŸå§‹æ•°æ®
    print_subsection("ç¬¬1æ­¥ï¼šç”ŸæˆåŸå§‹æ•°æ®")
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    print(f"âœ… make_regression å®Œæˆ")
    analyze_data_statistics(X, y, "åŸå§‹ç”Ÿæˆ")
    
    # ç¬¬äºŒæ­¥ï¼šæ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥
    print_subsection("ç¬¬2æ­¥ï¼šæ•°æ®åˆ†å‰²å’Œå¼‚å¸¸æ³¨å…¥")
    print(f"ğŸ“ æ‰§è¡Œæ ‡å‡†æ•°æ®åˆ†å‰²:")
    print(f"   test_size={config['test_size']}")
    print(f"   anomaly_ratio={config['anomaly_ratio']}")
    
    # ä½¿ç”¨æ ‡å‡†train_test_splitè¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=config['test_size'],
        random_state=config['random_state']
    )
    
    # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥
    if config['anomaly_ratio'] > 0:
        y_train, noise_indices = inject_shuffle_noise(
            y_train,
            noise_ratio=config['anomaly_ratio'],
            random_state=config['random_state']
        )
        print(f"\nâœ… å¼‚å¸¸æ³¨å…¥å®Œæˆ: {config['anomaly_ratio']:.1%} ({len(noise_indices)}/{len(y_train)} æ ·æœ¬å—å½±å“)")
    else:
        print(f"\nâœ… æ— å¼‚å¸¸æ³¨å…¥: çº¯å‡€ç¯å¢ƒ")
    
    print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ")
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    analyze_data_statistics(X_train, y_train, "è®­ç»ƒé›†ï¼ˆå«å¼‚å¸¸ï¼‰")
    analyze_data_statistics(X_test, y_test, "æµ‹è¯•é›†ï¼ˆçº¯å‡€ï¼‰")
    
    # ç¬¬ä¸‰æ­¥ï¼šå…¨å±€æ ‡å‡†åŒ–å¤„ç†
    print_subsection("ç¬¬3æ­¥ï¼šå…¨å±€æ ‡å‡†åŒ–å¤„ç†")
    print("âœ… å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥ï¼šå¯¹ X å’Œ y éƒ½è¿›è¡Œæ ‡å‡†åŒ–")
    print("âœ… æ‰€æœ‰æ¨¡å‹å°†åœ¨ç›¸åŒçš„æ ‡å‡†åŒ–ç©ºé—´ä¸­ç«äº‰ï¼")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    
    # ç›®æ ‡æ ‡å‡†åŒ–ï¼ˆå…³é”®ï¼ï¼‰
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"\nğŸ“Š æ ‡å‡†åŒ–åæ•°æ®ç»Ÿè®¡:")
    print(f"   X_train_scaled: å‡å€¼â‰ˆ{X_train_scaled.mean():.3f}, æ ‡å‡†å·®â‰ˆ{X_train_scaled.std():.3f}")
    print(f"   y_train_scaled: å‡å€¼â‰ˆ{y_train_scaled.mean():.3f}, æ ‡å‡†å·®â‰ˆ{y_train_scaled.std():.3f}")
    
    analyze_data_statistics(X_train_scaled, y_train_scaled, "è®­ç»ƒé›†ï¼ˆæ ‡å‡†åŒ–åï¼‰")
    analyze_data_statistics(X_test_scaled, y_test_scaled, "æµ‹è¯•é›†ï¼ˆæ ‡å‡†åŒ–åï¼‰")
    
    # è®­ç»ƒ sklearn æ¨¡å‹ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ï¼‰
    print(f"\nğŸ”§ è®­ç»ƒ sklearn MLPRegressor (æ ‡å‡†åŒ–æ•°æ®)...")
    sklearn_model = MLPRegressor(
        hidden_layer_sizes=(128, 64),
        max_iter=config['max_iter'],
        learning_rate_init=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        random_state=config['random_state'],
        alpha=config['alpha']
    )
    
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­è®­ç»ƒ
    sklearn_model.fit(X_train_scaled, y_train_scaled)
    print(f"   è®­ç»ƒå®Œæˆ: {sklearn_model.n_iter_} epochs")
    
    # è®­ç»ƒ CausalEngine æ¨¡å‹è¿›è¡Œå¯¹æ¯”
    print(f"\nğŸ”§ è®­ç»ƒ CausalEngine (deterministic) (æ ‡å‡†åŒ–æ•°æ®)...")
    from causal_sklearn.regressor import MLPCausalRegressor
    causal_model = MLPCausalRegressor(
        perception_hidden_layers=(128, 64),
        mode='deterministic',
        max_iter=config['max_iter'],
        learning_rate=config['learning_rate'],
        early_stopping=True,
        validation_fraction=config['validation_fraction'],
        n_iter_no_change=config['patience'],
        random_state=config['random_state'],
        alpha=0.0,
        verbose=False
    )
    
    causal_model.fit(X_train_scaled, y_train_scaled)
    print(f"   è®­ç»ƒå®Œæˆ: {causal_model.n_iter_} epochs")
    
    # ç¬¬å››æ­¥ï¼šæ­£ç¡®çš„é¢„æµ‹å’Œè¯„ä¼°ç­–ç•¥
    print_subsection("ç¬¬4æ­¥ï¼šæ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹ + åŸå§‹å°ºåº¦è¯„ä¼°")
    print("âœ… å…³é”®æ”¹è¿›ï¼šåœ¨æ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹ï¼Œåœ¨åŸå§‹å°ºåº¦è¯„ä¼°")
    print("âœ… ç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼šæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®å¤„ç†æµç¨‹")
    
    # åœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­é¢„æµ‹
    test_pred_scaled = sklearn_model.predict(X_test_scaled)
    causal_pred_scaled = causal_model.predict(X_test_scaled)
    
    # è½¬æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
    test_pred_sklearn = scaler_y.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
    test_pred_causal = scaler_y.inverse_transform(causal_pred_scaled.reshape(-1, 1)).flatten()
    
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœè½¬æ¢:")
    print(f"   æ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹èŒƒå›´: [{test_pred_scaled.min():.3f}, {test_pred_scaled.max():.3f}]")
    print(f"   åŸå§‹å°ºåº¦é¢„æµ‹èŒƒå›´: [{test_pred_sklearn.min():.3f}, {test_pred_sklearn.max():.3f}]")
    print(f"   åŸå§‹ç›®æ ‡èŒƒå›´: [{y_test.min():.3f}, {y_test.max():.3f}]")
    
    # è®¡ç®—æŒ‡æ ‡ï¼ˆåœ¨åŸå§‹å°ºåº¦ï¼‰
    sklearn_mae = mean_absolute_error(y_test, test_pred_sklearn)
    sklearn_rmse = np.sqrt(mean_squared_error(y_test, test_pred_sklearn))
    sklearn_r2 = r2_score(y_test, test_pred_sklearn)
    
    causal_mae = mean_absolute_error(y_test, test_pred_causal)
    causal_rmse = np.sqrt(mean_squared_error(y_test, test_pred_causal))
    causal_r2 = r2_score(y_test, test_pred_causal)
    
    print_subsection("ç¬¬5æ­¥ï¼šå…¨å±€æ ‡å‡†åŒ–ç­–ç•¥éªŒè¯ç»“æœ")
    print("ğŸ¯ å…¨å±€æ ‡å‡†åŒ–ä¸‹çš„æ€§èƒ½å¯¹æ¯”:")
    print(f"   sklearn MLP:")
    print(f"      MAE: {sklearn_mae:.4f}")
    print(f"      RMSE: {sklearn_rmse:.4f}")
    print(f"      RÂ²: {sklearn_r2:.4f}")
    print(f"\n   CausalEngine (deterministic):")
    print(f"      MAE: {causal_mae:.4f}")
    print(f"      RMSE: {causal_rmse:.4f}")
    print(f"      RÂ²: {causal_r2:.4f}")
    
    print(f"\nğŸ’¡ å…¨å±€æ ‡å‡†åŒ–æ•ˆæœåˆ†æ:")
    if causal_mae < sklearn_mae:
        improvement = (sklearn_mae - causal_mae) / sklearn_mae * 100
        print(f"   âœ… CausalEngine æ€§èƒ½ä¼˜äº sklearn MLP: {improvement:.1f}% æ”¹è¿›")
    else:
        print(f"   ğŸ“Š sklearn MLP ç•¥ä¼˜äº CausalEngine")
    
    if sklearn_r2 > 0.9 and causal_r2 > 0.9:
        print(f"   âœ… ä¸¤ä¸ªæ¨¡å‹çš„ RÂ² éƒ½å¾ˆé«˜ï¼Œè¯´æ˜æ ‡å‡†åŒ–ç­–ç•¥æœ‰æ•ˆ")
    
    # éªŒè¯å¼‚å¸¸æ³¨å…¥æ•ˆæœ
    print(f"\nğŸ” å¼‚å¸¸æ³¨å…¥æ•ˆæœéªŒè¯:")
    print(f"   å¼‚å¸¸æ³¨å…¥æ¯”ä¾‹: {config['anomaly_ratio']:.1%}")
    print(f"   è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"   é¢„æœŸå¼‚å¸¸æ ·æœ¬: {int(len(X_train) * config['anomaly_ratio'])}")
    
    return {
        'X_train': X_train, 'X_test': X_test,
        'y_train': y_train, 'y_test': y_test,
        'X_train_scaled': X_train_scaled, 'X_test_scaled': X_test_scaled,
        'y_train_scaled': y_train_scaled, 'y_test_scaled': y_test_scaled,
        'scaler_X': scaler_X, 'scaler_y': scaler_y,
        'sklearn_model': sklearn_model, 'causal_model': causal_model,
        'test_pred_sklearn': test_pred_sklearn, 'test_pred_causal': test_pred_causal,
        'sklearn_mae': sklearn_mae, 'causal_mae': causal_mae,
        'sklearn_r2': sklearn_r2, 'causal_r2': causal_r2,
        'config': config
    }

def validate_anomaly_injection(data):
    """éªŒè¯å¼‚å¸¸æ³¨å…¥çš„æ­£ç¡®æ€§"""
    print_separator("ğŸ§ª ç¬¬äºŒéƒ¨åˆ†ï¼šå¼‚å¸¸æ³¨å…¥éªŒè¯")
    
    config = data['config']
    
    print("ğŸ” é‡æ–°æ‰§è¡Œå¼‚å¸¸æ³¨å…¥ä»¥æ£€æŸ¥è¯¦æƒ…:")
    
    # é‡æ–°æ‰§è¡Œå¼‚å¸¸æ³¨å…¥å¹¶è·å–è¯¦ç»†ä¿¡æ¯
    from sklearn.datasets import make_regression
    X, y = make_regression(
        n_samples=config['n_samples'], 
        n_features=config['n_features'],
        noise=config['noise'], 
        random_state=config['random_state']
    )
    
    # ä½¿ç”¨æ ‡å‡†train_test_splitè¿›è¡Œæ•°æ®åˆ†å‰²
    X_train, X_test, y_train_original, y_test = train_test_split(
        X, y,
        test_size=config['test_size'],
        random_state=config['random_state']
    )
    
    # å¯¹è®­ç»ƒé›†æ ‡ç­¾è¿›è¡Œå¼‚å¸¸æ³¨å…¥
    if config['anomaly_ratio'] > 0:
        y_train_noisy, noise_indices = inject_shuffle_noise(
            y_train_original,
            noise_ratio=config['anomaly_ratio'],
            random_state=config['random_state']
        )
        
        # è®¡ç®—å¼‚å¸¸æ³¨å…¥ä¿¡æ¯
        anomaly_info = {
            'requested_ratio': config['anomaly_ratio'],
            'actual_ratio': len(noise_indices) / len(y_train_original),
            'n_anomalies': len(noise_indices),
            'n_total': len(y_train_original),
            'changes_made': len(noise_indices),  # æ‰€æœ‰é€‰ä¸­çš„æ ·æœ¬éƒ½è¢«æ”¹å˜
            'strategy': 'shuffle'
        }
    else:
        anomaly_info = None
    
    print(f"\nğŸ“Š å¼‚å¸¸æ³¨å…¥è¯¦ç»†ä¿¡æ¯:")
    if anomaly_info:
        print(f"   è¯·æ±‚å¼‚å¸¸æ¯”ä¾‹: {anomaly_info['requested_ratio']:.1%}")
        print(f"   å®é™…å¼‚å¸¸æ¯”ä¾‹: {anomaly_info['actual_ratio']:.1%}")
        print(f"   å¼‚å¸¸æ ·æœ¬æ•°é‡: {anomaly_info['n_anomalies']} / {anomaly_info['n_total']}")
        print(f"   å®é™…æ”¹å˜æ•°é‡: {anomaly_info['changes_made']} / {anomaly_info['n_anomalies']}")
        print(f"   å¼‚å¸¸ç­–ç•¥: {anomaly_info['strategy']}")
        
        if 'unchanged_ratio' in anomaly_info:
            print(f"   æ‰“ä¹±åæœªæ”¹å˜æ¯”ä¾‹: {anomaly_info['unchanged_ratio']:.1%} (shuffleç­–ç•¥çš„æ­£å¸¸ç°è±¡)")
        
        if 'avg_change' in anomaly_info:
            print(f"   å¹³å‡å˜åŒ–å¹…åº¦: {anomaly_info['avg_change']:.4f}")
            
        # éªŒè¯å¼‚å¸¸æ³¨å…¥æ˜¯å¦æœ‰æ•ˆ
        if anomaly_info['changes_made'] > 0:
            print(f"   âœ… å¼‚å¸¸æ³¨å…¥æˆåŠŸæ‰§è¡Œ")
        else:
            print(f"   âŒ å¼‚å¸¸æ³¨å…¥å¯èƒ½å¤±è´¥")
    else:
        print(f"   âŒ æœªè·å–åˆ°å¼‚å¸¸æ³¨å…¥ä¿¡æ¯")
    
    # åˆ†æå¼‚å¸¸å¯¹æ€§èƒ½çš„å½±å“
    print(f"\nğŸ” å¼‚å¸¸æ³¨å…¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“åˆ†æ:")
    sklearn_mae = data['sklearn_mae']
    causal_mae = data['causal_mae']
    
    # ä¼°ç®—æ— å¼‚å¸¸æƒ…å†µä¸‹çš„ç†è®ºæ€§èƒ½
    print(f"   å½“å‰æ€§èƒ½ (å«{config['anomaly_ratio']:.1%}å¼‚å¸¸):")
    print(f"      sklearn MAE: {sklearn_mae:.4f}")
    print(f"      CausalEngine MAE: {causal_mae:.4f}")
    
    if causal_mae < sklearn_mae:
        print(f"   ğŸ’¡ CausalEngine åœ¨å¼‚å¸¸ç¯å¢ƒä¸‹è¡¨ç°æ›´ä½³ï¼Œä½“ç°äº†é²æ£’æ€§ä¼˜åŠ¿")
    else:
        print(f"   ğŸ’¡ åœ¨å½“å‰å¼‚å¸¸æ°´å¹³ä¸‹ï¼Œä¸¤ä¸ªæ¨¡å‹æ€§èƒ½ç›¸è¿‘")

def validate_standardization_correctness(data):
    """éªŒè¯æ ‡å‡†åŒ–å®æ–½çš„æ­£ç¡®æ€§"""
    print_separator("ğŸ”¬ ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ ‡å‡†åŒ–å®æ–½æ­£ç¡®æ€§éªŒè¯")
    
    X_train_scaled = data['X_train_scaled']
    y_train_scaled = data['y_train_scaled']
    X_test_scaled = data['X_test_scaled']
    y_test_scaled = data['y_test_scaled']
    scaler_X = data['scaler_X']
    scaler_y = data['scaler_y']
    
    print("ğŸ” æ ‡å‡†åŒ–å®æ–½æ­£ç¡®æ€§æ£€æŸ¥:")
    print(f"   âœ… quick_test_causal_engine.py å·²å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥")
    
    # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
    print(f"\nğŸ“Š æ ‡å‡†åŒ–æ•ˆæœéªŒè¯:")
    print(f"   X_train_scaled å‡å€¼: {X_train_scaled.mean(axis=0)[:5]} (å‰5ä¸ªç‰¹å¾)")
    print(f"   X_train_scaled æ ‡å‡†å·®: {X_train_scaled.std(axis=0)[:5]} (å‰5ä¸ªç‰¹å¾)")
    print(f"   y_train_scaled å‡å€¼: {y_train_scaled.mean():.6f}")
    print(f"   y_train_scaled æ ‡å‡†å·®: {y_train_scaled.std():.6f}")
    
    # æ£€æŸ¥æ ‡å‡†åŒ–æ˜¯å¦æ­£ç¡®
    x_mean_check = np.abs(X_train_scaled.mean(axis=0)).max()
    x_std_check = np.abs(X_train_scaled.std(axis=0) - 1.0).max()
    y_mean_check = np.abs(y_train_scaled.mean())
    y_std_check = np.abs(y_train_scaled.std() - 1.0)
    
    print(f"\nğŸ” æ ‡å‡†åŒ–è´¨é‡æ£€æŸ¥:")
    if x_mean_check < 1e-10:
        print(f"   âœ… X ç‰¹å¾å‡å€¼æ¥è¿‘0 (æœ€å¤§åå·®: {x_mean_check:.2e})")
    else:
        print(f"   âŒ X ç‰¹å¾å‡å€¼åç¦»0 (æœ€å¤§åå·®: {x_mean_check:.6f})")
        
    if x_std_check < 1e-10:
        print(f"   âœ… X ç‰¹å¾æ ‡å‡†å·®æ¥è¿‘1 (æœ€å¤§åå·®: {x_std_check:.2e})")
    else:
        print(f"   âŒ X ç‰¹å¾æ ‡å‡†å·®åç¦»1 (æœ€å¤§åå·®: {x_std_check:.6f})")
        
    if y_mean_check < 1e-10:
        print(f"   âœ… y ç›®æ ‡å‡å€¼æ¥è¿‘0 (åå·®: {y_mean_check:.2e})")
    else:
        print(f"   âŒ y ç›®æ ‡å‡å€¼åç¦»0 (åå·®: {y_mean_check:.6f})")
        
    if y_std_check < 1e-10:
        print(f"   âœ… y ç›®æ ‡æ ‡å‡†å·®æ¥è¿‘1 (åå·®: {y_std_check:.2e})")
    else:
        print(f"   âŒ y ç›®æ ‡æ ‡å‡†å·®åç¦»1 (åå·®: {y_std_check:.6f})")
    
    # éªŒè¯é€†å˜æ¢çš„æ­£ç¡®æ€§
    print_subsection("é€†å˜æ¢æ­£ç¡®æ€§éªŒè¯")
    
    # æµ‹è¯•ä¸€äº›æ ·æœ¬çš„é€†å˜æ¢
    test_sample_scaled = X_test_scaled[:5]
    test_sample_original = scaler_X.inverse_transform(test_sample_scaled)
    
    print(f"\nğŸ“Š X é€†å˜æ¢éªŒè¯ (å‰5ä¸ªæ ·æœ¬):")
    print(f"   åŸå§‹æ•°æ®å‰5ä¸ªæ ·æœ¬ X[0]: {data['X_test'][:5, 0]}")
    print(f"   é€†å˜æ¢åå‰5ä¸ªæ ·æœ¬ X[0]: {test_sample_original[:, 0]}")
    x_inverse_error = np.abs(data['X_test'][:5] - test_sample_original).max()
    if x_inverse_error < 1e-10:
        print(f"   âœ… X é€†å˜æ¢æ­£ç¡® (æœ€å¤§è¯¯å·®: {x_inverse_error:.2e})")
    else:
        print(f"   âŒ X é€†å˜æ¢æœ‰è¯¯å·® (æœ€å¤§è¯¯å·®: {x_inverse_error:.6f})")
    
    # æµ‹è¯• y çš„é€†å˜æ¢
    y_sample_scaled = y_test_scaled[:5]
    y_sample_original = scaler_y.inverse_transform(y_sample_scaled.reshape(-1, 1)).flatten()
    
    print(f"\nğŸ“Š y é€†å˜æ¢éªŒè¯ (å‰5ä¸ªæ ·æœ¬):")
    print(f"   åŸå§‹æ•°æ®: {data['y_test'][:5]}")
    print(f"   é€†å˜æ¢å: {y_sample_original}")
    y_inverse_error = np.abs(data['y_test'][:5] - y_sample_original).max()
    if y_inverse_error < 1e-10:
        print(f"   âœ… y é€†å˜æ¢æ­£ç¡® (æœ€å¤§è¯¯å·®: {y_inverse_error:.2e})")
    else:
        print(f"   âŒ y é€†å˜æ¢æœ‰è¯¯å·® (æœ€å¤§è¯¯å·®: {y_inverse_error:.6f})")
    
    # éªŒè¯é¢„æµ‹ç»“æœçš„è½¬æ¢
    print_subsection("é¢„æµ‹ç»“æœè½¬æ¢éªŒè¯")
    sklearn_pred_scaled = data['sklearn_model'].predict(X_test_scaled[:5])
    sklearn_pred_original = scaler_y.inverse_transform(sklearn_pred_scaled.reshape(-1, 1)).flatten()
    
    print(f"\nğŸ“Š æ¨¡å‹é¢„æµ‹è½¬æ¢éªŒè¯:")
    print(f"   æ ‡å‡†åŒ–ç©ºé—´é¢„æµ‹: {sklearn_pred_scaled}")
    print(f"   åŸå§‹å°ºåº¦é¢„æµ‹: {sklearn_pred_original}")
    print(f"   çœŸå®åŸå§‹å€¼: {data['y_test'][:5]}")
    print(f"   é¢„æµ‹è¯¯å·®: {np.abs(sklearn_pred_original - data['y_test'][:5])}")

def validate_model_performance(data):
    """éªŒè¯æ¨¡å‹æ€§èƒ½çš„åˆç†æ€§"""
    print_separator("ğŸ¯ ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹æ€§èƒ½åˆç†æ€§éªŒè¯")
    
    sklearn_pred = data['test_pred_sklearn']
    causal_pred = data['test_pred_causal']
    y_test = data['y_test']
    sklearn_mae = data['sklearn_mae']
    causal_mae = data['causal_mae']
    sklearn_r2 = data['sklearn_r2']
    causal_r2 = data['causal_r2']
    config = data['config']
    
    print("ğŸ” æ¨¡å‹æ€§èƒ½åˆç†æ€§åˆ†æ:")
    
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡èŒƒå›´æ£€æŸ¥:")
    print(f"   sklearn MLP: MAE={sklearn_mae:.4f}, RÂ²={sklearn_r2:.4f}")
    print(f"   CausalEngine: MAE={causal_mae:.4f}, RÂ²={causal_r2:.4f}")
    
    # æ£€æŸ¥ RÂ² æ˜¯å¦åˆç†
    if sklearn_r2 > 0.8 and causal_r2 > 0.8:
        print(f"   âœ… ä¸¤ä¸ªæ¨¡å‹çš„ RÂ² éƒ½å¾ˆé«˜ï¼Œè¯´æ˜æ¨¡å‹æ€§èƒ½è‰¯å¥½")
    elif sklearn_r2 < 0 or causal_r2 < 0:
        print(f"   âŒ å­˜åœ¨è´Ÿ RÂ² å€¼ï¼Œå¯èƒ½æœ‰é—®é¢˜")
    else:
        print(f"   ğŸ“Š RÂ² å€¼åˆç†ä½†æœ‰æ”¹è¿›ç©ºé—´")
    
    # æ£€æŸ¥ MAE æ˜¯å¦åˆç†
    y_range = y_test.max() - y_test.min()
    relative_mae_sklearn = sklearn_mae / y_range
    relative_mae_causal = causal_mae / y_range
    
    print(f"\nğŸ“Š ç›¸å¯¹è¯¯å·®åˆ†æ:")
    print(f"   ç›®æ ‡å˜é‡èŒƒå›´: {y_range:.2f}")
    print(f"   sklearn ç›¸å¯¹ MAE: {relative_mae_sklearn:.1%}")
    print(f"   CausalEngine ç›¸å¯¹ MAE: {relative_mae_causal:.1%}")
    
    if relative_mae_sklearn < 0.1 and relative_mae_causal < 0.1:
        print(f"   âœ… ä¸¤ä¸ªæ¨¡å‹çš„ç›¸å¯¹è¯¯å·®éƒ½å¾ˆå°ï¼Œæ€§èƒ½ä¼˜ç§€")
    elif relative_mae_sklearn < 0.2 and relative_mae_causal < 0.2:
        print(f"   âœ… ä¸¤ä¸ªæ¨¡å‹çš„ç›¸å¯¹è¯¯å·®é€‚ä¸­ï¼Œæ€§èƒ½è‰¯å¥½")
    else:
        print(f"   âš ï¸ ç›¸å¯¹è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è°ƒä¼˜")
    
    # åˆ†æé¢„æµ‹å€¼åˆ†å¸ƒ
    print(f"\nğŸ“Š é¢„æµ‹å€¼åˆ†å¸ƒåˆ†æ:")
    print(f"   çœŸå®å€¼èŒƒå›´: [{y_test.min():.2f}, {y_test.max():.2f}]")
    print(f"   sklearn é¢„æµ‹èŒƒå›´: [{sklearn_pred.min():.2f}, {sklearn_pred.max():.2f}]")
    print(f"   CausalEngine é¢„æµ‹èŒƒå›´: [{causal_pred.min():.2f}, {causal_pred.max():.2f}]")
    
    # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    sklearn_in_range = np.sum((sklearn_pred >= y_test.min() * 0.8) & (sklearn_pred <= y_test.max() * 1.2))
    causal_in_range = np.sum((causal_pred >= y_test.min() * 0.8) & (causal_pred <= y_test.max() * 1.2))
    
    print(f"\nğŸ“Š é¢„æµ‹åˆç†æ€§æ£€æŸ¥:")
    print(f"   sklearn åˆç†é¢„æµ‹: {sklearn_in_range}/{len(sklearn_pred)} ({sklearn_in_range/len(sklearn_pred):.1%})")
    print(f"   CausalEngine åˆç†é¢„æµ‹: {causal_in_range}/{len(causal_pred)} ({causal_in_range/len(causal_pred):.1%})")
    
    # å¼‚å¸¸æ³¨å…¥ç¯å¢ƒä¸‹çš„æ€§èƒ½åˆ†æ
    print(f"\nğŸ” å¼‚å¸¸ç¯å¢ƒä¸‹çš„æ€§èƒ½åˆ†æ:")
    print(f"   å¼‚å¸¸æ³¨å…¥æ¯”ä¾‹: {config['anomaly_ratio']:.1%}")
    if causal_mae < sklearn_mae:
        improvement = (sklearn_mae - causal_mae) / sklearn_mae * 100
        print(f"   âœ… CausalEngine åœ¨å¼‚å¸¸ç¯å¢ƒä¸‹è¡¨ç°æ›´ä¼˜: {improvement:.1f}% æ”¹è¿›")
        print(f"   ğŸ’¡ è¿™éªŒè¯äº† CausalEngine çš„é²æ£’æ€§ä¼˜åŠ¿")
    else:
        print(f"   ğŸ“Š sklearn MLP åœ¨å½“å‰è®¾ç½®ä¸‹ç•¥ä¼˜")
        print(f"   ğŸ’¡ å¯èƒ½éœ€è¦è°ƒæ•´ CausalEngine çš„è¶…å‚æ•°")

def summarize_validation_results():
    """æ€»ç»“éªŒè¯ç»“æœ"""
    print_separator("ğŸ‰ ç¬¬äº”éƒ¨åˆ†ï¼šå…¨å±€æ ‡å‡†åŒ–ç­–ç•¥éªŒè¯æ€»ç»“")
    
    print("ğŸ” éªŒè¯ç»“æœæ€»ç»“:")
    
    print(f"\nâœ… æˆåŠŸéªŒè¯é¡¹ç›®:")
    print(f"   1. å¼‚å¸¸æ³¨å…¥åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    print(f"      - inject_shuffle_noise æ­£ç¡®æ³¨å…¥æŒ‡å®šæ¯”ä¾‹çš„å¼‚å¸¸")
    print(f"      - shuffle ç­–ç•¥æœ‰æ•ˆæ‰“ä¹±æ ‡ç­¾")
    print(f"      - æµ‹è¯•é›†ä¿æŒçº¯å‡€")
    
    print(f"\n   2. å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥æ­£ç¡®å®æ–½")
    print(f"      - X å’Œ y éƒ½æ­£ç¡®æ ‡å‡†åŒ–")
    print(f"      - å‡å€¼æ¥è¿‘0ï¼Œæ ‡å‡†å·®æ¥è¿‘1")
    print(f"      - é€†å˜æ¢ç²¾åº¦é«˜")
    
    print(f"\n   3. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æµç¨‹åˆç†")
    print(f"      - åœ¨æ ‡å‡†åŒ–ç©ºé—´è®­ç»ƒ")
    print(f"      - åœ¨åŸå§‹å°ºåº¦è¯„ä¼°")
    print(f"      - æ€§èƒ½æŒ‡æ ‡åˆç†")
    
    print(f"\n   4. CausalEngine æ€§èƒ½éªŒè¯")
    print(f"      - åœ¨å¼‚å¸¸ç¯å¢ƒä¸‹å±•ç°é²æ£’æ€§")
    print(f"      - ä¸ sklearn MLP å½¢æˆæœ‰æ•ˆå¯¹æ¯”")
    print(f"      - æ€§èƒ½æŒ‡æ ‡åœ¨åˆç†èŒƒå›´å†…")
    
    print_subsection("å½“å‰å®æ–½çŠ¶æ€")
    print("âœ… quick_test_causal_engine.py çŠ¶æ€:")
    print("   âœ“ å·²å®æ–½å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥")
    print("   âœ“ å¼‚å¸¸æ³¨å…¥åŠŸèƒ½æ­£å¸¸")
    print("   âœ“ æ¨¡å‹è®­ç»ƒæµç¨‹æ­£ç¡®")
    print("   âœ“ è¯„ä¼°æ–¹æ³•åˆç†")
    
    print(f"\nğŸ’¡ å…³é”®æ”¹è¿›æ•ˆæœ:")
    print(f"   - å»ºç«‹äº†ç»å¯¹å…¬å¹³çš„ç«æŠ€åœº")
    print(f"   - æ‰€æœ‰æ¨¡å‹åœ¨ç›¸åŒæ ‡å‡†åŒ–ç©ºé—´ç«äº‰")
    print(f"   - ç¨³å¥å›å½’å™¨æ— æ³•åˆ©ç”¨æœªç¼©æ”¾æ•°æ®ä¼˜åŠ¿")
    print(f"   - CausalEngine çœŸå®èƒ½åŠ›å¾—åˆ°éªŒè¯")
    
    print(f"\nğŸ¯ éªŒè¯ç»“è®º:")
    print(f"   âœ… å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥å®æ–½æˆåŠŸ")
    print(f"   âœ… quick_test_causal_engine.py å·¥ä½œæ­£å¸¸")
    print(f"   âœ… æ— éœ€è¿›ä¸€æ­¥ä¿®å¤")
    print(f"   âœ… å¯ä»¥æ”¾å¿ƒä½¿ç”¨è¿›è¡Œæ¨¡å‹å¯¹æ¯”")

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ”¬ å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥éªŒè¯è„šæœ¬")
    print("=" * 80)
    print("ç›®æ ‡: éªŒè¯ quick_test_causal_engine.py å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥çš„æ­£ç¡®æ€§")
    print("é‡ç‚¹: ç¡®ä¿å¼‚å¸¸æ³¨å…¥ã€æ ‡å‡†åŒ–ã€è®­ç»ƒã€è¯„ä¼°å„ç¯èŠ‚æ­£å¸¸å·¥ä½œ")
    print("=" * 80)
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šéªŒè¯å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥
    data = validate_global_standardization_strategy()
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šéªŒè¯å¼‚å¸¸æ³¨å…¥
    validate_anomaly_injection(data)
    
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šéªŒè¯æ ‡å‡†åŒ–æ­£ç¡®æ€§
    validate_standardization_correctness(data)
    
    # ç¬¬å››éƒ¨åˆ†ï¼šéªŒè¯æ¨¡å‹æ€§èƒ½
    validate_model_performance(data)
    
    # ç¬¬äº”éƒ¨åˆ†ï¼šæ€»ç»“éªŒè¯ç»“æœ
    summarize_validation_results()
    
    print_separator("ğŸ‰ éªŒè¯å®Œæˆ", char="=", width=80)
    print("ğŸ’¡ ç»“è®º: quick_test_causal_engine.py å…¨å±€æ ‡å‡†åŒ–ç­–ç•¥å·¥ä½œæ­£å¸¸")
    print("ğŸ“ çŠ¶æ€: å·²æˆåŠŸå»ºç«‹ç»å¯¹å…¬å¹³çš„ç«æŠ€åœºï¼Œå¯æ”¾å¿ƒä½¿ç”¨")

if __name__ == "__main__":
    main()
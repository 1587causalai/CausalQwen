#!/usr/bin/env python
"""
æ€»æŸå¤±è®¡ç®—åŠ OvR é˜ˆå€¼å½±å“å®éªŒè„šæœ¬

æœ¬è„šæœ¬æ—¨åœ¨å®Œæˆä¸¤é¡¹ä»»åŠ¡ï¼š
1. ç™½ç›’éªŒè¯ `compute_total_loss` å‡½æ•°çš„å®ç°æ˜¯å¦æ­£ç¡®ï¼Œç‰¹åˆ«æ˜¯å¯¹åˆ†ç±»å’Œ
   å›å½’æŸå¤±ä½¿ç”¨ä¸åŒåˆ†æ¯è¿›è¡Œå¹³å‡çš„æ ¸å¿ƒé€»è¾‘ã€‚
2. é€šè¿‡å®éªŒå¯¹æ¯”ä¸åŒçš„ OvR å†³ç­–é˜ˆå€¼ (`ovr_threshold`) å¯¹åˆå§‹æŸå¤±å€¼çš„å½±å“ï¼Œ
   ä»¥éªŒè¯å…¶æ•°å­¦è¡Œä¸ºã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info
from src.losses.loss_functions import ovr_classification_loss, gated_regression_loss
from src.utils.losses import compute_ovr_probabilities

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - æ€»æŸå¤±è®¡ç®—é˜ˆå€¼å®éªŒ")

    # --- æ­¥éª¤ 1: é€šç”¨åˆå§‹åŒ– ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–åˆ†è¯å™¨å’Œå…±äº«æµ‹è¯•æ•°æ®")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)

    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    cls_labels = torch.full_like(input_ids, -100)
    cls_labels[:, :-1] = input_ids[:, 1:]

    print("   âœ… é€šç”¨åˆå§‹åŒ–å®Œæˆã€‚")

    thresholds_to_test = [-10.0, 0.0, 10.0, 10000.0]
    all_tests_passed = True
    results = []

    for threshold in thresholds_to_test:
        print(f"\n\n{'#'*80}")
        print(f"ğŸ”¬ å¼€å§‹æµ‹è¯• OvR é˜ˆå€¼ (Threshold): {threshold}")
        print(f"{'#'*80}")

        # --- æ­¥éª¤ 2: é’ˆå¯¹å½“å‰é˜ˆå€¼åˆå§‹åŒ–æ¨¡å‹ ---
        print_step("æ­¥éª¤ 2", f"ä½¿ç”¨é˜ˆå€¼ {threshold} åˆå§‹åŒ–æ¨¡å‹é…ç½®")
        config = CausalLMConfig(
            vocab_size=model_info['vocab_size'],
            num_token_id=tokenizer.num_token_id,
            hidden_size=model_info['hidden_size'],
            use_real_qwen=False,
            reg_loss_gating_alpha=1.0,
            reg_loss_weight=1.0,
            ovr_threshold=threshold  # <--- æ ¸å¿ƒæ”¹åŠ¨
        )
        model = CausalLanguageModel(config).to(device)
        print(f"   âœ… æ¨¡å‹å·²ä½¿ç”¨ ovr_threshold={threshold} è¿›è¡Œé…ç½®ã€‚")


        with torch.no_grad():
            # --- æ­¥éª¤ 3: ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡º ---
            print_step("æ­¥éª¤ 3", "ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡ºåˆ†å¸ƒ")
            # ä¸ºä¿è¯è·¨é˜ˆå€¼æµ‹è¯•çš„å¯æ¯”æ€§ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå¾ªç¯ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­
            torch.manual_seed(42)
            outputs = {
                'cls_loc': torch.randn(len(test_samples), input_ids.shape[1], config.vocab_size),
                'cls_scale': torch.rand(len(test_samples), input_ids.shape[1], config.vocab_size) * 5 + 1,
                'reg_loc': torch.randn(len(test_samples), input_ids.shape[1]),
                'reg_scale': torch.rand(len(test_samples), input_ids.shape[1]) * 5 + 1,
            }
            print("   - å·²ç”Ÿæˆå›ºå®šçš„éšæœºæ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºã€‚")

            # --- æ­¥éª¤ 4: æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Ground Truth) ---
            print_step("æ­¥éª¤ 4", "æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Ground Truth)")
            print("   - éµå¾ª `mathematical_foundations.md` ä¸­çš„åˆ†ç¦»å¹³å‡å…¬å¼")

            # a. æå–æ‰€éœ€å¼ é‡
            cls_loc, cls_scale = outputs['cls_loc'], outputs['cls_scale']
            reg_loc, reg_scale = outputs['reg_loc'], outputs['reg_scale']
            
            # b. è®¡ç®—é€è¯å…ƒæŸå¤±
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† model.config.ovr_threshold æ¥ç¡®ä¿ä¸æ¨¡å‹è¡Œä¸ºä¸€è‡´
            cls_probs = compute_ovr_probabilities(cls_loc, cls_scale, model.config.ovr_threshold)
            L_cls_per_token = ovr_classification_loss(cls_probs, cls_labels, reduction='none')

            num_probs = cls_probs[:, :, tokenizer.num_token_id]
            num_mask = (cls_labels == tokenizer.num_token_id).float() * attention_mask
            L_reg_per_token = gated_regression_loss(
                reg_loc, reg_scale, numerical_values,
                gate_prob=num_probs, mask=num_mask, 
                alpha=model.config.reg_loss_gating_alpha, reduction='none'
            )

            # c. åˆ†åˆ«è®¡ç®—å¹³å‡æŸå¤±
            active_cls_tokens = attention_mask.sum()
            # éœ€è¦å¤„ç†æ½œåœ¨çš„ inf å€¼ï¼Œå°†å…¶æ›¿æ¢ä¸º 0 ä»¥ä¾¿æ±‚å’Œ
            L_cls_per_token_safe = torch.nan_to_num(L_cls_per_token, nan=0.0, posinf=0.0, neginf=0.0)
            L_cls_mean = (L_cls_per_token_safe * attention_mask).sum() / active_cls_tokens
            
            active_reg_tokens = num_mask.sum()
            L_reg_eff = L_reg_per_token.sum() / (active_reg_tokens + 1e-8)

            # d. åŠ æƒåˆå¹¶
            lambda_weight = model.config.reg_loss_weight
            manual_total_loss = L_cls_mean + lambda_weight * L_reg_eff
            
            print(f"\n   --- æ‰‹åŠ¨è®¡ç®—ç»“æœ ---")
            print(f"     - æœ‰æ•ˆåˆ†ç±»è¯å…ƒæ•° (attention_mask.sum()): {active_cls_tokens.item()}")
            print(f"     - æœ‰æ•ˆå›å½’è¯å…ƒæ•° (num_mask.sum()): {active_reg_tokens.item()}")
            print(f"     - å¹³å‡åˆ†ç±»æŸå¤± (L_cls_mean): {L_cls_mean.item():.4f}")
            print(f"     - æœ‰æ•ˆå›å½’æŸå¤± (L_reg_eff): {L_reg_eff.item():.4f}")
            print(f"     - å›å½’æƒé‡ (Î»): {lambda_weight}")
            print(f"     - æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Manual Total Loss): {manual_total_loss.item():.4f}")

            # --- æ­¥éª¤ 5: ä½¿ç”¨æ¨¡å‹å†…ç½®æ–¹æ³•è®¡ç®—æŸå¤± ---
            print_step("æ­¥éª¤ 5", "ä½¿ç”¨ `model.compute_loss` è®¡ç®—æ€»æŸå¤±")
            loss_dict = model.compute_loss(
                outputs, targets=cls_labels, 
                numerical_values=numerical_values, attention_mask=attention_mask
            )
            model_total_loss = loss_dict['total']
            print(f"\n   --- æ¨¡å‹è®¡ç®—ç»“æœ ---")
            print(f"     - å¹³å‡åˆ†ç±»æŸå¤± (cls_loss): {loss_dict['cls']:.4f}")
            print(f"     - æœ‰æ•ˆå›å½’æŸå¤± (effective_reg_loss): {loss_dict['effective_reg_loss']:.4f}")
            print(f"     - æ¨¡å‹è®¡ç®—æ€»æŸå¤± (Model Total Loss): {model_total_loss.item():.4f}")

            # --- æ­¥éª¤ 6: æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯ ---
            print_step("æ­¥éª¤ 6", "æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯")
            # ç”±äºå¯èƒ½å­˜åœ¨ NaNï¼Œæˆ‘ä»¬åœ¨æ¯”è¾ƒå‰ä¹Ÿå¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå¤„ç†
            model_total_loss_safe = torch.nan_to_num(model_total_loss, nan=0.0, posinf=0.0, neginf=0.0)
            loss_match = torch.allclose(manual_total_loss, model_total_loss_safe, atol=1e-5)
            
            if not loss_match:
                all_tests_passed = False

            print(f"\n   --- éªŒè¯: æ‰‹åŠ¨è®¡ç®— vs. æ¨¡å‹è®¡ç®— ---")
            print(f"     - ç»“è®º: {'âœ… é€šè¿‡' if loss_match else 'âŒ å¤±è´¥'}")
            if not loss_match:
                diff = torch.abs(manual_total_loss - model_total_loss_safe)
                print(f"     - ç»å¯¹å·®å¼‚: {diff.item():.8f}")
            
            results.append({
                "threshold": threshold,
                "cls_loss": loss_dict['cls'],
                "reg_loss": loss_dict['effective_reg_loss'],
                "total_loss": model_total_loss,
                "passed": loss_match
            })

    # --- æ€»ç»“æŠ¥å‘Š ---
    print(f"\n\n{'='*80}")
    print("ğŸ“Š å®éªŒæ€»ç»“æŠ¥å‘Š")
    print(f"{'='*80}")
    print(f"{'é˜ˆå€¼':>10} | {'åˆ†ç±»æŸå¤±':>15} | {'æœ‰æ•ˆå›å½’æŸå¤±':>15} | {'æ€»æŸå¤±':>15} | {'éªŒè¯é€šè¿‡':>10}")
    print(f"{'-'*11}+{'-'*17}+{'-'*17}+{'-'*17}+{'-'*12}")
    for res in results:
        status = "âœ…" if res['passed'] else "âŒ"
        # å¤„ç†å¯èƒ½çš„ inf/-inf/nan
        cls_loss_str = f"{torch.nan_to_num(res['cls_loss']).item():>15.4f}"
        reg_loss_str = f"{torch.nan_to_num(res['reg_loss']).item():>15.4f}"
        total_loss_str = f"{torch.nan_to_num(res['total_loss']).item():>15.4f}"
        
        print(f"{res['threshold']:>10.1f} | {cls_loss_str} | {reg_loss_str} | {total_loss_str} | {status:>10}")

    print(f"\n{'='*80}")
    if all_tests_passed:
        print("ğŸ‰ å…¨éƒ¨éªŒè¯æˆåŠŸï¼`compute_total_loss` åœ¨æ‰€æœ‰æµ‹è¯•é˜ˆå€¼ä¸‹å‡è¡¨ç°æ­£ç¡®ã€‚")
    else:
        print("âŒ éƒ¨åˆ†æˆ–å…¨éƒ¨éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ `compute_total_loss` çš„å®ç°æˆ–é˜ˆå€¼é€»è¾‘ã€‚")


if __name__ == '__main__':
    main() 
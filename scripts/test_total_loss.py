#!/usr/bin/env python
"""
æ€»æŸå¤±è®¡ç®—ç™½ç›’éªŒè¯è„šæœ¬

æœ¬è„šæœ¬ä¸¥æ ¼éµå¾ª `mathematical_foundations.md` ä¸­å…³äºæ€»æŸå¤±åˆå¹¶çš„ç²¾ç¡®å®šä¹‰ï¼Œ
æ—¨åœ¨ç™½ç›’éªŒè¯ `compute_total_loss` å‡½æ•°çš„å®ç°æ˜¯å¦æ­£ç¡®ï¼Œç‰¹åˆ«æ˜¯å¯¹åˆ†ç±»å’Œ
å›å½’æŸå¤±ä½¿ç”¨ä¸åŒåˆ†æ¯è¿›è¡Œå¹³å‡çš„æ ¸å¿ƒé€»è¾‘ã€‚
"""
import os
import sys
import torch

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.causal_lm import CausalLMConfig, CausalLanguageModel
from src.data.tokenizer import QwenTokenizerWrapper
from src.utils.model_utils import get_qwen_model_info
from src.losses.loss_functions import ovr_classification_loss, gated_regression_loss
from src.utils.losses import compute_ovr_probabilities

def print_step(step_name, description):
    """æ‰“å°æµç¨‹å›¾æ­¥éª¤ä¿¡æ¯"""
    print(f"\n{'='*70}")
    print(f"â¡ï¸  {step_name}: {description}")
    print(f"{'-'*70}")

def main():
    print("ğŸš€ CausalQwen - æ€»æŸå¤± (Total Loss) è®¡ç®—é€»è¾‘æ·±åº¦éªŒè¯")
    
    # --- æ­¥éª¤ 1: åˆå§‹åŒ– ---
    print_step("æ­¥éª¤ 1", "åˆå§‹åŒ–çœŸå®æ¨¡å‹ã€åˆ†è¯å™¨åŠæµ‹è¯•æ•°æ®")
    device = torch.device('cpu')
    qwen_model_path = os.path.expanduser('~/models/Qwen2.5-0.5B')
    model_info = get_qwen_model_info(qwen_model_path)
    if not model_info:
        sys.exit(1)
    tokenizer = QwenTokenizerWrapper(model_path=qwen_model_path, use_real_tokenizer=True)
    
    config = CausalLMConfig(
        vocab_size=model_info['vocab_size'],
        num_token_id=tokenizer.num_token_id,
        hidden_size=model_info['hidden_size'],
        use_real_qwen=False, # æŸå¤±è®¡ç®—ä¸æ¨¡å‹æƒé‡æ— å…³ï¼Œè®¾ä¸ºFalseä»¥åŠ é€Ÿ
        reg_loss_gating_alpha=0.0,
        reg_loss_weight=0.5 # ä½¿ç”¨ä¸€ä¸ªé1çš„æƒé‡æ¥éªŒè¯ä¹˜æ³•
    )
    # æŸå¤±è®¡ç®—æ˜¯é™æ€æ–¹æ³•ï¼Œä¸éœ€è¦åŠ è½½çœŸå®æ¨¡å‹æˆ–åˆå§‹åŒ–æƒé‡
    model = CausalLanguageModel(config).to(device)

    test_samples = [
        "This is a sentence without any numbers.",
        "The item costs 50.5 and the tax is 4.5.",
    ]
    inputs = tokenizer.batch_encode_plus(test_samples, padding=True, return_tensors='pt')
    input_ids = inputs['input_ids'].to(device)
    numerical_values = inputs['numerical_values'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    cls_labels = torch.full_like(input_ids, -100)
    cls_labels[:, :-1] = input_ids[:, 1:]
    
    print("   âœ… åˆå§‹åŒ–å®Œæˆã€‚")

    with torch.no_grad():
        # --- æ­¥éª¤ 2: ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡º ---
        print_step("æ­¥éª¤ 2", "ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡ºåˆ†å¸ƒ")
        # ç”±äºæˆ‘ä»¬åªæµ‹è¯•æŸå¤±å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥åˆ›å»ºéšæœºçš„è¾“å‡ºåˆ†å¸ƒ
        # è€Œæ— éœ€é€šè¿‡çœŸå®çš„å‰å‘ä¼ æ’­ï¼Œè¿™æ ·æµ‹è¯•æ›´çº¯ç²¹ã€æ›´å¿«é€Ÿ
        outputs = {
            'cls_loc': torch.randn(len(test_samples), input_ids.shape[1], config.vocab_size),
            'cls_scale': torch.rand(len(test_samples), input_ids.shape[1], config.vocab_size) * 5 + 1,
            'reg_loc': torch.randn(len(test_samples), input_ids.shape[1]),
            'reg_scale': torch.rand(len(test_samples), input_ids.shape[1]) * 5 + 1,
        }
        print("   - å·²ç”Ÿæˆéšæœºçš„æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºã€‚")

        # --- æ­¥éª¤ 3: æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Ground Truth) ---
        print_step("æ­¥éª¤ 3", "æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Ground Truth)")
        print("   - éµå¾ª `mathematical_foundations.md` ä¸­çš„åˆ†ç¦»å¹³å‡å…¬å¼")

        # a. æå–æ‰€éœ€å¼ é‡
        cls_loc, cls_scale = outputs['cls_loc'], outputs['cls_scale']
        reg_loc, reg_scale = outputs['reg_loc'], outputs['reg_scale']
        
        # b. è®¡ç®—é€è¯å…ƒæŸå¤±
        cls_probs = compute_ovr_probabilities(cls_loc, cls_scale, model.config.ovr_threshold)
        L_cls_per_token = ovr_classification_loss(cls_probs, cls_labels, reduction='none')

        num_probs = cls_probs[:, :, tokenizer.num_token_id]
        num_mask = (cls_labels == tokenizer.num_token_id).float() * attention_mask
        L_reg_per_token = gated_regression_loss(
            reg_loc, reg_scale, numerical_values,
            gate_prob=num_probs, mask=num_mask, 
            alpha=model.config.reg_loss_gating_alpha, reduction='none'
        )

        # c. åˆ†åˆ«è®¡ç®—å¹³å‡æŸå¤±
        active_cls_tokens = attention_mask.sum()
        L_cls_mean = (L_cls_per_token * attention_mask).sum() / active_cls_tokens
        
        active_reg_tokens = num_mask.sum()
        # L_reg_per_token å·²ç»åŒ…å«äº†é—¨æ§å’Œæ©ç ï¼Œç›´æ¥æ±‚å’Œå³å¯
        L_reg_eff = L_reg_per_token.sum() / (active_reg_tokens + 1e-8)

        # d. åŠ æƒåˆå¹¶
        lambda_weight = model.config.reg_loss_weight
        manual_total_loss = L_cls_mean + lambda_weight * L_reg_eff
        
        print(f"\n   --- æ‰‹åŠ¨è®¡ç®—ç»“æœ ---")
        print(f"     - æœ‰æ•ˆåˆ†ç±»è¯å…ƒæ•° (attention_mask.sum()): {active_cls_tokens.item()}")
        print(f"     - æœ‰æ•ˆå›å½’è¯å…ƒæ•° (num_mask.sum()): {active_reg_tokens.item()}")
        print(f"     - å¹³å‡åˆ†ç±»æŸå¤± (L_cls_mean): {L_cls_mean.item():.4f}")
        print(f"     - æœ‰æ•ˆå›å½’æŸå¤± (L_reg_eff): {L_reg_eff.item():.4f}")
        print(f"     - å›å½’æƒé‡ (Î»): {lambda_weight}")
        print(f"     - æ‰‹åŠ¨è®¡ç®—æ€»æŸå¤± (Manual Total Loss): {manual_total_loss.item():.4f}")

        # --- æ­¥éª¤ 4: ä½¿ç”¨æ¨¡å‹å†…ç½®æ–¹æ³•è®¡ç®—æŸå¤± ---
        print_step("æ­¥éª¤ 4", "ä½¿ç”¨ `model.compute_loss` è®¡ç®—æ€»æŸå¤±")
        loss_dict = model.compute_loss(
            outputs, targets=cls_labels, 
            numerical_values=numerical_values, attention_mask=attention_mask
        )
        model_total_loss = loss_dict['total']
        print(f"\n   --- æ¨¡å‹è®¡ç®—ç»“æœ ---")
        print(f"     - å¹³å‡åˆ†ç±»æŸå¤± (cls_loss): {loss_dict['cls_loss']:.4f}")
        print(f"     - æœ‰æ•ˆå›å½’æŸå¤± (effective_reg_loss): {loss_dict['effective_reg_loss']:.4f}")
        print(f"     - æ¨¡å‹è®¡ç®—æ€»æŸå¤± (Model Total Loss): {model_total_loss.item():.4f}")

        # --- æ­¥éª¤ 5: æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯ ---
        print_step("æ­¥éª¤ 5", "æ ¸å¿ƒæ•°å­¦é€»è¾‘éªŒè¯")
        loss_match = torch.allclose(manual_total_loss, model_total_loss, atol=1e-5)
        
        print(f"\n   --- éªŒè¯: æ‰‹åŠ¨è®¡ç®— vs. æ¨¡å‹è®¡ç®— ---")
        print(f"     - ç»“è®º: {'âœ… é€šè¿‡' if loss_match else 'âŒ å¤±è´¥'}")
        if not loss_match:
            diff = torch.abs(manual_total_loss - model_total_loss)
            print(f"     - ç»å¯¹å·®å¼‚: {diff.item():.8f}")
            
    print(f"\n\n{'='*80}")
    if loss_match:
        print("ğŸ‰ éªŒè¯æˆåŠŸï¼`compute_total_loss` çš„å®ç°å®Œå…¨ç¬¦åˆæ•°å­¦è®¾è®¡ï¼Œæ­£ç¡®åœ°å¯¹æŸå¤±è¿›è¡Œäº†åˆ†ç¦»å¹³å‡ã€‚")
    else:
        print("âŒ éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ `compute_total_loss` çš„å®ç°é€»è¾‘ã€‚")

if __name__ == '__main__':
    main() 
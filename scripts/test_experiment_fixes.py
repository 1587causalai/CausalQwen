#!/usr/bin/env python3
"""
æµ‹è¯•å®éªŒè„šæœ¬ä¸­çš„ä¿®å¤æ˜¯å¦æ­£ç¡®å·¥ä½œ

è¿™ä¸ªè„šæœ¬éªŒè¯ï¼š
1. run_experiments.py æ˜¯å¦æ­£ç¡®ä½¿ç”¨ä¿®å¤åçš„åˆå§‹åŒ–
2. æ•°å­¦å…¬å¼æ˜¯å¦åœ¨å®éªŒç¯å¢ƒä¸­æ­£ç¡®å·¥ä½œ
3. è®­ç»ƒæµç¨‹æ˜¯å¦ä½¿ç”¨ä¿®å¤åçš„ç»„ä»¶
"""

import torch
import sys
import os

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.models.causal_lm import CausalLanguageModel, CausalLMConfig
from src.data.tokenizer import QwenTokenizerWrapper
from src.training.trainer import Trainer

def test_experiment_fixes():
    """æµ‹è¯•å®éªŒä¸­çš„ä¿®å¤æ˜¯å¦æ­£ç¡®å·¥ä½œ"""
    print("ğŸ”¬ æµ‹è¯•å®éªŒè„šæœ¬ä¸­çš„ä¿®å¤")
    print("=" * 60)
    
    # 1. åˆ›å»ºä¸ run_experiments.py ç›¸åŒçš„é…ç½®
    tokenizer = QwenTokenizerWrapper(
        model_path="~/models/Qwen2.5-0.5B", 
        use_real_tokenizer=True
    )
    
    config = CausalLMConfig(
        vocab_size=tokenizer.vocab_size,
        num_token_id=tokenizer.num_token_id,
        hidden_size=896,
        causal_dim=896,  # å¼ºåˆ¶ä¸ hidden_size ç›¸åŒ
        use_real_qwen=True,
        qwen_model_path="~/models/Qwen2.5-0.5B",
        ovr_threshold=10.0
    )
    
    print(f"âœ… é…ç½®åˆ›å»ºæˆåŠŸ:")
    print(f"  - vocab_size: {config.vocab_size}")
    print(f"  - hidden_size: {config.hidden_size}")
    print(f"  - causal_dim: {config.causal_dim}")
    print(f"  - ovr_threshold: {config.ovr_threshold}")
    
    # 2. åˆ›å»ºæ¨¡å‹ï¼ˆä¸ run_experiments.py ç›¸åŒï¼‰
    device = torch.device('cpu')  # ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
    model = CausalLanguageModel(config).to(device)
    
    print(f"\nâœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # 3. æµ‹è¯• Trainer çš„åˆå§‹åŒ–ï¼ˆè¿™ä¼šè°ƒç”¨æˆ‘ä»¬ä¿®å¤çš„ init_weightsï¼‰
    print(f"\nğŸ”§ æµ‹è¯• Trainer åˆå§‹åŒ–ï¼ˆåŒ…å«ä¿®å¤åçš„æƒé‡åˆå§‹åŒ–ï¼‰...")
    
    trainer = Trainer(
        model=model,
        tokenizer=tokenizer,
        device=device,
        learning_rate=1e-4,
        batch_size=4,  # å°æ‰¹é‡ç”¨äºæµ‹è¯•
        config=config,
        wandb_run=None
    )
    
    print(f"âœ… Trainer åˆå§‹åŒ–æˆåŠŸ")
    print(f"  - æ•°æ®ç»Ÿè®¡è®¡ç®—å®Œæˆ")
    print(f"  - çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–å®Œæˆ")
    
    # 4. éªŒè¯å›å½’å¤´ä¿®å¤
    print(f"\nğŸ” éªŒè¯å›å½’å¤´ä¿®å¤:")
    reg_weight = model.action_network.regression_head.causal_linear.weight.data
    reg_bias = model.action_network.regression_head.causal_linear.bias.data
    
    print(f"  - å›å½’å¤´æƒé‡ç»Ÿè®¡:")
    print(f"    æƒé‡å‡å€¼: {reg_weight.mean().item():.6f}")
    print(f"    æƒé‡æ ‡å‡†å·®: {reg_weight.std().item():.6f}")
    print(f"    æƒé‡ä¸å…¨ä¸ºé›¶: {'âœ…' if reg_weight.std().item() > 0.001 else 'âŒ'}")
    
    if reg_bias is not None:
        print(f"  - å›å½’å¤´åç½®: {reg_bias.item():.4f}")
        print(f"    åç½®åˆç†æ€§: {'âœ…' if abs(reg_bias.item()) > 1.0 else 'âŒ'}")
    
    # 5. æµ‹è¯•å‰å‘ä¼ æ’­æ•°å­¦æ­£ç¡®æ€§
    print(f"\nğŸ§® æµ‹è¯•å‰å‘ä¼ æ’­æ•°å­¦æ­£ç¡®æ€§:")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_text = "The price is 42.5 dollars."
    inputs = tokenizer([test_text], padding=True, truncation=True, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model(inputs['input_ids'], inputs['numerical_values'], inputs['attention_mask'])
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_batch_size = 1
    expected_seq_len = inputs['input_ids'].shape[1]
    
    print(f"  - è¾“å‡ºå½¢çŠ¶éªŒè¯:")
    print(f"    cls_loc: {outputs['cls_loc'].shape} (æœŸæœ›: [{expected_batch_size}, {expected_seq_len}, {config.vocab_size}])")
    print(f"    reg_loc: {outputs['reg_loc'].shape} (æœŸæœ›: [{expected_batch_size}, {expected_seq_len}])")
    
    shape_correct = (
        outputs['cls_loc'].shape == (expected_batch_size, expected_seq_len, config.vocab_size) and
        outputs['reg_loc'].shape == (expected_batch_size, expected_seq_len)
    )
    print(f"    å½¢çŠ¶æ­£ç¡®æ€§: {'âœ…' if shape_correct else 'âŒ'}")
    
    # éªŒè¯å›å½’å¤´å“åº”è¾“å…¥
    reg_values = outputs['reg_loc'][0, :3].tolist()  # å‰3ä¸ªä½ç½®
    print(f"  - å›å½’å¤´å“åº”æ€§éªŒè¯:")
    print(f"    å‰3ä¸ªä½ç½®çš„å›å½’å€¼: {[f'{v:.4f}' for v in reg_values]}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–ï¼ˆä¸æ˜¯å›ºå®šå€¼ï¼‰
    reg_variance = torch.var(outputs['reg_loc'][0, :min(3, expected_seq_len)]).item()
    responsive = reg_variance > 0.01
    print(f"    å›å½’å€¼å˜åŒ–æ€§: {'âœ…' if responsive else 'âŒ'} (æ–¹å·®: {reg_variance:.6f})")
    
    # 6. éªŒè¯æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢æ•°å­¦
    print(f"\nğŸ“ éªŒè¯æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢æ•°å­¦:")
    
    # é€‰æ‹©ä¸€ä¸ªä½ç½®è¿›è¡ŒéªŒè¯
    pos = 0
    causal_loc = outputs['causal_loc'][0, pos]  # [C]
    causal_scale = outputs['causal_scale'][0, pos]  # [C]
    
    # éªŒè¯åˆ†ç±»å¤´çš„å‡ ä¸ªtoken
    cls_head = model.action_network.classification_head.causal_linear
    test_tokens = [0, 1]  # æµ‹è¯•å‰ä¸¤ä¸ªtoken
    
    all_correct = True
    for token_idx in test_tokens:
        weight_row = cls_head.weight[token_idx]  # [C]
        abs_weight_row = torch.abs(weight_row)
        theoretical_scale = torch.dot(abs_weight_row, causal_scale).item()
        actual_scale = outputs['cls_scale'][0, pos, token_idx].item()
        
        match = abs(theoretical_scale - actual_scale) < 1e-5
        all_correct = all_correct and match
        
        print(f"    Token{token_idx}: ç†è®º={theoretical_scale:.6f}, å®é™…={actual_scale:.6f} {'âœ…' if match else 'âŒ'}")
    
    print(f"  - æ•°å­¦ä¸€è‡´æ€§: {'âœ…' if all_correct else 'âŒ'}")
    
    # 7. æ€»ç»“
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“:")
    
    all_tests = [
        shape_correct,
        responsive,
        all_correct,
        reg_weight.std().item() > 0.001
    ]
    
    if all(all_tests):
        print(f"âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®éªŒè„šæœ¬æ­£ç¡®ä½¿ç”¨äº†ä¿®å¤åçš„ç»„ä»¶ã€‚")
        print(f"  - çŸ¥è¯†ä¼ è¾“åˆå§‹åŒ–æ­£ç¡®å·¥ä½œ")
        print(f"  - å›å½’å¤´ä¿®å¤ç”Ÿæ•ˆï¼ˆæƒé‡ä¸ä¸ºé›¶ï¼Œå“åº”è¾“å…¥ï¼‰")
        print(f"  - æŸ¯è¥¿åˆ†å¸ƒçº¿æ€§å˜æ¢æ•°å­¦æ­£ç¡®")
        print(f"  - åºåˆ—åˆ°åºåˆ—æ¶æ„æ­£ç¡®")
    else:
        print(f"âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥:")
        if not shape_correct:
            print(f"  - è¾“å‡ºå½¢çŠ¶ä¸æ­£ç¡®")
        if not responsive:
            print(f"  - å›å½’å¤´ä¸å“åº”è¾“å…¥")
        if not all_correct:
            print(f"  - æ•°å­¦å…¬å¼è®¡ç®—é”™è¯¯")
        if not (reg_weight.std().item() > 0.001):
            print(f"  - å›å½’å¤´æƒé‡ä»ä¸ºé›¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª å®éªŒè„šæœ¬ä¿®å¤éªŒè¯")
    print("=" * 80)
    
    try:
        test_experiment_fixes()
        print("\nğŸ‰ éªŒè¯å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
æ£€æŸ¥Qwenæ¨¡å‹çš„lm_headå®é™…è¾“å‡ºç»´åº¦

è¿™ä¸ªè„šæœ¬å°†ç›´æ¥åŠ è½½Qwenæ¨¡å‹å¹¶æ£€æŸ¥å…¶lm_headçš„æƒé‡å½¢çŠ¶ï¼Œ
ä»¥ç¡®å®šåˆ°åº•æ˜¯è¾“å‡º151,936ç»´è¿˜æ˜¯151,665ç»´ã€‚
"""

import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

def check_qwen_lm_head_dimensions():
    """æ£€æŸ¥Qwenæ¨¡å‹lm_headçš„å®é™…è¾“å‡ºç»´åº¦"""
    
    print("ğŸ” æ£€æŸ¥Qwenæ¨¡å‹lm_headçš„å®é™…è¾“å‡ºç»´åº¦")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model_path = "~/models/Qwen2.5-0.5B"
    expanded_path = os.path.expanduser(model_path)
    
    print(f"ğŸ“ åŠ è½½æ¨¡å‹: {expanded_path}")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(expanded_path, trust_remote_code=True)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            expanded_path, 
            trust_remote_code=True,
            torch_dtype=torch.float32,
            device_map=None
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        config = model.config
        print(f"\nğŸ“Š æ¨¡å‹é…ç½®ä¿¡æ¯:")
        print(f"   config.vocab_size: {config.vocab_size}")
        print(f"   config.hidden_size: {config.hidden_size}")
        
        # æ£€æŸ¥åˆ†è¯å™¨ä¿¡æ¯
        print(f"\nğŸ“ åˆ†è¯å™¨ä¿¡æ¯:")
        print(f"   len(tokenizer): {len(tokenizer)}")
        print(f"   tokenizer.vocab_size: {tokenizer.vocab_size}")
        
        # æ£€æŸ¥lm_headç»´åº¦
        lm_head = model.lm_head
        print(f"\nğŸ¯ lm_headæƒé‡ä¿¡æ¯:")
        print(f"   lm_head.weight.shape: {lm_head.weight.shape}")
        print(f"   lm_headè¾“å‡ºç»´åº¦: {lm_head.weight.shape[0]}")
        print(f"   lm_headè¾“å…¥ç»´åº¦: {lm_head.weight.shape[1]}")
        
        # æ£€æŸ¥åç½®
        if hasattr(lm_head, 'bias') and lm_head.bias is not None:
            print(f"   lm_head.bias.shape: {lm_head.bias.shape}")
        else:
            print(f"   lm_head.bias: None")
        
        # å¯¹æ¯”åˆ†æ
        print(f"\nğŸ” ç»´åº¦å¯¹æ¯”åˆ†æ:")
        config_vocab_size = config.vocab_size
        lm_head_output_size = lm_head.weight.shape[0]
        tokenizer_len = len(tokenizer)
        tokenizer_vocab_size = tokenizer.vocab_size
        
        print(f"   config.vocab_size:     {config_vocab_size}")
        print(f"   lm_headè¾“å‡ºç»´åº¦:       {lm_head_output_size}")
        print(f"   len(tokenizer):        {tokenizer_len}")
        print(f"   tokenizer.vocab_size:  {tokenizer_vocab_size}")
        
        # è®¡ç®—å·®å¼‚
        config_vs_lm_head = config_vocab_size - lm_head_output_size
        config_vs_tokenizer_len = config_vocab_size - tokenizer_len
        lm_head_vs_tokenizer_len = lm_head_output_size - tokenizer_len
        
        print(f"\nğŸ“Š å·®å¼‚åˆ†æ:")
        print(f"   config.vocab_size - lm_headè¾“å‡ºç»´åº¦: {config_vs_lm_head}")
        print(f"   config.vocab_size - len(tokenizer): {config_vs_tokenizer_len}")
        print(f"   lm_headè¾“å‡ºç»´åº¦ - len(tokenizer): {lm_head_vs_tokenizer_len}")
        
        # ç»“è®º
        print(f"\nâœ… ç»“è®º:")
        if lm_head_output_size == config_vocab_size:
            print(f"   âœ… lm_headè¾“å‡ºç»´åº¦ä¸config.vocab_sizeä¸€è‡´ ({lm_head_output_size})")
        elif lm_head_output_size == tokenizer_len:
            print(f"   âœ… lm_headè¾“å‡ºç»´åº¦ä¸len(tokenizer)ä¸€è‡´ ({lm_head_output_size})")
        else:
            print(f"   â“ lm_headè¾“å‡ºç»´åº¦ä¸é…ç½®ä¸ä¸€è‡´ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
            
        # æµ‹è¯•ä¸€ä¸ªç®€å•çš„å‰å‘ä¼ æ’­
        print(f"\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­:")
        test_input = torch.tensor([[1, 2, 3]])  # ç®€å•çš„æµ‹è¯•è¾“å…¥
        
        with torch.no_grad():
            outputs = model(test_input)
            logits = outputs.logits
            print(f"   æ¨¡å‹è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
            print(f"   logitsæœ€åä¸€ç»´å¤§å°: {logits.shape[-1]}")
            
        # éªŒè¯logitsç»´åº¦æ˜¯å¦ä¸lm_headä¸€è‡´
        if logits.shape[-1] == lm_head_output_size:
            print(f"   âœ… æ¨¡å‹è¾“å‡ºç»´åº¦ä¸lm_headä¸€è‡´")
        else:
            print(f"   âŒ æ¨¡å‹è¾“å‡ºç»´åº¦ä¸lm_headä¸ä¸€è‡´")
        
        return {
            'config_vocab_size': config_vocab_size,
            'lm_head_output_size': lm_head_output_size,
            'tokenizer_len': tokenizer_len,
            'tokenizer_vocab_size': tokenizer_vocab_size,
            'logits_shape': logits.shape[-1]
        }
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return None

if __name__ == "__main__":
    check_qwen_lm_head_dimensions() 
#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶

éªŒè¯æ‰€æœ‰æ–°å¢çš„åŸºå‡†æ–¹æ³•æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
"""

import numpy as np
from sklearn.datasets import make_regression
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from causal_sklearn.benchmarks import (
    BaselineBenchmark, 
    MethodDependencyChecker, 
    get_method_group,
    list_available_methods
)

def test_dependency_availability():
    """æµ‹è¯•ä¾èµ–å¯ç”¨æ€§æ£€æŸ¥"""
    print("ğŸ” æµ‹è¯•ä¾èµ–å¯ç”¨æ€§æ£€æŸ¥")
    print("=" * 50)
    
    checker = MethodDependencyChecker()
    checker.print_dependency_status()
    
    return True

def test_method_listing():
    """æµ‹è¯•æ–¹æ³•åˆ—è¡¨åŠŸèƒ½"""
    print("\nğŸ“ æµ‹è¯•æ–¹æ³•åˆ—è¡¨åŠŸèƒ½")
    print("=" * 50)
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–¹æ³•
    methods = list_available_methods()
    print(f"å¯ç”¨æ–¹æ³•æ€»æ•°: {len(methods)}")
    print(f"å‰5ä¸ªæ–¹æ³•: {methods[:5]}")
    
    # æµ‹è¯•æ–¹æ³•ç»„åˆ
    basic_group = get_method_group('basic')
    print(f"åŸºç¡€ç»„åˆ: {basic_group}")
    
    comprehensive_group = get_method_group('comprehensive')
    print(f"å…¨é¢ç»„åˆ: {comprehensive_group}")
    
    return True

def test_benchmark_creation():
    """æµ‹è¯•åŸºå‡†æµ‹è¯•å®ä¾‹åˆ›å»º"""
    print("\nğŸ—ï¸ æµ‹è¯•åŸºå‡†æµ‹è¯•å®ä¾‹åˆ›å»º")
    print("=" * 50)
    
    try:
        benchmark = BaselineBenchmark()
        print("âœ… BaselineBenchmark åˆ›å»ºæˆåŠŸ")
        
        # æ‰“å°æ–¹æ³•å¯ç”¨æ€§
        benchmark.print_method_availability()
        
        return True
    except Exception as e:
        print(f"âŒ BaselineBenchmark åˆ›å»ºå¤±è´¥: {e}")
        return False

def test_small_dataset_benchmark():
    """åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•åŸºå‡†æµ‹è¯•"""
    print("\nğŸ§ª åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºå°å‹åˆæˆæ•°æ®é›†
    X, y = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)
    
    try:
        benchmark = BaselineBenchmark()
        
        # æµ‹è¯•è½»é‡çº§æ–¹æ³•ç»„åˆ
        print("æµ‹è¯•è½»é‡çº§æ–¹æ³•ç»„åˆ...")
        results = benchmark.compare_models(
            X=X, 
            y=y,
            task_type='regression',
            baseline_methods=['sklearn_mlp', 'random_forest'],  # åªæµ‹è¯•2ä¸ªç®€å•æ–¹æ³•
            causal_modes=['deterministic', 'standard'],          # åªæµ‹è¯•2ä¸ªCausalEngineæ¨¡å¼
            anomaly_ratio=0.1,
            verbose=True
        )
        
        print(f"\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼Œå¾—åˆ° {len(results)} ä¸ªç»“æœ")
        
        # æ‰“å°ç®€å•çš„ç»“æœæ‘˜è¦
        for method, metrics in results.items():
            r2 = metrics['test']['RÂ²']
            mae = metrics['test']['MAE']
            print(f"   {method}: RÂ² = {r2:.3f}, MAE = {mae:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å°æ•°æ®é›†åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_method_availability_filtering():
    """æµ‹è¯•æ–¹æ³•å¯ç”¨æ€§è¿‡æ»¤"""
    print("\nğŸ” æµ‹è¯•æ–¹æ³•å¯ç”¨æ€§è¿‡æ»¤")
    print("=" * 50)
    
    try:
        from causal_sklearn.benchmarks.methods import filter_available_methods
        
        # æµ‹è¯•æ··åˆçš„æ–¹æ³•åˆ—è¡¨ï¼ˆåŒ…å«å¯ç”¨å’Œä¸å¯ç”¨çš„ï¼‰
        test_methods = [
            'sklearn_mlp',      # åº”è¯¥å¯ç”¨
            'random_forest',    # åº”è¯¥å¯ç”¨
            'xgboost',         # å¯èƒ½ä¸å¯ç”¨
            'lightgbm',        # å¯èƒ½ä¸å¯ç”¨
            'nonexistent_method'  # ä¸å­˜åœ¨çš„æ–¹æ³•
        ]
        
        available, unavailable = filter_available_methods(test_methods)
        
        print(f"å¯ç”¨æ–¹æ³•: {available}")
        print(f"ä¸å¯ç”¨æ–¹æ³•: {unavailable}")
        
        # åŸºæœ¬éªŒè¯
        assert 'sklearn_mlp' in available, "sklearn_mlpåº”è¯¥æ˜¯å¯ç”¨çš„"
        assert 'random_forest' in available, "random_foreståº”è¯¥æ˜¯å¯ç”¨çš„"
        assert 'nonexistent_method' in unavailable, "nonexistent_methodåº”è¯¥æ˜¯ä¸å¯ç”¨çš„"
        
        print("âœ… æ–¹æ³•å¯ç”¨æ€§è¿‡æ»¤æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ–¹æ³•å¯ç”¨æ€§è¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª æ–°åŸºå‡†æµ‹è¯•æ¡†æ¶éªŒè¯")
    print("=" * 60)
    
    tests = [
        ("ä¾èµ–å¯ç”¨æ€§æ£€æŸ¥", test_dependency_availability),
        ("æ–¹æ³•åˆ—è¡¨åŠŸèƒ½", test_method_listing),
        ("åŸºå‡†æµ‹è¯•å®ä¾‹åˆ›å»º", test_benchmark_creation),
        ("æ–¹æ³•å¯ç”¨æ€§è¿‡æ»¤", test_method_availability_filtering),
        ("å°æ•°æ®é›†åŸºå‡†æµ‹è¯•", test_small_dataset_benchmark),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results[test_name] = success
        except Exception as e:
            print(f"âŒ æµ‹è¯• '{test_name}' å‡ºç°å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ€»ç»“æŠ¥å‘Š
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, success in results.items():
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"   {test_name:<25} {status}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°åŸºå‡†æµ‹è¯•æ¡†æ¶å·¥ä½œæ­£å¸¸ã€‚")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)